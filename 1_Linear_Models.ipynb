{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Linear Models - Multiple Linear Regressions and Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Machine Learning Modelling Workflow Notebook 1_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content:\n",
    "1. Splitting the dataframe into train and test subsets;\n",
    "2. Retrieving the list of classes used for training the classification models;\n",
    "3. Ordinary Least Squares (OLS) Linear Regression;\n",
    "4. Ridge Linear Regression;\n",
    "5. Lasso Linear Regression;\n",
    "6. Elastic Net Linear Regression;\n",
    "7. Logistic Regression (binary classification);\n",
    "8. Getting a general feature ranking;\n",
    "9. Calculating metrics for regression models;\n",
    "10. Calculating metrics for classification models;\n",
    "11. Making predictions with the models;\n",
    "12. Calculating probabilities associated to each class;\n",
    "13. Performing the SHAP feature importance analysis;\n",
    "14. Time series visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267"
   },
   "source": [
    "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
    "- marcosoares.feq@gmail.com\n",
    "- marco.soares@bayer.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5dbf6196-9599-414f-a337-ec52f7f2122c",
    "id": "zGDOhNQbGzrr"
   },
   "source": [
    "Install statsmodels library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "11e929c6-8980-4298-8ded-b40a2080913e",
    "id": "7q1Bfti3Gzrs"
   },
   "outputs": [],
   "source": [
    "! pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5dbf6196-9599-414f-a337-ec52f7f2122c",
    "id": "zGDOhNQbGzrr"
   },
   "source": [
    "Install tensorflow library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "11e929c6-8980-4298-8ded-b40a2080913e",
    "id": "7q1Bfti3Gzrs"
   },
   "outputs": [],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1a5c1713-e549-4b18-bbaf-114c019c976d",
    "id": "wXnEEdHuGzru"
   },
   "source": [
    "Install Keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "dbd37f46-51f0-4a30-a812-4b27679ff1a1",
    "id": "N1OlfernGzrw"
   },
   "outputs": [],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e691d1ee-cb58-482b-9edb-d6baca45fdb3",
    "id": "Qowk3bTaGzrx"
   },
   "source": [
    "Install SHAP library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "245331e6-1f20-48ce-8ab9-00d47c128616",
    "id": "nC2bqVfxGzry"
   },
   "outputs": [],
   "source": [
    "! pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "7ec04518-6f8a-43a0-868e-3932f998886e",
    "id": "7ldt-mnjGzrz",
    "outputId": "b70c28af-c67c-4c61-a7b3-2a21bf4c6d64"
   },
   "outputs": [],
   "source": [
    "#check the version of the package\n",
    "! pip show shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e9286147-d907-490e-8596-ec38fc6c21c8",
    "id": "H9YCpVctGzr1"
   },
   "outputs": [],
   "source": [
    "# Upgrade to the most recent library versions, if a given module is not present and analysis cannot be\n",
    "# executed.\n",
    "! pip install pip --upgrade\n",
    "! pip install tensorflow --upgrade\n",
    "! pip install keras --upgrade\n",
    "! pip install shap --upgrade\n",
    "! pip install sklearn --upgrade\n",
    "! pip install pandas --upgrade\n",
    "! pip install numpy --upgrade\n",
    "! pip install matplotlib --upgrade\n",
    "! pip install seaborn --upgrade\n",
    "! pip install scipy --upgrade\n",
    "! pip install statsmodels --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea"
   },
   "source": [
    "## **Load Python Libraries in Global Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzZgOvXCyHHl",
    "tags": [
     "CELL_4"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels as sm\n",
    "import tensorflow as tf\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from xgboost import XGBRegressor, XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mount_storage_system (source = 'aws', path_to_store_imported_s3_bucket = '', s3_bucket_name = None, s3_obj_prefix = None):\n",
    "    \n",
    "    # source = 'google' for mounting the google drive;\n",
    "    # source = 'aws' for mounting an AWS S3 bucket.\n",
    "    \n",
    "    # THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN source == 'aws'\n",
    "    \n",
    "    # path_to_store_imported_s3_bucket: path of the Python environment to which the\n",
    "    # S3 bucket contents will be imported. If it is None, or if \n",
    "    # path_to_store_imported_s3_bucket = '/', bucket will be imported to the root path. \n",
    "    # Alternatively, input the path as a string (in quotes). e.g. \n",
    "    # path_to_store_imported_s3_bucket = 'copied_s3_bucket'\n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # s3_obj_prefix = None. Keep it None or as an empty string (s3_obj_key_prefix = '')\n",
    "    # to import the whole bucket content, instead of a single object from it.\n",
    "    # Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "    # Suppose that your bucket (admin-created) has four objects with the following object \n",
    "    # keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "    # s3-dg.pdf. The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "    # at the root level of the bucket. If you open the Development/ folder, you see \n",
    "    # the Projects.xlsx object in it.\n",
    "    # Check Amazon documentation:\n",
    "    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\n",
    "    \n",
    "    # In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "    # where 'bucket' is the bucket's name, key_prefix = 'my_path/.../', without the\n",
    "    # 'file.csv' (file name with extension) last part.\n",
    "    \n",
    "    # So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "    # a given folder (directory) of the bucket.\n",
    "    # DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "    # DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "    # Alternatively, provide the full path of a given file if you want to import only it:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "    # where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "    # Attention: after running this function for fetching AWS Simple Storage System (S3), \n",
    "    # your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "    # The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "    # other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "    # and the prefix. All of these are sensitive information from the organization.\n",
    "    # Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "    # and of removing such information from the strings.\n",
    "    # Remember that these data may contain privilege for accessing the information, so it should not\n",
    "    # be used for non-authorized people.\n",
    "\n",
    "    # Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "    # The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "    # workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "    # the notebook's workspace.\n",
    "    \n",
    "    \n",
    "    if (source == 'google'):\n",
    "        \n",
    "        from google.colab import drive\n",
    "        # Google Colab library must be imported only in case it is\n",
    "        # going to be used, for avoiding AWS compatibility issues.\n",
    "        \n",
    "        print(\"Associate the Python environment to your Google Drive account, and authorize the access in the opened window.\")\n",
    "        \n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        print(\"Now your Python environment is connected to your Google Drive: the root directory of your environment is now the root of your Google Drive.\")\n",
    "        print(\"In Google Colab, navigate to the folder icon (\\'Files\\') of the left navigation menu to find a specific folder or file in your Google Drive.\")\n",
    "        print(\"Click on the folder or file name and select the elipsis (...) icon on the right of the name to reveal the option \\'Copy path\\', which will give you the path to use as input for loading objects and files on your Python environment.\")\n",
    "        print(\"Caution: save your files into different directories of the Google Drive. If files are all saved in a same folder or directory, like the root path, they may not be accessible from your Python environment.\")\n",
    "        print(\"If you still cannot see the file after moving it to a different folder, reload the environment.\")\n",
    "    \n",
    "    elif (source == 'aws'):\n",
    "        \n",
    "        import os\n",
    "        import boto3\n",
    "        # boto3 is AWS S3 Python SDK\n",
    "        # sagemaker and boto3 libraries must be imported only in case \n",
    "        # they are going to be used, for avoiding \n",
    "        # Google Colab compatibility issues.\n",
    "        from getpass import getpass\n",
    "\n",
    "        # Check if path_to_store_imported_s3_bucket is None. If it is, make it the root directory:\n",
    "        if ((path_to_store_imported_s3_bucket is None)|(str(path_to_store_imported_s3_bucket) == \"/\")):\n",
    "            \n",
    "            # For the S3 buckets, the path should not start with slash. Assign the empty\n",
    "            # string instead:\n",
    "            path_to_store_imported_s3_bucket = \"\"\n",
    "            print(\"Bucket\\'s content will be copied to the notebook\\'s root directory.\")\n",
    "        \n",
    "        elif (str(path_to_store_imported_s3_bucket) == \"\"):\n",
    "            # Guarantee that the path is the empty string.\n",
    "            # Avoid accessing the else condition, what would raise an error\n",
    "            # since the empty string has no character of index 0\n",
    "            path_to_store_imported_s3_bucket = str(path_to_store_imported_s3_bucket)\n",
    "            print(\"Bucket\\'s content will be copied to the notebook\\'s root directory.\")\n",
    "        \n",
    "        else:\n",
    "            # Use the str attribute to guarantee that the path was read as a string:\n",
    "            path_to_store_imported_s3_bucket = str(path_to_store_imported_s3_bucket)\n",
    "            \n",
    "            if(path_to_store_imported_s3_bucket[0] == \"/\"):\n",
    "                # the first character is the slash. Let's remove it\n",
    "\n",
    "                # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "                # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "                # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "                # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "                # The slash is character 0. Then, we want all characters from character 1 (the\n",
    "                # second) to character len(str(path_to_store_imported_s3_bucket)) - 1, the index\n",
    "                # of the last character. So, we can slice the string from position 1 to position\n",
    "                # the slicing syntax is: string[1:] - all string characters from character 1\n",
    "                # string[:10] - all string characters from character 10-1 = 9 (including 9); or\n",
    "                # string[1:10] - characters from 1 to 9\n",
    "                # So, slice the whole string, starting from character 1:\n",
    "                path_to_store_imported_s3_bucket = path_to_store_imported_s3_bucket[1:]\n",
    "                # attention: even though strings may be seem as list of characters, that can be\n",
    "                # sliced, we cannot neither simply assign a character to a given position nor delete\n",
    "                # a character from a position.\n",
    "\n",
    "        # Ask the user to provide the credentials:\n",
    "        ACCESS_KEY = input(\"Enter your AWS Access Key ID here (in the right). It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "        print(\"\\n\") # line break\n",
    "        SECRET_KEY = getpass(\"Enter your password (Secret key) here (in the right). It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "        \n",
    "        # The use of 'getpass' instead of 'input' hide the password behind dots.\n",
    "        # So, the password is not visible by other users and cannot be copied.\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"WARNING: The bucket\\'s name, the prefix, the AWS access key ID, and the AWS Secret access key are all sensitive information, which may grant access to protected information from the organization.\\n\")\n",
    "        print(\"After copying data from S3 to your workspace, remember of removing these information from the notebook, specially if it is going to be shared. Also, remember of removing the files from the workspace.\\n\")\n",
    "        print(\"The cost for storing files in Simple Storage Service is quite inferior than the one for storing directly in SageMaker workspace. Also, files stored in S3 may be accessed for other users than those with access the notebook\\'s workspace.\\n\")\n",
    "\n",
    "        # Check if the user actually provided the mandatory inputs, instead\n",
    "        # of putting None or empty string:\n",
    "        if ((ACCESS_KEY is None) | (ACCESS_KEY == '')):\n",
    "            print(\"AWS Access Key ID is missing. It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "            return \"error\"\n",
    "        elif ((SECRET_KEY is None) | (SECRET_KEY == '')):\n",
    "            print(\"AWS Secret Access Key is missing. It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "            return \"error\"\n",
    "        elif ((s3_bucket_name is None) | (s3_bucket_name == '')):\n",
    "            print (\"Please, enter a valid S3 Bucket\\'s name. Do not add sub-directories or folders (prefixes), only the name of the bucket itself.\")\n",
    "            return \"error\"\n",
    "        \n",
    "        else:\n",
    "            # Use the str attribute to guarantee that all AWS parameters were properly read as strings, and not as\n",
    "            # other variables (like integers or floats):\n",
    "            ACCESS_KEY = str(ACCESS_KEY)\n",
    "            SECRET_KEY = str(SECRET_KEY)\n",
    "            s3_bucket_name = str(s3_bucket_name)\n",
    "        \n",
    "        if(s3_bucket_name[0] == \"/\"):\n",
    "                # the first character is the slash. Let's remove it\n",
    "\n",
    "                # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "                # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "                # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "                # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "                # So, slice the whole string, starting from character 1 (as did for \n",
    "                # path_to_store_imported_s3_bucket):\n",
    "                s3_bucket_name = s3_bucket_name[1:]\n",
    "\n",
    "        # Remove any possible trailing (white and tab spaces) spaces\n",
    "        # That may be present in the string. Use the Python string\n",
    "        # rstrip method, which is the equivalent to the Trim function:\n",
    "        # When no arguments are provided, the whitespaces and tabulations\n",
    "        # are the removed characters\n",
    "        # https://www.w3schools.com/python/ref_string_rstrip.asp?msclkid=ee2d05c3c56811ecb1d2189d9f803f65\n",
    "        s3_bucket_name = s3_bucket_name.rstrip()\n",
    "        ACCESS_KEY = ACCESS_KEY.rstrip()\n",
    "        SECRET_KEY = SECRET_KEY.rstrip()\n",
    "        # Since the user manually inputs the parameters ACCESS and SECRET_KEY,\n",
    "        # it is easy to input whitespaces without noticing that.\n",
    "\n",
    "        # Now process the non-obbligatory parameter.\n",
    "        # Check if a prefix was passed as input parameter. If so, we must select only the names that start with\n",
    "        # The prefix.\n",
    "        # Example: in the bucket 'my_bucket' we have a directory 'dir1'.\n",
    "        # In the main (root) directory, we have a file 'file1.json' like: '/file1.json'\n",
    "        # If we pass the prefix 'dir1', we want only the files that start as '/dir1/'\n",
    "        # such as: 'dir1/file2.json', excluding the file in the main (root) directory and excluding the files in other\n",
    "        # directories. Also, we want to eliminate the file names with no extensions, like 'dir1/' or 'dir1/dir2',\n",
    "        # since these object names represent folders or directories, not files.\t\n",
    "\n",
    "        if (s3_obj_prefix is None):\n",
    "            print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "            print (f\"Then, retrieving all content from the bucket \\'{s3_bucket_name}\\'.\\n\")\n",
    "        elif ((s3_obj_prefix == \"/\") | (s3_obj_prefix == '')):\n",
    "            # The root directory in the bucket must not be specified starting with the slash\n",
    "            # If the root \"/\" or the empty string '' is provided, make\n",
    "            # it equivalent to None (no directory)\n",
    "            s3_obj_prefix = None\n",
    "            print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "            print (f\"Then, retrieving all content from the bucket \\'{s3_bucket_name}\\'.\\n\")\n",
    "    \n",
    "        else:\n",
    "            # Since there is a prefix, use the str attribute to guarantee that the path was read as a string:\n",
    "            s3_obj_prefix = str(s3_obj_prefix)\n",
    "            \n",
    "            if(s3_obj_prefix[0] == \"/\"):\n",
    "                # the first character is the slash. Let's remove it\n",
    "\n",
    "                # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "                # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "                # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "                # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "                # So, slice the whole string, starting from character 1 (as did for \n",
    "                # path_to_store_imported_s3_bucket):\n",
    "                s3_obj_prefix = s3_obj_prefix[1:]\n",
    "\n",
    "            # Remove any possible trailing (white and tab spaces) spaces\n",
    "            # That may be present in the string. Use the Python string\n",
    "            # rstrip method, which is the equivalent to the Trim function:\n",
    "            s3_obj_prefix = s3_obj_prefix.rstrip()\n",
    "            \n",
    "            # Store the total characters in the prefix string after removing the initial slash\n",
    "            # and trailing spaces:\n",
    "            prefix_len = len(s3_obj_prefix)\n",
    "            \n",
    "            print(\"AWS Access Credentials, and bucket\\'s prefix, object or subdirectory provided.\\n\")\t\n",
    "\n",
    "            \n",
    "        print (\"Starting connection with the S3 bucket.\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Start S3 client as the object 's3_client'\n",
    "            s3_client = boto3.resource('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = SECRET_KEY)\n",
    "        \n",
    "            print(f\"Credentials accepted by AWS. S3 client successfully started.\\n\")\n",
    "            # An object 'data_table.xlsx' in the main (root) directory of the s3_bucket is stored in Python environment as:\n",
    "            # s3.ObjectSummary(bucket_name='bucket_name', key='data_table.xlsx')\n",
    "            # The name of each object is stored as the attribute 'key' of the object.\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect to AWS Simple Storage Service (S3). Review if your credentials are correct.\")\n",
    "            print(\"The variable \\'access_key\\' must be set as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"The variable \\'secret_key\\' must be set as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "        \n",
    "        try:\n",
    "            # Connect to the bucket specified as 'bucket_name'.\n",
    "            # The bucket is started as the object 's3_bucket':\n",
    "            s3_bucket = s3_client.Bucket(s3_bucket_name)\n",
    "            print(f\"Connection with bucket \\'{s3_bucket_name}\\' stablished.\\n\")\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect with the bucket, which usually happens when declaring a wrong bucket\\'s name.\") \n",
    "            print(\"Check the spelling of your bucket_name string and remember that it must be all in lower-case.\\n\")\n",
    "                \n",
    "\n",
    "        # Then, let's obtain a list of all objects in the bucket (list bucket_objects):\n",
    "        \n",
    "        bucket_objects_list = []\n",
    "\n",
    "        # Loop through all objects of the bucket:\n",
    "        for stored_obj in s3_bucket.objects.all():\n",
    "            \n",
    "            # Loop through all elements 'stored_obj' from s3_bucket.objects.all()\n",
    "            # Which stores the ObjectSummary for all objects in the bucket s3_bucket:\n",
    "            # Let's store only the key attribute and use the str function\n",
    "            # to guarantee that all values were stored as strings.\n",
    "            bucket_objects_list.append(str(stored_obj.key))\n",
    "        \n",
    "        # Now start a support list to store only the elements from\n",
    "        # bucket_objects_list that are not folders or directories\n",
    "        # (objects with extensions).\n",
    "        # If a prefix was provided, only files with that prefix should\n",
    "        # be added:\n",
    "        support_list = []\n",
    "        \n",
    "        for stored_obj in bucket_objects_list:\n",
    "            \n",
    "            # Loop through all elements 'stored_obj' from the list\n",
    "            # bucket_objects_list\n",
    "\n",
    "            # Check the file extension.\n",
    "            file_extension = os.path.splitext(stored_obj)[1][1:]\n",
    "            \n",
    "            # The os.path.splitext method splits the string into its FIRST dot (\".\") to\n",
    "            # separate the file extension from the full path. Example:\n",
    "            # \"C:/dir1/dir2/data_table.csv\" is split into:\n",
    "            # \"C:/dir1/dir2/data_table\" (root part) and '.csv' (extension part)\n",
    "            # https://www.geeksforgeeks.org/python-os-path-splitext-method/?msclkid=2d56198fc5d311ec820530cfa4c6d574\n",
    "\n",
    "            # os.path.splitext(stored_obj) is a tuple of strings: the first is the complete file\n",
    "            # root with no extension; the second is the extension starting with a point: '.txt'\n",
    "            # When we set os.path.splitext(stored_obj)[1], we are selecting the second element of\n",
    "            # the tuple. By selecting os.path.splitext(stored_obj)[1][1:], we are taking this string\n",
    "            # from the second character (index 1), eliminating the dot: 'txt'\n",
    "\n",
    "\n",
    "            # Check if the file extension is not an empty string '' (i.e., that it is different from != the empty\n",
    "            # string:\n",
    "            if (file_extension != ''):\n",
    "                    \n",
    "                    # The extension is different from the empty string, so it is not neither a folder nor a directory\n",
    "                    # The object is actually a file and may be copied if it satisfies the prefix condition. If there\n",
    "                    # is no prefix to check, we may simply copy the object to the list.\n",
    "\n",
    "                    # If there is a prefix, the first characters of the stored_obj must be the prefix:\n",
    "                    if not (s3_obj_prefix is None):\n",
    "                        \n",
    "                        # Check the characters from the position 0 (1st character) to the position\n",
    "                        # prefix_len - 1. Since a prefix was declared, we want only the objects that this first portion\n",
    "                        # corresponds to the prefix. string[i:j] slices the string from index i to index j-1\n",
    "                        # Then, the 1st portion of the string to check is: string[0:(prefix_len)]\n",
    "\n",
    "                        # Slice the string stored_obj from position 0 (1st character) to position prefix_len - 1,\n",
    "                        # The position that the prefix should end.\n",
    "                        obj_name_first_part = (stored_obj)[0:(prefix_len)]\n",
    "                        \n",
    "                        # If this first part is the prefix, then append the object to \n",
    "                        # support list:\n",
    "                        if (obj_name_first_part == (s3_obj_prefix)):\n",
    "\n",
    "                                support_list.append(stored_obj)\n",
    "\n",
    "                    else:\n",
    "                        # There is no prefix, so we can simply append the object to the list:\n",
    "                        support_list.append(stored_obj)\n",
    "\n",
    "            \n",
    "        # Make the objects list the support list itself:\n",
    "        bucket_objects_list = support_list\n",
    "            \n",
    "        # Now, bucket_objects_list contains the names of all objects from the bucket that must be copied.\n",
    "\n",
    "        print(\"Finished mapping objects to fetch. Now, all these objects from S3 bucket will be copied to the notebook\\'s workspace, in the specified directory.\\n\")\n",
    "        print(f\"A total of {len(bucket_objects_list)} files were found in the specified bucket\\'s prefix (\\'{s3_obj_prefix}\\').\")\n",
    "        print(f\"The first file found is \\'{bucket_objects_list[0]}\\'; whereas the last file found is \\'{bucket_objects_list[len(bucket_objects_list) - 1]}\\'.\")\n",
    "            \n",
    "        # Now, let's try copying the files:\n",
    "            \n",
    "        try:\n",
    "            \n",
    "            # Loop through all objects in the list bucket_objects and copy them to the workspace:\n",
    "            for copied_object in bucket_objects_list:\n",
    "\n",
    "                # Select the object in the bucket previously started as 's3_bucket':\n",
    "                selected_object = s3_bucket.Object(copied_object)\n",
    "            \n",
    "                # Now, copy this object to the workspace:\n",
    "                # Set the new file_path. Notice that by now, copied_object may be a string like:\n",
    "                # 'dir1/.../dirN/file_name.ext', where dirN is the n-th directory and ext is the file extension.\n",
    "                # We want only the file_name to joing with the path to store the imported bucket. So, we can use the\n",
    "                # str.split method specifying the separator sep = '/' to break the string into a list of substrings.\n",
    "                # The last element from this list will be 'file_name.ext'\n",
    "                # https://www.w3schools.com/python/ref_string_split.asp?msclkid=135399b6c63111ecada75d7d91add056\n",
    "\n",
    "                # 1. Break the copied_object full path into the list object_path_list, using the .split method:\n",
    "                object_path_list = copied_object.split(sep = \"/\")\n",
    "\n",
    "                # 2. Get the last element from this list. Since it has length len(object_path_list) and indexing starts from\n",
    "                # zero, the index of the last element is (len(object_path_list) - 1):\n",
    "                fetched_object = object_path_list[(len(object_path_list) - 1)]\n",
    "\n",
    "                # 3. Finally, join the string fetched_object with the new path (path on the notebook's workspace) to finish\n",
    "                # The new object's file_path:\n",
    "\n",
    "                file_path = os.path.join(path_to_store_imported_s3_bucket, fetched_object)\n",
    "\n",
    "                # Download the selected object to the workspace in the specified file_path\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" copies a xlsx file named 'my_table' to the notebook's main (root)\n",
    "                # directory\n",
    "                selected_object.download_file(Filename = file_path)\n",
    "\n",
    "                print(f\"The file \\'{fetched_object}\\' was successfully copied to notebook\\'s workspace.\\n\")\n",
    "\n",
    "                \n",
    "            print(\"Finished copying the files from the bucket to the notebook\\'s workspace. It may take a couple of minutes untill they be shown in SageMaker environment.\\n\") \n",
    "            print(\"Do not forget to delete these copies after finishing the analysis. They will remain stored in the bucket.\\n\")\n",
    "\n",
    "\n",
    "        except:\n",
    "\n",
    "            # Run this code for any other exception that may happen (no exception error\n",
    "            # specified, so any exception runs the following code).\n",
    "            # Check: https://pythonbasics.org/try-except/?msclkid=4f6b4540c5d011ecb1fe8a4566f632a6\n",
    "            # for seeing how to handle successive exceptions\n",
    "\n",
    "            print(\"Attention! The function raised an exception error, which is probably due to the AWS Simple Storage Service (S3) permissions.\")\n",
    "            print(\"Before running again this function, check this quick guide for configuring the permission roles in AWS.\\n\")\n",
    "            print(\"It is necessary to create an user with full access permissions to interact with S3 from SageMaker. To configure the User, go to the upper ribbon of AWS, click on Services, and select IAM – Identity and Access Management.\")\n",
    "            print(\"1. In IAM\\'s lateral panel, search for \\'Users\\' in the group of Access Management.\")\n",
    "            print(\"2. Click on the \\'Add users\\' button.\")\n",
    "            print(\"3. Set an user name in the text box \\'User name\\'.\")\n",
    "            print(\"Attention: users and S3 buckets cannot be written in upper case. Also, selecting a name already used by an Amazon user or bucket will raise an error message.\\n\")\n",
    "            print(\"4. In the field \\'Select type of Access to AWS\\'-\\'Select type of AWS credentials\\' select the option \\'Access key - Programmatic access\\'. After that, click on the button \\'Next: Permissions\\'.\")\n",
    "            print(\"5. In the field \\'Set Permissions\\', keep the \\'Add user to a group\\' button marked.\")\n",
    "            print(\"6. In the field \\'Add user to a group\\', click on \\'Create group\\' (alternatively, you can be added to a group already configured or copy the permissions of another user.\")\n",
    "            print(\"7. In the text box \\'Group\\'s name\\', set a name for the new group of permissions.\")\n",
    "            print(\"8. In the search bar below (\\'Filter politics\\'), search for a politics that fill your needs, and check the option button on the left of this politic. The politics \\'AmazonS3FullAccess\\' grants full access to the S3 content.\")\n",
    "            print(\"9. Finally, click on \\'Create a group\\'.\")\n",
    "            print(\"10. After the group is created, it will appear with a check box marked, over the previous groups. Keep it marked and click on the button \\'Next: Tags\\'.\")\n",
    "            print(\"11. Create and note down the Access key ID and Secret access key. You can also download a comma separated values (CSV) file containing the credentials for future use.\")\n",
    "            print(\"ATTENTION: These parameters are required for accessing the bucket\\'s content from any application, including AWS SageMaker.\")\n",
    "            print(\"12. Click on \\'Next: Review\\' and review the user credentials information and permissions.\")\n",
    "            print(\"13. Click on \\'Create user\\' and click on the download button to download the CSV file containing the user credentials information.\")\n",
    "            print(\"The headers of the CSV file (the stored fields) is: \\'User name, Password, Access key ID, Secret access key, Console login link\\'.\")\n",
    "            print(\"You need both the values indicated as \\'Access key ID\\' and as \\'Secret access key\\' to fetch the S3 bucket.\")\n",
    "            print(\"\\n\") # line break\n",
    "            print(\"After acquiring the necessary user privileges, use the boto3 library to fetch the bucket from the Python code. boto3 is AWS S3 Python SDK.\")\n",
    "            print(\"For fetching a specific bucket\\'s file use the following code:\\n\")\n",
    "            print(\"1. Set a variable \\'access_key\\' as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"2. Set a variable \\'secret_key\\' as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "            print(\"3. Set a variable \\'bucket_name\\' as a string containing only the name of the bucket. Do not add subdirectories, folders (prefixes), or file names.\")\n",
    "            print(\"Example: if your bucket is named \\'my_bucket\\' and its main directory contains folders like \\'folder1\\', \\'folder2\\', etc, do not declare bucket_name = \\'my_bucket/folder1\\', even if you only want files from folder1.\")\n",
    "            print(\"ALWAYS declare only the bucket\\'s name: bucket_name = \\'my_bucket\\'.\")\n",
    "            print(\"4. Set a variable \\'file_path\\' containing the path from the bucket\\'s subdirectories to the file you want to fetch. Include the file name and its extension.\")\n",
    "            print(\"If the file is stored in the bucket\\'s root (main) directory: file_path = \\\"my_file.ext\\\".\")\n",
    "            print(\"If the path of the file in the bucket is: \\'dir1/…/dirN/my_file.ext\\', where dirN is the N-th subdirectory, and dir1 is a folder or directory of the main (root) bucket\\'s directory: file_path = \\\"dir1/…/dirN/my_file.ext\\\".\")\n",
    "            print(\"Also, we say that \\'dir1/…/dirN/\\' is the file\\'s prefix. Notice that the name of the bucket is never declared here as the path for fetching its content from the Python code.\")\n",
    "            print(\"5. Set a variable named \\'new_path\\' to store the path of the file copied to the notebook’s workspace. This path must contain the file name and its extension.\")\n",
    "            print(\"Example: if you want to copy \\'my_file.ext\\' to the root directory of the notebook’s workspace, set: new_path = \\\"/my_file.ext\\\".\")\n",
    "            print(\"6. Finally, declare the following code, which refers to the defined variables:\\n\")\n",
    "\n",
    "            # Let's use triple quotes to declare a formated string\n",
    "            example_code = \"\"\"\n",
    "                import boto3\n",
    "                # Start S3 client as the object 's3_client'\n",
    "                s3_client = boto3.resource('s3', aws_access_key_id = access_key, aws_secret_access_key = secret_key)\n",
    "                # Connect to the bucket specified as 'bucket_name'.\n",
    "                # The bucket is started as the object 's3_bucket':\n",
    "                s3_bucket = s3_client.Bucket(bucket_name)\n",
    "                # Select the object in the bucket previously started as 's3_bucket':\n",
    "                selected_object = s3_bucket.Object(file_path)\n",
    "                # Download the selected object to the workspace in the specified file_path\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" copies a xlsx file named 'my_table' to the notebook's main (root)\n",
    "                # directory\n",
    "                selected_object.download_file(Filename = new_path)\n",
    "                \"\"\"\n",
    "\n",
    "            print(example_code)\n",
    "\n",
    "            print(\"An object \\'my_file.ext\\' in the main (root) directory of the s3_bucket is stored in Python environment as:\")\n",
    "            print(\"\"\"s3.ObjectSummary(bucket_name='bucket_name', key='my_file.ext'\"\"\") \n",
    "            # triple quotes to keep the internal quotes without using too much backslashes \"\\\" (the ignore next character)\n",
    "            print(\"Then, the name of each object is stored as the attribute \\'key\\' of the object. To view all objects, we can loop through their \\'key\\' attributes:\\n\")\n",
    "            example_code = \"\"\"\n",
    "                # Loop through all objects of the bucket:\n",
    "                for stored_obj in s3_bucket.objects.all():\t\t\n",
    "                    # Loop through all elements 'stored_obj' from s3_bucket.objects.all()\n",
    "                    # Which stores the ObjectSummary for all objects in the bucket s3_bucket:\n",
    "                    # Print the object’s names:\n",
    "                    print(stored_obj.key)\n",
    "                    \"\"\"\n",
    "\n",
    "            print(example_code)\n",
    "\n",
    "                \n",
    "    else:\n",
    "        \n",
    "        print(\"Select a valid source: \\'google\\' for mounting Google Drive; or \\'aws\\' for accessing AWS S3 Bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for loading the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pandas_dataframe (file_directory_path, file_name_with_extension, load_txt_file_with_json_format = False, how_missing_values_are_registered = None, has_header = True, decimal_separator = '.', txt_csv_col_sep = \"comma\", sheet_to_load = None, json_record_path = None, json_field_separator = \"_\", json_metadata_prefix_list = None):\n",
    "    \n",
    "    # Pandas documentation:\n",
    "    # pd.read_csv: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "    # pd.read_excel: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html\n",
    "    # pd.json_normalize: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html\n",
    "    # Python JSON documentation:\n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    \n",
    "    import os\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    \n",
    "    ## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "    ## JSON, txt, or CSV (comma separated values) files.\n",
    "    \n",
    "    # file_directory_path - (string, in quotes): input the path of the directory (e.g. folder path) \n",
    "    # where the file is stored. e.g. file_directory_path = \"/\" or file_directory_path = \"/folder\"\n",
    "    \n",
    "    # FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "    # extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "    # FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "    # Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
    "    \n",
    "    # load_txt_file_with_json_format = False. Set load_txt_file_with_json_format = True \n",
    "    # if you want to read a file with txt extension containing a text formatted as JSON \n",
    "    # (but not saved as JSON).\n",
    "    # WARNING: if load_txt_file_with_json_format = True, all the JSON file parameters of the \n",
    "    # function (below) must be set. If not, an error message will be raised.\n",
    "    \n",
    "    # HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "    # empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "    # This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "    # By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "    #‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "    # ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "    # If a different denomination is used, indicate it as a string. e.g.\n",
    "    # HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "    # HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "    # If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "    # only in column 'numeric_col', you can specify the following dictionary:\n",
    "    # how_missing_values_are_registered = {'numeric-col': 0}\n",
    "    \n",
    "    \n",
    "    # has_header = True if the the imported table has headers (row with columns names).\n",
    "    # Alternatively, has_header = False if the dataframe does not have header.\n",
    "    \n",
    "    # DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "    # the decimal separator. Alternatively, specify here the separator.\n",
    "    # e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "    # It manipulates the argument 'decimal' from Pandas functions.\n",
    "    \n",
    "    # txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "    # or 'csv'. It informs how the different columns are separated.\n",
    "    # Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "    # for columns separated by comma;\n",
    "    # txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "    # for columns separated by simple spaces.\n",
    "    # You can also set a specific separator as string. For example:\n",
    "    # txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "    # is used as separator for the columns - '\\t' represents the tab character).\n",
    "    \n",
    "    # sheet_to_load - This parameter has effect only when for Excel files.\n",
    "    # keep sheet_to_load = None not to specify a sheet of the file, so that the first sheet\n",
    "    # will be loaded.\n",
    "    # sheet_to_load may be an integer or an string (inside quotes). sheet_to_load = 0\n",
    "    # loads the first sheet (sheet with index 0); sheet_to_load = 1 loads the second sheet\n",
    "    # of the file (index 1); sheet_to_load = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "    # Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "    # name to load the sheet with that name.\n",
    "    \n",
    "    ## Parameters for loading JSON files:\n",
    "    \n",
    "    # json_record_path (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "    # Path in each object to list of records. If not passed, data will be assumed to \n",
    "    # be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "    # dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "    # 'books' stores a nested JSON, declare, json_record_path = 'books'\n",
    "    \n",
    "    # json_field_separator = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "    # Nested records will generate names separated by sep. \n",
    "    # e.g., for json_field_separator = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "    # Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "    # the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "    # separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "    \n",
    "    # json_metadata_prefix_list: list of strings (in quotes). Manipulates the parameter \n",
    "    # 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "    # table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "    # will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "    \n",
    "    # e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "    # 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "    # Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "    # are 'name' and 'last'.\n",
    "    # Then, json_record_path = 'books'\n",
    "    # json_metadata_prefix_list = ['name', 'last']\n",
    "    \n",
    "    \n",
    "    # Create the complete file path:\n",
    "    file_path = os.path.join(file_directory_path, file_name_with_extension)\n",
    "    # Extract the file extension\n",
    "    file_extension = os.path.splitext(file_path)[1][1:]\n",
    "    # os.path.splitext(file_path) is a tuple of strings: the first is the complete file\n",
    "    # root with no extension; the second is the extension starting with a point: '.txt'\n",
    "    # When we set os.path.splitext(file_path)[1], we are selecting the second element of\n",
    "    # the tuple. By selecting os.path.splitext(file_path)[1][1:], we are taking this string\n",
    "    # from the second character (index 1), eliminating the dot: 'txt'\n",
    "    \n",
    "    # Check if the decimal separator is None. If it is, set it as '.' (period):\n",
    "    if (decimal_separator is None):\n",
    "        decimal_separator = '.'\n",
    "    \n",
    "    if ((file_extension == 'txt') | (file_extension == 'csv')): \n",
    "        # The operator & is equivalent to 'And' (intersection).\n",
    "        # The operator | is equivalent to 'Or' (union).\n",
    "        # pandas.read_csv method must be used.\n",
    "        if (load_txt_file_with_json_format == True):\n",
    "            \n",
    "            print(\"Reading a txt file containing JSON parsed data. A reading error will be raised if you did not set the JSON parameters.\")\n",
    "            \n",
    "            with open(file_path, 'r') as opened_file:\n",
    "                # 'r' stands for read mode; 'w' stands for write mode\n",
    "                # read the whole file as a string named 'file_full_text'\n",
    "                file_full_text = opened_file.read()\n",
    "                # if we used the readlines() method, we would be reading the\n",
    "                # file by line, not the whole text at once.\n",
    "                # https://stackoverflow.com/questions/8369219/how-to-read-a-text-file-into-a-string-variable-and-strip-newlines?msclkid=a772c37bbfe811ec9a314e3629df4e1e\n",
    "                # https://www.tutorialkart.com/python/python-read-file-as-string/#:~:text=example.py%20%E2%80%93%20Python%20Program.%20%23open%20text%20file%20in,and%20prints%20it%20to%20the%20standard%20output.%20Output.?msclkid=a7723a1abfe811ecb68bba01a2b85bd8\n",
    "                \n",
    "            #Now, file_full_text is a string containing the full content of the txt file.\n",
    "            json_file = json.loads(file_full_text)\n",
    "            # json.load() : This method is used to parse JSON from URL or file.\n",
    "            # json.loads(): This method is used to parse string with JSON content.\n",
    "            # e.g. .json.loads() must be used to read a string with JSON and convert it to a flat file\n",
    "            # like a dataframe.\n",
    "            # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.\n",
    "            dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "        \n",
    "        else:\n",
    "            # Not a JSON txt\n",
    "        \n",
    "            if (has_header == True):\n",
    "\n",
    "                if ((txt_csv_col_sep == \"comma\") | (txt_csv_col_sep == \",\")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                    #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                    # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                    # parsing speed by 5-10x.\n",
    "\n",
    "                elif ((txt_csv_col_sep == \"whitespace\") | (txt_csv_col_sep == \" \")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, delim_whitespace = True, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                    #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                    # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                    # parsing speed by 5-10x.\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    try:\n",
    "                        \n",
    "                        # Try using the character specified as the argument txt_csv_col_sep:\n",
    "                        dataset = pd.read_csv(file_path, sep = txt_csv_col_sep, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                        # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                        #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                        # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                        # parsing speed by 5-10x.\n",
    "                    \n",
    "                    except:\n",
    "                        # An error was raised, the separator is not valid\n",
    "                        print(f\"Enter a valid column separator for the {file_extension} file, like: \\'comma\\' or \\'whitespace\\'.\")\n",
    "\n",
    "\n",
    "            else:\n",
    "                # has_header == False\n",
    "\n",
    "                if ((txt_csv_col_sep == \"comma\") | (txt_csv_col_sep == \",\")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                    #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                    # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                    # parsing speed by 5-10x.\n",
    "                    \n",
    "                elif ((txt_csv_col_sep == \"whitespace\") | (txt_csv_col_sep == \" \")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, delim_whitespace = True, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                    #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                    # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                    # parsing speed by 5-10x.\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    try:\n",
    "                        \n",
    "                        # Try using the character specified as the argument txt_csv_col_sep:\n",
    "                        dataset = pd.read_csv(file_path, sep = txt_csv_col_sep, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                        # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                        #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                        # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                        # parsing speed by 5-10x.\n",
    "                    \n",
    "                    except:\n",
    "                        # An error was raised, the separator is not valid\n",
    "                        print(f\"Enter a valid column separator for the {file_extension} file, like: \\'comma\\' or \\'whitespace\\'.\")\n",
    "\n",
    "    elif (file_extension == 'json'):\n",
    "        \n",
    "        with open(file_path, 'r') as opened_file:\n",
    "            \n",
    "            json_file = json.load(opened_file)\n",
    "            # The structure json_file = json.load(open(file_path)) relies on the GC to close the file. That's not a \n",
    "            # good idea: If someone doesn't use CPython the garbage collector might not be using refcounting (which \n",
    "            # collects unreferenced objects immediately) but e.g. collect garbage only after some time.\n",
    "            # Since file handles are closed when the associated object is garbage collected or closed \n",
    "            # explicitly (.close() or .__exit__() from a context manager) the file will remain open until \n",
    "            # the GC kicks in.\n",
    "            # Using 'with' ensures the file is closed as soon as the block is left - even if an exception \n",
    "            # happens inside that block, so it should always be preferred for any real application.\n",
    "            # source: https://stackoverflow.com/questions/39447362/equivalent-ways-to-json-load-a-file-in-python\n",
    "            \n",
    "        # json.load() : This method is used to parse JSON from URL or file.\n",
    "        # json.loads(): This method is used to parse string with JSON content.\n",
    "        # Then, json.load for a .json file\n",
    "        # and json.loads for text file containing json\n",
    "        # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.   \n",
    "        dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "    \n",
    "    else:\n",
    "        # If it is not neither a csv nor a txt file, let's assume it is one of different\n",
    "        # possible Excel files.\n",
    "        print(\"Excel file inferred. If an error message is shown, check if a valid file extension was used: \\'xlsx\\', \\'xls\\', etc.\")\n",
    "            \n",
    "        if (sheet_to_load is not None):        \n",
    "        #Case where the user specifies which sheet of the Excel file should be loaded.\n",
    "            \n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                # parsing speed by 5-10x.\n",
    "                \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                # parsing speed by 5-10x.\n",
    "        else:\n",
    "            #No sheet specified\n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                # parsing speed by 5-10x.\n",
    "                \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                # parsing speed by 5-10x.\n",
    "                \n",
    "    print(f\"Dataset extracted from {file_path}. Check the 10 first rows of this dataframe:\\n\")\n",
    "    print(dataset.head(10))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for converting JSON object to dataframe**\n",
    "- Objects may be:\n",
    "    - String with JSON formatted text;\n",
    "    - List with nested dictionaries (JSON formatted);\n",
    "    - Each dictionary may contain nested dictionaries, or nested lists of dictionaries (nested JSON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_obj_to_pandas_dataframe (json_obj_to_convert, json_obj_type = 'list', json_record_path = None, json_field_separator = \"_\", json_metadata_prefix_list = None):\n",
    "    \n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    \n",
    "    # JSON object in terms of Python structure: list of dictionaries, where each value of a\n",
    "    # dictionary may be a dictionary or a list of dictionaries (nested structures).\n",
    "    # example of highly nested structure saved as a list 'json_formatted_list'. Note that the same\n",
    "    # structure could be declared and stored into a string variable. For instance, if you have a txt\n",
    "    # file containing JSON, you could read the txt and save its content as a string.\n",
    "    # json_formatted_list = [{'field1': val1, 'field2': {'dict_val': dict_val}, 'field3': [{\n",
    "    # 'nest1': nest_val1}, {'nest2': nestval2}]}, {'field1': val1, 'field2': {'dict_val': dict_val}, \n",
    "    # 'field3': [{'nest1': nest_val1}, {'nest2': nestval2}]}]    \n",
    "\n",
    "    # json_obj_type = 'list', in case the object was saved as a list of dictionaries (JSON format)\n",
    "    # json_obj_type = 'string', in case it was saved as a string (text) containing JSON.\n",
    "\n",
    "    # json_obj_to_convert: object containing JSON, or string with JSON content to parse.\n",
    "    # Objects may be: string with JSON formatted text;\n",
    "    # list with nested dictionaries (JSON formatted);\n",
    "    # dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "    \n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html#pandas.json_normalize\n",
    "    \n",
    "    # json_record_path (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "    # Path in each object to list of records. If not passed, data will be assumed to \n",
    "    # be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "    # dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "    # 'books' stores a nested JSON, declare, json_record_path = 'books'\n",
    "    \n",
    "    # json_field_separator = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "    # Nested records will generate names separated by sep. \n",
    "    # e.g., for json_field_separator = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "    # Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "    # the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "    # separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "    \n",
    "    # json_metadata_prefix_list: list of strings (in quotes). Manipulates the parameter \n",
    "    # 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "    # table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "    # will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "    \n",
    "    # e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "    # 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "    # Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "    # are 'name' and 'last'.\n",
    "    # Then, json_record_path = 'books'\n",
    "    # json_metadata_prefix_list = ['name', 'last']\n",
    "\n",
    "    \n",
    "    if (json_obj_type == 'string'):\n",
    "        # Use the json.loads method to convert the string to json\n",
    "        json_file = json.loads(json_obj_to_convert)\n",
    "        # json.load() : This method is used to parse JSON from URL or file.\n",
    "        # json.loads(): This method is used to parse string with JSON content.\n",
    "        # e.g. .json.loads() must be used to read a string with JSON and convert it to a flat file\n",
    "        # like a dataframe.\n",
    "        # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.\n",
    "    \n",
    "    elif (json_obj_type == 'list'):\n",
    "        \n",
    "        # make the json_file the object itself:\n",
    "        json_file = json_obj_to_convert\n",
    "    \n",
    "    else:\n",
    "        print (\"Enter a valid JSON object type: \\'list\\', in case the JSON object is a list of dictionaries in JSON format; or \\'string\\', if the JSON is stored as a text (string variable).\")\n",
    "        return \"error\"\n",
    "    \n",
    "    dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "    \n",
    "    print(f\"JSON object {json_obj_to_convert} converted to a flat dataframe object. Check the 10 first rows of this dataframe:\\n\")\n",
    "    print(dataset.head(10))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for concatenating (SQL UNION) multiple dataframes**\n",
    "- Vertical concatenation of the dataframes.\n",
    "- Equivalent to SQL Union: vertical stack/append of the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNION_DATAFRAMES (list_of_dataframes, what_to_append = 'rows', ignore_index_on_union = True, sort_values_on_union = True, union_join_type = None):\n",
    "    \n",
    "    import pandas as pd\n",
    "    #JOIN can be 'inner' to perform an inner join, eliminating the missing values\n",
    "    #The default (None) is 'outer': the dataframes will be stacked on the columns with\n",
    "    #same names but, in case there is no correspondence, the row will present a missing\n",
    "    #value for the columns which are not present in one of the dataframes.\n",
    "    #When using the 'inner' method, only the common columns will remain\n",
    "    \n",
    "    #list_of_dataframes must be a list containing the dataframe objects\n",
    "    # example: list_of_dataframes = [df1, df2, df3, df4]\n",
    "    #Notice that the dataframes are objects, not strings. Therefore, they should not\n",
    "    # be declared inside quotes.\n",
    "    # There is no limit of dataframes. In this example, we will concatenate 4 dataframes.\n",
    "    # If list_of_dataframes = [df1, df2, df3] we would concatenate 3, and if\n",
    "    # list_of_dataframes = [df1, df2, df3, df4, df5] we would concatenate 5 dataframes.\n",
    "    \n",
    "    # what_to_append = 'rows' for appending the rows from one dataframe\n",
    "    # into the other; what_to_append = 'columns' for appending the columns\n",
    "    # from one dataframe into the other (horizontal or lateral append).\n",
    "    \n",
    "    # When what_to_append = 'rows', Pandas .concat method is defined as\n",
    "    # axis = 0, i.e., the operation occurs in the row level, so the rows\n",
    "    # of the second dataframe are added to the bottom of the first one.\n",
    "    # It is the SQL union, and creates a dataframe with more rows, and\n",
    "    # total of columns equals to the total of columns of the first dataframe\n",
    "    # plus the columns of the second one that were not in the first dataframe.\n",
    "    # When what_to_append = 'columns', Pandas .concat method is defined as\n",
    "    # axis = 1, i.e., the operation occurs in the column level: the two\n",
    "    # dataframes are laterally merged using the index as the key, \n",
    "    # preserving all columns from both dataframes. Therefore, the number of\n",
    "    # rows will be the total of rows of the dataframe with more entries,\n",
    "    # and the total of columns will be the sum of the total of columns of\n",
    "    # the first dataframe with the total of columns of the second dataframe.\n",
    "    \n",
    "    #The other parameters are the same from Pandas .concat method.\n",
    "    # ignore_index_on_union = ignore_index;\n",
    "    # sort_values_on_union = sort\n",
    "    # union_join_type = join\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\n",
    "    \n",
    "    #Check Datacamp course Joining Data with pandas, Chap.3, \n",
    "    # Advanced Merging and Concatenating\n",
    "    \n",
    "    # Check axis:\n",
    "    if (what_to_append == 'rows'):\n",
    "        \n",
    "        AXIS = 0\n",
    "    \n",
    "    elif (what_to_append == 'columns'):\n",
    "        \n",
    "        AXIS = 1\n",
    "    \n",
    "    else:\n",
    "        print(\"No valid string was input to what_to_append, so appending rows (vertical append, equivalent to SQL UNION).\")\n",
    "        AXIS = 0\n",
    "    \n",
    "    if (union_join_type == 'inner'):\n",
    "        \n",
    "        print(\"Warning: concatenating dataframes using the \\'inner\\' join method, that removes missing values.\")\n",
    "        concat_df = pd.concat(list_of_dataframes, axis = AXIS, ignore_index = ignore_index_on_union, sort = sort_values_on_union, join = union_join_type)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #In case None or an invalid value is provided, use the default 'outer', by simply\n",
    "        # not declaring the 'join':\n",
    "        concat_df = pd.concat(list_of_dataframes, axis = AXIS, ignore_index = ignore_index_on_union, sort = sort_values_on_union)\n",
    "    \n",
    "    # Pandas .head(Y) method results in a dataframe containing the first Y rows of the \n",
    "    # original dataframe. The default .head() is Y = 5. Print first 10 rows of the \n",
    "    # new dataframe:\n",
    "    print(\"Dataframes successfully concatenated. Check the 10 first rows of new dataframe:\\n\")\n",
    "    print(concat_df.head(10))\n",
    "    \n",
    "    #Now return the concatenated dataframe:\n",
    "    \n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for column filtering (selecting); or column renaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_filter_rename (df, cols_list, mode = 'filter'):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    #mode = 'filter' for filtering only the list of columns passed as cols_list;\n",
    "    #mode = 'rename' for renaming the columns with the names passed as cols_list.\n",
    "    \n",
    "    #cols_list = list of strings containing the names (headers) of the columns to select\n",
    "    # (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "    # For instance: cols_list = ['col1', 'col2', 'col3'] will \n",
    "    # select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "    # Declare the names inside quotes.\n",
    "    \n",
    "    print(f\"Original columns in the dataframe:\\n{df.columns}\")\n",
    "    \n",
    "    if (mode == 'filter'):\n",
    "        \n",
    "        #filter the dataframe so that it will contain only the cols_list.\n",
    "        df = df[cols_list]\n",
    "        print(\"Dataframe filtered according to the list provided.\")\n",
    "        \n",
    "    elif (mode == 'rename'):\n",
    "        \n",
    "        #Check if the number of columns of the dataset is equal to the number of elements\n",
    "        # of the new list. It will avoid raising an exception error.\n",
    "        boolean_filter = (len(cols_list) == len(df.columns))\n",
    "        \n",
    "        if (boolean_filter == False):\n",
    "            #Impossible to rename, number of elements are different.\n",
    "            print(\"The number of columns of the dataframe is different from the number of elements of the list. Please, provide a list with number of elements equals to the number of columns.\")\n",
    "        \n",
    "        else:\n",
    "            #Same number of elements, so that we can update the columns' names.\n",
    "            df.columns = cols_list\n",
    "            print(\"Dataframe columns renamed according to the list provided.\")\n",
    "            print(\"Warning: the substitution is element-wise: the first element of the list is now the name of the first column, and so on, ..., so that the last element is the name of the last column.\")\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"Enter a valid mode: \\'filter\\' or \\'rename\\'.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for plotting the bar chart**\n",
    "- Bars may be vertically or horizontally oriented.\n",
    "- Bar charts are plotted after selecting an aggregation function, and the cumulative percent curve may be obtained and plotted with the bars (in secondary axis).\n",
    "- To obtain a **Pareto chart**, keep `aggregate_function = 'sum'`, `plot_cumulative_percent = True`, and `orientation = 'vertical'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "CELL_7"
    ]
   },
   "outputs": [],
   "source": [
    "def bar_chart (df, categorical_var_name, response_var_name, aggregate_function = 'sum', add_suffix_to_aggregated_col = True, suffix = None, calculate_and_plot_cumulative_percent = True, orientation = 'vertical', limit_of_plotted_categories = None, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # df: dataframe being analyzed\n",
    "    \n",
    "    # categorical_var_name: string (inside quotes) containing the name \n",
    "    # of the column to be analyzed. e.g. \n",
    "    # categorical_var_name = \"column1\"\n",
    "    \n",
    "    # response_var_name: string (inside quotes) containing the name \n",
    "    # of the column that stores the response correspondent to the\n",
    "    # categories. e.g. response_var_name = \"response_feature\" \n",
    "    \n",
    "    # aggregate_function = 'sum': String defining the aggregation \n",
    "    # method that will be applied. Possible values:\n",
    "    # 'median', 'mean', 'mode', 'sum', 'min', 'max', 'variance',\n",
    "    # 'standard_deviation','10_percent_quantile', '20_percent_quantile',\n",
    "    # '25_percent_quantile', '30_percent_quantile', '40_percent_quantile',\n",
    "    # '50_percent_quantile', '60_percent_quantile', '70_percent_quantile',\n",
    "    # '75_percent_quantile', '80_percent_quantile', '90_percent_quantile',\n",
    "    # and '95_percent_quantile'.\n",
    "    # To use another aggregate function, the method must be added to the\n",
    "    # dictionary of methods agg_methods_dict, defined in the function.\n",
    "    # If None or an invalid function is input, 'sum' will be used.\n",
    "    \n",
    "    # add_suffix_to_aggregated_col = True will add a suffix to the\n",
    "    # aggregated column. e.g. 'responseVar_mean'. If add_suffix_to_aggregated_col \n",
    "    # = False, the aggregated column will have the original column name.\n",
    "    \n",
    "    # suffix = None. Keep it None if no suffix should be added, or if\n",
    "    # the name of the aggregate function should be used as suffix, after\n",
    "    # \"_\". Alternatively, set it as a string. As recommendation, put the\n",
    "    # \"_\" sign in the beginning of this string to separate the suffix from\n",
    "    # the original column name. e.g. if the response variable is 'Y' and\n",
    "    # suffix = '_agg', the new aggregated column will be named as 'Y_agg'\n",
    "    \n",
    "    # calculate_and_plot_cumulative_percent = True to calculate and plot\n",
    "    # the line of cumulative percent, or \n",
    "    # calculate_and_plot_cumulative_percent = False to omit it.\n",
    "    # This feature is only shown when aggregate_function = 'sum', 'median',\n",
    "    # 'mean', or 'mode'. So, it will be automatically set as False if \n",
    "    # another aggregate is selected.\n",
    "    \n",
    "    # orientation = 'vertical' is the standard, and plots vertical bars\n",
    "    # (perpendicular to the X axis). In this case, the categories are shown\n",
    "    # in the X axis, and the correspondent responses are in Y axis.\n",
    "    # Alternatively, orientation = 'horizontal' results in horizontal bars.\n",
    "    # In this case, categories are in Y axis, and responses in X axis.\n",
    "    # If None or invalid values are provided, orientation is set as 'vertical'.\n",
    "    \n",
    "    # Note: to obtain a Pareto chart, keep aggregate_function = 'sum',\n",
    "    # plot_cumulative_percent = True, and orientation = 'vertical'.\n",
    "    \n",
    "    # limit_of_plotted_categories: integer value that represents\n",
    "    # the maximum of categories that will be plot. Keep it None to plot\n",
    "    # all categories. Alternatively, set an integer value. e.g.: if\n",
    "    # limit_of_plotted_categories = 4, but there are more categories,\n",
    "    # the dataset will be sorted in descending order and: 1) The remaining\n",
    "    # categories will be sum in a new category named 'others' if the\n",
    "    # aggregate function is 'sum'; 2) Or the other categories will be simply\n",
    "    # omitted from the plot, for other aggregate functions. Notice that\n",
    "    # it limits only the variables in the plot: all of them will be\n",
    "    # returned in the dataframe.\n",
    "    # Use this parameter to obtain a cleaner plot. Notice that the remaining\n",
    "    # columns will be aggregated as 'others' even if there is a single column\n",
    "    # beyond the limit.\n",
    "    \n",
    "    \n",
    "    # Create a local copy of the dataframe to manipulate:\n",
    "    \n",
    "    DATASET = df\n",
    "    \n",
    "    # Create the dictionary of possible aggregates, to define the\n",
    "    # aggregation method, according to the set by the user:\n",
    "    agg_methods_dict = {\n",
    "        \n",
    "        'median': DATASET.groupby(categorical_var_name)[response_var_name].median(),\n",
    "        'mean': DATASET.groupby(categorical_var_name)[response_var_name].mean(),\n",
    "        'mode': DATASET.groupby(categorical_var_name)[response_var_name].mode(),\n",
    "        'sum': DATASET.groupby(categorical_var_name)[response_var_name].sum(),\n",
    "        'min': DATASET.groupby(categorical_var_name)[response_var_name].min(),\n",
    "        'max': DATASET.groupby(categorical_var_name)[response_var_name].max(),\n",
    "        'variance': DATASET.groupby(categorical_var_name)[response_var_name].var(),\n",
    "        'standard_deviation': DATASET.groupby(categorical_var_name)[response_var_name].std(),\n",
    "        '10_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.10),\n",
    "        '20_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.20),\n",
    "        '25_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.25),\n",
    "        '30_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.30),\n",
    "        '40_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.40),\n",
    "        '50_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.50),\n",
    "        '60_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.60),\n",
    "        '70_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.70),\n",
    "        '75_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.75),\n",
    "        '80_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.80),\n",
    "        '90_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.90),\n",
    "        '95_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.95)\n",
    "    }\n",
    "    \n",
    "    # check if the function was not set in the dictionary. If not,\n",
    "    # use 'sum'\n",
    "    if (aggregate_function not in (agg_methods_dict.keys())):\n",
    "        \n",
    "        aggregate_function = 'sum'\n",
    "        print(\"Invalid or no aggregation function input, so using the default \\'sum\\'.\")\n",
    "    \n",
    "    # Select the method in the dictionary and apply it. To access a value\n",
    "    # 'val' correspondent to the key 'key' from a dictionary dict, we\n",
    "    # declare: dict['key'], just as accessing a column from a dataframe.\n",
    "    \n",
    "    # The value will be the application of the method itself, i.e., the\n",
    "    # dataset will be aggregated:\n",
    "    DATASET = agg_methods_dict[aggregate_function]\n",
    "    \n",
    "    # If an aggregate function different from 'sum', 'mean', 'median' or 'mode' \n",
    "    # is used with plot_cumulative_percent = True, \n",
    "    # set plot_cumulative_percent = False:\n",
    "    # (check if aggregate function is not in the list of allowed values):\n",
    "    if ((aggregate_function not in ['sum', 'mean', 'median', 'mode']) & (calculate_and_plot_cumulative_percent == True)):\n",
    "        \n",
    "        calculate_and_plot_cumulative_percent = False\n",
    "        print(\"The cumulative percent is only calculated when aggregate_function = \\'sum\\', \\'mean\\', \\'median\\', or \\'mode\\'. So, plot_cumulative_percent was set as False.\")\n",
    "    \n",
    "    # Guarantee that the columns from the aggregated dataset have the correct\n",
    "    \n",
    "    # Let's create a list of the new column names\n",
    "    # The first element is categorical_var_name, which is not modified:\n",
    "    list_of_cols = [categorical_var_name]\n",
    "    \n",
    "    # Check if add_suffix_to_aggregated_col is False. If it is, simply\n",
    "    # repeat the original response_var_name:\n",
    "    if (add_suffix_to_aggregated_col == False):\n",
    "        \n",
    "        list_of_cols.append(response_var_name)\n",
    "    \n",
    "    else:\n",
    "        # Let's add a suffix. Check if suffix is None. If it is,\n",
    "        # set \"_\" + aggregate_function as suffix:\n",
    "        \n",
    "        if (suffix is None):\n",
    "            suffix = \"_\" + aggregate_function\n",
    "        \n",
    "        # Now, append response_var_name + suffix to the list to\n",
    "        # create the name of the new aggregated column:\n",
    "        response_var_name = response_var_name + suffix\n",
    "        list_of_cols.append(response_var_name)\n",
    "    \n",
    "    # Now, rename the columns of the aggregated dataset as the list\n",
    "    # list_of_cols:\n",
    "    DATASET.columns = list_of_cols\n",
    "    \n",
    "    # Let's sort the dataframe.\n",
    "    \n",
    "    # Order the dataframe in descending order by the response.\n",
    "    # If there are equal responses, order them by category, in\n",
    "    # ascending order; put the missing values in the first position\n",
    "    # To pass multiple columns and multiple types of ordering, we use\n",
    "    # lists. If there was a single column to order by, we would declare\n",
    "    # it as a string. If only one order of ascending was used, we would\n",
    "    # declare it as a simple boolean\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html\n",
    "    \n",
    "    DATASET = DATASET.sort_values(by = [response_var_name, categorical_var_name], ascending = [False, True], na_position = 'first')\n",
    "    \n",
    "    # Now, reset index positions:\n",
    "    DATASET = DATASET.reset_index(drop = True)\n",
    "    \n",
    "    # plot_cumulative_percent = True, create a column to store the\n",
    "    # cumulative percent:\n",
    "    if (calculate_and_plot_cumulative_percent): \n",
    "        # Run the following code if the boolean value is True (implicity)\n",
    "        \n",
    "        # Calculate the total sum of the array correspondent to\n",
    "        # the column (series) response_var_name\n",
    "        total_sum = np.sum(np.array(DATASET[response_var_name]))\n",
    "        \n",
    "        # Create a column series for the cumulative sum:\n",
    "        cumsum_col = response_var_name + \"_cumsum\"\n",
    "        DATASET[cumsum_col] = DATASET[response_var_name].cumsum()\n",
    "        \n",
    "        # Now, create a column for the accumulated percent\n",
    "        # by dividing cumsum_col by total_sum and multiplying it by\n",
    "        # 100 (%):\n",
    "        cum_pct_col = response_var_name + \"_cum_pct\"\n",
    "        DATASET[cum_pct_col] = (DATASET[cumsum_col])/(total_sum)*100\n",
    "        print(f\"Successfully calculated cumulative sum and cumulative percent correspondent to the response variable {response_var_name}.\")\n",
    "    \n",
    "    print(\"Successfully aggregated and ordered the dataset to plot. Check the 10 first rows of this returned dataset:\\n\")\n",
    "    print(DATASET.head(10))\n",
    "    \n",
    "    # Check if the total of plotted categories is limited:\n",
    "    if not (limit_of_plotted_categories is None):\n",
    "        \n",
    "        # Since the value is not None, we have to limit it\n",
    "        # Check if the limit is lower than or equal to the length of the dataframe.\n",
    "        # If it is, we simply copy the columns to the series (there is no need of\n",
    "        # a memory-consuming loop or of applying the head method to a local copy\n",
    "        # of the dataframe):\n",
    "        df_length = len(DATASET)\n",
    "            \n",
    "        if (limit_of_plotted_categories <= df_length):\n",
    "            # Simply copy the columns to the graphic series:\n",
    "            categories = DATASET[categorical_var_name]\n",
    "            responses = DATASET[response_var_name]\n",
    "            # If there is a cum_pct column, copy it to a series too:\n",
    "            if (calculate_and_plot_cumulative_percent):\n",
    "                cum_pct = plotted_df[cum_pct_col]\n",
    "        \n",
    "        else:\n",
    "            # The limit is lower than the total of categories,\n",
    "            # so we actually have to limit the size of plotted df:\n",
    "        \n",
    "            # If aggregate_function is not 'sum', we simply apply\n",
    "            # the head method to obtain the first rows (number of\n",
    "            # rows input as parameter; if no parameter is input, the\n",
    "            # number of 5 rows is used):\n",
    "            if (aggregate_function != 'sum'):\n",
    "                # Limit to the number limit_of_plotted_categories:\n",
    "                # create another local copy of the dataframe not to\n",
    "                # modify the returned dataframe object:\n",
    "                plotted_df = DATASET.head(limit_of_plotted_categories)\n",
    "\n",
    "                # Create the series of elements to plot:\n",
    "                categories = plotted_df[categorical_var_name]\n",
    "                responses = plotted_df[response_var_name]\n",
    "                # If the cumulative percent was obtained, create the series for it:\n",
    "                if (calculate_and_plot_cumulative_percent):\n",
    "                    cum_pct = plotted_df[cum_pct_col]\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Firstly, copy the elements that will be kept to x, y and (possibly) cum_pct\n",
    "                # lists.\n",
    "                # Start the lists:\n",
    "                categories = []\n",
    "                responses = []\n",
    "                if (calculate_and_plot_cumulative_percent):\n",
    "                    cum_pct = [] # start this list only if its needed to save memory\n",
    "\n",
    "                for i in range (0, limit_of_plotted_categories):\n",
    "                    # i goes from 0 (first index) to limit_of_plotted_categories - 1\n",
    "                    # (index of the last category to be kept):\n",
    "                    # copy the elements from the DATASET to the list\n",
    "                    # category is the 1st column (column 0); response is the 2nd (col 1);\n",
    "                    # and cumulative percent is the 4th (col 3):\n",
    "                    categories.append(DATASET.iloc[i, 0])\n",
    "                    responses.append(DATASET.iloc[i, 1])\n",
    "                    \n",
    "                    if (calculate_and_plot_cumulative_percent):\n",
    "                        cum_pct.append(DATASET.iloc[i, 3]) # only if there is something to iloc\n",
    "                    \n",
    "                # Now, i = limit_of_plotted_categories - 1\n",
    "                # Create a variable to store the sum of other responses\n",
    "                other_responses = 0\n",
    "                # loop from i = limit_of_plotted_categories to i = df_length-1, index\n",
    "                # of the last element. Notice that this loop may have a single call, if there\n",
    "                # is only one element above the limit:\n",
    "                for i in range (limit_of_plotted_categories, (df_length - 1)):\n",
    "                    \n",
    "                    other_responses = other_responses + (DATASET.iloc[i, 1])\n",
    "                \n",
    "                # Now, add the last elements to the series:\n",
    "                # The last category is named 'others':\n",
    "                categories.append('others')\n",
    "                # The correspondent aggregated response is the value \n",
    "                # stored in other_responses:\n",
    "                responses.append(other_responses)\n",
    "                # The cumulative percent is 100%, since this must be the sum of all\n",
    "                # elements (the previous ones plus the ones aggregated as 'others'\n",
    "                # must totalize 100%).\n",
    "                # On the other hand, the cumulative percent is stored only if needed:\n",
    "                cum_pct.append(100)\n",
    "    \n",
    "    else:\n",
    "        # This is the situation where there is no limit of plotted categories. So, we\n",
    "        # simply copy the columns to the plotted series (it is equivalent to the \n",
    "        # situation where there is a limit, but the limit is equal or inferior to the\n",
    "        # size of the dataframe):\n",
    "        categories = DATASET[categorical_var_name]\n",
    "        responses = DATASET[response_var_name]\n",
    "        # If there is a cum_pct column, copy it to a series too:\n",
    "        if (calculate_and_plot_cumulative_percent):\n",
    "            cum_pct = plotted_df[cum_pct_col]\n",
    "    \n",
    "    \n",
    "    # Now the data is prepared and we only have to plot \n",
    "    # categories, responses, and cum_pct:\n",
    "    \n",
    "    # Set labels and titles for the case they are None\n",
    "    if (plot_title is None):\n",
    "        plot_title = f\"Bar_chart_for_{response_var_name}_by_{categorical_var_name}\"\n",
    "    \n",
    "    if (horizontal_axis_title is None):\n",
    "\n",
    "        horizontal_axis_title = categorical_var_name\n",
    "\n",
    "    if (vertical_axis_title is None):\n",
    "        # Notice that response_var_name already has the suffix indicating the\n",
    "        # aggregation function\n",
    "        vertical_axis_title = response_var_name\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 70 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    plt.title(plot_title)\n",
    "    ax1.set_xlabel(horizontal_axis_title)\n",
    "    ax1.set_ylabel(vertical_axis_title, color = 'blue')\n",
    "    \n",
    "    if (orientation == 'horizontal'):\n",
    "        \n",
    "        # Horizontal bars used - barh method (bar horizontal):\n",
    "        # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.barh.html\n",
    "        # Now, the categorical variables stored in series categories must be\n",
    "        # positioned as the vertical axis Y, whereas the correspondent responses\n",
    "        # must be in the horizontal axis X.\n",
    "        ax1.barh(categories, responses, color = 'blue', label = categorical_var_name)\n",
    "        #.barh(y, x, ...)\n",
    "        \n",
    "        if (calculate_and_plot_cumulative_percent):\n",
    "            # Let's plot the line for the cumulative percent\n",
    "            # Set the grid for the bar chart as False. If it is True, there will\n",
    "            # be to grids, one for the bars and other for the percents, making \n",
    "            # the image difficult to interpretate:\n",
    "            ax1.grid(False)\n",
    "            \n",
    "            # Create the twin plot for the cumulative percent:\n",
    "            ax2 = ax1.twinx()\n",
    "            # Here, the x axis must be the cum_pct value, and the Y\n",
    "            # axis must be categories (it must be correspondent to the\n",
    "            # bar chart)\n",
    "            ax2.plot(cum_pct, categories, '-ro', color = 'red', label = \"cumulative\\npercent\")\n",
    "            #.plot(x, y, ...)\n",
    "            ax2.tick_params('x', color = 'red')\n",
    "            ax2.set_ylabel(\"Cumulative Percent (\\%)\", color = 'red')\n",
    "            ax2.legend()\n",
    "            ax2.grid(grid) # shown if user set grid = True\n",
    "            # If user wants to see the grid, it is shown only for the cumulative line.\n",
    "        \n",
    "        else:\n",
    "            # There is no cumulative line, so the parameter grid must control \n",
    "            # the bar chart's grid\n",
    "            ax1.legend()\n",
    "            ax1.grid(grid)\n",
    "        \n",
    "    else: \n",
    "        # If None or an invalid orientation was used, set it as vertical\n",
    "        # Use Matplotlib standard bar method (vertical bar):\n",
    "        # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html#matplotlib.pyplot.bar\n",
    "        \n",
    "        # In this standard case, the categorical variables (categories) are positioned\n",
    "        # as X, and the responses as Y:\n",
    "        ax1.bar(categories, responses, color = 'blue', label = categorical_var_name)\n",
    "        #.bar(x, y, ...)\n",
    "        \n",
    "        if (calculate_and_plot_cumulative_percent):\n",
    "            # Let's plot the line for the cumulative percent\n",
    "            # Set the grid for the bar chart as False. If it is True, there will\n",
    "            # be to grids, one for the bars and other for the percents, making \n",
    "            # the image difficult to interpretate:\n",
    "            ax1.grid(False)\n",
    "            \n",
    "            # Create the twin plot for the cumulative percent:\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.plot(categories, cum_pct, '-ro', color = 'red', label = \"cumulative\\npercent\")\n",
    "            #.plot(x, y, ...)\n",
    "            ax2.tick_params('y', color = 'red')\n",
    "            ax2.set_ylabel(\"Cumulative Percent (\\%)\", color = 'red')\n",
    "            ax2.legend()\n",
    "            ax2.grid(grid) # shown if user set grid = True\n",
    "            # If user wants to see the grid, it is shown only for the cumulative line.\n",
    "        \n",
    "        else:\n",
    "            # There is no cumulative line, so the parameter grid must control \n",
    "            # the bar chart's grid\n",
    "            ax1.legend()\n",
    "            ax1.grid(grid)\n",
    "    \n",
    "    # Notice that the .plot method is used for generating the plot for both orientations.\n",
    "    # It is different from .bar and .barh, which specify the orientation of a bar; or\n",
    "    # .hline (creation of an horizontal constant line); or .vline (creation of a vertical\n",
    "    # constant line).\n",
    "    \n",
    "    # Now the parameters specific to the configurations are finished, so we can go back\n",
    "    # to the general code:\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "        \n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"/\"\n",
    "        \n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"bar_chart\"\n",
    "        \n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 110 dpi\n",
    "            png_resolution_dpi = 110\n",
    "        \n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "        \n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "    \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for time series visualization**\n",
    "        x1, y1, lab1: blue\n",
    "        x2, y2, lab2: red\n",
    "        x3, y3, lab3: green\n",
    "        x4, y4, lab4: black\n",
    "        x5, y5, lab5: magenta\n",
    "        x6, y6, lab6: yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "azdata_cell_guid": "ef571494-3eb2-4dcc-9ebb-00c1fd6e9aad",
    "tags": [
     "CELL_7"
    ]
   },
   "outputs": [],
   "source": [
    "def time_series_vis (x1 = None, y1 = None, x2 = None, y2 = None, x3 = None, y3 = None, x4 = None, y4 = None, x5 = None, y5 = None, x6 = None, y6 = None, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, add_splines_lines = True, add_scatter_dots = False, lab1 = None, lab2 = None, lab3 = None, lab4 = None, lab5 = None, lab6 = None, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    if (add_splines_lines == True):\n",
    "        line_value = '-'\n",
    "    else:\n",
    "        line_value = ''\n",
    "    \n",
    "    if (add_scatter_dots == True):\n",
    "        marker_value = 'o'\n",
    "    else:\n",
    "        marker_value = ''\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    \n",
    "    if not (lab1 is None):\n",
    "        \n",
    "        label_1 = lab1\n",
    "    \n",
    "    else:\n",
    "        label_1 = \"Y1\"\n",
    "\n",
    "    if not (x1 is None):\n",
    "        ax.plot(x1, y1, linestyle = line_value, marker = marker_value, color='blue', label=label_1)\n",
    "    \n",
    "    if not (x2 is None):\n",
    "        #runs only when both are present\n",
    "        if not (lab2 is None):\n",
    "            label_2 = lab2\n",
    "        else:\n",
    "            label_2 = \"Y2\"\n",
    "        \n",
    "        ax.plot(x2, y2, linestyle = line_value, marker = marker_value, color='red', label=label_2)\n",
    "    \n",
    "    if not (x3 is None):\n",
    "                \n",
    "        if not (lab3 is None):\n",
    "            label_3 = lab3\n",
    "        else:\n",
    "            label_3 = \"Y3\"\n",
    "        \n",
    "        ax.plot(x3, y3, linestyle = line_value, marker = marker_value, color='green', label=label_3)\n",
    "    \n",
    "    if not (x4 is None):\n",
    "                \n",
    "        if not (lab4 is None):\n",
    "            label_4 = lab4\n",
    "        else:\n",
    "            label_4 = \"Y4\"\n",
    "        \n",
    "        ax.plot(x4, y4, linestyle = line_value, marker = marker_value, color='black', label=label_4)\n",
    "    \n",
    "    if not (x5 is None):\n",
    "               \n",
    "        if not (lab5 is None):\n",
    "            label_5 = lab5\n",
    "        else:\n",
    "            label_5 = \"Y5\"\n",
    "        \n",
    "        ax.plot(x5, y5, linestyle = line_value, marker = marker_value, color='magenta', label=label_5)\n",
    "   \n",
    "    if not (x6 is None):\n",
    "               \n",
    "        if not (lab6 is None):\n",
    "            label_6 = lab6\n",
    "        else:\n",
    "            label_6 = \"Y6\"\n",
    "        \n",
    "        ax.plot(x6, y6, linestyle = line_value, marker = marker_value, color='yellow', label=label_6)\n",
    "   \n",
    "    if not (plot_title is None):\n",
    "        #graphic's title\n",
    "        ax.set_title(plot_title) \n",
    "    \n",
    "    if not (horizontal_axis_title is None):\n",
    "        #X-axis title\n",
    "        ax.set_xlabel(horizontal_axis_title)\n",
    "    \n",
    "    if not (vertical_axis_title is None):\n",
    "        #Y-axis title\n",
    "        ax.set_ylabel(vertical_axis_title)\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 70 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    ax.grid(grid)\n",
    "    ax.legend()\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "        \n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"/\"\n",
    "        \n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"time_series_vis\"\n",
    "        \n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 110 dpi\n",
    "            png_resolution_dpi = 110\n",
    "        \n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "        \n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "    \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for splitting the dataframe into train and test subsets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_into_train_and_test (X, y, percent_of_data_used_for_model_training = 75):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # X = subset of predictive variables (dataframe).\n",
    "    # y = subset of response variable (series).\n",
    "    \n",
    "    # percent_of_data_used_for_model_training: float from 0 to 100,\n",
    "    # representing the percent of data used for training the model\n",
    "    \n",
    "    # Convert the percent to fraction.\n",
    "    train_fraction = (percent_of_data_used_for_model_training / 100)\n",
    "    # Calculate the test fraction:\n",
    "    test_fraction = (1 - train_fraction)\n",
    "    \n",
    "    #Funcao para dividir os dados (split em treino e teste)\n",
    "    X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size = test_fraction, random_state = 0)\n",
    "    #test_size: proportion: 0.25 used for test\n",
    "    #test_size = 0.25 = 25% of data used for tests \n",
    "    #-> then, 0.75 = 75% of data used for training the Machine Learning model\n",
    "    \n",
    "    print(f\"X and y successfully splitted into train: X_train, y_train ({percent_of_data_used_for_model_training}\\% of data); and test subsets: X_test, y_test ({100 - percent_of_data_used_for_model_training}\\% of data).\")\n",
    "    # the slash is used to pass a prohibited character % to the string: it is ignorated, so it is printed.\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for retrieving the list of classes used for training the classification models**\n",
    "- The number of classes that are actually in the dataset used for training may be lower than the actual number of possible classes. \n",
    "    - So, also use this function to check if the training set is representing all the classes.\n",
    "- In deep-learning models, we must set the number of neurons of the output layer as the actual number of possible classes. Therefore, it is important that the training is performed with a representative dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_classes_used_for_training (model_type = 'generic_model', y_train = None, sklearn_model_object = None):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    print(\"Attention: this function will return the list of classes (labels) effectively used for training the classifier. Use the list output from this function as input for the function \\'calculate_class_probability\\'.\")\n",
    "    print(\"Use this function to verify if the training was actually performed using all possible classes.\")\n",
    "    print(\"There is no meaning in using this function with a regression model, where the output is a scalar (real number).\")\n",
    "    \n",
    "    # sklearn_model_object: object containing the Scikit-learn model \n",
    "    # that will be analyzed. \n",
    "    # e.g. sklearn_model_object = logistic_reg_model\n",
    "    # This parameter should be provided for sklearn models; for other\n",
    "    # models (XGBoost, Keras, etc) keep it as None.\n",
    "    \n",
    "    # MODEL_TYPE = 'sklearn', model_type = 'xgb', for analyzing an\n",
    "    # object returned from the training of data with a Sckit-learn\n",
    "    # or a XGBoost model;\n",
    "    # or MODEL_TYPE = 'generic_model' ('generic_model' can be used\n",
    "    # for any type of model, including 'sklearn' and 'xgb' themselves\n",
    "    # and deep learning models. That is because it relies on a more\n",
    "    # generic programming that analyzes the training data itself, not\n",
    "    # the model object).\n",
    "    \n",
    "    # For 'sklearn' or 'xgb' models, \n",
    "    # the list of classes is retrieved through the .classes_ attribute.\n",
    "    # for model_type = 'generic_model', the list is retrieved \n",
    "    # from y_train itself, by searching the unique values in this series.\n",
    "    # Notice that the model_type = 'generic_model' can be used for\n",
    "    # any type of classification model, including 'sklearn' and 'xgb',\n",
    "    # since it does not rely on any particular attribute from the\n",
    "    # model object.\n",
    "    \n",
    "    # y_train = subset of response variable (series).\n",
    "    # keep y_train = None if model_type = 'sklearn'\n",
    "    \n",
    "    if ((model_type == 'sklearn') | (model_type == 'xgb')):\n",
    "        # Access the .classes_ attribute from sklearn or XGBoost model\n",
    "        print(f\"Searching .classes_ attribute from {model_type} model object.\\n\")\n",
    "        \n",
    "        # check if the model object is provided\n",
    "        \n",
    "        if not (model_object is None):\n",
    "            # get list of classes from .classes_ attribute:\n",
    "            list_of_classes = model_object.classes_\n",
    "            print(\"List of classes retrieved from .classes_ attribute.\")\n",
    "            print(\"Attention: if you already converted this series to numeric values, the list will contain only the numbers.\")\n",
    "            \n",
    "            # Now use the list attribute to convert the array to a list:\n",
    "            list_of_classes = list(list_of_classes)\n",
    "            number_of_classes = len(list_of_classes)\n",
    "            print(\"\\n\") # line break\n",
    "            print(f\"Number of different classes in the training set = {number_of_classes}\")\n",
    "            print(\"\\n\") # line break\n",
    "            \n",
    "            print(\"List of classes returned as \\'list_of_classes\\'. Check it below:\")\n",
    "            print(list_of_classes)\n",
    "            \n",
    "            return number_of_classes, list_of_classes\n",
    "        \n",
    "        else:\n",
    "            print (\"No valid model object provided, so changing model type to \\'generic_model\\'.\")\n",
    "            model_type = 'generic_model'\n",
    "    \n",
    "    # do not use elif (else if): elif would not run the case where no valid\n",
    "    # model object was provided and the model_type was modified (because the\n",
    "    # first if was run). If the if's are separated, the modification of the\n",
    "    # model type runs the next if:\n",
    "    if (model_type == 'generic_model'):\n",
    "        \n",
    "        print(\"Searching the unique classes (values) of the y_train series.\\n\")\n",
    "        # check if y_train was provided:\n",
    "        if not (y_train is None):\n",
    "            \n",
    "            # Use numpy.unique to collect the unique classes, in the\n",
    "            # order they appear:\n",
    "            # They are the unique values from series xgb_y_train\n",
    "            # https://numpy.org/doc/stable/reference/generated/numpy.unique.html?msclkid=ce35d85ec24511ec82dc9f13c97be8ce\n",
    "            list_of_classes = np.unique(y_train)\n",
    "            print(\"List of classes retrieved from the series of labels used for training, in the order they appear.\")\n",
    "            print(\"Attention: if you already converted this series to numeric values, the list will contain only the numbers.\")\n",
    "            \n",
    "            # Now use the list attribute to convert the array to a list:\n",
    "            list_of_classes = list(list_of_classes)\n",
    "            number_of_classes = len(list_of_classes)\n",
    "            print(\"\\n\") # line break\n",
    "            print(f\"Number of different classes in the training set = {number_of_classes}\")\n",
    "            print(\"\\n\") # line break\n",
    "            \n",
    "            print(\"List of classes returned as \\'list_of_classes\\'. Check it below:\")\n",
    "            print(list_of_classes)\n",
    "\n",
    "            return number_of_classes, list_of_classes\n",
    "        \n",
    "        else:\n",
    "            print (\"Please, input a valid y_train series of labels used for the model to train.\")\n",
    "            return \"error\"\n",
    "    \n",
    "    else:\n",
    "        print(\"Please, enter a valid model type, \\'sklearn\\' for Scikit-learn models; \\'xgb\\' for XGBoost models; or \\'generic_model\\' for any type of models, including Deep Learning ones.\")\n",
    "        return \"error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for Ordinary Least Squares (OLS) Linear Regression**\n",
    "    - This function runs the 'bar_chart' function. Certify that this function was properly loaded.\n",
    "- Fits a linear model with coefficients w = (w1, …, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_linear_reg (X_train, y_train):\n",
    "    \n",
    "    # check Scikit-learn documentation: \n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html?msclkid=636b4046c01b11ec973dee34641f67b0\n",
    "    # This function runs the 'bar_chart' function. Certify that this function was properly loaded.\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    # X_train = subset of predictive variables (dataframe).\n",
    "    # y_train = subset of response variable (series).\n",
    "    \n",
    "    # Create an instance (object) from the class LinearRegression:\n",
    "    # There is no parameter to pass to the constructor of this class:\n",
    "    ols_linear_reg_model = LinearRegression()\n",
    "    \n",
    "    # Fit the model:\n",
    "    ols_linear_reg_model = ols_linear_reg_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Set the list of the predictors:\n",
    "    # Use the list attribute to guarantee that it is a list:\n",
    "    predictive_features = list(X_train.columns)\n",
    "    # Append the 'intercept' to this list:\n",
    "    predictive_features.append('intercept')\n",
    "    \n",
    "    # Get the list of coefficients. Apply the list method to convert the\n",
    "    # array from .coef_ to a list:\n",
    "    reg_coefficients = list(ols_linear_reg_model.coef_)\n",
    "    \n",
    "    # Append the intercept coefficient to this list:\n",
    "    reg_coefficients.append(ols_linear_reg_model.intercept_)\n",
    "    \n",
    "    # Create the regression dictionary:\n",
    "    reg_dict = {'predictive_features': predictive_features,\n",
    "               'regression_coefficients': reg_coefficients}\n",
    "    \n",
    "    # Convert it to a Pandas dataframe:\n",
    "    ols_feature_importance_df = pd.DataFrame(data = reg_dict)\n",
    "    \n",
    "    # Now sort the dataframe in descending order of coefficient, and ascending order of\n",
    "    # feature (when sorting by multiple columns, we pass a list of columns to by and a \n",
    "    # list of booleans to ascending, instead of passing a simple string to by and a boolean\n",
    "    # to ascending. The element on a given index from the list by corresponds to the boolean\n",
    "    # with the same index in ascending):\n",
    "    ols_feature_importance_df = ols_feature_importance_df.sort_values(by = ['regression_coefficients', 'predictive_features'], ascending = [False, True])\n",
    "    \n",
    "    # Now that the dataframe is sorted in descending order, it represents the feature\n",
    "    # importance ranking.\n",
    "    \n",
    "    # Restart the indices:\n",
    "    ols_feature_importance_df = ols_feature_importance_df.reset_index(drop = True)\n",
    "    \n",
    "    print(\"Successfully obtained the linear regression.\")\n",
    "    print(f\"R² = {ols_linear_reg_model.score(X_train, y_train)}\\n\")\n",
    "    print(\"Check the parameters of the estimator:\")\n",
    "    print(ols_linear_reg_model.get_params(deep = True))\n",
    "    print(\"Returning the model object \\'ols_linear_reg_model\\' and the dataframe \\'ols_feature_importance_df\\' with the feature importance ranking (regression coefficients in descending order). Check the ranking below (first 20 features):\\n\")\n",
    "    print(ols_feature_importance_df.head(20))\n",
    "    \n",
    "    print(\"\\n\") #line break\n",
    "    print(\"To predict the model output y_pred for a dataframe X, declare: y_pred = ols_linear_reg_model.predict(X)\\n\")\n",
    "    print(\"For a one-dimensional correlation, the one-dimension array or list with format X_train = [x1, x2, ...] must be converted into a dataframe subset, X_train = [[x1, x2, ...]] before the prediction. To do so, create a list with X_train as its element: X_train = [X_train], or use the numpy.reshape(-1,1):\")\n",
    "    print(\"X_train = np.reshape(np.array(X_train), (-1, 1))\")\n",
    "    # numpy reshape: https://numpy.org/doc/1.21/reference/generated/numpy.reshape.html?msclkid=5de33f8bc02c11ec803224a6bd588362\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Check the feature importance bar chart:\\n\")\n",
    "    \n",
    "    # Obtain the bar chart. Set the local variables for using as bar_chart function parameters:\n",
    "    DATASET = ols_feature_importance_df\n",
    "    CATEGORICAL_VAR_NAME = 'predictive_features'\n",
    "    RESPONSE_VAR_NAME = 'regression_coefficients'\n",
    "    AGGREGATE_FUNCTION = 'sum'\n",
    "    ADD_SUFFIX_TO_AGGREGATED_COL = True\n",
    "    SUFFIX = None\n",
    "    CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = False\n",
    "    ORIENTATION = 'vertical'\n",
    "    X_AXIS_ROTATION = 70\n",
    "    Y_AXIS_ROTATION = 0\n",
    "    GRID = True\n",
    "    HORIZONTAL_AXIS_TITLE = 'Feature'\n",
    "    VERTICAL_AXIS_TITLE = 'Regression_coefficients'\n",
    "    PLOT_TITLE = 'Feature_ranking'\n",
    "    EXPORT_PNG = False\n",
    "    DIRECTORY_TO_SAVE = None\n",
    "    FILE_NAME = None\n",
    "    PNG_RESOLUTION_DPI = 110\n",
    "    \n",
    "    # use underscore to ignore the dataframe, and simply obtain the plot:\n",
    "    _ = bar_chart (df = DATASET, categorical_var_name = CATEGORICAL_VAR_NAME, response_var_name = RESPONSE_VAR_NAME, aggregate_function = AGGREGATE_FUNCTION, add_suffix_to_aggregated_col = ADD_SUFFIX_TO_AGGREGATED_COL, suffix = SUFFIX, calculate_and_plot_cumulative_percent = CALCULATE_AND_PLOT_CUMULATIVE_PERCENT, orientation = ORIENTATION, limit_of_plotted_categories = LIMIT_OF_PLOTTED_CATEGORIES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)\n",
    "\n",
    "    return ols_linear_reg_model, ols_feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for Ridge Linear Regression**\n",
    "    - This function runs the 'bar_chart' function. Certify that this function was properly loaded.\n",
    "- Linear least squares with l2 regularization.\n",
    "- Minimizes the objective function: `||y - Xw||^2_2 + alpha * ||w||^2_2`\n",
    "- This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. \n",
    "- Also known as Ridge Regression or Tikhonov regularization.\n",
    "- This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape (n_samples, n_targets))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_linear_reg (X_train, y_train, alpha_hyperparameter = 1.0, maximum_of_allowed_iterations = 20000):\n",
    "    \n",
    "    # check Scikit-learn documentation: \n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge\n",
    "    # This function runs the 'bar_chart' function. Certify that this function was properly loaded.\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import Ridge\n",
    "    \n",
    "    # X_train = subset of predictive variables (dataframe).\n",
    "    # y_train = subset of response variable (series).\n",
    "    \n",
    "    # hyperparameters: alpha = ALPHA_HYPERPARAMETER and MAXIMUM_OF_ALLOWED_ITERATIONS = max_iter\n",
    "\n",
    "    # MAXIMUM_OF_ALLOWED_ITERATIONS = integer representing the maximum number of iterations\n",
    "    # that the optimization algorithm can perform. Depending on data, convergence may not be\n",
    "    # reached within this limit, so you may need to increase this hyperparameter.\n",
    "\n",
    "    # alpha is the regularization strength and must be a positive float value. \n",
    "    # Regularization improves the conditioning of the problem and reduces the variance \n",
    "    # of the estimates. Larger values specify stronger regularization.\n",
    "    \n",
    "    # alpha = 0 is equivalent to an ordinary least square, solved by the LinearRegression \n",
    "    # object. For numerical reasons, using alpha = 0 is not advised. \n",
    "    # Given this, you should use the ols_linear_reg function instead.\n",
    "    \n",
    "    # Create an instance (object) from the class Ridge:\n",
    "    # Pass the appropriate parameters to the class constructor:\n",
    "    ridge_linear_reg_model = Ridge(alpha = alpha_hyperparameter, max_iter = maximum_of_allowed_iterations)\n",
    "    \n",
    "    # Fit the model:\n",
    "    ridge_linear_reg_model = ridge_linear_reg_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Set the list of the predictors:\n",
    "    # Use the list attribute to guarantee that it is a list:\n",
    "    predictive_features = list(X_train.columns)\n",
    "    # Append the 'intercept' to this list:\n",
    "    predictive_features.append('intercept')\n",
    "    \n",
    "    # Get the list of coefficients. Apply the list method to convert the\n",
    "    # array from .coef_ to a list:\n",
    "    reg_coefficients = list(ridge_linear_reg_model.coef_)\n",
    "    \n",
    "    # Append the intercept coefficient to this list:\n",
    "    reg_coefficients.append(ridge_linear_reg_model.intercept_)\n",
    "    \n",
    "    # Create the regression dictionary:\n",
    "    reg_dict = {'predictive_features': predictive_features,\n",
    "               'regression_coefficients': reg_coefficients}\n",
    "    \n",
    "    # Convert it to a Pandas dataframe:\n",
    "    ridge_feature_importance_df = pd.DataFrame(data = reg_dict)\n",
    "    \n",
    "    # Now sort the dataframe in descending order of coefficient, and ascending order of\n",
    "    # feature (when sorting by multiple columns, we pass a list of columns to by and a \n",
    "    # list of booleans to ascending, instead of passing a simple string to by and a boolean\n",
    "    # to ascending. The element on a given index from the list by corresponds to the boolean\n",
    "    # with the same index in ascending):\n",
    "    ridge_feature_importance_df = ridge_feature_importance_df.sort_values(by = ['regression_coefficients', 'predictive_features'], ascending = [False, True])\n",
    "    \n",
    "    # Now that the dataframe is sorted in descending order, it represents the feature\n",
    "    # importance ranking.\n",
    "    \n",
    "    # Restart the indices:\n",
    "    ridge_feature_importance_df = ridge_feature_importance_df.reset_index(drop = True)\n",
    "    \n",
    "    print(\"Successfully obtained the linear regression.\")\n",
    "    print(f\"R² = {ridge_linear_reg_model.score(X_train, y_train)}\\n\")\n",
    "    print(f\"Total of iterations to fit the model = {ridge_linear_reg_model.n_iter_}\")\n",
    "    \n",
    "    if (ridge_linear_reg_model.n_iter_ == maximum_of_allowed_iterations):\n",
    "        print(\"Warning! Total of iterations equals to the maximum allowed. It indicates that the convergence was not reached yet. Try to increase the maximum number of allowed iterations.\")\n",
    "    \n",
    "    print(\"Check the parameters of the estimator:\")\n",
    "    print(ridge_linear_reg_model.get_params(deep = True))\n",
    "    print(\"Returning the model object \\'ridge_linear_reg_model\\' and the dataframe \\'ridge_feature_importance_df\\' with the feature importance ranking (regression coefficients in descending order). Check the ranking below (first 20 features):\\n\")\n",
    "    print(ridge_feature_importance_df.head(20))\n",
    "    \n",
    "    print(\"\\n\") #line break\n",
    "    print(\"To predict the model output y_pred for a dataframe X, declare: y_pred = ridge_linear_reg_model.predict(X)\\n\")\n",
    "    print(\"For a one-dimensional correlation, the one-dimension array or list with format X_train = [x1, x2, ...] must be converted into a dataframe subset, X_train = [[x1, x2, ...]] before the prediction. To do so, create a list with X_train as its element: X_train = [X_train], or use the numpy.reshape(-1,1):\")\n",
    "    print(\"X_train = np.reshape(np.array(X_train), (-1, 1))\")\n",
    "    # numpy reshape: https://numpy.org/doc/1.21/reference/generated/numpy.reshape.html?msclkid=5de33f8bc02c11ec803224a6bd588362\n",
    "      \n",
    "    print(\"\\n\")\n",
    "    print(\"Check the feature importance bar chart:\\n\")\n",
    "    \n",
    "    # Obtain the bar chart. Set the local variables for using as bar_chart function parameters:\n",
    "    DATASET = ridge_feature_importance_df\n",
    "    CATEGORICAL_VAR_NAME = 'predictive_features'\n",
    "    RESPONSE_VAR_NAME = 'regression_coefficients'\n",
    "    AGGREGATE_FUNCTION = 'sum'\n",
    "    ADD_SUFFIX_TO_AGGREGATED_COL = True\n",
    "    SUFFIX = None\n",
    "    CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = False\n",
    "    ORIENTATION = 'vertical'\n",
    "    X_AXIS_ROTATION = 70\n",
    "    Y_AXIS_ROTATION = 0\n",
    "    GRID = True\n",
    "    HORIZONTAL_AXIS_TITLE = 'Feature'\n",
    "    VERTICAL_AXIS_TITLE = 'Regression_coefficients'\n",
    "    PLOT_TITLE = 'Feature_ranking'\n",
    "    EXPORT_PNG = False\n",
    "    DIRECTORY_TO_SAVE = None\n",
    "    FILE_NAME = None\n",
    "    PNG_RESOLUTION_DPI = 110\n",
    "    \n",
    "    # use underscore to ignore the dataframe, and simply obtain the plot:\n",
    "    _ = bar_chart (df = DATASET, categorical_var_name = CATEGORICAL_VAR_NAME, response_var_name = RESPONSE_VAR_NAME, aggregate_function = AGGREGATE_FUNCTION, add_suffix_to_aggregated_col = ADD_SUFFIX_TO_AGGREGATED_COL, suffix = SUFFIX, calculate_and_plot_cumulative_percent = CALCULATE_AND_PLOT_CUMULATIVE_PERCENT, orientation = ORIENTATION, limit_of_plotted_categories = LIMIT_OF_PLOTTED_CATEGORIES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)\n",
    "\n",
    "    return ridge_linear_reg_model, ridge_feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for Lasso Linear Regression**\n",
    "    - This function runs the 'bar_chart' function. Certify that this function was properly loaded.\n",
    "- Linear Model trained with L1 prior as regularizer (aka the Lasso).\n",
    "- The optimization objective for Lasso is: `(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1`\n",
    "- Technically the Lasso model is optimizing the same objective function as the Elastic Net with l1_ratio=1.0 (no L2 penalty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_linear_reg (X_train, y_train, alpha_hyperparameter = 1.0, maximum_of_allowed_iterations = 20000):\n",
    "    \n",
    "    # check Scikit-learn documentation: \n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso\n",
    "    # This function runs the 'bar_chart' function. Certify that this function was properly loaded.\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import Lasso\n",
    "    \n",
    "    # X_train = subset of predictive variables (dataframe).\n",
    "    # y_train = subset of response variable (series).\n",
    "    \n",
    "    # hyperparameters: alpha = ALPHA_HYPERPARAMETER and MAXIMUM_OF_ALLOWED_ITERATIONS = max_iter\n",
    "\n",
    "    # MAXIMUM_OF_ALLOWED_ITERATIONS = integer representing the maximum number of iterations\n",
    "    # that the optimization algorithm can perform. Depending on data, convergence may not be\n",
    "    # reached within this limit, so you may need to increase this hyperparameter.\n",
    "\n",
    "    # alpha is the regularization strength and must be a positive float value. \n",
    "    # Regularization improves the conditioning of the problem and reduces the variance \n",
    "    # of the estimates. Larger values specify stronger regularization.\n",
    "    \n",
    "    # alpha = 0 is equivalent to an ordinary least square, solved by the LinearRegression \n",
    "    # object. For numerical reasons, using alpha = 0 is not advised. \n",
    "    # Given this, you should use the ols_linear_reg function instead.\n",
    "    \n",
    "    # Create an instance (object) from the class Lasso:\n",
    "    # Pass the appropriate parameters to the class constructor:\n",
    "    lasso_linear_reg_model = Lasso(alpha = alpha_hyperparameter, max_iter = maximum_of_allowed_iterations, verbose = True)\n",
    "    # verbose = True to debug mode (show training status during training)\n",
    "    \n",
    "    # Fit the model:\n",
    "    lasso_linear_reg_model = lasso_linear_reg_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Set the list of the predictors:\n",
    "    # Use the list attribute to guarantee that it is a list:\n",
    "    predictive_features = list(X_train.columns)\n",
    "    # Append the 'intercept' to this list:\n",
    "    predictive_features.append('intercept')\n",
    "    \n",
    "    # Get the list of coefficients. Apply the list method to convert the\n",
    "    # array from .coef_ to a list:\n",
    "    reg_coefficients = list(lasso_linear_reg_model.coef_)\n",
    "    \n",
    "    # Append the intercept coefficient to this list:\n",
    "    reg_coefficients.append(lasso_linear_reg_model.intercept_)\n",
    "    \n",
    "    # Create the regression dictionary:\n",
    "    reg_dict = {'predictive_features': predictive_features,\n",
    "               'regression_coefficients': reg_coefficients}\n",
    "    \n",
    "    # Convert it to a Pandas dataframe:\n",
    "    lasso_feature_importance_df = pd.DataFrame(data = reg_dict)\n",
    "    \n",
    "    # Now sort the dataframe in descending order of coefficient, and ascending order of\n",
    "    # feature (when sorting by multiple columns, we pass a list of columns to by and a \n",
    "    # list of booleans to ascending, instead of passing a simple string to by and a boolean\n",
    "    # to ascending. The element on a given index from the list by corresponds to the boolean\n",
    "    # with the same index in ascending):\n",
    "    lasso_feature_importance_df = lasso_feature_importance_df.sort_values(by = ['regression_coefficients', 'predictive_features'], ascending = [False, True])\n",
    "    \n",
    "    # Now that the dataframe is sorted in descending order, it represents the feature\n",
    "    # importance ranking.\n",
    "    \n",
    "    # Restart the indices:\n",
    "    lasso_feature_importance_df = lasso_feature_importance_df.reset_index(drop = True)\n",
    "    \n",
    "    print(\"Successfully obtained the linear regression.\")\n",
    "    print(f\"R² = {lasso_linear_reg_model.score(X_train, y_train)}\\n\")\n",
    "    print(f\"Total of iterations to fit the model = {lasso_linear_reg_model.n_iter_}\")\n",
    "    \n",
    "    if (lasso_linear_reg_model.n_iter_ == maximum_of_allowed_iterations):\n",
    "        print(\"Warning! Total of iterations equals to the maximum allowed. It indicates that the convergence was not reached yet. Try to increase the maximum number of allowed iterations.\")\n",
    "    \n",
    "    print(\"Check the parameters of the estimator:\")\n",
    "    print(lasso_linear_reg_model.get_params(deep = True))\n",
    "    print(\"Returning the model object \\'lasso_linear_reg_model\\' and the dataframe \\'lasso_feature_importance_df\\' with the feature importance ranking (regression coefficients in descending order). Check the ranking below (first 20 features):\\n\")\n",
    "    print(lasso_feature_importance_df.head(20))\n",
    "    \n",
    "    print(\"\\n\") #line break\n",
    "    print(\"To predict the model output y_pred for a dataframe X, declare: y_pred = lasso_linear_reg_model.predict(X)\\n\")\n",
    "    print(\"For a one-dimensional correlation, the one-dimension array or list with format X_train = [x1, x2, ...] must be converted into a dataframe subset, X_train = [[x1, x2, ...]] before the prediction. To do so, create a list with X_train as its element: X_train = [X_train], or use the numpy.reshape(-1,1):\")\n",
    "    print(\"X_train = np.reshape(np.array(X_train), (-1, 1))\")\n",
    "    # numpy reshape: https://numpy.org/doc/1.21/reference/generated/numpy.reshape.html?msclkid=5de33f8bc02c11ec803224a6bd588362\n",
    "      \n",
    "    print(\"\\n\")\n",
    "    print(\"Check the feature importance bar chart:\\n\")\n",
    "    \n",
    "    # Obtain the bar chart. Set the local variables for using as bar_chart function parameters:\n",
    "    DATASET = lasso_feature_importance_df\n",
    "    CATEGORICAL_VAR_NAME = 'predictive_features'\n",
    "    RESPONSE_VAR_NAME = 'regression_coefficients'\n",
    "    AGGREGATE_FUNCTION = 'sum'\n",
    "    ADD_SUFFIX_TO_AGGREGATED_COL = True\n",
    "    SUFFIX = None\n",
    "    CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = False\n",
    "    ORIENTATION = 'vertical'\n",
    "    X_AXIS_ROTATION = 70\n",
    "    Y_AXIS_ROTATION = 0\n",
    "    GRID = True\n",
    "    HORIZONTAL_AXIS_TITLE = 'Feature'\n",
    "    VERTICAL_AXIS_TITLE = 'Regression_coefficients'\n",
    "    PLOT_TITLE = 'Feature_ranking'\n",
    "    EXPORT_PNG = False\n",
    "    DIRECTORY_TO_SAVE = None\n",
    "    FILE_NAME = None\n",
    "    PNG_RESOLUTION_DPI = 110\n",
    "    \n",
    "    # use underscore to ignore the dataframe, and simply obtain the plot:\n",
    "    _ = bar_chart (df = DATASET, categorical_var_name = CATEGORICAL_VAR_NAME, response_var_name = RESPONSE_VAR_NAME, aggregate_function = AGGREGATE_FUNCTION, add_suffix_to_aggregated_col = ADD_SUFFIX_TO_AGGREGATED_COL, suffix = SUFFIX, calculate_and_plot_cumulative_percent = CALCULATE_AND_PLOT_CUMULATIVE_PERCENT, orientation = ORIENTATION, limit_of_plotted_categories = LIMIT_OF_PLOTTED_CATEGORIES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)\n",
    "\n",
    "    return lasso_linear_reg_model, lasso_feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for Elastic Net Linear Regression**\n",
    "    - This function runs the 'bar_chart' function. Certify that this function was properly loaded.\n",
    "- Linear Model trained with combined L1 and L2 priors as regularizer.\n",
    "- Minimizes the objective function: `1 / (2 * n_samples) * ||y - Xw||^2_2 + alpha * l1_ratio * ||w||_1 + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2`\n",
    "- If you are interested in controlling the L1 and L2 penalty separately, keep in mind that this is equivalent to: `a * ||w||_1 + 0.5 * b * ||w||_2^2`\n",
    "- where: `alpha = a + b and l1_ratio = a / (a + b)`\n",
    "- The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable, unless you supply your own sequence of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_net_linear_reg (X_train, y_train, alpha_hyperparameter = 1.0, l1_ratio_hyperparameter = 0.5, maximum_of_allowed_iterations = 20000):\n",
    "    \n",
    "    # check Scikit-learn documentation: \n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet\n",
    "    # This function runs the 'bar_chart' function. Certify that this function was properly loaded.\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    \n",
    "    # X_train = subset of predictive variables (dataframe).\n",
    "    # y_train = subset of response variable (series).\n",
    "    \n",
    "    # hyperparameters: alpha = alpha_hyperparameter; maximum_of_allowed_iterations = max_iter;\n",
    "    # and l1_ratio_hyperparameter = l1_ratio\n",
    "\n",
    "    # MAXIMUM_OF_ALLOWED_ITERATIONS = integer representing the maximum number of iterations\n",
    "    # that the optimization algorithm can perform. Depending on data, convergence may not be\n",
    "    # reached within this limit, so you may need to increase this hyperparameter.\n",
    "\n",
    "    # alpha is the regularization strength and must be a positive float value. \n",
    "    # Regularization improves the conditioning of the problem and reduces the variance \n",
    "    # of the estimates. Larger values specify stronger regularization.\n",
    "    \n",
    "    # l1_ratio is The ElasticNet mixing parameter (float), with 0 <= l1_ratio <= 1. \n",
    "    # For l1_ratio = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is an L1 penalty. \n",
    "    # For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n",
    "    \n",
    "    # alpha = 0 and l1_ratio = 0 is equivalent to an ordinary least square, solved by \n",
    "    # the LinearRegression object. For numerical reasons, using alpha = 0 and \n",
    "    # l1_ratio = 0 is not advised. Given this, you should use the ols_linear_reg function instead.\n",
    "    \n",
    "    # Create an instance (object) from the class ElasticNet:\n",
    "    # Pass the appropriate parameters to the class constructor:\n",
    "    elastic_net_linear_reg_model = ElasticNet(alpha = alpha_hyperparameter, l1_ratio = l1_ratio_hyperparameter, max_iter = maximum_of_allowed_iterations, verbose = True)\n",
    "    # verbose = True to debug mode (show training status during training)\n",
    "    \n",
    "    # Fit the model:\n",
    "    elastic_net_linear_reg_model = elastic_net_linear_reg_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Set the list of the predictors:\n",
    "    # Use the list attribute to guarantee that it is a list:\n",
    "    predictive_features = list(X_train.columns)\n",
    "    # Append the 'intercept' to this list:\n",
    "    predictive_features.append('intercept')\n",
    "    \n",
    "    # Get the list of coefficients. Apply the list method to convert the\n",
    "    # array from .coef_ to a list:\n",
    "    reg_coefficients = list(elastic_net_linear_reg_model.coef_)\n",
    "    \n",
    "    # Append the intercept coefficient to this list:\n",
    "    reg_coefficients.append(elastic_net_linear_reg_model.intercept_)\n",
    "    \n",
    "    # Create the regression dictionary:\n",
    "    reg_dict = {'predictive_features': predictive_features,\n",
    "               'regression_coefficients': reg_coefficients}\n",
    "    \n",
    "    # Convert it to a Pandas dataframe:\n",
    "    elastic_net_feature_importance_df = pd.DataFrame(data = reg_dict)\n",
    "    \n",
    "    # Now sort the dataframe in descending order of coefficient, and ascending order of\n",
    "    # feature (when sorting by multiple columns, we pass a list of columns to by and a \n",
    "    # list of booleans to ascending, instead of passing a simple string to by and a boolean\n",
    "    # to ascending. The element on a given index from the list by corresponds to the boolean\n",
    "    # with the same index in ascending):\n",
    "    elastic_net_feature_importance_df = elastic_net_feature_importance_df.sort_values(by = ['regression_coefficients', 'predictive_features'], ascending = [False, True])\n",
    "    \n",
    "    # Now that the dataframe is sorted in descending order, it represents the feature\n",
    "    # importance ranking.\n",
    "    \n",
    "    # Restart the indices:\n",
    "    elastic_net_feature_importance_df = elastic_net_feature_importance_df.reset_index(drop = True)\n",
    "    \n",
    "    print(\"Successfully obtained the linear regression.\")\n",
    "    print(f\"R² = {elastic_net_linear_reg_model.score(X_train, y_train)}\\n\")\n",
    "    print(f\"Total of iterations to fit the model = {elastic_net_linear_reg_model.n_iter_}\")\n",
    "    \n",
    "    if (elastic_net_linear_reg_model.n_iter_ == maximum_of_allowed_iterations):\n",
    "        print(\"Warning! Total of iterations equals to the maximum allowed. It indicates that the convergence was not reached yet. Try to increase the maximum number of allowed iterations.\")\n",
    "    \n",
    "    print(\"Check the parameters of the estimator:\")\n",
    "    print(elastic_net_linear_reg_model.get_params(deep = True))\n",
    "    print(\"Returning the model object \\'elastic_net_linear_reg_model\\' and the dataframe \\'elastic_net_feature_importance_df\\' with the feature importance ranking (regression coefficients in descending order). Check the ranking below (first 20 features):\\n\")\n",
    "    print(elastic_net_feature_importance_df.head(20))\n",
    "    \n",
    "    print(\"\\n\") #line break\n",
    "    print(\"To predict the model output y_pred for a dataframe X, declare: y_pred = elastic_net_linear_reg_model.predict(X)\\n\")\n",
    "    print(\"For a one-dimensional correlation, the one-dimension array or list with format X_train = [x1, x2, ...] must be converted into a dataframe subset, X_train = [[x1, x2, ...]] before the prediction. To do so, create a list with X_train as its element: X_train = [X_train], or use the numpy.reshape(-1,1):\")\n",
    "    print(\"X_train = np.reshape(np.array(X_train), (-1, 1))\")\n",
    "    # numpy reshape: https://numpy.org/doc/1.21/reference/generated/numpy.reshape.html?msclkid=5de33f8bc02c11ec803224a6bd588362\n",
    "     \n",
    "    print(\"\\n\")\n",
    "    print(\"Check the feature importance bar chart:\\n\")\n",
    "    \n",
    "    # Obtain the bar chart. Set the local variables for using as bar_chart function parameters:\n",
    "    DATASET = elastic_net_feature_importance_df\n",
    "    CATEGORICAL_VAR_NAME = 'predictive_features'\n",
    "    RESPONSE_VAR_NAME = 'regression_coefficients'\n",
    "    AGGREGATE_FUNCTION = 'sum'\n",
    "    ADD_SUFFIX_TO_AGGREGATED_COL = True\n",
    "    SUFFIX = None\n",
    "    CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = False\n",
    "    ORIENTATION = 'vertical'\n",
    "    X_AXIS_ROTATION = 70\n",
    "    Y_AXIS_ROTATION = 0\n",
    "    GRID = True\n",
    "    HORIZONTAL_AXIS_TITLE = 'Feature'\n",
    "    VERTICAL_AXIS_TITLE = 'Regression_coefficients'\n",
    "    PLOT_TITLE = 'Feature_ranking'\n",
    "    EXPORT_PNG = False\n",
    "    DIRECTORY_TO_SAVE = None\n",
    "    FILE_NAME = None\n",
    "    PNG_RESOLUTION_DPI = 110\n",
    "    \n",
    "    # use underscore to ignore the dataframe, and simply obtain the plot:\n",
    "    _ = bar_chart (df = DATASET, categorical_var_name = CATEGORICAL_VAR_NAME, response_var_name = RESPONSE_VAR_NAME, aggregate_function = AGGREGATE_FUNCTION, add_suffix_to_aggregated_col = ADD_SUFFIX_TO_AGGREGATED_COL, suffix = SUFFIX, calculate_and_plot_cumulative_percent = CALCULATE_AND_PLOT_CUMULATIVE_PERCENT, orientation = ORIENTATION, limit_of_plotted_categories = LIMIT_OF_PLOTTED_CATEGORIES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)\n",
    "\n",
    "    return elastic_net_linear_reg_model, elastic_net_feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for Logistic Regression (binary classification)**\n",
    "    - This function runs the 'bar_chart' function. Certify that this function was properly loaded.\n",
    "- This linear Model may be trained with combined L1 and L2 priors as regularizer.\n",
    "- The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable, unless you supply your own sequence of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_reg (X_train, y_train, l2_penalty = 0.0, l1_ratio_hyperparameter = 0.0, maximum_of_allowed_iterations = 20000):\n",
    "    \n",
    "    # check Scikit-learn documentation: \n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?msclkid=6bede8a8c1a011ecad332ec5eb711355\n",
    "    # This function runs the 'bar_chart' function. Certify that this function was properly loaded.\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "    # X_train = subset of predictive variables (dataframe).\n",
    "    # y_train = subset of response variable (series).\n",
    "    \n",
    "    # hyperparameters: l2_penalty = L2_PENALTY; maximum_of_allowed_iterations = max_iter;\n",
    "    # and l1_ratio_hyperparameter = l1_ratio\n",
    "\n",
    "    # MAXIMUM_OF_ALLOWED_ITERATIONS = integer representing the maximum number of iterations\n",
    "    # that the optimization algorithm can perform. Depending on data, convergence may not be\n",
    "    # reached within this limit, so you may need to increase this hyperparameter.\n",
    "\n",
    "    # l2_penalty is the regularization strength and must be a positive float value. \n",
    "    # Regularization improves the conditioning of the problem and reduces the variance \n",
    "    # of the estimates. Larger values specify stronger regularization.\n",
    "    \n",
    "    # l1_ratio is The ElasticNet mixing parameter (float), with 0 <= l1_ratio <= 1. \n",
    "    # For l1_ratio = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is an L1 penalty. \n",
    "    # For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n",
    "    \n",
    "    # if you do not want to add l2 penalty, keep l1_ratio = 0\n",
    "    # if you do not want to add l1 penalty, keep l1_penalty = 0\n",
    "    \n",
    "    print(\"Attention: logistic regression is a binary classifier. It results in probabilities, instead of on scalar (real numbers) like other regression algorithms from linear models class.\")\n",
    "    \n",
    "    # Set penalty \n",
    "    \n",
    "    if (l2_penalty != 0):\n",
    "        \n",
    "        # there is a l2_penalty\n",
    "        \n",
    "        C_HYPERPARAMETER = 1/(l2_penalty)\n",
    "        # C is a float, default = 1.0\n",
    "        # It is the inverse of regularization strength; \n",
    "        # must be a positive float. Like in support vector \n",
    "        # machines, smaller values specify stronger \n",
    "        # regularization.\n",
    "        \n",
    "        if (l1_ratio_hyperparameter != 0):\n",
    "            # l1 and l2 penalties are present\n",
    "            PENALTY = 'elasticnet'\n",
    "        \n",
    "        else:\n",
    "            # only l2 penalty\n",
    "            PENALTY = 'l2'\n",
    "    \n",
    "    else: \n",
    "        # There is no penalty\n",
    "        C_HYPERPARAMETER = 1.0 # default\n",
    "        PENALTY = 'none'\n",
    "    \n",
    "    # Create an instance (object) from the class ElasticNet:\n",
    "    # Pass the appropriate parameters to the class constructor:\n",
    "    \n",
    "    # 'saga' supports all penalties, and is faster for larger datasets\n",
    "    logistic_reg_model = LogisticRegression(penalty = PENALTY, C = C_HYPERPARAMETER, solver = 'saga', l1_ratio = l1_ratio_hyperparameter, max_iter = maximum_of_allowed_iterations)\n",
    "    # verbose = 1 to debug mode is not available for 'saga' solver\n",
    "    \n",
    "    # Fit the model:\n",
    "    logistic_reg_model = logistic_reg_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Set the list of the predictors:\n",
    "    # Use the list attribute to guarantee that it is a list:\n",
    "    predictive_features = list(X_train.columns)\n",
    "    # Append the 'intercept' to this list:\n",
    "    predictive_features.append('intercept')\n",
    "    \n",
    "    # Get the list of coefficients. Apply the list method to convert the\n",
    "    # array from .coef_ to a list:\n",
    "    reg_coefficients = list(logistic_reg_model.coef_)\n",
    "    \n",
    "    # Append the intercept coefficient to this list:\n",
    "    reg_coefficients.append(logistic_reg_model.intercept_)\n",
    "    \n",
    "    # Create the regression dictionary:\n",
    "    reg_dict = {'predictive_features': predictive_features,\n",
    "               'regression_coefficients': reg_coefficients}\n",
    "    \n",
    "    # Convert it to a Pandas dataframe:\n",
    "    logistic_reg_feature_importance_df = pd.DataFrame(data = reg_dict)\n",
    "    \n",
    "    # Now sort the dataframe in descending order of coefficient, and ascending order of\n",
    "    # feature (when sorting by multiple columns, we pass a list of columns to by and a \n",
    "    # list of booleans to ascending, instead of passing a simple string to by and a boolean\n",
    "    # to ascending. The element on a given index from the list by corresponds to the boolean\n",
    "    # with the same index in ascending):\n",
    "    logistic_reg_feature_importance_df = logistic_reg_feature_importance_df.sort_values(by = ['regression_coefficients', 'predictive_features'], ascending = [False, True])\n",
    "    \n",
    "    # Now that the dataframe is sorted in descending order, it represents the feature\n",
    "    # importance ranking.\n",
    "    \n",
    "    # Restart the indices:\n",
    "    logistic_reg_feature_importance_df = logistic_reg_feature_importance_df.reset_index(drop = True)\n",
    "    \n",
    "    print(\"Successfully obtained the logistic regression.\")\n",
    "    print(f\"Mean accuracy = {logistic_reg_model.score(X_train, y_train)}\\n\")\n",
    "    print(f\"Total of iterations to fit the model = {logistic_reg_model.n_iter_}\")\n",
    "    \n",
    "    if (logistic_reg_model.n_iter_ == maximum_of_allowed_iterations):\n",
    "        print(\"Warning! Total of iterations equals to the maximum allowed. It indicates that the convergence was not reached yet. Try to increase the maximum number of allowed iterations.\")\n",
    "    \n",
    "    print(\"Check the parameters of the estimator:\")\n",
    "    print(logistic_reg_model.get_params(deep = True))\n",
    "    print(\"Returning the model object \\'logistic_reg_model\\' and the dataframe \\'logistic_reg_feature_importance_df\\' with the feature importance ranking (regression coefficients in descending order). Check the ranking below (first 20 features):\\n\")\n",
    "    print(logistic_reg_feature_importance_df.head(20))\n",
    "    \n",
    "    print(\"\\n\") #line break\n",
    "    print(\"To predict the model output y_pred for a dataframe X, declare: y_pred = logistic_reg_model.predict(X)\\n\")\n",
    "    print(\"For a one-dimensional correlation, the one-dimension array or list with format X_train = [x1, x2, ...] must be converted into a dataframe subset, X_train = [[x1, x2, ...]] before the prediction. To do so, create a list with X_train as its element: X_train = [X_train], or use the numpy.reshape(-1,1):\")\n",
    "    print(\"X_train = np.reshape(np.array(X_train), (-1, 1))\")\n",
    "    # numpy reshape: https://numpy.org/doc/1.21/reference/generated/numpy.reshape.html?msclkid=5de33f8bc02c11ec803224a6bd588362\n",
    "    \n",
    "    print(\"To predict the probabilities associated to each class for the set X_train, use the .predict_proba(X) method:\")\n",
    "    print(\"y_pred_probabilities = logistic_reg_model.predict_proba(X_train)\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Check the feature importance bar chart:\\n\")\n",
    "    \n",
    "    # Obtain the bar chart. Set the local variables for using as bar_chart function parameters:\n",
    "    DATASET = logistic_reg_feature_importance_df\n",
    "    CATEGORICAL_VAR_NAME = 'predictive_features'\n",
    "    RESPONSE_VAR_NAME = 'regression_coefficients'\n",
    "    AGGREGATE_FUNCTION = 'sum'\n",
    "    ADD_SUFFIX_TO_AGGREGATED_COL = True\n",
    "    SUFFIX = None\n",
    "    CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = False\n",
    "    ORIENTATION = 'vertical'\n",
    "    X_AXIS_ROTATION = 70\n",
    "    Y_AXIS_ROTATION = 0\n",
    "    GRID = True\n",
    "    HORIZONTAL_AXIS_TITLE = 'Feature'\n",
    "    VERTICAL_AXIS_TITLE = 'Regression_coefficients'\n",
    "    PLOT_TITLE = 'Feature_ranking'\n",
    "    EXPORT_PNG = False\n",
    "    DIRECTORY_TO_SAVE = None\n",
    "    FILE_NAME = None\n",
    "    PNG_RESOLUTION_DPI = 110\n",
    "    \n",
    "    # use underscore to ignore the dataframe, and simply obtain the plot:\n",
    "    _ = bar_chart (df = DATASET, categorical_var_name = CATEGORICAL_VAR_NAME, response_var_name = RESPONSE_VAR_NAME, aggregate_function = AGGREGATE_FUNCTION, add_suffix_to_aggregated_col = ADD_SUFFIX_TO_AGGREGATED_COL, suffix = SUFFIX, calculate_and_plot_cumulative_percent = CALCULATE_AND_PLOT_CUMULATIVE_PERCENT, orientation = ORIENTATION, limit_of_plotted_categories = LIMIT_OF_PLOTTED_CATEGORIES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)\n",
    "\n",
    "    return logistic_reg_model, logistic_reg_feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for getting a general feature ranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_feature_ranking (dictionary_of_feature_rankings_dataframes, eliminate_non_correspondence = False, limit_of_ranked_features = None):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # dictionary_of_feature_rankings_dataframes\n",
    "    # The key of this dictionary must be the model name or the name of the ranking.\n",
    "    # This key will be used to identify the column on the new dataframe (it will be\n",
    "    # used as suffix). The correspondent value must be the feature importance ranking\n",
    "    # dataframe, with configuration similar to reg_dict: it must have a column\n",
    "    # named, 'predictive_features', which will be used as key for merging.\n",
    "    \n",
    "    # For instance, for a dictionary = {'ols_linear_regression': ols_feature_df,\n",
    "    # 'ridge_linear_regression': ridge_feature_df}, the columns 'regression_coefficients'\n",
    "    # will be identified as: 'regression_coefficients_ols_linear_regression' and\n",
    "    # 'regression_coefficients_ridge_linear_regression'. Notice that the underscore (\"_\")\n",
    "    # is used as suffix separator.\n",
    "    \n",
    "    # eliminate_non_correspondence = False. Since the dataframes will be merged using an\n",
    "    # \"outer\" join, all entries from all dataframes will be added, possibly resulting in\n",
    "    # missing values (pandas \"outer\" is a full outer join). \n",
    "    # Then, set eliminate_non_correspondence = True to eliminate all missing values\n",
    "    # (rows with entries without correspondence).\n",
    "    \n",
    "    # limit_of_ranked_features = None. Alternatively, set as an integer to limit the number\n",
    "    # of ranked features. e.g. limit_of_ranked_features = 20 will return a dataset with only\n",
    "    # 20 features. Notice that the features are sorted in accordance to their order in the\n",
    "    # input dictionary. Then, the most important ranking will be the one from the first dataframe.\n",
    "    \n",
    "    \n",
    "    # Get the list of keys from the dictionary. This will generate an array dict_keys([])\n",
    "    # that cannot be referenced throgh indexing. So, use the list attribute to convert it to\n",
    "    # an indexable list:\n",
    "    list_of_keys = list(dictionary_of_feature_rankings_dataframes.keys())\n",
    "    \n",
    "    # Get the total of dataframes that will be merged. It is the length of the list_of_keys\n",
    "    total_dfs = len(list_of_keys)\n",
    "    \n",
    "    # Get the first key. It is the first element of the list.\n",
    "    suffix_left = list_of_keys[0]\n",
    "    \n",
    "    # Get the correspondent dataframe by accessing the dictionary value with this key:\n",
    "    df_left = dictionary_of_feature_rankings_dataframes[suffix_left]\n",
    "    \n",
    "    # Now let's convert suffix_left to an appropriate suffix for merging.\n",
    "    # Use the str attribute to guarantee that the key was properly read as a string instead\n",
    "    # of other type. Then, there will be no concatenation errors:\n",
    "    suffix_left = str(suffix_left)\n",
    "    # Concatenate an underscore on the left of suffix_left to obtain the suffix for merging.\n",
    "    suffix_left = \"_\" + suffix_left\n",
    "    \n",
    "    # Now we have the first dataframe and the first suffix. We must loop through the rest of\n",
    "    # list_of_keys list starting from index i = 1 (second dataframe) to merge all with this\n",
    "    # first one:\n",
    "    \n",
    "    for i in range (1, total_dfs):\n",
    "        \n",
    "        # goes from i = 1 to i = (total_dfs - 1), index of the last dictionary key.\n",
    "        # Get the new dataframe to merge. It is the i-th key from list of keys:\n",
    "        suffix_right = list_of_keys[i]\n",
    "        \n",
    "        # Access this dataframe on the dictionary:\n",
    "        df_right = dictionary_of_feature_rankings_dataframes[suffix_right]\n",
    "        \n",
    "        # Now let's convert suffix_right to an appropriate suffix for merging.\n",
    "        # Use the str attribute to guarantee that the key was properly read as a string instead\n",
    "        # of other type. Then, there will be no concatenation errors:\n",
    "        suffix_right = str(suffix_right)\n",
    "        # Concatenate an underscore on the left of suffix_left to obtain the suffix for merging.\n",
    "        suffix_right = \"_\" + suffix_right\n",
    "        \n",
    "        # Create the tuple of suffixes:\n",
    "        SUFFIXES = (suffix_left, suffix_right)\n",
    "        \n",
    "        # Apply the merge method to update the left_df my merging it to right_df.\n",
    "        # The merged left_df will continue to be update on the following iterations:\n",
    "        # notice that we could specify the arguments left_on = 'predictive_features',\n",
    "        # and right_on = 'predictive_features'. Since the columns have the same name,\n",
    "        # we omit this parameter and simply specify on = 'predictive_features'\n",
    "        # We also modify the standard 'inner' to 'outer' join, so that no entries are\n",
    "        # removed at first (pandas \"outer\" is a full outer join: it keeps all rows from\n",
    "        # both dataframes)\n",
    "        df_left = df_left.merge(df_right, on = 'predictive_features', how = \"outer\", suffixes = SUFFIXES)\n",
    "        \n",
    "        # Update the left_suffix as _merge_i. Since i is an integer, we again use the\n",
    "        # str attribute to convert it to a string. So the string concatenation is allowed:\n",
    "        suffix_left = \"_merge_\" + str(i) \n",
    "    \n",
    "    # Now we must order the dataframe df_left in descending order by all of its columns except \n",
    "    # 'predictive_features'. This one will be in ascending (alphabetic) order.\n",
    "    # The order of importance will be the same order of the dictionary, i.e., the order of the\n",
    "    # dataframes passed. The last important column for sorting will be 'predictive_features'.\n",
    "    # Then, we must create a list of columns in their order of importance, and a corresponding\n",
    "    # list of booleans for the sorting order. This list will have value False for the columns\n",
    "    # sorted in descending order; and value True for the column sorted in ascending order.\n",
    "    \n",
    "    # Start the list of columns importance and the list of booleans for ascending or descending\n",
    "    # order:\n",
    "    cols_importance = []\n",
    "    asc_booleans = []\n",
    "    \n",
    "    # loop through all new columns of the dataframe:\n",
    "    \n",
    "    for column in df_left.columns:\n",
    "        # loop through each element, named 'column' from the list df_left.columns (list of columns\n",
    "        # of the merged dataframe):\n",
    "        \n",
    "        # check if the column is different from 'predictive_features'. \n",
    "        # If it is, add it to col_importance list and add a False value to asc_booleans list:\n",
    "        if (column != 'predictive_features'):\n",
    "            \n",
    "            cols_importance.append(column)\n",
    "            asc_booleans.append(False)\n",
    "    \n",
    "    # Now, we finished adding the columns to sort in descending order.\n",
    "    # Then, we simply append 'predictive_features' to the cols_importance list, and append True\n",
    "    # to the asc_booleans list:\n",
    "    cols_importance.append('predictive_features')\n",
    "    asc_booleans.append(True)\n",
    "    \n",
    "    # Now we can sort the df_left dataframe using the pandas sort_values method:\n",
    "    # Since we are sorting by a subset, we pass the list of columns to by instead of passing a simple\n",
    "    # string (case when we are sorting by a single column); and instead of passing a simple boolean\n",
    "    # to ascending (case for single column)\n",
    "    df_left = df_left.sort_values(by = cols_importance, ascending = asc_booleans)\n",
    "    #sort by the cols_importance list; ascending is set on asc_booleans: if its True, column is \n",
    "    # sorted in ascending order; if it is False, sorting is performed in descending order.\n",
    "    \n",
    "    # Check if missing values should be dropped:\n",
    "    if (eliminate_non_correspondence): # runs if the boolean is True:\n",
    "        # drop rows (axis = 0) with missing values:\n",
    "        # axis = 1 would drop columns containing missing values\n",
    "        df_cleaned = df_left.dropna(axis = 0)\n",
    "    \n",
    "    # Now, reset index positions:\n",
    "    df_left = df_left.reset_index(drop = True)\n",
    "    \n",
    "    # If limit_of_ranked_features is not None, use the head method to limit the output\n",
    "    # from the dataframe:\n",
    "    \n",
    "    if not (limit_of_ranked_features is None):\n",
    "        \n",
    "        df_left = df_left.head(limit_of_ranked_features)\n",
    "        # The dataframe will have only the 'limit_of_ranked_features' first entries\n",
    "        print(f\"General feature ranking successfully returned. It was limited to {limit_of_ranked_features}. Check the new dataframe:\\n\")\n",
    "        print(df_left)\n",
    "        \n",
    "        return df_left\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print(\"General feature ranking successfully returned. Check the 20 first features:\\n\")\n",
    "        print(df_left.head(20))\n",
    "        \n",
    "        return df_left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for calculating metrics for regression models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_models_metrics (model_object, X_train, y_train, X_test = None, y_test = None):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import r2_score\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    from xgboost import XGBRegressor\n",
    "    \n",
    "    # model_object: object containing the model that will be analyzed. e.g.\n",
    "    # model_object = elastic_net_linear_reg_model\n",
    "    # X_train = subset of predictive variables (dataframe).\n",
    "    # y_train = subset of response variable (series).\n",
    "    # X_test = subset of predictive variables (test dataframe, in case the \n",
    "    # original one was split into train and test).\n",
    "    # y_test = subset of response variable (test series, in case the \n",
    "    # original one was split into train and test).\n",
    "    \n",
    "    y_pred = model_object.predict(X_train)\n",
    "    \n",
    "    # Start a dictionary for storing the metrics:\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    #METRICS FOR TRAINING\n",
    "    \n",
    "    #mean squared error- MSE\n",
    "    mse_train = mean_squared_error(y_train, y_pred)\n",
    "    # Save this metric in the dictionary:\n",
    "    metrics_dict.update({'MSE_train': mse_train})\n",
    "    print(f\"Mean squared error (MSE) for training = {mse_train}\")\n",
    "    \n",
    "    #Root mean squared error - RMSE\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    metrics_dict.update({'RMSE_train': rmse_train})\n",
    "    print(f\"Root mean squared error (RMSE) for training = {rmse_train}\")\n",
    "    \n",
    "    #R²\n",
    "    val_r2 = r2_score(y_train, y_pred)\n",
    "    metrics_dict.update({'R2_train': val_r2})\n",
    "    print(f\"Correlation coefficient R2 for training = {val_r2}\")\n",
    "\n",
    "    # n_size_train = number of sample size\n",
    "    # k_model = number of independent variables of the defined model\n",
    "    \n",
    "    # the number of predictive features is the length of the list of the predictors:\n",
    "    # Use the list attribute to guarantee that it is a list:\n",
    "    k_model = len(list(X_train.columns)) # numer of X columns (variables)\n",
    "\n",
    "    n_size_train = len(y_train)\n",
    "    #numer of rows\n",
    "\n",
    "    Adj_r2_train = 1 - (1 - val_r2)*(n_size_train - 1)/(n_size_train - k_model - 1)\n",
    "    metrics_dict.update({'Adj_R2_train': Adj_r2_train})\n",
    "    print(f\"Adjusted coefficient of correlation Adj R2 for training = {Adj_r2_train}\")\n",
    "    \n",
    "    if not ((X_test is None) & (y_test is None)):\n",
    "        \n",
    "        # calculate the metrics for the tests:\n",
    "        y_pred_test = model_object.predict(X_test)\n",
    "        \n",
    "        #METRICS FOR TESTING\n",
    "        #mean squared error- MSE\n",
    "        mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "        # Save this metric in the dictionary:\n",
    "        metrics_dict.update({'MSE_test': mse_test})\n",
    "        print(f\"Mean squared error (MSE) for testing = {mse_test}\")\n",
    "        #Root mean squared error - RMSE\n",
    "        rmse_test = np.sqrt(mse_test)\n",
    "        metrics_dict.update({'RMSE_test': rmse_test})\n",
    "        print(f\"Root mean squared error (RMSE) for testing = {rmse_test}\")\n",
    "        #R²\n",
    "        val_r2_test = r2_score(y_test, y_pred_test)\n",
    "        metrics_dict.update({'R2_test': val_r2_test})\n",
    "        print(f\"Correlation coefficient R2 for testing = {val_r2_test}\")\n",
    "        \n",
    "        n_size_test = len(y_test)\n",
    "        \n",
    "        Adj_r2_test = 1 - (1 - val_r2_test)*(n_size_test - 1)/(n_size_test - k_model - 1)\n",
    "        metrics_dict.update({'Adj_R2_test': Adj_r2_test})\n",
    "        print(f\"Adjusted coefficient of correlation Adj R2 for testing = {Adj_r2_test}\")\n",
    "    \n",
    "    print(\"\\n\") # line break\n",
    "    print(\"Dictionary with metrics returned as metrics_dict.\")\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for calculating metrics for classification models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_models_metrics (model_object, X_train, y_train, X_test = None, y_test = None, show_confusion_matrix_values = True, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import tensorflow as tf\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score \n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    # model_object: object containing the model that will be analyzed. e.g.\n",
    "    # model_object = elastic_net_linear_reg_model\n",
    "    # X_train = subset of predictive variables (dataframe).\n",
    "    # y_train = subset of response variable (series).\n",
    "    # X_test = subset of predictive variables (test dataframe, in case the \n",
    "    # original one was split into train and test).\n",
    "    # y_test = subset of response variable (test series, in case the \n",
    "    # original one was split into train and test).\n",
    "    \n",
    "    # show_confusion_matrix_values = True will plot the numeric values in\n",
    "    # the confusion matrices. show_confusion_matrix_values = False will\n",
    "    # omit the values and show only the heatmap.\n",
    "    \n",
    "    # Attention: if the confusion matrix is exported and both the matrices for\n",
    "    # train and test are calculated, then both files will have the same name, but\n",
    "    # the file name of the matrix related to training will get a suffix \"_train\"; \n",
    "    # whereas the matrix related to the test set will get the suffix '_test'\n",
    "    \n",
    "    print(\"Metrics definitions:\")\n",
    "    print(\"True Positive (TP): the model correctly predicts a positive class output, i.e., it correctly predicts that the classified element belongs to that class (in binary classification, like in logistic regression, the model predicts the output 1 and the real output is also 1).\")\n",
    "    print(\"True Negative (TN): the model correctly predicts a negative class output, i.e., it correctly predicts that the classified element do not belong to that class (in binary classification, the model predicts the output 0 and the real output is also 0).\")\n",
    "    print(\"False Positive (FP, type 1 error): the model incorrectly predicts a positive class for a negative class-element, i.e., it predicts that the element belongs to that class, but it actually does not (in binary classification, the model predicts an output 1, but the correct output is 0).\")\n",
    "    print(\"False Negative (FN, type 2 error): the model incorrectly predicts a negative class for a positive class-element, i.e., it predicts that the element does not belong to that class, but it actually does (in binary classification, the model predicts an output 0, but the correct output is 1).\")\n",
    "    print(\"Naturally, the total number (TOTAL) of classifications is the sum of total correct predictions with total incorrect predictions, i.e., TOTAL = TP + TN + FP + FN\")\n",
    "    print(\"\\n\") # line break\n",
    "    print(\"Accuracy: relation between the total number of correct classifications and the total number of classifications performed, i.e., Accuracy = (TP + TN)/(TOTAL)\")\n",
    "    print(\"Precision: it is referrent to the attempt of answering the question: \\'What is the proportion of positive identifications that were actually correct?\\'.\")\n",
    "    print(\"In other words, Precision is the relation between the number of true positives and the total of positively-labelled classifications (true and false positives), i.e., Precision = (TP)/(TP + FP)\")\n",
    "    print(\"Recall: it is referrent to the attempt of answering the question: \\'What is the proportion of elements from positive class that were correctly classified?\\'.\")\n",
    "    print(\"In other words, Recall is the relation between the number of true positives and the total of elements from the positive class (true positives and false negatives), i.e., Recall = (TP)/(TP + FN)\")\n",
    "    print(\"F1: is the ROC-AUC score. In a generic classification problem, this metric is representative of the capability of the model in distinguishing classes.\")     \n",
    "    print(\"F1 =2/((1/Precision)+(1/Recall)) = (2*(Precision)*(Recall))/(Precision + Recall)\")\n",
    "    print(\"\\n\") # line break\n",
    "    # Check:\n",
    "    # https://towardsdatascience.com/how-to-evaluate-your-machine-learning-models-with-python-code-5f8d2d8d945b\n",
    "    \n",
    "    print(\"Confusion Matrix Interpretation:\")\n",
    "    print(\"The confusion matrix is a table commonly used for describing the performance of a classification model (a classifier). It visually compares the model outputs with the correct data labels.\")\n",
    "    print(\"The matrix is divided into several sectors. For a binary classifier, it is divided into 4 quadrants.\")\n",
    "    print(\"Each sector represents a given classification: in the vertical (Y) axis, the real observed labels are shown; whereas the predicted classes (model's outputs) are represented in the horizontal (X) axis.\")\n",
    "    print(\"Then, for each possible class, the following situations may happen: 1. The model predicted that the element belong to a given class, but it does not (incorrect prediction); or 2. The model predicted that the element belong to a given class, and it does (correct prediction).\")\n",
    "    print(\"If the output predicted y_pred (X-coordinate in the confusion matrix = y_pred) is the real label, then the Y-coordinate in the confusion matrix is also y_pred. For an element to have X and Y coordinates equal, it must be positioned on the principal diagonal of the matrix.\")\n",
    "    print(\"\\n\") #line break to highlight the next sentence\n",
    "    print(\"So, we conclude that all the correct predictions of the model are positioned on the main or principal diagonal of the confusion matrix.\")\n",
    "    print(\"\\n\") # line break\n",
    "    print(\"We also may conclude that an increase on model general accuracy is observed as an increase on the values shown in the main diagonal of the confusion matrix.\")\n",
    "    print(\"Notice that this interpretation takes in account a matrix organized starting from the bottom to the top of Y axis (i.e., lower classes on the origin), and from the left to the right of the X-axis, with lower classes closer to the origin. If the order was the opposite, then the secondary diagonal that would contain the correct predictions.\")\n",
    "    print(\"If we have N possible classifications, than we have N values on axis X, and N values in axis Y. So, we have N x N = N2 (N squared) sectors (values) in the confusion matrix.\\n\")\n",
    "    print(\"Confusion matrix for a binary classifier:\")\n",
    "    print(\"For a binary classifier, we have to possible outputs: 0 (the origin of the matrix) and 1. In the vertical axis, 1 is the topper value; in the horizontal axis, 1 is the value on the extreme right (the positions more distant from the origin).\")\n",
    "    print(\"Since N = 2, we have 2 x 2 = 4 quadrants (sectors or values).Starting from the origin, clockwise, we have 4 situations:\")\n",
    "    print(\"Situation 1: X = 0 and Y = 0 - the model correctly predicted a negative output (it is a true negative prediction, TN).\")\n",
    "    print(\"Situation 2: X = 0 and Y = 1 - the model predicted a negative output for a positive class element (it is a false negative, FN).\")\n",
    "    print(\"Situation 3: X = 1 and Y = 1 - the model correctly predicted a positive class (TP).\")\n",
    "    print(\"Situation 4: X = 1 and Y = 0 - the model predicted a positive output for a negative class element (FP)\\n\")\n",
    "    print(\"Each position of the confusion matrix represents the total of elements in each of the possible situations. Then, the sum of all values must be equal to the total of elements classified, and the relation between the sum of the main diagonal and the total of elements must be the accuracy.\")\n",
    "    print(\"So, use the confusion matrix to analyze the performance of the model in classifying each class, separately, and to observe the false negatives and false positives. Also, the confusion matrix will reveal if the classes are balanced, or ir a given class has much more elements than the other, what could impart the capability on differentiating the classes.\")\n",
    "    print(\"For some models, the proportion of false positives may be very different from the proportion of false negatives. It is not a problem, though, and depend on the application of the classifier.\")\n",
    "    print(\"It is an important situation that would be masked by the general metrics that take in account all the predictions, without seggregating them through the classes.\")\n",
    "    print(\"A classical example: suppose the classifier is used for predicting cancer. In this case, the model must have a proportion of false negatives much inferior than the proportion of false positives. That is because the risk associated to a false negative output is much higher.\")\n",
    "    print(\"A person who is incorrectly classified as having cancer will perform several more detailed exams to confirm the diagnosis, so the false positive may get detected without a great problem (in fact, the patient will probably feel good about it and keep taking care of the health). But a person incorrectly classified as not having cancer (when he has cancer) may feel comfortable, not taking care of his health and not making other exams (because he trusts the algorithm). Then, it may be too late when he founds out that was a false negative.\")\n",
    "    print(\"\\n\") # line break\n",
    "    \n",
    "    y_pred = model_object.predict(X_train)\n",
    "    \n",
    "    # Start a dictionary for storing the metrics:\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    #METRICS FOR TRAINING\n",
    "    print(\"Metrics for the training set:\\n\")\n",
    "    \n",
    "    # Accuracy\n",
    "    acc_train = accuracy_score(y_train, y_pred)\n",
    "    # Save this metric in the dictionary:\n",
    "    metrics_dict.update({'Accuracy_train': acc_train})\n",
    "    print(f\"Accuracy score for training = {acc_train} = {100*acc_train}\\%\")\n",
    "    \n",
    "    # Precision\n",
    "    precision_train = precision_score(y_train, y_pred)\n",
    "    # Save this metric in the dictionary:\n",
    "    metrics_dict.update({'Precision_train': precision_train})\n",
    "    print(f\"Precision score for training = {precision_train}\")\n",
    "    \n",
    "    # Recall\n",
    "    recall_train = recall_score(y_train, y_pred)\n",
    "    # Save this metric in the dictionary:\n",
    "    metrics_dict.update({'Recall_train': recall_train})\n",
    "    print(f\"Recall score for training = {recall_train}\")\n",
    "    \n",
    "    # ROC-AUC\n",
    "    roc_auc_train = roc_auc_score(y_train, y_pred)\n",
    "    # Save this metric in the dictionary:\n",
    "    metrics_dict.update({'ROC_AUC_train': roc_auc_train})\n",
    "    print(f\"ROC-AUC score for training = {roc_auc_train}\")\n",
    "    \n",
    "    print(\"Check the general metrics report for the training set:\\n\")\n",
    "    report_train = classification_report(y_train, y_pred)\n",
    "    print(report_train)\n",
    "    print(\"\\n\") # Line break\n",
    "    \n",
    "    print(\"Now, check the confusion matrix for training:\\n\")\n",
    "    cm_train = confusion_matrix(y_train, y_pred)\n",
    "    # Save this table in the dictionary:\n",
    "    metrics_dict.update({'Confusion_matrix_train': cm_train})\n",
    "    \n",
    "    fig1, ax1 = plt.figure()\n",
    "    # possible color schemes (cmap) for the heat map: None, 'Blues_r',\n",
    "    # \"YlGnBu\",\n",
    "    # https://seaborn.pydata.org/generated/seaborn.heatmap.html?msclkid=73d24a00c1b211ec8aa1e7ab656e3ff4\n",
    "    # http://seaborn.pydata.org/tutorial/color_palettes.html?msclkid=daa091f1c1b211ec8c74553348177b45\n",
    "    ax1 = sns.heatmap(cm_train, annot = show_confusion_matrix_values, fmt = \".0f\", linewidths = .5, square = True, cmap = 'Blues_r');\n",
    "    #annot = True: shows the number corresponding to each square\n",
    "    #annot = False: do not show the number\n",
    "    plot_title = \"Accuracy Score for Training = %.2f\" %(acc_train)\n",
    "    ax1.set_title(plot_title)\n",
    "    ax1.set_ylabel('Actual class')\n",
    "    ax1.set_xlabel('Predicted class')\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "        \n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"/\"\n",
    "        \n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"confusion_matrix_train\"\n",
    "        \n",
    "        else:\n",
    "            # add the train suffix, to differentiate from the test matrix:\n",
    "            file_name = file_name + \"_train\"\n",
    "        \n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 110 dpi\n",
    "            png_resolution_dpi = 110\n",
    "        \n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "        \n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "    \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    if not ((X_test is None) & (y_test is None)):\n",
    "        \n",
    "        # calculate the metrics for the tests:\n",
    "        y_pred_test = model_object.predict(X_test)\n",
    "        \n",
    "        #METRICS FOR TESTING\n",
    "        print(\"Metrics for the testing set:\\n\")\n",
    "\n",
    "        # Accuracy\n",
    "        acc_test = accuracy_score(y_test, y_pred_test)\n",
    "        # Save this metric in the dictionary:\n",
    "        metrics_dict.update({'Accuracy_test': acc_test})\n",
    "        print(f\"Accuracy score for validation (test) = {acc_test} = {100*acc_test}\\%\")\n",
    "        # Precision\n",
    "        precision_test = precision_score(y_test, y_pred_test)\n",
    "        # Save this metric in the dictionary:\n",
    "        metrics_dict.update({'Precision_test': precision_test})\n",
    "        print(f\"Precision score for validation (test) = {precision_test}\")\n",
    "        # Recall\n",
    "        recall_test = recall_score(y_test, y_pred_test)\n",
    "        # Save this metric in the dictionary:\n",
    "        metrics_dict.update({'Recall_test': recall_test})\n",
    "        print(f\"Recall score for validation (test) = {recall_test}\")\n",
    "        # ROC-AUC\n",
    "        roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "        # Save this metric in the dictionary:\n",
    "        metrics_dict.update({'ROC_AUC_test': roc_auc_test})\n",
    "        print(f\"ROC-AUC score for validation (test) = {roc_auc_test}\")\n",
    "        print(\"Check the general metrics report for the test set:\\n\")\n",
    "        report_test = classification_report(y_test, y_pred_test)\n",
    "        print(report_test)\n",
    "        print(\"\\n\") # Line break\n",
    "        print(\"Now, check the confusion matrix for validation (test):\\n\")\n",
    "        cm_test = confusion_matrix(y_test, y_pred_test)\n",
    "        # Save this table in the dictionary:\n",
    "        metrics_dict.update({'Confusion_matrix_test': cm_test})\n",
    "\n",
    "        fig2, ax2 = plt.figure()\n",
    "        ax2 = sns.heatmap(cm_test, annot = show_confusion_matrix_values, fmt = \".0f\", linewidths = .5, square = True, cmap = 'Blues_r');\n",
    "        plot_title = \"Accuracy Score for Validation = %.2f\" %(acc_test)\n",
    "        ax1.set_title(plot_title)\n",
    "        ax1.set_ylabel('Actual class')\n",
    "        ax1.set_xlabel('Predicted class')\n",
    "\n",
    "        if (export_png == True):\n",
    "            # Image will be exported\n",
    "            import os\n",
    "\n",
    "            #check if the user defined a directory path. If not, set as the default root path:\n",
    "            if (directory_to_save is None):\n",
    "                #set as the default\n",
    "                directory_to_save = \"/\"\n",
    "\n",
    "            #check if the user defined a file name. If not, set as the default name for this\n",
    "            # function.\n",
    "            if (file_name is None):\n",
    "                #set as the default\n",
    "                file_name = \"confusion_matrix_test\"\n",
    "\n",
    "            else:\n",
    "                # add the train suffix, to differentiate from the train matrix:\n",
    "                file_name = file_name + \"_test\"\n",
    "\n",
    "            #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "            # resolution.\n",
    "            if (png_resolution_dpi is None):\n",
    "                #set as 110 dpi\n",
    "                png_resolution_dpi = 110\n",
    "\n",
    "            #Get the new_file_path\n",
    "            new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "            #Export the file to this new path:\n",
    "            # The extension will be automatically added by the savefig method:\n",
    "            plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "            #quality could be set from 1 to 100, where 100 is the best quality\n",
    "            #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "            #transparent = True or False\n",
    "            # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "            print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.show()\n",
    "\n",
    "    print(\"\\n\") # line break\n",
    "    print(\"Dictionary with metrics returned as metrics_dict.\")\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for making predictions with the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_predictions (model_object, X, dataframe_for_concatenating_predictions = None, col_with_predictions_suffix = None):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBRegressor\n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    # predict_for = 'subset' or predict_for = 'single_entry'\n",
    "    # The function will automatically detect if it is dealing with lists, NumPy arrays\n",
    "    # or Pandas dataframes. If X is a list or a single-dimension array, predict_for\n",
    "    # will be set as 'single_entry'. If X is a multi-dimension NumPy array (as the\n",
    "    # outputs for preparing data - even single_entry - for deep learning models), or if\n",
    "    # it is a Pandas dataframe, the function will set predict_for = 'subset'\n",
    "    \n",
    "    # X = subset of predictive variables (dataframe, NumPy array, or list).\n",
    "    # If PREDICT_FOR = 'single_entry', X should be a list of parameters values.\n",
    "    # e.g. X = [1.2, 3, 4] (dot is the decimal case separator, comma separate values). \n",
    "    # Notice that the list should contain only the numeric values, in the same order of the\n",
    "    # correspondent columns.\n",
    "    # If PREDICT_FOR = 'subset' (prediction for multiple entries), X should be a dataframe \n",
    "    # (subset) or a multi-dimensional NumPy array of the parameters values, as usual.\n",
    "    \n",
    "    # model_object: object containing the model that will be analyzed. e.g.\n",
    "    # model_object = elastic_net_linear_reg_model\n",
    "    \n",
    "    # dataframe_for_concatenating_predictions: if you want to concatenate the predictions\n",
    "    # to a dataframe, pass it here:\n",
    "    # e.g. dataframe_for_concatenating_predictions = df\n",
    "    # If the dataframe must be the same one passed as X, repeat the dataframe object here:\n",
    "    # X = dataset, dataframe_for_concatenating_predictions = dataset.\n",
    "    # Alternatively, if dataframe_for_concatenating_predictions = None, \n",
    "    # the prediction will be returned as a series or NumPy array, depending on the input format.\n",
    "    # Notice that the concatenated predictions will be added as a new column.\n",
    "    \n",
    "    # col_with_predictions_suffix = None. If the predictions are added as a new column\n",
    "    # of the dataframe dataframe_for_concatenating_predictions, you can declare this\n",
    "    # parameter as string with a suffix for identifying the model. If no suffix is added, the new\n",
    "    # column will be named 'y_pred'.\n",
    "    # e.g. col_with_predictions_suffix = '_keras' will create a column named \"y_pred_keras\". This\n",
    "    # parameter is useful when working with multiple models. Always start the suffix with underscore\n",
    "    # \"_\" so that no blank spaces are added; the suffix will not be merged to the column; and there\n",
    "    # will be no confusion with the dot (.) notation for methods, JSON attributes, etc.\n",
    "    \n",
    "    \n",
    "    # Check the type of input: if we are predicting the output for a subset (NumPy array reshaped\n",
    "    # for deep learning models or Pandas dataframe); or predicting for a single entry (single-\n",
    "    # dimension NumPy array or Python list).\n",
    "    \n",
    "    # 1. Check if a list was input. Lists do not have the attribute shape, present in dataframes\n",
    "    # and NumPy arrays. Accessing the attribute shape from a list will raise the Exception error\n",
    "    # named AttributeError\n",
    "    # Try to access the attribute shape. If the error AttributeError is raised, it is a list, so\n",
    "    # set predict_for = 'single_entry':\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Try accessing the shape attribute\n",
    "        X_shape = X.shape\n",
    "        \n",
    "        # Now, check the type of the object X: if it is a dataframe or a numpy array:\n",
    "        X_type = type(X)\n",
    "        \n",
    "        # type(X) == numpy.ndarray (or np.ndarray if NumPy was imported as np) if it is\n",
    "        # an array\n",
    "        # type(X) == pandas.core.frame.DataFrame (or pd.core.frame.DataFrame if Pandas\n",
    "        # was imported as pd) if it is a pandas dataframe.\n",
    "        # Notice that the object type is not a string, so it should not be declared in quotes.\n",
    "        \n",
    "        if (X_type == np.ndarray):\n",
    "            \n",
    "            # It is a NumPy array\n",
    "            # If this array was previously manipulated for the deep learning models, it has 3\n",
    "            # dimensions, so: X_shape = (N, M, 1), N = number of arrays (the number of rows\n",
    "            # of the original dataset), and M = number of elements on each array (the number\n",
    "            # of columns of the original dataset)\n",
    "            \n",
    "            # If the array has the 3rd dimension, we should consider the prediction for 'subset',\n",
    "            # even if it is for a single entry. That is because the array is already reshaped\n",
    "            # and the single_entry code would reshape again.\n",
    "            \n",
    "            # Let's try to access the 3rd dimension as X_shape[2]. \n",
    "            # If there is no 3rd dimension, the exception error IndexError will be raised, since\n",
    "            # there is no index 2:\n",
    "            try:\n",
    "                \n",
    "                # Try accessing the 3rd dimension:\n",
    "                third_dim = X_shape[2]\n",
    "                \n",
    "                # Since it was accessed, the array is already in the correct shape, so set\n",
    "                # prediction for subset:\n",
    "                predict_for = 'subset'\n",
    "            \n",
    "            except IndexError:\n",
    "                \n",
    "                # The index error was raised because there is no 3rd dimension. Then, we are\n",
    "                # dealing with a numpy array equivalent to a list. Set prediction for single_entry.\n",
    "                # It is true even if there are two dimensions like (N, 1) - (2nd dimension added\n",
    "                # by the function for correcting the array format for deep learning).\n",
    "                predict_for = 'single_entry'\n",
    "        \n",
    "        else:\n",
    "            # It is a Pandas dataframe\n",
    "            # Set prediction for a subset:\n",
    "            predict_for = 'subset'\n",
    "        \n",
    "        \n",
    "    except AttributeError:\n",
    "        \n",
    "        # The AttributeError is raised when there is no attribute. \n",
    "        # Since Python lists do not have the shape attribute, \n",
    "        # the input of a list raises this error when trying to access the object's shape.\n",
    "        # Since it is a list, set predict_for = 'single_entry':\n",
    "        predict_for = 'single_entry'\n",
    "        \n",
    "    \n",
    "    if (predict_for == 'single_entry'):\n",
    "        \n",
    "        print(\"Making prediction for a single entry X.\")\n",
    "        print(\"X must be a list with values in the order of the correspondent columns of the dataset.\")\n",
    "        print(\"In other words: declare X as a Python list of values correspondent to each variable, using the same order of variables (columns) used in the dataset.\")\n",
    "        \n",
    "        # Get reshaped list for making the prediction:\n",
    "        X_reshaped = np.reshape(np.array(X), (-1, 1))\n",
    "        \n",
    "        y_pred = model_object.predict(X_reshaped)\n",
    "            \n",
    "        print(f\"Output value predicted for the entry parameters = {y_pred}\\n\")\n",
    "        print(\"Attention: for classification with Keras/TensorFlow and other deep learning frameworks, this output will not be a class, but an array of probabilities correspondent to the probability that the entry belongs to each class. In this case, it is better to use the function calculate_class_probability below, setting model_type == \\'deep_learning\\'. This function will result into dataframes containing the classes as columns and the probabilities in the respective row.\")\n",
    "        print(\"The output class from the deep learning model is the class with higher probability indicated by the predict method. Again, the order of classes is the order they appear in the training dataset. For instance, when using the ImageDataGenerator, the 1st class is the name of the 1st read directory, the 2nd class is the 2nd directory, and so on.\")\n",
    "            \n",
    "        print(\"Returning only the predicted value.\")\n",
    "            \n",
    "        return y_pred\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # prediction for a subset\n",
    "        y_pred = model_object.predict(X)\n",
    "        print(\"Attention: for classification with Keras/TensorFlow and other deep learning frameworks, this output will not be a class, but an array of probabilities correspondent to the probability that the entry belongs to each class. In this case, it is better to use the function calculate_class_probability below, setting model_type == \\'deep_learning\\'. This function will result into dataframes containing the classes as columns and the probabilities in the respective row.\")\n",
    "        print(\"The output class from the deep learning model is the class with higher probability indicated by the predict method. Again, the order of classes is the order they appear in the training dataset. For instance, when using the ImageDataGenerator, the 1st class is the name of the 1st read directory, the 2nd class is the 2nd directory, and so on.\")\n",
    "        \n",
    "        # If y_pred came from a RNN with the parameter return_sequences = True and/or\n",
    "        # return_states = True, then the hidden and/or cell states from the LSTMs\n",
    "        # were returned. So, the returned array has at least one extra dimensions (two\n",
    "        # if both parameters are True). On the other hand, we want only the first dimension,\n",
    "        # correspondent to the actual output.\n",
    "        \n",
    "        # Remember that, due to the reshapes for preparing data for deep learning models,\n",
    "        # y_pred must have at least 2 dimensions: (N, 1), where N is the number of rows of\n",
    "        # the original dataset. But y_pred returned from a model with return_sequences = True\n",
    "        # or return_states = True will be of dimension (N, N, 1). If both parameters are True,\n",
    "        # the dimension is (N, N, N, 1), since there are extra arrays for both the hidden and\n",
    "        # cell states.\n",
    "        \n",
    "        # The conclusion is that there is a third dimension only for models where return_sequences\n",
    "        # = True or return_states = True\n",
    "        \n",
    "        # Check if y_pred is a numpy array, instead of a Pandas dataframe:\n",
    "        \n",
    "        if (type(y_pred) == np.ndarray):\n",
    "            \n",
    "                # Try accessing the array's 3rd dimension. If there is no 3rd dimension,\n",
    "                # the exception error IndexError will be raised.\n",
    "                # Notice: if 4 or more dimensions are present, we can still access\n",
    "                # the 3rd dimension (naturally).\n",
    "                try:\n",
    "                    \n",
    "                    third_dim = y_pred.shape[2]\n",
    "                \n",
    "                    # If we could access the third_dimension, than return_states and\n",
    "                    # or return_sequences = True\n",
    "                    \n",
    "                    # We want only the values stored as the 1st dimension\n",
    "                    # y_pred is an array where each element is an array with two elements. \n",
    "                    # To get only the first elements:\n",
    "                    # (slice the arrays: get all values only for dimension 0, the 1st dim):\n",
    "                    y_pred = y_pred[:,0]\n",
    "                    # if we used y_pred[:,1] we would get the second element, \n",
    "                    # which is the hidden state h (input of the next LSTM unit).\n",
    "                    # It happens because of the parameter return_sequences = True. \n",
    "                    # If return_states = True, there would be a third element, corresponding \n",
    "                    # to the cell state c.\n",
    "                    # Notice that we want only the 1st dimension (0), no matter the case.\n",
    "                \n",
    "                except IndexError:\n",
    "                \n",
    "                    # The index error was raised because there is no 3rd dimension. Then,\n",
    "                    # we do not have to worry with the returned states\n",
    "                    # simply set y_pred as itself:\n",
    "                    y_pred = y_pred\n",
    "                    # Even though the slicing y_pred = y_pred[:,0] would not generate an\n",
    "                    # error, it would unecessarily modify the shape of the array (extra\n",
    "                    # critical step).\n",
    "                    \n",
    "                    # Also, the array obtained as y_pred[:,0] when there are 3 or more \n",
    "                    # dimensions has same shape as y_pred when there are only 1 or 2 \n",
    "                    # dimensions. So, the extra modification of the shape would eliminate\n",
    "                    # this correspondence.\n",
    "                \n",
    "                # If we wanted only the first array, we could set y_pred = y_pred[0]\n",
    "        \n",
    "        # Check if there is a dataframe to concatenate the predictions\n",
    "        if not (dataframe_for_concatenating_predictions is None):\n",
    "            \n",
    "            # there is a dataframe for concatenating the predictions\n",
    "            \n",
    "            # concatenate the predicted values with dataframe_for_concatenating_predictions.\n",
    "            # Add the predicted values as a column:\n",
    "            \n",
    "            # check if there is a suffix:\n",
    "            if not (col_with_predictions_suffix is None):\n",
    "                # There is a suffix declared\n",
    "                # Since there is a suffix, concatenate it to 'y_pred':\n",
    "                col_name = \"y_pred\" + col_with_predictions_suffix\n",
    "            \n",
    "            else:\n",
    "                # Create the column name as the standard.\n",
    "                # The name of the new column is simply 'y_pred'\n",
    "                col_name = \"y_pred\"\n",
    "            \n",
    "            # Set a local copy of the dataframe to manipulate:\n",
    "            X_copy = dataframe_for_concatenating_predictions\n",
    "            \n",
    "            # Add the predictions as the new column named col_name:\n",
    "            X_copy[col_name] = y_pred\n",
    "            \n",
    "            print(f\"The prediction was added as the new column {col_name} of the dataframe, and this dataframe was returned. Check its 10 first rows:\\n\")\n",
    "            print(X_copy.head(10))\n",
    "            \n",
    "            return X_copy\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            print(\"Returning only the predicted values. Check the 10 first values of the series:\\n\")\n",
    "            print(y_pred[:10]) # slice until 10th element from the series or list\n",
    "            # dataset[:,10]: all rows for column 10 of dataset\n",
    "            # dataset[1,:] - slice of all rows for row 1 of dataset.\n",
    "            \n",
    "            return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for calculating probabilities associated to each class**\n",
    "- Set the list_of_classes returned from function `retrieve_classes_used_to_train` as the input of this function.\n",
    "- The predictions (outputs) from deep learning models (e.g. Keras/TensorFlow models) are themselves the probabilities associated to each possible class.\n",
    "    - For Scikit-learn and XGBoost, we must use a specific method for retrieving the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_probability (model_object, X, list_of_classes, type_of_model = 'other', dataframe_for_concatenating_predictions = None):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    # predict_for = 'subset' or predict_for = 'single_entry'\n",
    "    # The function will automatically detect if it is dealing with lists, NumPy arrays\n",
    "    # or Pandas dataframes. If X is a list or a single-dimension array, predict_for\n",
    "    # will be set as 'single_entry'. If X is a multi-dimension NumPy array (as the\n",
    "    # outputs for preparing data - even single_entry - for deep learning models), or if\n",
    "    # it is a Pandas dataframe, the function will set predict_for = 'subset'\n",
    "    \n",
    "    # X = subset of predictive variables (dataframe, NumPy array, or list).\n",
    "    # If PREDICT_FOR = 'single_entry', X should be a list of parameters values.\n",
    "    # e.g. X = [1.2, 3, 4] (dot is the decimal case separator, comma separate values). \n",
    "    # Notice that the list should contain only the numeric values, in the same order of the\n",
    "    # correspondent columns.\n",
    "    # If PREDICT_FOR = 'subset' (prediction for multiple entries), X should be a dataframe \n",
    "    # (subset) or a multi-dimensional NumPy array of the parameters values, as usual.\n",
    "    \n",
    "    # model_object: object containing the model that will be analyzed. e.g.\n",
    "    # model_object = elastic_net_linear_reg_model\n",
    "    \n",
    "    # list_of_classes is the list of classes effectively used for training\n",
    "    # the model. Set this parameter as the object returned from function\n",
    "    # retrieve_classes_used_to_train\n",
    "    \n",
    "    # type_of_model = 'other' or type_of_model = 'deep_learning'\n",
    "    \n",
    "    # Notice that the output will be an array of probabilities, where each\n",
    "    # element corresponds to a possible class, in the order classes appear.\n",
    "    \n",
    "    # dataframe_for_concatenating_predictions: if you want to concatenate the predictions\n",
    "    # to a dataframe, pass it here:\n",
    "    # e.g. dataframe_for_concatenating_predictions = df\n",
    "    # If the dataframe must be the same one passed as X, repeat the dataframe object here:\n",
    "    # X = dataset, dataframe_for_concatenating_predictions = dataset.\n",
    "    # Alternatively, if dataframe_for_concatenating_predictions = None, \n",
    "    # the prediction will be returned as a series or NumPy array, depending on the input format.\n",
    "    # Notice that the concatenated predictions will be added as a new column.\n",
    "    \n",
    "    # All of the new columns (appended or not) will have the prefix \"prob_class_\" followed\n",
    "    # by the correspondent class name to identify them.\n",
    "    \n",
    "       \n",
    "    # 1. Check if a list was input. Lists do not have the attribute shape, present in dataframes\n",
    "    # and NumPy arrays. Accessing the attribute shape from a list will raise the Exception error\n",
    "    # named AttributeError\n",
    "    # Try to access the attribute shape. If the error AttributeError is raised, it is a list, so\n",
    "    # set predict_for = 'single_entry':\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Try accessing the shape attribute\n",
    "        X_shape = X.shape\n",
    "        \n",
    "        # Now, check the type of the object X: if it is a dataframe or a numpy array:\n",
    "        X_type = type(X)\n",
    "        \n",
    "        # type(X) == numpy.ndarray (or np.ndarray if NumPy was imported as np) if it is\n",
    "        # an array\n",
    "        # type(X) == pandas.core.frame.DataFrame (or pd.core.frame.DataFrame if Pandas\n",
    "        # was imported as pd) if it is a pandas dataframe.\n",
    "        # Notice that the object type is not a string, so it should not be declared in quotes.\n",
    "        \n",
    "        if (X_type == np.ndarray):\n",
    "            \n",
    "            # It is a NumPy array\n",
    "            # If this array was previously manipulated for the deep learning models, it has 3\n",
    "            # dimensions, so: X_shape = (N, M, 1), N = number of arrays (the number of rows\n",
    "            # of the original dataset), and M = number of elements on each array (the number\n",
    "            # of columns of the original dataset)\n",
    "            \n",
    "            # If the array has the 3rd dimension, we should consider the prediction for 'subset',\n",
    "            # even if it is for a single entry. That is because the array is already reshaped\n",
    "            # and the single_entry code would reshape again.\n",
    "            \n",
    "            # Let's try to access the 3rd dimension as X_shape[2]. \n",
    "            # If there is no 3rd dimension, the exception error IndexError will be raised, since\n",
    "            # there is no index 2:\n",
    "            try:\n",
    "                \n",
    "                # Try accessing the 3rd dimension:\n",
    "                third_dim = X_shape[2]\n",
    "                \n",
    "                # Since it was accessed, the array is already in the correct shape, so set\n",
    "                # prediction for subset:\n",
    "                predict_for = 'subset'\n",
    "            \n",
    "            except IndexError:\n",
    "                \n",
    "                # The index error was raised because there is no 3rd dimension. Then, we are\n",
    "                # dealing with a numpy array equivalent to a list. Set prediction for single_entry.\n",
    "                # It is true even if there are two dimensions like (N, 1) - (2nd dimension added\n",
    "                # by the function for correcting the array format for deep learning).\n",
    "                predict_for = 'single_entry'\n",
    "        \n",
    "        else:\n",
    "            # It is a Pandas dataframe\n",
    "            # Set prediction for a subset:\n",
    "            predict_for = 'subset'\n",
    "        \n",
    "        \n",
    "    except AttributeError:\n",
    "        \n",
    "        # The AttributeError is raised when there is no attribute. \n",
    "        # Since Python lists do not have the shape attribute, \n",
    "        # the input of a list raises this error when trying to access the object's shape.\n",
    "        # Since it is a list, set predict_for = 'single_entry':\n",
    "        predict_for = 'single_entry'\n",
    "        \n",
    "        \n",
    "    # Check if it is a keras or other deep learning framework; or if it is a sklearn or xgb model:\n",
    "    boolean_check = (type_of_model == 'deep_learning')\n",
    "    \n",
    "    if (boolean_check): # run if it is True\n",
    "        print(\"The predictions (outputs) from deep learning models are themselves the probabilities associated to each possible class.\")\n",
    "        print(\"\\n\") #line break\n",
    "        print(\"The output will be an array of float values: each float represents the probability of one class, in the order the classes appear. For a binary classifier, the first element will correspond to class 0; and the second element will be the probability of class 1.\")\n",
    "    \n",
    "    \n",
    "    if (predict_for == 'single_entry'):\n",
    "        \n",
    "        print(\"Calculating probabilities for a single entry X.\")\n",
    "        print(\"X must be a list with values in the order of the correspondent columns of the dataset.\")\n",
    "        print(\"In other words: declare X as a Python list of values correspondent to each variable, using the same order of variables (columns) used in the dataset.\")\n",
    "        \n",
    "        # Get reshaped list for making the prediction:\n",
    "        X_reshaped = np.reshape(np.array(X), (-1, 1))\n",
    "        \n",
    "        if (boolean_check): \n",
    "            # Use the predict method itself for deep learning models.\n",
    "            # These models do not have the predict_proba method.\n",
    "            # Their output is itself the probability for each class.\n",
    "            y_pred_probabilities = model_object.predict(X_reshaped)\n",
    "        \n",
    "        else:\n",
    "            # use the predict_proba method from sklearn and xgboost:\n",
    "            y_pred_probabilities = model_object.predict_proba(X_reshaped)\n",
    "        \n",
    "        print(\"Probabilities calculated using the entry parameters.\") \n",
    "        print(f\"Probabilities calculated for each one of the classes {list_of_classes} (in the order of classes) = {y_pred_probabilities}\\n\")\n",
    "        \n",
    "        # create a dictionary with the possible classes and the correspondent probabilities:\n",
    "        # Use the list attribute to guarantee that the probabilities are\n",
    "        # retrieved as a list:\n",
    "        probability_dict = {'class': list_of_classes,\n",
    "                            'probability': list(y_pred_probabilities)}\n",
    "            \n",
    "        # Convert it to a Pandas dataframe:\n",
    "        probabilities_df = pd.DataFrame(data = probability_dict)\n",
    "            \n",
    "        print(\"Returning a dataframe containing the classes and the probabilities calculated for the entry to belong to each class. Check it below:\")\n",
    "        print(probabilities_df)\n",
    "            \n",
    "        return probabilities_df\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # prediction for a subset\n",
    "        \n",
    "        if (boolean_check): \n",
    "            # Use the predict method itself for deep learning models.\n",
    "            # These models do not have the predict_proba method.\n",
    "            # Their output is itself the probability for each class.\n",
    "            y_pred_probabilities = model_object.predict(X)\n",
    "            \n",
    "            # If y_pred_probabilities came from a RNN with the parameter return_sequences = True \n",
    "            # and/or return_states = True, then the hidden and/or cell states from the LSTMs\n",
    "            # were returned. So, the returned array has at least one extra dimensions (two\n",
    "            # if both parameters are True). On the other hand, we want only the first dimension,\n",
    "            # correspondent to the actual output.\n",
    "\n",
    "            # Remember that, due to the reshapes for preparing data for deep learning models,\n",
    "            # y_pred_probabilities must have at least 2 dimensions: (N, 1), where N is the number \n",
    "            # of rows of the original dataset. But y_pred_probabilities returned from a model \n",
    "            # with return_sequences = True or return_states = True will be of dimension (N, N, 1). \n",
    "            # If both parameters are True, the dimension is (N, N, N, 1), since there are extra \n",
    "            # arrays for both the hidden and cell states.\n",
    "\n",
    "            # The conclusion is that there is a third dimension only for models where \n",
    "            # return_sequences = True or return_states = True\n",
    "\n",
    "            # Check if y_pred_probabilities is a numpy array, instead of a Pandas dataframe:\n",
    "\n",
    "            if (type(y_pred_probabilities) == np.ndarray):\n",
    "\n",
    "                    # Try accessing the array's 3rd dimension. If there is no 3rd dimension,\n",
    "                    # the exception error IndexError will be raised.\n",
    "                    # Notice: if 4 or more dimensions are present, we can still access\n",
    "                    # the 3rd dimension (naturally).\n",
    "                    try:\n",
    "\n",
    "                        third_dim = y_pred_probabilities.shape[2]\n",
    "\n",
    "                        # If we could access the third_dimension, than return_states and\n",
    "                        # or return_sequences = True\n",
    "\n",
    "                        # We want only the values stored as the 1st dimension\n",
    "                        # y_pred_probabilities is an array where each element is an array with \n",
    "                        # two elements. To get only the first elements:\n",
    "                        # (slice the arrays: get all values only for dimension 0, the 1st dim):\n",
    "                        y_pred_probabilities = y_pred_probabilities[:,0]\n",
    "                        # if we used y_pred_probabilities[:,1] we would get the second element, \n",
    "                        # which is the hidden state h (input of the next LSTM unit).\n",
    "                        # It happens because of the parameter return_sequences = True. \n",
    "                        # If return_states = True, there would be a third element, corresponding \n",
    "                        # to the cell state c.\n",
    "                        # Notice that we want only the 1st dimension (0), no matter the case.\n",
    "\n",
    "                    except IndexError:\n",
    "\n",
    "                        # The index error was raised because there is no 3rd dimension. Then,\n",
    "                        # we do not have to worry with the returned states\n",
    "                        # simply set y_pred_probabilities as itself:\n",
    "                        y_pred_probabilities = y_pred_probabilities\n",
    "                        # Even though the slicing y_pred = y_pred[:,0] would not generate an\n",
    "                        # error, it would unecessarily modify the shape of the array (extra\n",
    "                        # critical step).\n",
    "\n",
    "                        # Also, the array obtained as y_pred[:,0] when there are 3 or more \n",
    "                        # dimensions has same shape as y_pred when there are only 1 or 2 \n",
    "                        # dimensions. So, the extra modification of the shape would eliminate\n",
    "                        # this correspondence.\n",
    "\n",
    "                    # If we wanted only the first array, we could set \n",
    "                    # y_pred_probabilities = y_pred_probabilities[0]\n",
    "        \n",
    "        else:\n",
    "            # use the predict_proba method from sklearn and xgboost:\n",
    "            y_pred_probabilities = model_object.predict_proba(X)\n",
    "        \n",
    "        # y_pred_probabilities is a column containing arrays of probabilities\n",
    "        # Let's create a dataframe separating each element of the array into\n",
    "        # a separate column\n",
    "        \n",
    "        # Get the size of each array. It is the total of elements from\n",
    "        # list_of_classes (total of possible classes):\n",
    "        total_of_classes = len(list_of_classes)\n",
    "        \n",
    "        # Get the total of rows. It is the length of X:\n",
    "        \n",
    "        # If X is a NumPy array, get its first dimension:\n",
    "        if (X_type == np.ndarray):\n",
    "            \n",
    "            # Get the first dimension of the array (dimension 0)\n",
    "            # This dimension is the total of arrays, i.e., the total\n",
    "            # of rows on the original dataset:\n",
    "            # X.shape = (N, M, 1), N = total of arrays (rows of the original\n",
    "            # dataset); M = total of elements in each array (columns of the\n",
    "            # original dataset). Analogously, y.shape = (N, 1)\n",
    "            total_rows = X.shape[0]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # X is a dataframe, so the number of rows is its length\n",
    "            total_rows = len(X)\n",
    "        \n",
    "        # Starts a dictionary. This dictionary will have the class as the\n",
    "        # key and a list of the probabilities that the element belong to that\n",
    "        # class as the value (in the dataframe, the class will be column,\n",
    "        # with its calculated probability in each row):\n",
    "        probability_dict = {}\n",
    "        \n",
    "        # Loop through each possible class:\n",
    "        for i in range (total_of_classes):\n",
    "            # loops from i = 0 (first index) \n",
    "            # to i = (total_of_classes - 1), index of the last element of list\n",
    "            # 'list_of_classes'\n",
    "            \n",
    "            # Retrieve the name of the class in the list 'list_of_classes'.\n",
    "            # It is the i-th element from list list_of_classes:\n",
    "            class_name = list_of_classes[i]\n",
    "            # Let's concatenate the prefix \"prob_class_\" to this strings.\n",
    "            # This string will be used as column name, so it will be clear \n",
    "            # in the output dataframe that the column is referrent to the \n",
    "            # probability calculated for the class. Since the elements may \n",
    "            # have been saved as numbers use the str attribute to guarantee \n",
    "            # that the element was read as a string, and concatenate the\n",
    "            # prefix to its left:\n",
    "            class_name = \"prob_class_\" + str(class_name)\n",
    "            \n",
    "            # Start a list of probabilities:\n",
    "            prob_list = []\n",
    "            \n",
    "            # Now loop through each row j from the dataframe\n",
    "            # to retrieve the array in the column y_pred_probabilities:\n",
    "            \n",
    "            for j in len(total_rows):\n",
    "                # goes from j = 0 (first row of the dataframe) to\n",
    "                # j = total_rows - 1, index of the last row\n",
    "                # Get the array of probabilities for that row:\n",
    "                prob_array = y_pred_probabilities[j]\n",
    "                \n",
    "                # Append the i-th element of that array in prob_list\n",
    "                # The i-th position of the array is the probability\n",
    "                # of the class being analyzed in the i-th iteration of\n",
    "                # the main loop\n",
    "                prob_list.append(prob_array[i])\n",
    "            \n",
    "            # Now that the probabilities for the class correspondent to\n",
    "            # each row were retrieved as the list prob_list, update the\n",
    "            # dictionary. Use the class name saved as class_name as the\n",
    "            # key, and put the prob_list as the correspondent value:\n",
    "            probability_dict.update({class_name: prob_list})\n",
    "        \n",
    "        # Now that we finished the loop, the probability dictionary contains\n",
    "        # each one of the classes as its keys, and the list of probabilities\n",
    "        # for each row as the correspondent values. \n",
    "        # Also, the keys are identified with the prefix 'prob_class' to\n",
    "        # indicate that they are referrent to the probability of belonging to\n",
    "        # one class. Let's convert this dictionary to a Pandas dataframe:\n",
    "        \n",
    "        probabilities_df = pd.DataFrame(data = probability_dict)\n",
    "        \n",
    "        # Check if there is a dataframe to concatenate the predictions\n",
    "        if not (dataframe_for_concatenating_predictions is None):\n",
    "            \n",
    "            # there is a dataframe for concatenating the predictions.\n",
    "            \n",
    "            # Set a local copy of the dataframe to manipulate:\n",
    "            X_copy = X\n",
    "            \n",
    "            # Append the columns from probabilities_df with Pandas concat\n",
    "            # method, setting axis = 1 (axis = 0  appends rows)\n",
    "            # Use the pandas 'inner' join, which removes entries without\n",
    "            # correspondence. It is the same strategy used for concatenating\n",
    "            # the dataframe obtained from One-Hot Encoding transformation in the\n",
    "            # ETL Workflow (3_Dataset_Transformation)\n",
    "            X_copy = pd.concat([X_copy, probabilities_df], axis = 1, join = \"inner\")\n",
    "      \n",
    "            print(f\"The dataframe X was concatenated to the probabilities calculated for each class and returned. Check its first 10 entries:\\n\")\n",
    "            print(X_copy.head(10))\n",
    "            \n",
    "            return X_copy\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            print(\"Returning only the dataframe with the probabilities calculated for each class. Check its first 10 entries:\\n\")\n",
    "            print(probabilities_df.head(10))\n",
    "            \n",
    "            return probabilities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for performing the SHAP feature importance analysis**\n",
    "- SHAP was developed by a mathematician from Washington University.\n",
    "- It combines the obtained machine learning model with Game Theory algorithms to analyze the relative importance of each variable, as well as the **interactions between variables**.\n",
    "- SHAP returns us a SHAP value that represents the relative importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_feature_analysis (model_object, X_train, model_type = 'linear', total_of_shap_points = 40, plot_type = 'waterfall', max_number_of_features_shown = 10):\n",
    "    \n",
    "    # An introduction to explainable AI with Shapley values:\n",
    "    # https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import shap\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBRegressor\n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    # model_object: object containing the model that will be analyzed. e.g.\n",
    "    # model_object = elastic_net_linear_reg_model\n",
    "    # X_train = subset of predictive variables (dataframe).\n",
    "    \n",
    "    # total_of_shap_points (integer): number of points from the \n",
    "    # subset X_train that will be randomly selected for the SHAP \n",
    "    # analysis. If the kernel is taking too long, reduce this value.\n",
    "    \n",
    "    # MODEL_TYPE = 'linear' for linear models (OLS, Ridge, Lasso, ElasticNet,\n",
    "    # Logistic Regression)\n",
    "    # MODEL_TYPE = 'tree' for tree-based models (Random Forest and XGBoost)\n",
    "    # MODEL_TYPE = 'ann' for artificial neural networks\n",
    "    \n",
    "    # PLOT_TYPE = 'waterfall', 'beeswarm', 'bar', 'heatmap' \n",
    "    # 'scatter', 'force_plt' or 'summary': \n",
    "    # sets the type of shap plot that will be shown\n",
    "    \n",
    "    # If clustering is used, it is possible to plot the dendogram with\n",
    "    # the bar chart: shap.plots.bar(shap_values, clustering=clustering, clustering_cutoff=1.8)\n",
    "    # Also, SHAP can be used for text analysis (in the next example, it\n",
    "    # is used to analyze the first sentence - index 0):\n",
    "    # shap.plots.text(shap_values[0])\n",
    "    \n",
    "    # MAX_NUMBER_OF_FEATURES_SHOWN = 10: (integer) limiting the number\n",
    "    # of features shown in the plot.\n",
    "\n",
    "    \n",
    "    # check if a invalid number (None, zero or negative value) was input\n",
    "    # as max_number_of_features. Firstly, use the int attribute to guarantee\n",
    "    # that the value is an integer:\n",
    "    max_number_of_features = int(max_number_of_features)\n",
    "    \n",
    "    boolean_check = (max_number_of_features is None) | (max_number_of_features <= 0)\n",
    "    \n",
    "    if (boolean_check): #run if it is True\n",
    "        print(\"Invalid value input as max_number_of_features. Setting it to 10.\")\n",
    "        max_number_of_features = 10\n",
    "    \n",
    "    # Start SHAP:\n",
    "    shap.initjs()\n",
    "    \n",
    "    print(f\"Randomly sampling {total_of_shap_points} points from the dataset to perform SHAP analysis.\")\n",
    "    print(\"If the kernel takes too long, cancel the application and reduce the integer value input as \\'total_of_shap_points\\'. On the other hand, if it is possible, increase the value to obtain higher precision on the analysis.\")\n",
    "    \n",
    "    # sample the number of points passed as total_of_shap_points\n",
    "    # from the dataset X_train, and store these points as X_shap:\n",
    "    X_shap = shap.sample(X_train, total_of_shap_points)\n",
    "    \n",
    "    if (model_type == 'linear'):\n",
    "        \n",
    "        print(\"Analyzing Scikit-learn linear model.\")\n",
    "        \n",
    "        # Create an object from the Linear explainer class:\n",
    "        shap_explainer = shap.explainers.Linear(model_object)\n",
    "        # Documentation:\n",
    "        # https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/linear_models/Math%20behind%20LinearExplainer%20with%20correlation%20feature%20perturbation.html\n",
    "        # https://shap.readthedocs.io/en/latest/generated/shap.explainers.Linear.html#shap.explainers.Linear\n",
    "        # https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/linear_models/Sentiment%20Analysis%20with%20Logistic%20Regression.html\n",
    "        \n",
    "        # Apply .shap_values method to obtain the shap values:\n",
    "        shap_vals = shap_explainer.shap_values(X_shap)\n",
    "        # shap_vals is a list or array of calculated values.\n",
    "        \n",
    "    elif (model_type == 'tree'):\n",
    "        \n",
    "        print(\"Analyzing tree-based Scikit-learn or XGBoost model.\")\n",
    "    \n",
    "        # Create an object from the Tree explainer class:\n",
    "        shap_explainer = shap.explainers.Tree(model_object)\n",
    "        # Documentation:\n",
    "        # https://shap.readthedocs.io/en/latest/generated/shap.explainers.Tree.html#shap.explainers.Tree\n",
    "        # Apply .shap_values method to obtain the shap values:\n",
    "        shap_vals = shap_explainer.shap_values(X_shap)\n",
    "        # shap_vals is a list or array of calculated values.\n",
    "        \n",
    "    else:\n",
    "        # In any other case, use the KernelExplainer\n",
    "        # Create an object from KernelExplainer class:\n",
    "        shap_explainer = shap.KernelExplainer(model_object.predict, X_shap)\n",
    "        # https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/neural_networks/Census%20income%20classification%20with%20Keras.html\n",
    "        # Alternatively: model_object.predict(X)\n",
    "        # Apply .shap_values method to obtain the shap values:\n",
    "        shap_vals = shap_explainer.shap_values(X_shap)\n",
    "        # shap_vals is a list or array of calculated values.\n",
    "        \n",
    "    if (plot_type == 'waterfall'):\n",
    "        \n",
    "        print(\"SHAP waterfall plot:\\n\")\n",
    "        shap.plots.waterfall(shap_values, max_display = max_number_of_features)\n",
    "    \n",
    "    elif (plot_type == 'beeswarm'):\n",
    "        \n",
    "        print(\"SHAP beeswarm plot:\\n\")\n",
    "        shap.plots.beeswarm(shap_values, max_display = max_number_of_features)\n",
    "    \n",
    "    elif (plot_type == 'bar'):\n",
    "        \n",
    "        print(\"SHAP bar plot:\\n\")\n",
    "        shap.plots.bar(shap_values, max_display = max_number_of_features)\n",
    "    \n",
    "    elif (plot_type == 'heatmap'):\n",
    "        \n",
    "        print(\"SHAP heatmap. Warning: do not use more than 1000 data on this plot:\\n\")\n",
    "        # to limit the amount of data, slice the subset until 1000:\n",
    "        # shap.plots.heatmap(shap_values[:1000])\n",
    "        shap.plots.heatmap(shap_values, max_display = max_number_of_features)\n",
    "    \n",
    "    elif (plot_type == 'scatter'):\n",
    "        \n",
    "        print(\"SHAP scatter plot. It does not support the setting of maximum displayed features:\\n\")\n",
    "        shap.plots.scatter(shap_values)\n",
    "    \n",
    "    elif (plot_type == 'force_plot'):\n",
    "        \n",
    "        # force plot for a single entry:\n",
    "        # shap_values = explainer.shap_values(X.iloc[299,:], nsamples=500)\n",
    "        # shap.force_plot(explainer.expected_value, shap_values, X_display.iloc[299,:])\n",
    "        \n",
    "        print(\"SHAP force plot. It does not support the setting of maximum displayed features:\\n\")\n",
    "        shap.force_plot(shap_explainer.expected_value, shap_values)\n",
    "    \n",
    "    else:\n",
    "        # If any other valid (including invalids) was provided:\n",
    "        print(\"SHAP summary plot. It does not support the setting of maximum displayed features:\\n\")\n",
    "        shap.summary_plot(shap_vals, X_shap)\n",
    "    \n",
    "    # Create a dictionary with the explainer and the shap_vals:\n",
    "    shap_dict = {\n",
    "        'SHAP_kernel_explainer': shap_explainer,\n",
    "        'SHAP_values': shap_vals\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\") # line break\n",
    "    print(\"Dictionary with SHAP explainer and SHAP values returned as \\'shap_dict\\'.\")\n",
    "    \n",
    "    print(\"\\n\") # line break\n",
    "    print(\"SHAP Interpretation:\")\n",
    "    print(\"SHAP returns us a SHAP value that represents the relative importance.\")\n",
    "    print(\"The features are displayed in order of importance, from the most important (top of the plot) to the less important (bottom of the plot).\")\n",
    "    print(\"A feature which is shown on the right side of the plot results in positive impact on the model, whereas a feature on the left results into a negative impact in the response.\")\n",
    "    print(\"The relative impact is shown by the color scale: a tone closer to red indicates a higher impact, whereas the proximity to blue indicates low relative impact.\")\n",
    "        \n",
    "    return shap_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for importing or exporting models and dictionaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_export_model_or_dict (action = 'import', objects_manipulated = 'model_only', model_file_name = None, dictionary_file_name = None, directory_path = '/', model_type = 'keras', dict_to_export = None, model_to_export = None, use_colab_memory = False):\n",
    "    \n",
    "    import os\n",
    "    import pickel as pkl\n",
    "    import dill\n",
    "    import tensorflow as tf\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "    from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "    from xgboost import XGBRegressor, XGBClassifier\n",
    "    from statsmodels.tsa.arima.model import ARIMA, ARIMAResults\n",
    "    \n",
    "    # action = 'import' for importing a model and/or a dictionary;\n",
    "    # action = 'export' for exporting a model and/or a dictionary.\n",
    "    \n",
    "    # objects_manipulated = 'model_only' if only a model will be manipulated.\n",
    "    # objects_manipulated = 'dict_only' if only a dictionary will be manipulated.\n",
    "    # objects_manipulated = 'model_and_dict' if both a model and a dictionary will be\n",
    "    # manipulated.\n",
    "    \n",
    "    #model_file_name: string with the name of the file containing the model (for 'import');\n",
    "    # or of the name that the exported file will have (for 'export')\n",
    "    # e.g. model_file_name = 'model'\n",
    "    # WARNING: Do not add the file extension.\n",
    "    # Keep it in quotes. Keep model_file_name = None if no model will be manipulated.\n",
    "    \n",
    "    # dictionary_file_name: string with the name of the file containing the dictionary \n",
    "    # (for 'import');\n",
    "    # or of the name that the exported file will have (for 'export')\n",
    "    # e.g. dictionary_file_name = 'history_dict'\n",
    "    # WARNING: Do not add the file extension.\n",
    "    # Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no \n",
    "    # dictionary will be manipulated.\n",
    "    \n",
    "    # DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "    # or from which the model will be retrieved. If no value is provided,\n",
    "    # the DIRECTORY_PATH will be the root: \"/\"\n",
    "    # Notice that the model and the dictionary must be stored in the same path.\n",
    "    # If a model and a dictionary will be exported, they will be stored in the same\n",
    "    # DIRECTORY_PATH.\n",
    "    \n",
    "    # model_type: This parameter has effect only when a model will be manipulated.\n",
    "    # model_type: 'keras' for deep learning keras/ tensorflow models with extension .h5\n",
    "    # model_type = 'sklearn' for models from scikit-learn (non-deep learning)\n",
    "    # model_type = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "    # model_type = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "    # model_type = 'arima' for ARIMA model (Statsmodels)\n",
    "    \n",
    "    # dict_to_export and model_to_export: \n",
    "    # These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "    # must be declared. If ACTION == 'export', keep:\n",
    "    # dict_to_export = None, \n",
    "    # model_to_export = None\n",
    "    # If one of these objects will be exported, substitute None by the name of the object\n",
    "    # e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "    # model_to_export = keras_model. Notice that it must be declared without quotes, since\n",
    "    # it is not a string, but an object.\n",
    "    # For exporting a dictionary named as 'dict':\n",
    "    # dict_to_export = dict\n",
    "    \n",
    "    # use_colab_memory: this parameter has only effect when using Google Colab (or it will\n",
    "    # raise an error). Set as use_colab_memory = True if you want to use the instant memory\n",
    "    # from Google Colaboratory: you will update or download the file and it will be available\n",
    "    # only during the time when the kernel is running. It will be excluded when the kernel\n",
    "    # dies, for instance, when you close the notebook.\n",
    "    \n",
    "    # If action == 'export' and use_colab_memory == True, then the file will be downloaded\n",
    "    # to your computer (running the cell will start the download).\n",
    "    \n",
    "    # Check the directory path\n",
    "    if (directory_path is None):\n",
    "        # set as the root:\n",
    "        directory_path = \"/\"\n",
    "        \n",
    "        \n",
    "    bool_check1 = (objects_manipulated != 'model_only')\n",
    "    # bool_check1 == True if a dictionary will be manipulated\n",
    "    \n",
    "    bool_check2 = (objects_manipulated != 'dict_only')\n",
    "    # bool_check1 == True if a dictionary will be manipulated\n",
    "    \n",
    "    if (bool_check1 == True):\n",
    "        #manipulate a dictionary\n",
    "        \n",
    "        if (dictionary_file_name is None):\n",
    "            print(\"Please, enter a name for the dictionary.\")\n",
    "            return \"error1\"\n",
    "        \n",
    "        else:\n",
    "            # Create the file path for the dictionary:\n",
    "            dict_path = os.path.join(directory_path, dictionary_file_name)\n",
    "            # Extract the file extension\n",
    "            dict_extension = 'pkl'\n",
    "            #concatenate:\n",
    "            dict_path = dict_path + \".\" + dict_extension\n",
    "            \n",
    "    \n",
    "    if (bool_check2 == True):\n",
    "        #manipulate a model\n",
    "        \n",
    "        if (model_file_name is None):\n",
    "            print(\"Please, enter a name for the model.\")\n",
    "            return \"error1\"\n",
    "        \n",
    "        else:\n",
    "            # Create the file path for the dictionary:\n",
    "            model_path = os.path.join(directory_path, model_file_name)\n",
    "            # Extract the file extension\n",
    "            \n",
    "            #check model_type:\n",
    "            if (model_type == 'keras'):\n",
    "                model_extension = 'h5'\n",
    "            \n",
    "            elif (model_type == 'sklearn'):\n",
    "                model_extension = 'dill'\n",
    "                #it could be 'pkl', though\n",
    "            \n",
    "            elif (model_type == 'xgb_regressor'):\n",
    "                model_extension = 'json'\n",
    "                #it could be 'ubj', though\n",
    "            \n",
    "            elif (model_type == 'xgb_classifier'):\n",
    "                model_extension = 'json'\n",
    "                #it could be 'ubj', though\n",
    "            \n",
    "            elif (model_tyoe == 'arima'):\n",
    "                model_extension = 'pkl'\n",
    "            \n",
    "            else:\n",
    "                print(\"Enter a valid model_type: keras, sklearn_xgb, or arima.\")\n",
    "                return \"error2\"\n",
    "            \n",
    "            #concatenate:\n",
    "            model_path = model_path +  \".\" + model_extension\n",
    "            \n",
    "    # Now we have the full paths for the dictionary and for the model.\n",
    "    \n",
    "    if (action == 'import'):\n",
    "        \n",
    "        if (use_colab_memory == True):\n",
    "             \n",
    "            from google.colab import files\n",
    "            # google.colab library must be imported only in case \n",
    "            # it is going to be used, for avoiding \n",
    "            # AWS compatibility issues.\n",
    "            \n",
    "            print(\"Click on the button for file selection and select the files from your machine that will be uploaded in the Colab environment.\")\n",
    "            print(\"Warning: the files will be removed from Colab memory after the Kernel dies or after the notebook is closed.\")\n",
    "            # this functionality requires the previous declaration:\n",
    "            ## from google.colab import files\n",
    "            colab_files_dict = files.upload()\n",
    "            # The files are stored into a dictionary called colab_files_dict where the keys\n",
    "            # are the names of the files and the values are the files themselves.\n",
    "            ## e.g. if you upload a single file named \"dictionary.pkl\", the dictionary will be\n",
    "            ## colab_files_dict = {'dictionary.pkl': file}, where file is actually a big string\n",
    "            ## representing the contents of the file. The length of this value is the size of the\n",
    "            ## uploaded file, in bytes.\n",
    "            ## To access the file is like accessing a value from a dictionary: \n",
    "            ## d = {'key1': 'val1'}, d['key1'] == 'val1'\n",
    "            ## we simply declare the key inside brackets and quotes, the same way we would do for\n",
    "            ## accessing the column of a dataframe.\n",
    "            ## In this example, colab_files_dict['dictionary.pkl'] access the content of the \n",
    "            ## .pkl file, and len(colab_files_dict['dictionary.pkl']) is the size of the .pkl\n",
    "            ## file in bytes.\n",
    "            ## To check the dictionary keys, apply the method .keys() to the dictionary (with empty\n",
    "            ## parentheses): colab_files_dict.keys()\n",
    "            \n",
    "            for key in colab_files_dict.keys():\n",
    "                #loop through each element of the list of keys of the dictionary\n",
    "                # (list colab_files_dict.keys()). Each element is named 'key'\n",
    "                print(f\"User uploaded file {key} with length {len(colab_files_dict[key])} bytes.\")\n",
    "                # The key is the name of the file, and the length of the value\n",
    "                ## correspondent to the key is the file's size in bytes.\n",
    "                ## Notice that the content of the uploaded object must be passed \n",
    "                ## as argument for a proper function to be interpreted. \n",
    "                ## For instance, the content of a xlsx file should be passed as\n",
    "                ## argument for Pandas .read_excel function; the pkl file must be passed as\n",
    "                ## argument for pickle.\n",
    "                ## e.g., if you uploaded 'table.xlsx' and stored it into colab_files_dict you should\n",
    "                ## declare df = pd.read_excel(colab_files_dict['table.xlsx']) to obtain a dataframe\n",
    "                ## df from the uploaded table. Notice that is the value, not the key, that is the\n",
    "                ## argument.\n",
    "        \n",
    "        if (bool_check1 == True):\n",
    "            #manipulate a dictionary\n",
    "            if (use_colab_memory == True):\n",
    "                key = dictionary_file_name + \".\" + dict_extension\n",
    "                #Use the key to access the file content, and pass the file content\n",
    "                # to pickle:\n",
    "                with open(colab_files_dict[key], 'rb') as opened_file:\n",
    "            \n",
    "                    imported_dict = pkl.load(opened_file)\n",
    "                    # The structure imported_dict = pkl.load(open(colab_files_dict[key], 'rb')) relies \n",
    "                    # on the GC to close the file. That's not a good idea: If someone doesn't use \n",
    "                    # CPython the garbage collector might not be using refcounting (which collects \n",
    "                    # unreferenced objects immediately) but e.g. collect garbage only after some time.\n",
    "                    # Since file handles are closed when the associated object is garbage collected or \n",
    "                    # closed explicitly (.close() or .__exit__() from a context manager) the file \n",
    "                    # will remain open until the GC kicks in.\n",
    "                    # Using 'with' ensures the file is closed as soon as the block is left - even if \n",
    "                    # an exception happens inside that block, so it should always be preferred for any \n",
    "                    # real application.\n",
    "                    # source: https://stackoverflow.com/questions/39447362/equivalent-ways-to-json-load-a-file-in-python\n",
    "\n",
    "                print(f\"Dictionary {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "            else:\n",
    "                #standard method\n",
    "                with open(dict_path, 'rb') as opened_file:\n",
    "            \n",
    "                    imported_dict = pkl.load(opened_file)\n",
    "                \n",
    "                # 'rb' stands for read binary (read mode). For writing mode, 'wb', 'write binary'\n",
    "                print(f\"Dictionary successfully imported from {dict_path}.\")\n",
    "                \n",
    "        if (bool_chek2 == True):\n",
    "            #manipulate a model\n",
    "            # select the proper model\n",
    "        \n",
    "            if (model_type == 'keras'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = tf.keras.models.load_model(colab_files_dict[key])\n",
    "                    print(f\"Keras/TensorFlow model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # We previously declared:\n",
    "                    # from keras.models import load_model\n",
    "                    model = tf.keras.models.load_model(model_path)\n",
    "                    print(f\"Keras/TensorFlow model successfully imported from {model_path}.\")\n",
    "\n",
    "            elif (model_type == 'sklearn'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    \n",
    "                    with open(colab_files_dict[key], 'rb') as opened_file:\n",
    "            \n",
    "                        model = dill.load(opened_file)\n",
    "                    \n",
    "                    print(f\"Scikit-learn model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    with open(model_path, 'rb') as opened_file:\n",
    "            \n",
    "                        model = dill.load(opened_file)\n",
    "                \n",
    "                    print(f\"Scikit-learn model successfully imported from {model_path}.\")\n",
    "                    # For loading a pickle model:\n",
    "                    ## model = pkl.load(open(model_path, 'rb'))\n",
    "                    # 'rb' stands for read binary (read mode). For writing mode, 'wb', 'write binary'\n",
    "\n",
    "            elif (model_type == 'xgb_regressor'):\n",
    "                \n",
    "                # Create an instance (object) from the class XGBRegressor:\n",
    "                \n",
    "                model = XGBRegressor()\n",
    "                # Now we can apply the load_model method from this class:\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = model.load_model(colab_files_dict[key])\n",
    "                    print(f\"XGBoost regression model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model = model.load_model(model_path)\n",
    "                    print(f\"XGBoost regression model successfully imported from {model_path}.\")\n",
    "                    # model.load_model(\"model.json\") or model.load_model(\"model.ubj\")\n",
    "                    # .load_model is a method from xgboost object\n",
    "            \n",
    "             elif (model_type == 'xgb_classifier'):\n",
    "                \n",
    "                # Create an instance (object) from the class XGBClassifier:\n",
    "                \n",
    "                model = XGBClassifier()\n",
    "                # Now we can apply the load_model method from this class:\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = model.load_model(colab_files_dict[key])\n",
    "                    print(f\"XGBoost classification model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model = model.load_model(model_path)\n",
    "                    print(f\"XGBoost classification model successfully imported from {model_path}.\")\n",
    "                    # model.load_model(\"model.json\") or model.load_model(\"model.ubj\")\n",
    "                    # .load_model is a method from xgboost object\n",
    "\n",
    "            elif (model_type == 'arima'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = ARIMAResults.load(colab_files_dict[key])\n",
    "                    print(f\"ARIMA model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # We previously declared:\n",
    "                    # from statsmodels.tsa.arima.model import ARIMAResults\n",
    "                    model = ARIMAResults.load(model_path)\n",
    "                    print(f\"ARIMA model successfully imported from {model_path}.\")\n",
    "            \n",
    "            if (objects_manipulated == 'model_only'):\n",
    "                # only the model should be returned\n",
    "                return model\n",
    "            \n",
    "            elif (objects_manipulated == 'dict_only'):\n",
    "                # only the dictionary should be returned:\n",
    "                return imported_dict\n",
    "            \n",
    "            else:\n",
    "                # Both objects are returned:\n",
    "                return model, imported_dict\n",
    "\n",
    "    \n",
    "    elif (action == 'export'):\n",
    "        \n",
    "        #Let's export the models or dictionary:\n",
    "        if (use_colab_memory == True):\n",
    "            \n",
    "            from google.colab import files\n",
    "            # google.colab library must be imported only in case \n",
    "            # it is going to be used, for avoiding \n",
    "            # AWS compatibility issues.\n",
    "            \n",
    "            print(\"The files will be downloaded to your computer.\")\n",
    "        \n",
    "        if (bool_check1 == True):\n",
    "            #manipulate a dictionary\n",
    "            if (use_colab_memory == True):\n",
    "                ## Download the dictionary\n",
    "                key = dictionary_file_name + \".\" + dict_extension\n",
    "                \n",
    "                with open(key, 'wb') as opened_file:\n",
    "            \n",
    "                    pkl.dump(dict_to_export, opened_file)\n",
    "                \n",
    "                # this functionality requires the previous declaration:\n",
    "                ## from google.colab import files\n",
    "                files.download(key)\n",
    "                \n",
    "                print(f\"Dictionary {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "            else:\n",
    "                #standard method \n",
    "                with open(dict_path, 'wb') as opened_file:\n",
    "            \n",
    "                    pkl.dump(dict_to_export, opened_file)\n",
    "                \n",
    "                #to save the file, the mode must be set as 'wb' (write binary)\n",
    "                print(f\"Dictionary successfully exported as {dict_path}.\")\n",
    "                \n",
    "        if (bool_chek2 == True):\n",
    "            #manipulate a model\n",
    "            # select the proper model\n",
    "        \n",
    "            if (model_type == 'keras'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"Keras/TensorFlow model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save(model_path)\n",
    "                    print(f\"Keras/TensorFlow model successfully exported as {model_path}.\")\n",
    "\n",
    "            elif (model_type == 'sklearn'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    \n",
    "                    with open(key, 'wb') as opened_file:\n",
    "\n",
    "                        dill.dump(model_to_export, opened_file)\n",
    "                    \n",
    "                    #to save the file, the mode must be set as 'wb' (write binary)\n",
    "                    files.download(key)\n",
    "                    print(f\"Scikit-learn model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    with open(model_path, 'wb') as opened_file:\n",
    "\n",
    "                        dill.dump(model_to_export, opened_file)\n",
    "                    \n",
    "                    print(f\"Scikit-learn model successfully exported as {model_path}.\")\n",
    "                    # For exporting a pickle model:\n",
    "                    ## pkl.dump(model_to_export, open(model_path, 'wb'))\n",
    "            \n",
    "            elif ((model_type == 'xgb_regressor')|(model_type == 'xgb_classifier')):\n",
    "                # In both cases, the XGBoost object is already loaded in global\n",
    "                # context memory. So there is already the object for using the\n",
    "                # save_model method, available for both classes (XGBRegressor and\n",
    "                # XGBClassifier).\n",
    "                # We can simply check if it is one type OR the other, since the\n",
    "                # method is the same:\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save_model(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"XGBoost model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save_model(model_path)\n",
    "                    print(f\"XGBoost model successfully exported as {model_path}.\")\n",
    "                    # For exporting a pickle model:\n",
    "                    ## pkl.dump(model_to_export, open(model_path, 'wb'))\n",
    "            \n",
    "            elif (model_type == 'arima'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"ARIMA model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save(model_path)\n",
    "                    print(f\"ARIMA model successfully exported as {model_path}.\")\n",
    "        \n",
    "        print(\"Export of files completed.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Enter a valid action, import or export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for exporting the dataframe as CSV File (to notebook's workspace)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_pd_dataframe_as_csv (dataframe_obj_to_be_exported, new_file_name_without_extension, file_directory_path = None):\n",
    "    \n",
    "    import os\n",
    "    import pandas as pd\n",
    "    \n",
    "    ## WARNING: all files exported from this function are .csv (comma separated values)\n",
    "    \n",
    "    # dataframe_obj_to_be_exported: dataframe object that is going to be exported from the\n",
    "    # function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "    # example: dataframe_obj_to_be_exported = dataset will export the dataset object.\n",
    "    # ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "    \n",
    "    # FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "    # (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"/\" \n",
    "    # or FILE_DIRECTORY_PATH = \"/folder\"\n",
    "    # If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "    # In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "    # new_file_name_without_extension - (string, in quotes): input the name of the \n",
    "    # file without the extension. e.g. new_file_name_without_extension = \"my_file\" \n",
    "    # will export a file 'my_file.csv' to notebook's workspace.\n",
    "    \n",
    "    # Create the complete file path:\n",
    "    file_path = os.path.join(file_directory_path, new_file_name_without_extension)\n",
    "    # Concatenate the extension \".csv\":\n",
    "    file_path = file_path + \".csv\"\n",
    "\n",
    "    dataframe_obj_to_be_exported.to_csv(file_path, index = False)\n",
    "\n",
    "    print(f\"Dataframe {new_file_name_without_extension} exported as CSV file to notebook\\'s workspace as \\'{file_path}\\'.\")\n",
    "    print(\"Warning: if there was a file in this file path, it was replaced by the exported dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_or_download_file_from_colab (action = 'download', file_to_download_from_colab = None):\n",
    "    \n",
    "    # action = 'download' to download the file to the local machine\n",
    "    # action = 'upload' to upload a file from local machine to\n",
    "    # Google Colab's instant memory\n",
    "    \n",
    "    # file_to_download_from_colab = None. This parameter is obbligatory when\n",
    "    # action = 'download'. \n",
    "    # Declare as file_to_download_from_colab the file that you want to download, with\n",
    "    # the correspondent extension.\n",
    "    # It should not be declared in quotes.\n",
    "    # e.g. to download a dictionary named dict, object_to_download_from_colab = 'dict.pkl'\n",
    "    # To download a dataframe named df, declare object_to_download_from_colab = 'df.csv'\n",
    "    # To export a model named keras_model, declare object_to_download_from_colab = 'keras_model.h5'\n",
    " \n",
    "    from google.colab import files\n",
    "    # google.colab library must be imported only in case \n",
    "    # it is going to be used, for avoiding \n",
    "    # AWS compatibility issues.\n",
    "        \n",
    "    if (action == 'upload'):\n",
    "            \n",
    "        print(\"Click on the button for file selection and select the files from your machine that will be uploaded in the Colab environment.\")\n",
    "        print(\"Warning: the files will be removed from Colab memory after the Kernel dies or after the notebook is closed.\")\n",
    "        # this functionality requires the previous declaration:\n",
    "        ## from google.colab import files\n",
    "            \n",
    "        colab_files_dict = files.upload()\n",
    "            \n",
    "        # The files are stored into a dictionary called colab_files_dict where the keys\n",
    "        # are the names of the files and the values are the files themselves.\n",
    "        ## e.g. if you upload a single file named \"dictionary.pkl\", the dictionary will be\n",
    "        ## colab_files_dict = {'dictionary.pkl': file}, where file is actually a big string\n",
    "        ## representing the contents of the file. The length of this value is the size of the\n",
    "        ## uploaded file, in bytes.\n",
    "        ## To access the file is like accessing a value from a dictionary: \n",
    "        ## d = {'key1': 'val1'}, d['key1'] == 'val1'\n",
    "        ## we simply declare the key inside brackets and quotes, the same way we would do for\n",
    "        ## accessing the column of a dataframe.\n",
    "        ## In this example, colab_files_dict['dictionary.pkl'] access the content of the \n",
    "        ## .pkl file, and len(colab_files_dict['dictionary.pkl']) is the size of the .pkl\n",
    "        ## file in bytes.\n",
    "        ## To check the dictionary keys, apply the method .keys() to the dictionary (with empty\n",
    "        ## parentheses): colab_files_dict.keys()\n",
    "            \n",
    "        for key in colab_files_dict.keys():\n",
    "            #loop through each element of the list of keys of the dictionary\n",
    "            # (list colab_files_dict.keys()). Each element is named 'key'\n",
    "            print(f\"User uploaded file {key} with length {len(colab_files_dict[key])} bytes.\")\n",
    "            # The key is the name of the file, and the length of the value\n",
    "            ## correspondent to the key is the file's size in bytes.\n",
    "            ## Notice that the content of the uploaded object must be passed \n",
    "            ## as argument for a proper function to be interpreted. \n",
    "            ## For instance, the content of a xlsx file should be passed as\n",
    "            ## argument for Pandas .read_excel function; the pkl file must be passed as\n",
    "            ## argument for pickle.\n",
    "            ## e.g., if you uploaded 'table.xlsx' and stored it into colab_files_dict you should\n",
    "            ## declare df = pd.read_excel(colab_files_dict['table.xlsx']) to obtain a dataframe\n",
    "            ## df from the uploaded table. Notice that is the value, not the key, that is the\n",
    "            ## argument.\n",
    "                \n",
    "            print(\"The uploaded files are stored into a dictionary object named as colab_files_dict.\")\n",
    "            print(\"Each key from this dictionary is the name of an uploaded file. The value correspondent to that key is the file itself.\")\n",
    "            print(\"The structure of a general Python dictionary is dict = {\\'key1\\': value1}. To access value1, declare file = dict[\\'key1\\'], as if you were accessing a column from a dataframe.\")\n",
    "            print(\"Then, if you uploaded a file named \\'table.xlsx\\', you can access this file as:\")\n",
    "            print(\"uploaded_file = colab_files_dict[\\'table.xlsx\\']\")\n",
    "            print(\"Notice, though, that the object uploaded_file is the whole file content, not a Python object already converted. To convert to a Python object, pass this element as argument for a proper function or method.\")\n",
    "            print(\"In this example, to convert the object uploaded_file to a dataframe, Pandas pd.read_excel function could be used. In the following line, a df dataframe object is obtained from the uploaded file:\")\n",
    "            print(\"df = pd.read_excel(uploaded_file)\")\n",
    "            print(\"Also, the uploaded file itself will be available in the Colaboratory Notebook\\'s workspace.\")\n",
    "            \n",
    "            return colab_files_dict\n",
    "        \n",
    "    elif (action == 'download'):\n",
    "            \n",
    "        if (file_to_download_from_colab is None):\n",
    "                \n",
    "            #No object was declared\n",
    "            print(\"Please, inform a file to download from the notebook\\'s workspace. It should be declared in quotes and with the extension: e.g. \\'table.csv\\'.\")\n",
    "            \n",
    "        else:\n",
    "                \n",
    "            print(\"The file will be downloaded to your computer.\")\n",
    "\n",
    "            files.download(file_to_download_from_colab)\n",
    "\n",
    "            print(f\"File {file_to_download_from_colab} successfully downloaded from Colab environment.\")\n",
    "\n",
    "    else:\n",
    "            \n",
    "            print(\"Please, select a valid action, \\'download\\' or \\'upload\\'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for exporting a list of files from notebook's workspace to AWS Simple Storage Service (S3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_files_to_s3 (list_of_file_names_with_extensions, directory_of_notebook_workspace_storing_files_to_export = None, s3_bucket_name = None, s3_obj_prefix = None):\n",
    "    \n",
    "    import os\n",
    "    import boto3\n",
    "    # boto3 is AWS S3 Python SDK\n",
    "    # sagemaker and boto3 libraries must be imported only in case \n",
    "    # they are going to be used, for avoiding \n",
    "    # Google Colab compatibility issues.\n",
    "    from getpass import getpass\n",
    "    \n",
    "    # list_of_file_names_with_extensions: list containing all the files to export to S3.\n",
    "    # Declare it as a list even if only a single file will be exported.\n",
    "    # It must be a list of strings containing the file names followed by the extensions.\n",
    "    # Example, to a export a single file my_file.ext, where my_file is the name and ext is the\n",
    "    # extension:\n",
    "    # list_of_file_names_with_extensions = ['my_file.ext']\n",
    "    # To export 3 files, file1.ext1, file2.ext2, and file3.ext3:\n",
    "    # list_of_file_names_with_extensions = ['file1.ext1', 'file2.ext2', 'file3.ext3']\n",
    "    # Other examples:\n",
    "    # list_of_file_names_with_extensions = ['Screen_Shot.png', 'dataset.csv']\n",
    "    # list_of_file_names_with_extensions = [\"dictionary.pkl\", \"model.h5\"]\n",
    "    # list_of_file_names_with_extensions = ['doc.pdf', 'model.dill']\n",
    "    \n",
    "    # directory_of_notebook_workspace_storing_files_to_export: directory from notebook's workspace\n",
    "    # from which the files will be exported to S3. Keep it None, or\n",
    "    # directory_of_notebook_workspace_storing_files_to_export = \"/\"; or\n",
    "    # directory_of_notebook_workspace_storing_files_to_export = '' (empty string) to export from\n",
    "    # the root (main) directory.\n",
    "    # Alternatively, set as a string containing only the directories and folders, not the file names.\n",
    "    # Examples: directory_of_notebook_workspace_storing_files_to_export = 'folder1';\n",
    "    # directory_of_notebook_workspace_storing_files_to_export = 'folder1/folder2/'\n",
    "    \n",
    "    # For this function, all exported files must be located in the same directory.\n",
    "    \n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # s3_obj_prefix = None. Keep it None or as an empty string (s3_obj_key_prefix = '')\n",
    "    # to import the whole bucket content, instead of a single object from it.\n",
    "    # Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "    # Suppose that your bucket (admin-created) has four objects with the following object \n",
    "    # keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "    # s3-dg.pdf. The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "    # at the root level of the bucket. If you open the Development/ folder, you see \n",
    "    # the Projects.xlsx object in it.\n",
    "    # Check Amazon documentation:\n",
    "    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\n",
    "    \n",
    "    # In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "    # where 'bucket' is the bucket's name, key_prefix = 'my_path/.../', without the\n",
    "    # 'file.csv' (file name with extension) last part.\n",
    "    \n",
    "    # So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "    # a given folder (directory) of the bucket.\n",
    "    # DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "    # DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "    # Alternatively, provide the full path of a given file if you want to import only it:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "    # where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "    # Attention: after running this function for connecting with AWS Simple Storage System (S3), \n",
    "    # your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "    # The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "    # other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "    # and the prefix. All of these are sensitive information from the organization.\n",
    "    # Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "    # and of removing such information from the strings.\n",
    "    # Remember that these data may contain privilege for accessing the information, so it should not\n",
    "    # be used for non-authorized people.\n",
    "\n",
    "    # Also, remember of deleting the exported from the workspace after finishing the analysis.\n",
    "    # The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "    # workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "    # the notebook's workspace.\n",
    "    \n",
    "    \n",
    "    # Check if directory_of_notebook_workspace_storing_files_to_export is None. \n",
    "    # If it is, make it the root directory:\n",
    "    if ((directory_of_notebook_workspace_storing_files_to_export is None)|(str(directory_of_notebook_workspace_storing_files_to_export) == \"/\")):\n",
    "            \n",
    "            # For the S3 buckets, the path should not start with slash. Assign the empty\n",
    "            # string instead:\n",
    "            directory_of_notebook_workspace_storing_files_to_export = \"\"\n",
    "            print(\"The files will be exported from the notebook\\'s root directory to S3.\")\n",
    "    \n",
    "    elif (str(directory_of_notebook_workspace_storing_files_to_export) == \"\"):\n",
    "        \n",
    "            # Guarantee that the path is the empty string.\n",
    "            # Avoid accessing the else condition, what would raise an error\n",
    "            # since the empty string has no character of index 0\n",
    "            directory_of_notebook_workspace_storing_files_to_export = str(directory_of_notebook_workspace_storing_files_to_export)\n",
    "            print(\"The files will be exported from the notebook\\'s root directory to S3.\")\n",
    "          \n",
    "    else:\n",
    "        # Use the str attribute to guarantee that the path was read as a string:\n",
    "        directory_of_notebook_workspace_storing_files_to_export = str(directory_of_notebook_workspace_storing_files_to_export)\n",
    "            \n",
    "        if(directory_of_notebook_workspace_storing_files_to_export[0] == \"/\"):\n",
    "            # the first character is the slash. Let's remove it\n",
    "\n",
    "            # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "            # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "            # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "            # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "            # The slash is character 0. Then, we want all characters from character 1 (the\n",
    "            # second) to character len(str(path_to_store_imported_s3_bucket)) - 1, the index\n",
    "            # of the last character. So, we can slice the string from position 1 to position\n",
    "            # the slicing syntax is: string[1:] - all string characters from character 1\n",
    "            # string[:10] - all string characters from character 10-1 = 9 (including 9); or\n",
    "            # string[1:10] - characters from 1 to 9\n",
    "            # So, slice the whole string, starting from character 1:\n",
    "            directory_of_notebook_workspace_storing_files_to_export = directory_of_notebook_workspace_storing_files_to_export[1:]\n",
    "            # attention: even though strings may be seem as list of characters, that can be\n",
    "            # sliced, we cannot neither simply assign a character to a given position nor delete\n",
    "            # a character from a position.\n",
    "\n",
    "    # Ask the user to provide the credentials:\n",
    "    ACCESS_KEY = input(\"Enter your AWS Access Key ID here (in the right). It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "    print(\"\\n\") # line break\n",
    "    SECRET_KEY = getpass(\"Enter your password (Secret key) here (in the right). It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "        \n",
    "    # The use of 'getpass' instead of 'input' hide the password behind dots.\n",
    "    # So, the password is not visible by other users and cannot be copied.\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    print(\"WARNING: The bucket\\'s name, the prefix, the AWS access key ID, and the AWS Secret access key are all sensitive information, which may grant access to protected information from the organization.\\n\")\n",
    "    print(\"After finish exporting data to S3, remember of removing these information from the notebook, specially if it is going to be shared. Also, remember of removing the files from the workspace.\\n\")\n",
    "    print(\"The cost for storing files in Simple Storage Service is quite inferior than the one for storing directly in SageMaker workspace. Also, files stored in S3 may be accessed for other users than those with access the notebook\\'s workspace.\\n\")\n",
    "\n",
    "    # Check if the user actually provided the mandatory inputs, instead\n",
    "    # of putting None or empty string:\n",
    "    if ((ACCESS_KEY is None) | (ACCESS_KEY == '')):\n",
    "        print(\"AWS Access Key ID is missing. It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "        return \"error\"\n",
    "    elif ((SECRET_KEY is None) | (SECRET_KEY == '')):\n",
    "        print(\"AWS Secret Access Key is missing. It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "        return \"error\"\n",
    "    elif ((s3_bucket_name is None) | (s3_bucket_name == '')):\n",
    "        print (\"Please, enter a valid S3 Bucket\\'s name. Do not add sub-directories or folders (prefixes), only the name of the bucket itself.\")\n",
    "        return \"error\"\n",
    "    \n",
    "    else:\n",
    "        # Use the str attribute to guarantee that all AWS parameters were properly read as strings, and not as\n",
    "        # other variables (like integers or floats):\n",
    "        ACCESS_KEY = str(ACCESS_KEY)\n",
    "        SECRET_KEY = str(SECRET_KEY)\n",
    "        s3_bucket_name = str(s3_bucket_name)\n",
    "\n",
    "    if(s3_bucket_name[0] == \"/\"):\n",
    "        # the first character is the slash. Let's remove it\n",
    "\n",
    "        # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "        # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "        # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "        # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "        # So, slice the whole string, starting from character 1 (as did for \n",
    "        # path_to_store_imported_s3_bucket):\n",
    "        s3_bucket_name = s3_bucket_name[1:]\n",
    "\n",
    "    # Remove any possible trailing (white and tab spaces) spaces\n",
    "    # That may be present in the string. Use the Python string\n",
    "    # rstrip method, which is the equivalent to the Trim function:\n",
    "    # When no arguments are provided, the whitespaces and tabulations\n",
    "    # are the removed characters\n",
    "    # https://www.w3schools.com/python/ref_string_rstrip.asp?msclkid=ee2d05c3c56811ecb1d2189d9f803f65\n",
    "    s3_bucket_name = s3_bucket_name.rstrip()\n",
    "    ACCESS_KEY = ACCESS_KEY.rstrip()\n",
    "    SECRET_KEY = SECRET_KEY.rstrip()\n",
    "    # Since the user manually inputs the parameters ACCESS and SECRET_KEY,\n",
    "    # it is easy to input whitespaces without noticing that.\n",
    "\n",
    "    # Now process the non-obbligatory parameter.\n",
    "    # Check if a prefix was passed as input parameter. If so, we must select only the names that start with\n",
    "    # The prefix.\n",
    "    # Example: in the bucket 'my_bucket' we have a directory 'dir1'.\n",
    "    # In the main (root) directory, we have a file 'file1.json' like: '/file1.json'\n",
    "    # If we pass the prefix 'dir1', we want only the files that start as '/dir1/'\n",
    "    # such as: 'dir1/file2.json', excluding the file in the main (root) directory and excluding the files in other\n",
    "    # directories. Also, we want to eliminate the file names with no extensions, like 'dir1/' or 'dir1/dir2',\n",
    "    # since these object names represent folders or directories, not files.\t\n",
    "\n",
    "    if (s3_obj_prefix is None):\n",
    "        print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "        print (f\"Then, exporting to \\'{s3_bucket_name}\\' root (main) directory.\\n\")\n",
    "        # s3_path: path that the file should have in S3:\n",
    "        s3_path = \"\" # empty string for the root directory\n",
    "    elif ((s3_obj_prefix == \"/\") | (s3_obj_prefix == '')):\n",
    "        # The root directory in the bucket must not be specified starting with the slash\n",
    "        # If the root \"/\" or the empty string '' is provided, make\n",
    "        # it equivalent to None (no directory)\n",
    "        print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "        print (f\"Then, exporting to \\'{s3_bucket_name}\\' root (main) directory.\\n\")\n",
    "        # s3_path: path that the file should have in S3:\n",
    "        s3_path = \"\" # empty string for the root directory\n",
    "    \n",
    "    else:\n",
    "        # Since there is a prefix, use the str attribute to guarantee that the path was read as a string:\n",
    "        s3_obj_prefix = str(s3_obj_prefix)\n",
    "            \n",
    "        if(s3_obj_prefix[0] == \"/\"):\n",
    "            # the first character is the slash. Let's remove it\n",
    "\n",
    "            # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "            # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "            # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "            # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "            # So, slice the whole string, starting from character 1 (as did for \n",
    "            # path_to_store_imported_s3_bucket):\n",
    "            s3_obj_prefix = s3_obj_prefix[1:]\n",
    "\n",
    "        # Remove any possible trailing (white and tab spaces) spaces\n",
    "        # That may be present in the string. Use the Python string\n",
    "        # rstrip method, which is the equivalent to the Trim function:\n",
    "        s3_obj_prefix = s3_obj_prefix.rstrip()\n",
    "            \n",
    "        # s3_path: path that the file should have in S3:\n",
    "        # Make the path the prefix itself, since there is a prefix:\n",
    "        s3_path = s3_obj_prefix\n",
    "            \n",
    "        print(\"AWS Access Credentials, and bucket\\'s prefix, object or subdirectory provided.\\n\")\t\n",
    "\n",
    "            \n",
    "        print (\"Starting connection with the S3 bucket.\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Start S3 client as the object 's3_client'\n",
    "            s3_client = boto3.resource('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = SECRET_KEY)\n",
    "        \n",
    "            print(f\"Credentials accepted by AWS. S3 client successfully started.\\n\")\n",
    "            # An object 'data_table.xlsx' in the main (root) directory of the s3_bucket is stored in Python environment as:\n",
    "            # s3.ObjectSummary(bucket_name='bucket_name', key='data_table.xlsx')\n",
    "            # The name of each object is stored as the attribute 'key' of the object.\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect to AWS Simple Storage Service (S3). Review if your credentials are correct.\")\n",
    "            print(\"The variable \\'access_key\\' must be set as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"The variable \\'secret_key\\' must be set as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            # Connect to the bucket specified as 'bucket_name'.\n",
    "            # The bucket is started as the object 's3_bucket':\n",
    "            s3_bucket = s3_client.Bucket(s3_bucket_name)\n",
    "            print(f\"Connection with bucket \\'{s3_bucket_name}\\' stablished.\\n\")\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect with the bucket, which usually happens when declaring a wrong bucket\\'s name.\") \n",
    "            print(\"Check the spelling of your bucket_name string and remember that it must be all in lower-case.\\n\")\n",
    "                \n",
    "        # Now, let's obtain the lists of all file paths in the notebook's workspace and\n",
    "        # of the paths that the files should have in S3, after being exported.\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            # start the lists:\n",
    "            workspace_full_paths = []\n",
    "            s3_full_paths = []\n",
    "            \n",
    "            # Get the total of files in list_of_file_names_with_extensions:\n",
    "            total_of_files = len(list_of_file_names_with_extensions)\n",
    "            \n",
    "            # And Loop through all elements, named 'my_file' from the list\n",
    "            for my_file in list_of_file_names_with_extensions:\n",
    "                \n",
    "                # Get the full path in the notebook's workspace:\n",
    "                workspace_file_full_path = os.path.join(directory_of_notebook_workspace_storing_files_to_export, my_file)\n",
    "                # Get the full path that the file will have in S3:\n",
    "                s3_file_full_path = os.path.join(s3_path, my_file)\n",
    "                \n",
    "                # Append these paths to the correspondent lists:\n",
    "                workspace_full_paths.append(workspace_file_full_path)\n",
    "                s3_full_paths.append(s3_file_full_path)\n",
    "                \n",
    "            # Now, both lists have the same number of elements. For an element (file) i,\n",
    "            # workspace_full_paths has the full file path in notebook's workspace, and\n",
    "            # s3_full_paths has the path that the new file should have in S3 bucket.\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"The function returned an error when trying to access the list of files. Declare it as a list of strings, even if there is a single element in the list.\")\n",
    "            print(\"Example: list_of_file_names_with_extensions = [\\'my_file.ext\\']\\n\")\n",
    "            return \"error\"\n",
    "        \n",
    "        \n",
    "        # Now, loop through all elements i from the lists.\n",
    "        # The first elements of the lists have index 0; the last elements have index\n",
    "        # total_of_files - 1, since there are 'total_of_files' elements:\n",
    "        \n",
    "        # Then, export the correspondent element to S3:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            for i in range(total_of_files):\n",
    "                # goes from i = 0 to i = total_of_files - 1\n",
    "\n",
    "                # get the element from list workspace_file_full_path \n",
    "                # (original path of file i, from which it will be exported):\n",
    "                PATH_IN_WORKSPACE = workspace_full_paths[i]\n",
    "\n",
    "                # get the correspondent element of list s3_full_paths\n",
    "                # (path that the file i should have in S3, after being exported):\n",
    "                S3_FILE_PATH = s3_full_paths[i]\n",
    "\n",
    "                # Start the new object in the bucket previously started as 's3_bucket'.\n",
    "                # Start it with the specified prefix, in S3_FILE_PATH:\n",
    "                new_s3_object = s3_bucket.Object(S3_FILE_PATH)\n",
    "                \n",
    "                # Finally, upload the file in PATH_IN_WORKSPACE.\n",
    "                # Make new_s3_object the exported file:\n",
    "            \n",
    "                # Upload the selected object from the workspace path PATH_IN_WORKSPACE\n",
    "                # to the S3 path specified as S3_FILE_PATH.\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" exports a xlsx file named 'my_table' to the notebook's main (root)\n",
    "                # directory\n",
    "                new_s3_object.upload_file(Filename = PATH_IN_WORKSPACE)\n",
    "\n",
    "                print(f\"The file \\'{list_of_file_names_with_extensions[i]}\\' was successfully exported from notebook\\'s workspace to AWS Simple Storage Service (S3).\\n\")\n",
    "\n",
    "                \n",
    "            print(\"Finished exporting the files from the the notebook\\'s workspace to S3 bucket. It may take a couple of minutes untill they be shown in S3 environment.\\n\") \n",
    "            print(\"Do not forget to delete these copies after finishing the analysis. They will remain stored in the bucket.\\n\")\n",
    "\n",
    "\n",
    "        except:\n",
    "\n",
    "            # Run this code for any other exception that may happen (no exception error\n",
    "            # specified, so any exception runs the following code).\n",
    "            # Check: https://pythonbasics.org/try-except/?msclkid=4f6b4540c5d011ecb1fe8a4566f632a6\n",
    "            # for seeing how to handle successive exceptions\n",
    "\n",
    "            print(\"Attention! The function raised an exception error, which is probably due to the AWS Simple Storage Service (S3) permissions.\")\n",
    "            print(\"Before running again this function, check this quick guide for configuring the permission roles in AWS.\\n\")\n",
    "            print(\"It is necessary to create an user with full access permissions to interact with S3 from SageMaker. To configure the User, go to the upper ribbon of AWS, click on Services, and select IAM – Identity and Access Management.\")\n",
    "            print(\"1. In IAM\\'s lateral panel, search for \\'Users\\' in the group of Access Management.\")\n",
    "            print(\"2. Click on the \\'Add users\\' button.\")\n",
    "            print(\"3. Set an user name in the text box \\'User name\\'.\")\n",
    "            print(\"Attention: users and S3 buckets cannot be written in upper case. Also, selecting a name already used by an Amazon user or bucket will raise an error message.\\n\")\n",
    "            print(\"4. In the field \\'Select type of Access to AWS\\'-\\'Select type of AWS credentials\\' select the option \\'Access key - Programmatic access\\'. After that, click on the button \\'Next: Permissions\\'.\")\n",
    "            print(\"5. In the field \\'Set Permissions\\', keep the \\'Add user to a group\\' button marked.\")\n",
    "            print(\"6. In the field \\'Add user to a group\\', click on \\'Create group\\' (alternatively, you can be added to a group already configured or copy the permissions of another user.\")\n",
    "            print(\"7. In the text box \\'Group\\'s name\\', set a name for the new group of permissions.\")\n",
    "            print(\"8. In the search bar below (\\'Filter politics\\'), search for a politics that fill your needs, and check the option button on the left of this politic. The politics \\'AmazonS3FullAccess\\' grants full access to the S3 content.\")\n",
    "            print(\"9. Finally, click on \\'Create a group\\'.\")\n",
    "            print(\"10. After the group is created, it will appear with a check box marked, over the previous groups. Keep it marked and click on the button \\'Next: Tags\\'.\")\n",
    "            print(\"11. Create and note down the Access key ID and Secret access key. You can also download a comma separated values (CSV) file containing the credentials for future use.\")\n",
    "            print(\"ATTENTION: These parameters are required for accessing the bucket\\'s content from any application, including AWS SageMaker.\")\n",
    "            print(\"12. Click on \\'Next: Review\\' and review the user credentials information and permissions.\")\n",
    "            print(\"13. Click on \\'Create user\\' and click on the download button to download the CSV file containing the user credentials information.\")\n",
    "            print(\"The headers of the CSV file (the stored fields) is: \\'User name, Password, Access key ID, Secret access key, Console login link\\'.\")\n",
    "            print(\"You need both the values indicated as \\'Access key ID\\' and as \\'Secret access key\\' to fetch the S3 bucket.\")\n",
    "            print(\"\\n\") # line break\n",
    "            print(\"After acquiring the necessary user privileges, use the boto3 library to export the file from the notebook’s workspace to the bucket (i.e., to upload a file to the bucket).\")\n",
    "            print(\"For exporting the file as a new bucket\\'s file use the following code:\\n\")\n",
    "            print(\"1. Set a variable \\'access_key\\' as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"2. Set a variable \\'secret_key\\' as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "            print(\"3. Set a variable \\'bucket_name\\' as a string containing only the name of the bucket. Do not add subdirectories, folders (prefixes), or file names.\")\n",
    "            print(\"Example: if your bucket is named \\'my_bucket\\' and its main directory contains folders like \\'folder1\\', \\'folder2\\', etc, do not declare bucket_name = \\'my_bucket/folder1\\', even if you only want files from folder1.\")\n",
    "            print(\"ALWAYS declare only the bucket\\'s name: bucket_name = \\'my_bucket\\'.\")\n",
    "            print(\"4. Set a variable \\'file_path_in_workspace\\' containing the path of the file in notebook’s workspace. The file will be exported from “file_path_in_workspace” to the S3 bucket.\")\n",
    "            print(\"If the file is stored in the notebook\\'s root (main) directory: file_path = \\\"my_file.ext\\\".\")\n",
    "            print(\"If the path of the file in the notebook workspace is: \\'dir1/…/dirN/my_file.ext\\', where dirN is the N-th subdirectory, and dir1 is a folder or directory of the main (root) bucket\\'s directory: file_path = \\\"dir1/…/dirN/my_file.ext\\\".\")\n",
    "            print(\"5. Set a variable named \\'file_path_in_s3\\' containing the path from the bucket’s subdirectories to the file you want to fetch. Include the file name and its extension.\")\n",
    "            print(\"6. Finally, declare the following code, which refers to the defined variables:\\n\")\n",
    "\n",
    "            # Let's use triple quotes to declare a formated string\n",
    "            example_code = \"\"\"\n",
    "                import boto3\n",
    "                # Start S3 client as the object 's3_client'\n",
    "                s3_client = boto3.resource('s3', aws_access_key_id = access_key, aws_secret_access_key = secret_key)\n",
    "                # Connect to the bucket specified as 'bucket_name'.\n",
    "                # The bucket is started as the object 's3_bucket':\n",
    "                s3_bucket = s3_client.Bucket(bucket_name)\n",
    "                # Start the new object in the bucket previously started as 's3_bucket'.\n",
    "                # Start it with the specified prefix, in file_path_in_s3:\n",
    "                new_s3_object = s3_bucket.Object(file_path_in_s3)\n",
    "                # Finally, upload the file in file_path_in_workspace.\n",
    "                # Make new_s3_object the exported file:\n",
    "                # Upload the selected object from the workspace path file_path_in_workspace\n",
    "                # to the S3 path specified as file_path_in_s3.\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" exports a xlsx file named 'my_table' to \n",
    "                # the notebook's main (root) directory.\n",
    "                new_s3_object.upload_file(Filename = file_path_in_workspace)\n",
    "                \"\"\"\n",
    "\n",
    "            print(example_code)\n",
    "\n",
    "            print(\"An object \\'my_file.ext\\' in the main (root) directory of the s3_bucket is stored in Python environment as:\")\n",
    "            print(\"\"\"s3.ObjectSummary(bucket_name='bucket_name', key='my_file.ext'\"\"\") \n",
    "            # triple quotes to keep the internal quotes without using too much backslashes \"\\\" (the ignore next character)\n",
    "            print(\"Then, the name of each object is stored as the attribute \\'key\\' of the object. To view all objects, we can loop through their \\'key\\' attributes:\\n\")\n",
    "            example_code = \"\"\"\n",
    "                # Loop through all objects of the bucket:\n",
    "                for stored_obj in s3_bucket.objects.all():\t\t\n",
    "                    # Loop through all elements 'stored_obj' from s3_bucket.objects.all()\n",
    "                    # Which stores the ObjectSummary for all objects in the bucket s3_bucket:\n",
    "                    # Print the object’s names:\n",
    "                    print(stored_obj.key)\n",
    "                    \"\"\"\n",
    "\n",
    "            print(example_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "## **Call the functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = 'aws'\n",
    "# SOURCE = 'google' for mounting the google drive;\n",
    "# SOURCE = 'aws' for accessing an AWS S3 bucket\n",
    "\n",
    "## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN SOURCE == 'aws':\n",
    "\n",
    "PATH_TO_STORE_IMPORTED_S3_BUCKET = ''\n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET: path of the Python environment to which the\n",
    "# S3 bucket contents will be imported. If it is None; or if it is an empty string; or if \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = '/', bucket will be imported to the root path. \n",
    "# Alternatively, input the path as a string (in quotes). e.g. \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = 'copied_s3_bucket'\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for fetching AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "mount_storage_system (source = SOURCE, path_to_store_imported_s3_bucket = PATH_TO_STORE_IMPORTED_S3_BUCKET, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "azdata_cell_guid": "0d4292a3-e6c6-41b2-bbda-0743ee04195e",
    "id": "g3zcdInJyHHm"
   },
   "outputs": [],
   "source": [
    "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "## JSON, txt, or CSV (comma separated values) files.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
    "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "# FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "# Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
    "\n",
    "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
    "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True \n",
    "# if you want to read a file with txt extension containing a text formatted as JSON \n",
    "# (but not saved as JSON).\n",
    "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the \n",
    "# function (below) must be set. If not, an error message will be raised.\n",
    "\n",
    "HOW_MISSING_VALUES_ARE_REGISTERED = None\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "# empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "# This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "#‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "# ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "# If a different denomination is used, indicate it as a string. e.g.\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "# If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "# only in column 'numeric_col', you can specify the following dictionary:\n",
    "# how_missing_values_are_registered = {'numeric-col': 0}\n",
    "\n",
    "    \n",
    "HAS_HEADER = True\n",
    "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
    "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
    "\n",
    "DECIMAL_SEPARATOR = '.'\n",
    "# DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "# the decimal separator. Alternatively, specify here the separator.\n",
    "# e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "# It manipulates the argument 'decimal' from Pandas functions.\n",
    "\n",
    "TXT_CSV_COL_SEP = \"comma\"\n",
    "# txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "# or 'csv'. It informs how the different columns are separated.\n",
    "# Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "# for columns separated by comma;\n",
    "# txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "# for columns separated by simple spaces.\n",
    "# You can also set a specific separator as string. For example:\n",
    "# txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "# is used as separator for the columns - '\\t' represents the tab character).\n",
    "\n",
    "SHEET_TO_LOAD = None\n",
    "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
    "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
    "# will be loaded.\n",
    "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
    "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
    "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "# name to load the sheet with that name.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = load_pandas_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, how_missing_values_are_registered = HOW_MISSING_VALUES_ARE_REGISTERED, has_header = HAS_HEADER, decimal_separator = DECIMAL_SEPARATOR, txt_csv_col_sep = TXT_CSV_COL_SEP, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Converting JSON object to dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "azdata_cell_guid": "0d4292a3-e6c6-41b2-bbda-0743ee04195e",
    "id": "g3zcdInJyHHm"
   },
   "outputs": [],
   "source": [
    "# JSON object in terms of Python structure: list of dictionaries, where each value of a\n",
    "# dictionary may be a dictionary or a list of dictionaries (nested structures).\n",
    "# example of highly nested structure saved as a list 'json_formatted_list'. Note that the same\n",
    "# structure could be declared and stored into a string variable. For instance, if you have a txt\n",
    "# file containing JSON, you could read the txt and save its content as a string.\n",
    "# json_formatted_list = [{'field1': val1, 'field2': {'dict_val': dict_val}, 'field3': [{\n",
    "# 'nest1': nest_val1}, {'nest2': nestval2}]}, {'field1': val1, 'field2': {'dict_val': dict_val}, \n",
    "# 'field3': [{'nest1': nest_val1}, {'nest2': nestval2}]}]\n",
    "\n",
    "JSON_OBJ_TO_CONVERT = json_object #Alternatively: object containing the JSON to be converted\n",
    "\n",
    "# JSON_OBJ_TO_CONVERT: object containing JSON, or string with JSON content to parse.\n",
    "# Objects may be: string with JSON formatted text;\n",
    "# list with nested dictionaries (JSON formatted);\n",
    "# dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "\n",
    "JSON_OBJ_TYPE = 'list'\n",
    "# JSON_OBJ_TYPE = 'list', in case the object was saved as a list of dictionaries (JSON format)\n",
    "# JSON_OBJ_TYPE = 'string', in case it was saved as a string (text) containing JSON.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: [{'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]}]\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = json_obj_to_pandas_dataframe (json_obj_to_convert = JSON_OBJ_TO_CONVERT, json_obj_type = JSON_OBJ_TYPE, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Concatenating (SQL UNION) multiple dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_DATAFRAMES = [dataset1, dataset2]\n",
    "# LIST_OF_DATAFRAMES must be a list containing the dataframe objects\n",
    "# example: list_of_dataframes = [df1, df2, df3, df4]\n",
    "# Notice that the dataframes are objects, not strings. Therefore, they should not\n",
    "# be declared inside quotes.\n",
    "# There is no limit of dataframes. In this example, we will concatenate 4 dataframes.\n",
    "# If LIST_OF_DATAFRAMES = [df1, df2, df3] we would concatenate 3, and if\n",
    "# LIST_OF_DATAFRAMES = [df1, df2, df3, df4, df5] we would concatenate 5 dataframes.\n",
    "\n",
    "WHAT_TO_APPEND = 'rows'\n",
    "# WHAT_TO_APPEND = 'rows' for appending the rows from one dataframe\n",
    "# into the other; WHAT_TO_APPEND = 'columns' for appending the columns\n",
    "# from one dataframe into the other (horizontal or lateral append).\n",
    "\n",
    "IGNORE_INDEX_ON_UNION = True # Alternatively: True or False\n",
    "\n",
    "SORT_VALUES_ON_UNION = True # Alternatively: True or False\n",
    "\n",
    "UNION_JOIN_TYPE = None\n",
    "# JOIN can be 'inner' to perform an inner join, eliminating the missing values\n",
    "# The default (None) is 'outer': the dataframes will be stacked on the columns with\n",
    "# same names but, in case there is no correspondence, the row will present a missing\n",
    "# value for the columns which are not present in one of the dataframes.\n",
    "# When using the 'inner' method, only the common columns will remain.\n",
    "# Alternatively, keep UNION_JOIN_TYPE = None for the standard outer join; or set\n",
    "# UNION_JOIN_TYPE = \"inner\" (inside quotes) for using the inner join.\n",
    "    \n",
    "#These 3 last parameters are the same from Pandas .concat method:\n",
    "# IGNORE_INDEX_ON_UNION = ignore_index;\n",
    "# SORT_VALUES_ON_UNION = sort\n",
    "# UNION_JOIN_TYPE = join\n",
    "# Check Datacamp course Joining Data with pandas, Chap.3, \n",
    "# Advanced Merging and Concatenating\n",
    "    \n",
    "\n",
    "#New dataframe saved as concat_df. Simply modify this object on the left of equality:\n",
    "concat_df = UNION_DATAFRAMES (list_of_dataframes = LIST_OF_DATAFRAMES, what_to_append = WHAT_TO_APPEND, ignore_index_on_union = IGNORE_INDEX_ON_UNION, sort_values_on_union = SORT_VALUES_ON_UNION, union_join_type = UNION_JOIN_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Filtering (selecting); or renaming columns of the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "MODE = 'filter'\n",
    "# MODE = 'filter' for filtering only the list of columns passed as cols_list;\n",
    "# MODE = 'rename' for renaming the columns with the names passed as cols_list.\n",
    "\n",
    "COLS_LIST = ['column1', 'column2', 'column3']\n",
    "# COLS_LIST = list of strings containing the names (headers) of the columns to select\n",
    "# (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "# For instance: COLS_LIST = ['col1', 'col2', 'col3'] will \n",
    "# select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "# Declare the names inside quotes.\n",
    "# Simply substitute the list by the list of columns that you want to select; or the\n",
    "# list of the new names you want to give to the dataset columns.\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = col_filter_rename (df = DATASET, cols_list = COLS_LIST, mode = MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Splitting the dataframe into train and test subsets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = X\n",
    "# X_df = subset of predictive variables (dataframe). Alternatively, modify X, not X_df\n",
    "Y = y\n",
    "# Y = subset of response variable (series). Alternatively, modify y, not Y\n",
    "\n",
    "PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 75   \n",
    "# percent_of_data_used_for_model_training: float from 0 to 100,\n",
    "# representing the percent of data used for training the model\n",
    "\n",
    "# Subset and series destined to training returned as X_train, y_train;\n",
    "# Subset and series separated for testing returned as X_test, y_test;\n",
    "# Simply modify these objects on the left of equality:\n",
    "X_train, X_test, y_train, y_test = split_data_into_train_and_test (X = X_df, y = Y, percent_of_data_used_for_model_training = PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Retrieving the list of classes used for training the classification models**\n",
    "- The number of classes that are actually in the dataset used for training may be lower than the actual number of possible classes. \n",
    "    - So, also use this function to check if the training set is representing all the classes.\n",
    "- In deep-learning models, we must set the number of neurons of the output layer as the actual number of possible classes. Therefore, it is important that the training is performed with a representative dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = 'generic_model'\n",
    "# MODEL_TYPE = 'sklearn', model_type = 'xgb', for analyzing an\n",
    "# object returned from the training of data with a Sckit-learn\n",
    "# or a XGBoost model;\n",
    "# or MODEL_TYPE = 'generic_model' ('generic_model' can be used\n",
    "# for any type of model, including 'sklearn' and 'xgb' themselves\n",
    "# and deep learning models. That is because it relies on a more\n",
    "# generic programming that analyzes the training data itself, not\n",
    "# the model object).\n",
    "\n",
    "Y_TRAIN = None\n",
    "# Y_TRAIN = subset of response variable (train series)\n",
    "# keep Y_TRAIN = None if model_type = 'sklearn'\n",
    "# Alternatively, modify y_train (or None), not Y_TRAIN\n",
    "# e.g. Y_TRAIN = y_train\n",
    "\n",
    "SKLEARN_MODEL_OBJECT = logistic_reg_model\n",
    "# SKLEARN_MODEL_OBJECT: object containing the Scikit-learn model \n",
    "# that will be analyzed. \n",
    "# e.g. SKLEARN_MODEL_OBJECT = logistic_reg_model\n",
    "# This parameter should be provided for sklearn models; \n",
    "# For other models (XGBoost, Keras, etc): \n",
    "# set SKLEARN_MODEL_OBJECT = None.\n",
    "\n",
    "# Total (number) of classes present in the training set returned as\n",
    "# number_of_classes;\n",
    "# List of classes used to train the model returned as list_of_classes;\n",
    "# Simply modify these objects on the left of equality:\n",
    "number_of_classes, list_of_classes = retrieve_classes_used_for_training (model_type = MODEL_TYPE, y_train = Y_TRAIN, sklearn_model_object = SKLEARN_MODEL_OBJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ordinary Least Squares (OLS) Linear Regression**\n",
    "- This function runs the 'bar_chart' function. Certify that this function was properly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN = X_train\n",
    "# X_TRAIN = subset of predictive variables (dataframe).\n",
    "# Alternatively, modify X_train, not X_TRAIN\n",
    "Y_TRAIN = y_train\n",
    "# Y_TRAIN = subset of response variable (series).\n",
    "# Alternatively, modify y_train, not Y_TRAIN\n",
    "\n",
    "# Model object returned as ols_linear_reg_model;\n",
    "# Feature ranking dataframe returned as ols_feature_importance_df;\n",
    "# Simply modify these objects on the left of equality:\n",
    "ols_linear_reg_model, ols_feature_importance_df = ols_linear_reg (X_train = X_TRAIN, y_train = Y_TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ridge Linear Regression**\n",
    "- This function runs the 'bar_chart' function. Certify that this function was properly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN = X_train\n",
    "# X_TRAIN = subset of predictive variables (dataframe).\n",
    "# Alternatively, modify X_train, not X_TRAIN\n",
    "Y_TRAIN = y_train\n",
    "# Y_TRAIN = subset of response variable (series).\n",
    "# Alternatively, modify y_train, not Y_TRAIN\n",
    "\n",
    "ALPHA_HYPERPARAMETER = 1.0\n",
    "MAXIMUM_OF_ALLOWED_ITERATIONS = 20000\n",
    "# hyperparameters: alpha = ALPHA_HYPERPARAMETER and MAXIMUM_OF_ALLOWED_ITERATIONS = max_iter\n",
    "\n",
    "# MAXIMUM_OF_ALLOWED_ITERATIONS = integer representing the maximum number of iterations\n",
    "# that the optimization algorithm can perform. Depending on data, convergence may not be\n",
    "# reached within this limit, so you may need to increase this hyperparameter.\n",
    "\n",
    "# ALPHA_HYPERPARAMETER is the regularization strength and must be a positive float value. \n",
    "# Regularization improves the conditioning of the problem and reduces the variance \n",
    "# of the estimates. Larger values specify stronger regularization.\n",
    "# ALPHA_HYPERPARAMETER = 0 is equivalent to an ordinary least square, solved by the \n",
    "# LinearRegression object. For numerical reasons, using ALPHA_HYPERPARAMETER = 0 \n",
    "# is not advised. Given this, you should use the ols_linear_reg function instead.\n",
    "\n",
    "# Model object returned as ridge_linear_reg_model;\n",
    "# Feature ranking dataframe returned as ridge_feature_importance_df;\n",
    "# Simply modify these objects on the left of equality:\n",
    "ridge_linear_reg_model, ridge_feature_importance_df = ridge_linear_reg (X_train = X_TRAIN, y_train = Y_TRAIN, alpha_hyperparameter = ALPHA_HYPERPARAMETER, maximum_of_allowed_iterations = MAXIMUM_OF_ALLOWED_ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Lasso Linear Regression**\n",
    "- This function runs the 'bar_chart' function. Certify that this function was properly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN = X_train\n",
    "# X_TRAIN = subset of predictive variables (dataframe).\n",
    "# Alternatively, modify X_train, not X_TRAIN\n",
    "Y_TRAIN = y_train\n",
    "# Y_TRAIN = subset of response variable (series).\n",
    "# Alternatively, modify y_train, not Y_TRAIN\n",
    "\n",
    "ALPHA_HYPERPARAMETER = 1.0\n",
    "MAXIMUM_OF_ALLOWED_ITERATIONS = 20000\n",
    "# hyperparameters: alpha = ALPHA_HYPERPARAMETER and MAXIMUM_OF_ALLOWED_ITERATIONS = max_iter\n",
    "\n",
    "# MAXIMUM_OF_ALLOWED_ITERATIONS = integer representing the maximum number of iterations\n",
    "# that the optimization algorithm can perform. Depending on data, convergence may not be\n",
    "# reached within this limit, so you may need to increase this hyperparameter.\n",
    "\n",
    "# ALPHA_HYPERPARAMETER is the regularization strength and must be a positive float value. \n",
    "# Regularization improves the conditioning of the problem and reduces the variance \n",
    "# of the estimates. Larger values specify stronger regularization.\n",
    "# ALPHA_HYPERPARAMETER = 0 is equivalent to an ordinary least square, solved by the \n",
    "# LinearRegression object. For numerical reasons, using ALPHA_HYPERPARAMETER = 0 \n",
    "# is not advised. Given this, you should use the ols_linear_reg function instead.\n",
    "\n",
    "# Model object returned as lasso_linear_reg_model;\n",
    "# Feature ranking dataframe returned as lasso_feature_importance_df;\n",
    "# Simply modify these objects on the left of equality:\n",
    "lasso_linear_reg_model, lasso_feature_importance_df = lasso_linear_reg (X_train = X_TRAIN, y_train = Y_TRAIN, alpha_hyperparameter = ALPHA_HYPERPARAMETER, maximum_of_allowed_iterations = MAXIMUM_OF_ALLOWED_ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Elastic Net Linear Regression**\n",
    "- This function runs the 'bar_chart' function. Certify that this function was properly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN = X_train\n",
    "# X_TRAIN = subset of predictive variables (dataframe).\n",
    "# Alternatively, modify X_train, not X_TRAIN\n",
    "Y_TRAIN = y_train\n",
    "# Y_TRAIN = subset of response variable (series).\n",
    "# Alternatively, modify y_train, not Y_TRAIN\n",
    "\n",
    "ALPHA_HYPERPARAMETER = 1.0\n",
    "MAXIMUM_OF_ALLOWED_ITERATIONS = 20000\n",
    "L1_RATIO_HYPERPARAMETER = 0.5\n",
    "# hyperparameters: alpha = ALPHA_HYPERPARAMETER; MAXIMUM_OF_ALLOWED_ITERATIONS = max_iter\n",
    "# and L1_RATIO_HYPERPARAMETER = l1_ratio\n",
    "\n",
    "# MAXIMUM_OF_ALLOWED_ITERATIONS = integer representing the maximum number of iterations\n",
    "# that the optimization algorithm can perform. Depending on data, convergence may not be\n",
    "# reached within this limit, so you may need to increase this hyperparameter.\n",
    "\n",
    "# ALPHA_HYPERPARAMETER is the regularization strength and must be a positive float value. \n",
    "# Regularization improves the conditioning of the problem and reduces the variance \n",
    "# of the estimates. Larger values specify stronger regularization.\n",
    "\n",
    "# L1_RATIO_HYPERPARAMETER is The ElasticNet mixing parameter (float), with 0 <= l1_ratio <= 1. \n",
    "# For L1_RATIO_HYPERPARAMETER = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is an L1 penalty. \n",
    "# For 0 < L1_RATIO_HYPERPARAMETER < 1, the penalty is a combination of L1 and L2.\n",
    "\n",
    "# ALPHA_HYPERPARAMETER = 0 and L1_RATIO_HYPERPARAMETER = 0 is equivalent to an ordinary \n",
    "# least square, solved by the LinearRegression object. For numerical reasons, \n",
    "# using ALPHA_HYPERPARAMETER = 0 and L1_RATIO_HYPERPARAMETER = 0 is not advised. \n",
    "# Given this, you should use the ols_linear_reg function instead.\n",
    "\n",
    "# Model object returned as elastic_net_linear_reg_model;\n",
    "# Feature ranking dataframe returned as elastic_net_feature_importance_df;\n",
    "# Simply modify these objects on the left of equality:\n",
    "elastic_net_linear_reg_model, elastic_net_feature_importance_df = elastic_net_linear_reg (X_train = X_TRAIN, y_train = Y_TRAIN, alpha_hyperparameter = ALPHA_HYPERPARAMETER, l1_ratio_hyperparameter = L1_RATIO_HYPERPARAMETER, maximum_of_allowed_iterations = MAXIMUM_OF_ALLOWED_ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Logistic Regression (binary classification)**\n",
    "- This function runs the 'bar_chart' function. Certify that this function was properly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN = X_train\n",
    "# X_TRAIN = subset of predictive variables (dataframe).\n",
    "# Alternatively, modify X_train, not X_TRAIN\n",
    "Y_TRAIN = y_train\n",
    "# Y_TRAIN = subset of response variable (series).\n",
    "# Alternatively, modify y_train, not Y_TRAIN\n",
    "\n",
    "L2_PENALTY = 0.0\n",
    "MAXIMUM_OF_ALLOWED_ITERATIONS = 20000\n",
    "L1_RATIO_HYPERPARAMETER = 0.0\n",
    "# hyperparameters: l2_penalty = L2_PENALTY; MAXIMUM_OF_ALLOWED_ITERATIONS = max_iter\n",
    "# and L1_RATIO_HYPERPARAMETER = l1_ratio\n",
    "\n",
    "# MAXIMUM_OF_ALLOWED_ITERATIONS = integer representing the maximum number of iterations\n",
    "# that the optimization algorithm can perform. Depending on data, convergence may not be\n",
    "# reached within this limit, so you may need to increase this hyperparameter.\n",
    "\n",
    "# L2_PENALTY is the regularization strength and must be a positive float value. \n",
    "# Regularization improves the conditioning of the problem and reduces the variance \n",
    "# of the estimates. Larger values specify stronger regularization.\n",
    "\n",
    "# L1_RATIO_HYPERPARAMETER is The ElasticNet mixing parameter (float), with 0 <= l1_ratio <= 1. \n",
    "# For L1_RATIO_HYPERPARAMETER = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is an L1 penalty. \n",
    "# For 0 < L1_RATIO_HYPERPARAMETER < 1, the penalty is a combination of L1 and L2.\n",
    "\n",
    "# if you do not want to add l2 penalty, keep l1_ratio = 0\n",
    "# if you do not want to add l1 penalty, keep l1_penalty = 0\n",
    "\n",
    "\n",
    "# Model object returned as logistic_reg_model;\n",
    "# Feature ranking dataframe returned as logistic_reg_feature_importance_df ;\n",
    "# Simply modify these objects on the left of equality:\n",
    "logistic_reg_model, logistic_reg_feature_importance_df = logistic_reg (X_train = X_TRAIN, y_train = Y_TRAIN, l2_penalty = L2_PENALTY, l1_ratio_hyperparameter = L1_RATIO_HYPERPARAMETER, maximum_of_allowed_iterations = MAXIMUM_OF_ALLOWED_ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Getting a general feature ranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICTIONARY_OF_FEATURE_RANKINGS_DATAFRAMES = {\n",
    "    \n",
    "    'ols_linear_reg': ols_feature_importance_df,\n",
    "    'ridge_linear_reg': ridge_feature_importance_df,\n",
    "    'lasso_linear_reg': lasso_feature_importance_df,\n",
    "    'elastic_net_linear_reg': elastic_net_feature_importance_df\n",
    "    \n",
    "}\n",
    "# DICTIONARY_OF_FEATURE_RANKINGS_DATAFRAMES\n",
    "# The key of this dictionary must be the model name or the name of the ranking.\n",
    "# This key will be used to identify the column on the new dataframe (it will be\n",
    "# used as suffix). The correspondent value must be the feature importance ranking\n",
    "# dataframe, with configuration similar to reg_dict: it must have a column\n",
    "# named, 'predictive_features', which will be used as key for merging.\n",
    "# For instance, for a dictionary = {'ols_linear_regression': ols_feature_df,\n",
    "# 'ridge_linear_regression': ridge_feature_df}, the columns 'regression_coefficients'\n",
    "# will be identified as: 'regression_coefficients_ols_linear_regression' and\n",
    "# 'regression_coefficients_ridge_linear_regression'. Notice that the underscore (\"_\")\n",
    "# is used as suffix separator.\n",
    "\n",
    "ELIMINATE_NON_CORRESPONDENCE = False\n",
    "# ELIMINATE_NON_CORRESPONDENCE = False. Since the dataframes will be merged using an\n",
    "# \"outer\" join, all entries from all dataframes will be added, possibly resulting in\n",
    "# missing values (pandas \"outer\" is a full outer join). \n",
    "# Then, set ELIMINATE_NON_CORRESPONDENCE = True to eliminate all missing values\n",
    "# (rows with entries without correspondence).\n",
    "LIMIT_OF_RANKED_FEATURES = None\n",
    "# LIMIT_OF_RANKED_FEATURES = None. Alternatively, set as an integer to limit the number\n",
    "# of ranked features. e.g. LIMIT_OF_RANKED_FEATURES = 20 will return a dataset with only\n",
    "# 20 features. Notice that the features are sorted in accordance to their order in the\n",
    "# input dictionary. Then, the most important ranking will be the one from the first dataframe.\n",
    "\n",
    "# General feature ranking dataframe returned as general_feature_ranking_df;\n",
    "# Simply modify this object on the left of equality:\n",
    "general_feature_ranking_df = (dictionary_of_feature_rankings_dataframes = DICTIONARY_OF_FEATURE_RANKINGS_DATAFRAMES, eliminate_non_correspondence = ELIMINATE_NON_CORRESPONDENCE, limit_of_ranked_features = LIMIT_OF_RANKED_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculating metrics for regression models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OBJECT = ols_linear_reg_model # Alternatively: object storing another model\n",
    "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
    "# MODEL_OBJECT = elastic_net_linear_reg_model\n",
    "\n",
    "X_TRAIN = X_train\n",
    "# X_TRAIN = subset of predictive variables (dataframe).\n",
    "# Alternatively, modify X_train, not X_TRAIN\n",
    "Y_TRAIN = y_train\n",
    "# Y_TRAIN = subset of response variable (series).\n",
    "# Alternatively, modify y_train, not Y_TRAIN\n",
    "\n",
    "X_TEST = None\n",
    "# X_TEST = subset of predictive variables (test dataframe, in case the original one was \n",
    "# split into train and test). Alternatively, modify X_test (or None), not X_TEST\n",
    "# e.g. X_TEST = X_test\n",
    "Y_TEST = None\n",
    "# Y_TEST = subset of response variable (test series, in case the original one was \n",
    "# split into train and test). Alternatively, modify y_test (or None), not Y_TEST\n",
    "# e.g. Y_TEST = y_test\n",
    "\n",
    "# Dictionary containing calculated metrics returned as metrics_dict;\n",
    "# Simply modify this object on the left of equality:\n",
    "metrics_dict = regression_models_metrics (model_object = MODEL_OBJECT, X_train = X_TRAIN, y_train = Y_TRAIN, X_test = X_TEST, y_test = Y_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculating metrics for classification models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OBJECT = logistic_reg_model # Alternatively: object storing another model\n",
    "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
    "# MODEL_OBJECT = xgb_classifier\n",
    "\n",
    "X_TRAIN = X_train\n",
    "# X_TRAIN = subset of predictive variables (dataframe).\n",
    "# Alternatively, modify X_train, not X_TRAIN\n",
    "Y_TRAIN = y_train\n",
    "# Y_TRAIN = subset of response variable (series).\n",
    "# Alternatively, modify y_train, not Y_TRAIN\n",
    "\n",
    "X_TEST = None\n",
    "# X_TEST = subset of predictive variables (test dataframe, in case the original one was \n",
    "# split into train and test). Alternatively, modify X_test (or None), not X_TEST\n",
    "# e.g. X_TEST = X_test\n",
    "Y_TEST = None\n",
    "# Y_TEST = subset of response variable (test series, in case the original one was \n",
    "# split into train and test). Alternatively, modify y_test (or None), not Y_TEST\n",
    "# e.g. Y_TEST = y_test\n",
    "\n",
    "SHOW_CONFUSION_MATRIX_VALUES = True\n",
    "# SHOW_CONFUSION_MATRIX_VALUES = True will plot the numeric values in\n",
    "# the confusion matrices. SHOW_CONFUSION_MATRIX_VALUES = False will\n",
    "# omit the values and show only the heatmap.\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "# ATTENTION: if the confusion matrix is exported and both the matrices for\n",
    "# train and test are calculated, then both files will have the same name, but\n",
    "# the file name of the matrix related to training will get a suffix \"_train\"; \n",
    "# whereas the matrix related to the test set will get the suffix '_test'\n",
    "\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'time_series_vis.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "# Dictionary containing calculated metrics returned as metrics_dict;\n",
    "# Simply modify this object on the left of equality:\n",
    "metrics_dict = classification_models_metrics (model_object = MODEL_OBJECT, X_train = X_TRAIN, y_train = Y_TRAIN, X_test = X_TEST, y_test = Y_TEST, show_confusion_matrix_values = SHOW_CONFUSION_MATRIX_VALUES, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Making predictions with the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OBJECT = ols_linear_reg_model # Alternatively: object storing another model\n",
    "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
    "# MODEL_OBJECT = elastic_net_linear_reg_model\n",
    "\n",
    "X_df = X\n",
    "# predict_for = 'subset' or predict_for = 'single_entry'\n",
    "# The function will automatically detect if it is dealing with lists, NumPy arrays\n",
    "# or Pandas dataframes. If X_df is a list or a single-dimension array, predict_for\n",
    "# will be set as 'single_entry'. If X is a multi-dimension NumPy array (as the\n",
    "# outputs for preparing data - even single_entry - for deep learning models), or if\n",
    "# it is a Pandas dataframe, the function will set predict_for = 'subset'\n",
    "    \n",
    "# X_df = subset of predictive variables (dataframe, NumPy array, or list).\n",
    "# If PREDICT_FOR = 'single_entry', X_df should be a list of parameters values.\n",
    "# e.g. X_df = [1.2, 3, 4] (dot is the decimal case separator, comma separate values). \n",
    "# Notice that the list should contain only the numeric values, in the same order of the\n",
    "# correspondent columns.\n",
    "# If PREDICT_FOR = 'subset' (prediction for multiple entries), X_df should be a dataframe \n",
    "# (subset) or a multi-dimensional NumPy array of the parameters values, as usual.\n",
    "\n",
    "DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset  \n",
    "# DATAFRAME_FOR_CONCATENATING_PREDICTIONS: if you want to concatenate the predictions\n",
    "# to a dataframe, pass it here:\n",
    "# e.g. DATAFRAME_FOR_CONCATENATING_PREDICTIONS = df\n",
    "# If the dataframe must be the same one passed as X, repeat the dataframe object here:\n",
    "# X_df = dataset, DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset.\n",
    "# Alternatively, if DATAFRAME_FOR_CONCATENATING_PREDICTIONS = None, \n",
    "# the prediction will be returned as a series or NumPy array, depending on the input format.\n",
    "# Notice that the concatenated predictions will be added as a new column.\n",
    "\n",
    "COLUMN_WITH_PREDICTIONS_SUFFIX = None\n",
    "# COLUMN_WITH_PREDICTIONS_SUFFIX = None. If the predictions are added as a new column\n",
    "# of the dataframe DATAFRAME_FOR_CONCATENATING_PREDICTIONS, you can declare this\n",
    "# parameter as string with a suffix for identifying the model. If no suffix is added, the new\n",
    "# column will be named 'y_pred'.\n",
    "# e.g. COLUMN_WITH_PREDICTIONS_SUFFIX = '_keras' will create a column named \"y_pred_keras\". This\n",
    "# parameter is useful when working with multiple models. Always start the suffix with underscore\n",
    "# \"_\" so that no blank spaces are added; the suffix will not be merged to the column; and there\n",
    "# will be no confusion with the dot (.) notation for methods, JSON attributes, etc.\n",
    "\n",
    "# Predictions returned as prediction_output\n",
    "# Simply modify this object (or variable) on the left of equality:\n",
    "prediction_output = make_model_predictions (model_object = MODEL_OBJECT, X = X_df, dataframe_for_concatenating_predictions = DATAFRAME_FOR_CONCATENATING_PREDICTIONS, col_with_predictions_suffix = COLUMN_WITH_PREDICTIONS_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculating probabilities associated to each class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OBJECT = logistic_reg_model # Alternatively: object storing another model\n",
    "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
    "# MODEL_OBJECT = mlp_model\n",
    "\n",
    "X_df = X\n",
    "# predict_for = 'subset' or predict_for = 'single_entry'\n",
    "# The function will automatically detect if it is dealing with lists, NumPy arrays\n",
    "# or Pandas dataframes. If X_df is a list or a single-dimension array, predict_for\n",
    "# will be set as 'single_entry'. If X is a multi-dimension NumPy array (as the\n",
    "# outputs for preparing data - even single_entry - for deep learning models), or if\n",
    "# it is a Pandas dataframe, the function will set predict_for = 'subset'\n",
    "    \n",
    "# X_df = subset of predictive variables (dataframe, NumPy array, or list).\n",
    "# If PREDICT_FOR = 'single_entry', X_df should be a list of parameters values.\n",
    "# e.g. X_df = [1.2, 3, 4] (dot is the decimal case separator, comma separate values). \n",
    "# Notice that the list should contain only the numeric values, in the same order of the\n",
    "# correspondent columns.\n",
    "# If PREDICT_FOR = 'subset' (prediction for multiple entries), X_df should be a dataframe \n",
    "# (subset) or a multi-dimensional NumPy array of the parameters values, as usual.\n",
    "\n",
    "LIST_OF_CLASSES = list_of_classes\n",
    "# LIST_OF_CLASSES is the list of classes effectively used for training\n",
    "# the model. Set this parameter as the object returned from function\n",
    "# retrieve_classes_used_to_train\n",
    "\n",
    "TYPE_OF_MODEL = 'other'\n",
    "# TYPE_OF_MODEL = 'deep_learning' if Keras/TensorFlow or other deep learning\n",
    "# framework was used to obtain the model;\n",
    "# TYPE_OF_MODEL = 'other' for Scikit-learn or XGBoost models.\n",
    "\n",
    "DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset  \n",
    "# DATAFRAME_FOR_CONCATENATING_PREDICTIONS: if you want to concatenate the predictions\n",
    "# to a dataframe, pass it here:\n",
    "# e.g. DATAFRAME_FOR_CONCATENATING_PREDICTIONS = df\n",
    "# If the dataframe must be the same one passed as X, repeat the dataframe object here:\n",
    "# X_df = dataset, DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset.\n",
    "# Alternatively, if DATAFRAME_FOR_CONCATENATING_PREDICTIONS = None, \n",
    "# the prediction will be returned as a series or NumPy array, depending on the input format.\n",
    "# Notice that the concatenated predictions will be added as a new column.    \n",
    "# All of the new columns (appended or not) will have the prefix \"prob_class_\" followed\n",
    "# by the correspondent class name to identify them.\n",
    "\n",
    "\n",
    "# Probabilities returned as calculated_probability\n",
    "# Simply modify this object (or variable) on the left of equality:\n",
    "calculated_probability = calculate_class_probability (model_object = MODEL_OBJECT, X = X_df, list_of_classes = LIST_OF_CLASSES, type_of_model = TYPE_OF_MODEL, dataframe_for_concatenating_predictions = DATAFRAME_FOR_CONCATENATING_PREDICTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Performing the SHAP feature importance analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OBJECT = ols_linear_reg_model # Alternatively: object storing another model\n",
    "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
    "# MODEL_OBJECT = elastic_net_linear_reg_model\n",
    "\n",
    "X_TRAIN = X_train\n",
    "# X_TRAIN = subset of predictive variables (dataframe).\n",
    "# Alternatively, modify X_train, not X_TRAIN\n",
    "\n",
    "MODEL_TYPE = 'linear'\n",
    "# MODEL_TYPE = 'linear' for linear models (OLS, Ridge, Lasso, ElasticNet,\n",
    "# Logistic Regression)\n",
    "# MODEL_TYPE = 'tree' for tree-based models (Random Forest and XGBoost)\n",
    "# MODEL_TYPE = 'ann' for artificial neural networks\n",
    "\n",
    "TOTAL_OF_SHAP_POINTS = 40\n",
    "# TOTAL_OF_SHAP_POINTS (integer): number of points from the \n",
    "# subset X_train that will be randomly selected for the SHAP \n",
    "# analysis. If the kernel is taking too long, reduce this value.\n",
    "\n",
    "PLOT_TYPE = 'waterfall'\n",
    "# PLOT_TYPE = 'waterfall', 'beeswarm', 'bar', 'heatmap' \n",
    "# 'scatter', 'force_plt' or 'summary': \n",
    "# sets the type of shap plot that will be shown\n",
    "\n",
    "MAX_NUMBER_OF_FEATURES_SHOWN = 10\n",
    "# MAX_NUMBER_OF_FEATURES_SHOWN = 10: (integer) limiting the number\n",
    "# of features shown in the plot.\n",
    "\n",
    "# Dictionary containing calculated metrics returned as shap_dict;\n",
    "# Simply modify this object on the left of equality:\n",
    "shap_dict = shap_feature_analysis (model_object = MODEL_OBJECT, X_train = X_TRAIN, model_type = MODEL_TYPE, total_of_shap_points = TOTAL_OF_SHAP_POINTS, plot_type = PLOT_TYPE, max_number_of_features_shown = MAX_NUMBER_OF_FEATURES_SHOWN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualizing time series**\n",
    "        x1, y1, lab1: blue\n",
    "        x2, y2, lab2: red\n",
    "        x3, y3, lab3: green\n",
    "        x4, y4, lab4: black\n",
    "        x5, y5, lab5: magenta\n",
    "        x6, y6, lab6: yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "#X1 = dataset.index to use the index as the axis itself\n",
    "X1 = (DATASET['DATE']).astype('datetime64[D]') \n",
    "#Alternatively: None; or other column in quotes, substituting 'DATE'\n",
    "# WARNING: Modify only the object in the first parenthesis: DATASET['DATE']\n",
    "# Do not modify the method .astype('datetime64[D]')\n",
    "#Remove .astype('datetime64[D]') if it is not a datetime.\n",
    "# e.g. X1 = DATASET['Time'] for a X variable named 'Time', if 'Time' is a float, not a\n",
    "# a datetime64. If 'Time' should be interpreted as a timestamp, then, we would declare as:\n",
    "\n",
    "# X1 = (DATASET['Time']).astype('datetime64[D]')\n",
    "\n",
    "# In summary: apply the method .astype('datetime64[D]') if you want the value to be\n",
    "# interpreted (correctly) as a timestamp.\n",
    "\n",
    "#Notice that there is a data transforming step to guarantee that the 'DATE' was interpreted as a timestamp, not as object or string.\n",
    "#The astype method defines the type of variable as 'datetime64[D]'. If we wanted the timestamps to be resolved in seconds, we should use\n",
    "# 'datetime64[ns]'.\n",
    "Y1 = DATASET['Y1'] \n",
    "#Alternatively: None; or other column in quotes, substituting 'Y1'\n",
    "# e.g. Y1 = DATASET['Speed'] for a Y variable named 'Speed'\n",
    "\n",
    "X2 = None #Alternatively: series for X2 (analogous to X1)\n",
    "Y2 = None #Alternatively: series for Y2 (analogous to Y1)\n",
    "X3 = None #Alternatively: series for X3 (analogous to X1)\n",
    "Y3 = None #Alternatively: series for Y3 (analogous to Y1)\n",
    "X4 = None #Alternatively: series for X4 (analogous to X1)\n",
    "Y4 = None #Alternatively: series for Y4 (analogous to Y1)\n",
    "X5 = None #Alternatively: series for X5 (analogous to X1)\n",
    "Y5 = None #Alternatively: series for Y5 (analogous to Y1)\n",
    "X6 = None #Alternatively: series for X6 (analogous to X1)\n",
    "Y6 = None #Alternatively: series for Y6 (analogous to Y1)\n",
    "# Warning: if X2, X3, X4, X5, and X6 were timestamps, do not forget to use the method\n",
    "# .astype('datetime64[D]'). e.g.: X2 = (DATASET['DATE']).astype('datetime64[D]')\n",
    "# If all X axis are the same, you can also declare: X2 = X1, X3 = X1, X4 = X1, X5 = X1\n",
    "# and X6 = X1.\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "ADD_SPLINE_LINES = True #Alternatively: True or False\n",
    "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
    "ADD_SCATTER_DOTS = False #Alternatively: True or False\n",
    "# If ADD_SCATTER_DOTS = False, the dots (scatter plot) are omitted, so only the lines\n",
    "# correspondent to the series are shown.\n",
    "\n",
    "# Notice that adding the dots and omitting the spline lines is equivalent to obtain a\n",
    "# scatter plot. If you want to do so, consider using the scatter_plot_lin_reg function, \n",
    "# capable of calculating the linear regressions.\n",
    "\n",
    "LAB1 = None #Alternatively: string inside quotes containing the label for series 1\n",
    "LAB2 = None #Alternatively: string inside quotes containing the label for series 2\n",
    "LAB3 = None #Alternatively: string inside quotes containing the label for series 3\n",
    "LAB4 = None #Alternatively: string inside quotes containing the label for series 4\n",
    "LAB5 = None #Alternatively: string inside quotes containing the label for series 5\n",
    "LAB6 = None #Alternatively: string inside quotes containing the label for series 6\n",
    "#e.g. LAB1 = \"Y1_values\"\n",
    "\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'time_series_vis.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "time_series_vis (x1 = X1, y1 = Y1, x2 = X2, y2 = Y2, x3 = X3, y3 = Y3, x4 = X4, y4 = Y4, x5 = X5, y5 = Y5, x6 = X6, y6 = Y6, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, add_scatter_dots = ADD_SCATTER_DOTS, lab1 = LAB1, lab2 = LAB2, lab3 = LAB3, lab4 = LAB4, lab5 = LAB5, lab6 = LAB6, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing or exporting models and dictionaries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: import only a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_FILE_NAME = None\n",
    "# DICTIONARY_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no dictionary will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'sklearn'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "#Model object saved as model.\n",
    "# Simply modify this object on the left of equality:\n",
    "model = import_export_model_or_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_file_name = DICTIONARY_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_to_export = DICT_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2: import only a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'dict_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_FILE_NAME = None\n",
    "# DICTIONARY_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no dictionary will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'sklearn'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Dictionary saved as imported_dict.\n",
    "# Simply modify this object on the left of equality:\n",
    "imported_dict = import_export_model_or_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_file_name = DICTIONARY_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_to_export = DICT_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 3: import a model and a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_and_dict'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_FILE_NAME = None\n",
    "# DICTIONARY_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no dictionary will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'sklearn'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model. Dictionary saved as imported_dict.\n",
    "# Simply modify these objects on the left of equality:\n",
    "model, imported_dict = import_export_model_or_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_file_name = DICTIONARY_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_to_export = DICT_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 4: export a model and/or a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'export'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_FILE_NAME = None\n",
    "# DICTIONARY_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no dictionary will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'sklearn'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "import_export_model_or_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_file_name = DICTIONARY_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_to_export = DICT_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exporting the dataframe as CSV file (to notebook's workspace)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING: all files exported from this function are .csv (comma separated values)\n",
    "\n",
    "DATAFRAME_OBJ_TO_BE_EXPORTED = dataset\n",
    "# Alternatively: object containing the dataset to be exported.\n",
    "# DATAFRAME_OBJ_TO_BE_EXPORTED: dataframe object that is going to be exported from the\n",
    "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "# example: DATAFRAME_OBJ_TO_BE_EXPORTED = dataset will export the dataset object.\n",
    "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "NEW_FILE_NAME_WITHOUT_EXTENSION = \"dataset\"\n",
    "# NEW_FILE_NAME_WITHOUT_EXTENSION - (string, in quotes): input the name of the \n",
    "# file without the extension. e.g. set NEW_FILE_NAME_WITHOUT_EXTENSION = \"my_file\" \n",
    "# to export the CSV file 'my_file.csv' to notebook's workspace.\n",
    "\n",
    "export_pd_dataframe_as_csv (dataframe_obj_to_be_exported = DATAFRAME_OBJ_TO_BE_EXPORTED, new_file_name_without_extension = NEW_FILE_NAME_WITHOUT_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: upload a file to Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model named keras_model, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'keras_model.h5'\n",
    "\n",
    "# Dictionary storing the uploaded files returned as colab_files_dict.\n",
    "# Simply modify this object on the left of the equality:\n",
    "colab_files_dict = upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2: download a file from Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'download'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model nameACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exporting a list of files from notebook's workspace to AWS Simple Storage Service (S3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['s3_file1.txt', 's3_file2.txt']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS: list containing all the files to export to S3.\n",
    "# Declare it as a list even if only a single file will be exported.\n",
    "# It must be a list of strings containing the file names followed by the extensions.\n",
    "# Example, to a export a single file my_file.ext, where my_file is the name and ext is the\n",
    "# extension:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['my_file.ext']\n",
    "# To export 3 files, file1.ext1, file2.ext2, and file3.ext3:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['file1.ext1', 'file2.ext2', 'file3.ext3']\n",
    "# Other examples:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['Screen_Shot.png', 'dataset.csv']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = [\"dictionary.pkl\", \"model.h5\"]\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['doc.pdf', 'model.dill']\n",
    "\n",
    "DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = ''\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT: directory from notebook's workspace\n",
    "# from which the files will be exported to S3. Keep it None, or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = \"/\"; or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = '' (empty string) to export from\n",
    "# the root (main) directory.\n",
    "# Alternatively, set as a string containing only the directories and folders, not the file names.\n",
    "# Examples: DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1';\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1/folder2/'\n",
    "    \n",
    "# For this function, all exported files must be located in the same directory.\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for connecting with AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "export_files_to_s3 (list_of_file_names_with_extensions = LIST_OF_FILE_NAMES_WITH_EXTENSIONS, directory_of_notebook_workspace_storing_files_to_export = DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plotting a bar chart**\n",
    "- Bars may be vertically or horizontally oriented.\n",
    "- Bar charts are plotted after selecting an aggregation function, and the cumulative percent curve may be obtained and plotted with the bars (in secondary axis).\n",
    "- To obtain a Pareto chart, keep aggregate_function = 'sum', plot_cumulative_percent = True, and orientation = 'vertical'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "CATEGORICAL_VAR_NAME = 'categorical_column_name'\n",
    "# CATEGORICAL_VAR_NAME: string (inside quotes) containing the name \n",
    "# of the column to be analyzed. e.g. \n",
    "# CATEGORICAL_VAR_NAME = \"column1\"\n",
    "\n",
    "RESPONSE_VAR_NAME = \"response_column_name\"\n",
    "# RESPONSE_VAR_NAME: string (inside quotes) containing the name \n",
    "# of the column that stores the response correspondent to the\n",
    "# categories. e.g. RESPONSE_VAR_NAME = \"response_feature\"\n",
    "\n",
    "AGGREGATE_FUNCTION = 'sum'\n",
    "# AGGREGATE_FUNCTION = 'sum': String defining the aggregation \n",
    "# method that will be applied. Possible values:\n",
    "# 'median', 'mean', 'mode', 'sum', 'min', 'max', 'variance',\n",
    "# 'standard_deviation','10_percent_quantile', '20_percent_quantile',\n",
    "# '25_percent_quantile', '30_percent_quantile', '40_percent_quantile',\n",
    "# '50_percent_quantile', '60_percent_quantile', '70_percent_quantile',\n",
    "# '75_percent_quantile', '80_percent_quantile', '90_percent_quantile',\n",
    "# and '95_percent_quantile'.\n",
    "# To use another aggregate function, the method must be added to the\n",
    "# dictionary of methods agg_methods_dict, defined in the function.\n",
    "# If None or an invalid function is input, 'sum' will be used.\n",
    "\n",
    "ADD_SUFFIX_TO_AGGREGATED_COL = True\n",
    "# ADD_SUFFIX_TO_AGGREGATED_COL = True will add a suffix to the\n",
    "# aggregated column. e.g. 'responseVar_mean'. If ADD_SUFFIX_TO_AGGREGATED_COL\n",
    "# = False, the aggregated column will have the original column name.\n",
    "SUFFIX = None\n",
    "# suffix = None. Keep it None if no suffix should be added, or if\n",
    "# the name of the aggregate function should be used as suffix, after\n",
    "# \"_\". Alternatively, set it as a string. As recommendation, put the\n",
    "# \"_\" sign in the beginning of this string to separate the suffix from\n",
    "# the original column name. e.g. if the response variable is 'Y' and\n",
    "# suffix = '_agg', the new aggregated column will be named as 'Y_agg'\n",
    "CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = True\n",
    "# CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = True to calculate and plot\n",
    "# the line of cumulative percent, or \n",
    "# CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = False to omit it.\n",
    "# This feature is only shown when AGGREGATE_FUNCTION = 'sum', 'median',\n",
    "# 'mean', or 'mode'. So, it will be automatically set as False if \n",
    "# another aggregate is selected.\n",
    "ORIENTATION = 'vertical'\n",
    "# ORIENTATION = 'vertical' is the standard, and plots vertical bars\n",
    "# (perpendicular to the X axis). In this case, the categories are shown\n",
    "# in the X axis, and the correspondent responses are in Y axis.\n",
    "# Alternatively, ORIENTATION = 'horizontal' results in horizontal bars.\n",
    "# In this case, categories are in Y axis, and responses in X axis.\n",
    "# If None or invalid values are provided, orientation is set as 'vertical'.\n",
    "LIMIT_OF_PLOTTED_CATEGORIES = None\n",
    "# LIMIT_OF_PLOTTED_CATEGORIES: integer value that represents\n",
    "# the maximum of categories that will be plot. Keep it None to plot\n",
    "# all categories. Alternatively, set an integer value. e.g.: if\n",
    "# LIMIT_OF_PLOTTED_CATEGORIES = 4, but there are more categories,\n",
    "# the dataset will be sorted in descending order and: 1) The remaining\n",
    "# categories will be sum in a new category named 'others' if the\n",
    "# aggregate function is 'sum'; 2) Or the other categories will be simply\n",
    "# omitted from the plot, for other aggregate functions. Notice that\n",
    "# it limits only the variables in the plot: all of them will be\n",
    "# returned in the dataframe.\n",
    "# Use this parameter to obtain a cleaner plot. Notice that the remaining\n",
    "# columns will be aggregated as 'others' even if there is a single column\n",
    "# beyond the limit.\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'scatter_plot_lin_reg.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "# New dataframe saved as aggregated_sorted_df. \n",
    "# Simply modify this object on the left of equality:\n",
    "aggregated_sorted_df = bar_chart (df = DATASET, categorical_var_name = CATEGORICAL_VAR_NAME, response_var_name = RESPONSE_VAR_NAME, aggregate_function = AGGREGATE_FUNCTION, add_suffix_to_aggregated_col = ADD_SUFFIX_TO_AGGREGATED_COL, suffix = SUFFIX, calculate_and_plot_cumulative_percent = CALCULATE_AND_PLOT_CUMULATIVE_PERCENT, orientation = ORIENTATION, limit_of_plotted_categories = LIMIT_OF_PLOTTED_CATEGORIES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1"
   },
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification metrics - Background**"
   ]
  },
  {
   "attachments": {
    "TP%20x%20FP%201-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "CELL_23"
    ]
   },
   "source": [
    "Explaining graphic:\n",
    "\n",
    "![TP%20x%20FP%201-2.png](attachment:TP%20x%20FP%201-2.png)\n",
    "\n",
    "https://towardsdatascience.com/how-to-evaluate-your-machine-learning-models-with-python-code-5f8d2d8d945b"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
