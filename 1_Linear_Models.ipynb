{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "instance_type": "ml.t3.medium",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcosoares-92/IndustrialDataScience_ML_Modelling_Workflow/blob/main/1_Linear_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Linear Models - Multiple Linear Regressions and Logistic Regression**"
      ],
      "metadata": {
        "azdata_cell_guid": "1700b7f0-044e-4c81-be92-a60206d9f1e2",
        "id": "G9R2NHkZf7eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _Machine Learning Modelling Workflow Notebook 1_"
      ],
      "metadata": {
        "azdata_cell_guid": "8e3d3f0e-0b85-44e8-b5a4-f4d7a77841d5",
        "id": "bUbCFXu4f7ej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content:\n",
        "\n",
        "1. Separate and prepare features and responses tensors;\n",
        "2. Splitting features and responses into train and test tensors;\n",
        "3. Splitting time series into train and test tensors;\n",
        "4. Creating a TensorFlow windowed dataset from a time series;\n",
        "4. Retrieving the list of classes used for training the classification models;\n",
        "5. Ordinary Least Squares (OLS) Linear Regression;\n",
        "6. Ridge Linear Regression;\n",
        "7. Lasso Linear Regression;\n",
        "8. Elastic Net Linear Regression;\n",
        "9. Logistic Regression (binary classification);\n",
        "10. Getting a general feature ranking;\n",
        "10. Calculating metrics for regression models;\n",
        "11. Calculating metrics for classification models;\n",
        "12. Making predictions with the models;\n",
        "13. Calculating probabilities associated to each class;\n",
        "14. Performing the SHAP feature importance analysis;\n",
        "15. Time series visualization."
      ],
      "metadata": {
        "azdata_cell_guid": "ddda8dca-f505-45df-8b0e-6c619ee4bde2",
        "id": "4cRBv3opf7ej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
        "- marcosoares.feq@gmail.com\n",
        "- marco.soares@bayer.com"
      ],
      "metadata": {
        "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267",
        "id": "LZdKoQWwf7ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Python Libraries in Global Context**"
      ],
      "metadata": {
        "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea",
        "id": "T4hyKVrwf7el"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import load\n",
        "from idsw import *"
      ],
      "metadata": {
        "azdata_cell_guid": "30c47ae7-5a45-4ee5-bc33-0ab312dadcec",
        "id": "bzZgOvXCyHHl",
        "language": "python",
        "tags": [
          "CELL_4"
        ]
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Call the functions**"
      ],
      "metadata": {
        "azdata_cell_guid": "45132e9c-8a92-4374-b511-fa092004dc24",
        "id": "zHUhoX1XyHHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing the dataset**"
      ],
      "metadata": {
        "azdata_cell_guid": "a03e1dae-be1f-4b90-91d0-cb9297ad015d",
        "id": "MAlnu3gpf7eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt),\n",
        "## JSON, txt, or CSV (comma separated values) files. Tables in webpages or html files can also be read.\n",
        "\n",
        "FILE_DIRECTORY_PATH = \"\"\n",
        "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\"\n",
        "# or FILE_DIRECTORY_PATH = \"folder\"\n",
        "\n",
        "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
        "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the\n",
        "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or,\n",
        "# FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
        "# Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
        "# Also, html files and webpages may be also read.\n",
        "\n",
        "# You may input the path for an HTML file containing a table to be read; or\n",
        "# a string containing the address for a webpage containing the table. The address must start\n",
        "# with www or htpp. If a website is input, the full address can be input as FILE_DIRECTORY_PATH\n",
        "# or as FILE_NAME_WITH_EXTENSION.\n",
        "\n",
        "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
        "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True\n",
        "# if you want to read a file with txt extension containing a text formatted as JSON\n",
        "# (but not saved as JSON).\n",
        "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the\n",
        "# function (below) must be set. If not, an error message will be raised.\n",
        "\n",
        "HOW_MISSING_VALUES_ARE_REGISTERED = None\n",
        "# HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
        "# empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
        "# This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
        "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’,\n",
        "#‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’,\n",
        "# ‘n/a’, ‘nan’, ‘null’.\n",
        "\n",
        "# If a different denomination is used, indicate it as a string. e.g.\n",
        "# HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
        "# HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
        "\n",
        "# If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
        "# only in column 'numeric_col', you can specify the following dictionary:\n",
        "# how_missing_values_are_registered = {'numeric-col': 0}\n",
        "\n",
        "\n",
        "HAS_HEADER = True\n",
        "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
        "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
        "\n",
        "DECIMAL_SEPARATOR = '.'\n",
        "# DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
        "# the decimal separator. Alternatively, specify here the separator.\n",
        "# e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
        "# It manipulates the argument 'decimal' from Pandas functions.\n",
        "\n",
        "TXT_CSV_COL_SEP = \"comma\"\n",
        "# txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
        "# or 'csv'. It informs how the different columns are separated.\n",
        "# Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\"\n",
        "# for columns separated by comma;\n",
        "# txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \"\n",
        "# for columns separated by simple spaces.\n",
        "# You can also set a specific separator as string. For example:\n",
        "# txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
        "# is used as separator for the columns - '\\t' represents the tab character).\n",
        "\n",
        "## Parameters for loading Excel files:\n",
        "\n",
        "LOAD_ALL_SHEETS_AT_ONCE = False\n",
        "# LOAD_ALL_SHEETS_AT_ONCE = False - This parameter has effect only when for Excel files.\n",
        "# If LOAD_ALL_SHEETS_AT_ONCE = True, the function will return a list of dictionaries, each\n",
        "# dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
        "# value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
        "# and its value will be the pandas dataframe object obtained from that sheet.\n",
        "# This argument has preference over SHEET_TO_LOAD. If it is True, all sheets will be loaded.\n",
        "\n",
        "SHEET_TO_LOAD = None\n",
        "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
        "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
        "# will be loaded.\n",
        "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
        "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
        "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
        "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
        "# name to load the sheet with that name.\n",
        "\n",
        "## Parameters for loading JSON files:\n",
        "\n",
        "JSON_RECORD_PATH = None\n",
        "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
        "# Path in each object to list of records. If not passed, data will be assumed to\n",
        "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
        "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
        "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
        "\n",
        "JSON_FIELD_SEPARATOR = \"_\"\n",
        "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
        "# Nested records will generate names separated by sep.\n",
        "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
        "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
        "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
        "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
        "\n",
        "JSON_METADATA_PREFIX_LIST = None\n",
        "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter\n",
        "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting\n",
        "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
        "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
        "\n",
        "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
        "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
        "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
        "# are 'name' and 'last'.\n",
        "# Then, JSON_RECORD_PATH = 'books'\n",
        "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
        "\n",
        "\n",
        "# The dataframe will be stored in the object named 'dataset':\n",
        "# Simply modify this object on the left of equality:\n",
        "dataset = load_pandas_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, how_missing_values_are_registered = HOW_MISSING_VALUES_ARE_REGISTERED, has_header = HAS_HEADER, decimal_separator = DECIMAL_SEPARATOR, txt_csv_col_sep = TXT_CSV_COL_SEP, load_all_sheets_at_once = LOAD_ALL_SHEETS_AT_ONCE, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)\n",
        "\n",
        "# OBS: If an Excel file is loaded and LOAD_ALL_SHEETS_AT_ONCE = True, then the object\n",
        "# dataset will be a list of dictionaries, with 'sheet' as key containing the sheet name; and 'df'\n",
        "# as key correspondent to the Pandas dataframe. So, to access the 3rd dataframe (index 2, since\n",
        "# indexing starts from zero): df = dataframe[2]['df'], where dataframe is the list returned."
      ],
      "metadata": {
        "azdata_cell_guid": "7c5408c3-c382-4010-b090-312686bf8d1a",
        "language": "python",
        "id": "wgN4f2BHf7eo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Separating and preparing features and responses tensors**"
      ],
      "metadata": {
        "azdata_cell_guid": "4912cfd4-e950-4a53-ac11-45f1e89d352f",
        "id": "QlMxFdx4f7ep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = dataset  #Alternatively: object containing the dataset to be analyzed\n",
        "\n",
        "FEATURES_COLUMNS = ['col1', 'col2']\n",
        "# FEATURES_COLUMNS: list of strings or string containing the names of columns\n",
        "# with predictive variables in the original dataframe.\n",
        "# Example: FEATURES_COLUMNS = ['col1', 'col2']; FEATURES_COLUMNS = 'predictor';\n",
        "# FEATURES_COLUMNS = ['predictor'].\n",
        "\n",
        "RESPONSE_COLUMNS = \"response\"\n",
        "# RESPONSE_COLUMNS: list of strings or string containing the names of columns\n",
        "# with response variables in the original dataframe.\n",
        "# Example: RESPONSE_COLUMNS= ['col3', 'col4']; RESPONSE_COLUMNS = 'response';\n",
        "# RESPONSE_COLUMNS = ['response']\n",
        "\n",
        "# Arrays or tensors containing features and responses returned as X and y, respectively.\n",
        "# Mapping dictionary correlating the position in array or tensor to the original column name\n",
        "# returned as column_map_dict.\n",
        "# Simply modify these objects on the left of equality:\n",
        "X, y, column_map_dict = separate_and_prepare_features_and_responses (df = DATASET, features_columns = FEATURES_COLUMNS, response_columns = RESPONSE_COLUMNS)"
      ],
      "metadata": {
        "azdata_cell_guid": "caea052f-209e-49c0-b221-ade6594f19d5",
        "language": "python",
        "id": "JLY7k9Iaf7ep"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Converting a whole dataframe or array-like object to tensor**"
      ],
      "metadata": {
        "azdata_cell_guid": "4912cfd4-e950-4a53-ac11-45f1e89d352f",
        "id": "Cf6rS0RHf7eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_OR_ARRAY_TO_CONVERT = dataset\n",
        "# Alternatively: object containing the dataset or array-like object to be converted and reshaped.\n",
        "\n",
        "COLUMNS_TO_CONVERT = None\n",
        "# ATTENTION: This argument only works for Pandas dataframes.\n",
        "# COLUMNS_TO_CONVERT: list of strings or string containing the names of columns\n",
        "# that you want to convert. Use this if you want to convert only a subset of the dataframe.\n",
        "# Example: COLUMNS_TO_CONVERT = ['col1', 'col2']; COLUMNS_TO_CONVERT = 'predictor';\n",
        "# COLUMNS_TO_CONVERT = ['predictor'] will create a tensor with only the specified columns;\n",
        "# If None, the whole dataframe will be converted.\n",
        "\n",
        "COLUMNS_TO_EXCLUDE = None\n",
        "# ATTENTION: This argument only works for Pandas dataframes.\n",
        "# COLUMNS_TO_EXCLUDE: Alternative parameter.\n",
        "# list of strings or string containing the names of columns that you want to exclude from the\n",
        "# returned tensor. Use this if you want to convert only a subset of the dataframe.\n",
        "# Example: COLUMNS_TO_EXCLUDE = ['col1', 'col2']; COLUMNS_TO_EXCLUDE = 'predictor';\n",
        "# COLUMNS_TO_EXCLUDE = ['predictor'] will create a tensor with all columns from the dataframe\n",
        "# except the specified ones. This argument will only be used if the previous one was not.\n",
        "\n",
        "\n",
        "# Array or tensor returned as X. Mapping dictionary correlating the position in array or tensor\n",
        "# to the original column name returned as column_map_dict.\n",
        "# Simply modify these objects on the left of equality:\n",
        "X, column_map_dict = convert_to_tensor (df_or_array_to_convert = DATASET_OR_ARRAY_TO_CONVERT, columns_to_convert = COLUMNS_TO_CONVERT, columns_to_exclude = COLUMNS_TO_EXCLUDE)"
      ],
      "metadata": {
        "azdata_cell_guid": "caea052f-209e-49c0-b221-ade6594f19d5",
        "language": "python",
        "id": "is9dfajVf7eq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Splitting features and responses into train and test tensors**"
      ],
      "metadata": {
        "azdata_cell_guid": "4912cfd4-e950-4a53-ac11-45f1e89d352f",
        "id": "Qqve0zpmf7eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_tensor = X\n",
        "# X_df = tensor or array of predictive variables. Alternatively, modify X, not X_tensor.\n",
        "Y_tensor = y\n",
        "# Y = tensor or array of response variables. Alternatively, modify y, not Y_tensor.\n",
        "\n",
        "PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 75\n",
        "# percent_of_data_used_for_model_training: float from 0 to 100,\n",
        "# representing the percent of data used for training the model\n",
        "\n",
        "PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 0\n",
        "# If you want to use cross-validation, separate a percent of the training data for validation.\n",
        "# Declare this percent as PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION (float from 0 to 100).\n",
        "\n",
        "# Subset and series destined to training, testing and/or validation returned in the dictionary split_dictionary;\n",
        "# Simply modify this object on the left of equality:\n",
        "split_dictionary = split_data_into_train_and_test (X = X_tensor, y = Y_tensor, percent_of_data_used_for_model_training = PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING, percent_of_training_data_used_for_model_validation = PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION)"
      ],
      "metadata": {
        "azdata_cell_guid": "9134b868-05f8-4ca6-9cec-c655423a0fa4",
        "language": "python",
        "id": "cs783lWcf7er"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Splitting time series into train and test tensors**"
      ],
      "metadata": {
        "azdata_cell_guid": "4912cfd4-e950-4a53-ac11-45f1e89d352f",
        "id": "i4SYBYTdf7er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_tensor = X\n",
        "# X_df = tensor or array of predictive variables. Alternatively, modify X, not X_tensor.\n",
        "Y_tensor = y\n",
        "# Y = tensor or array of response variables. Alternatively, modify y, not Y_tensor.\n",
        "\n",
        "PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 75\n",
        "# percent_of_data_used_for_model_training: float from 0 to 100,\n",
        "# representing the percent of data used for training the model\n",
        "\n",
        "PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 0\n",
        "# If you want to use cross-validation, separate a percent of the training data for validation.\n",
        "# Declare this percent as PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION (float from 0 to 100).\n",
        "\n",
        "\n",
        "# Subset and series destined to training, testing and/or validation returned in the dictionary split_dictionary;\n",
        "# Simply modify this object on the left of equality:\n",
        "split_dictionary = time_series_train_test_split (X = X_tensor, y = Y_tensor, percent_of_data_used_for_model_training = PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING, percent_of_training_data_used_for_model_validation = PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION)"
      ],
      "metadata": {
        "azdata_cell_guid": "3b14a884-4ffa-4e24-89f5-aca069f17291",
        "language": "python",
        "id": "bAoyQUjmf7er"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creating a TensorFlow windowed dataset from a time series**"
      ],
      "metadata": {
        "azdata_cell_guid": "4912cfd4-e950-4a53-ac11-45f1e89d352f",
        "id": "ufj4kRDxf7er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_tensor = y\n",
        "# Y = tensor or array of response variables. Alternatively, modify y, not Y_tensor.\n",
        "\n",
        "WINDOW_SIZE = 20\n",
        "# WINDOW_SIZE (integer): number of rows/ size of the time window used.\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "# BATCH_SIZE (integer): number of rows/ size of the batches used for training.\n",
        "\n",
        "SHUFFLE_BUFFER_SIZE = 100\n",
        "# SHUFFLE_BUFFER_SIZE (integer): number of rows/ size used for shuffling the entries.\n",
        "\n",
        "# TensorFlow Dataset obtained from the time series returned as dataset_from_time_series.\n",
        "# Simply modify this object on the left of equality:\n",
        "dataset_from_time_series = windowed_dataset_from_time_series (y = Y_tensor, window_size = WINDOW_SIZE, batch_size = BATCH_SIZE, shuffle_buffer_size = SHUFFLE_BUFFER_SIZE)"
      ],
      "metadata": {
        "azdata_cell_guid": "db98832b-6b3a-41cf-93cb-1e1ab3415e75",
        "language": "python",
        "id": "fPCvVOWHf7er"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creating a TensorFlow windowed dataset from multiple-feature time series**"
      ],
      "metadata": {
        "azdata_cell_guid": "4912cfd4-e950-4a53-ac11-45f1e89d352f",
        "id": "5mmcrfyjf7es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = dataset\n",
        "# Alternatively: object containing the Pandas dataframe to be converted and reshaped.\n",
        "\n",
        "RESPONSE_COLUMNS = 'response_variable'\n",
        "# RESPONSE_COLUMNS: string or list of strings with the response columns\n",
        "\n",
        "SEQUENCE_STRIDE = 1\n",
        "SAMPLING_RATE = 1\n",
        "SHIFT = 1\n",
        "# SHIFT, SAMPLING_RATE, and SEQUENCE_STRIDE: integers\n",
        "\n",
        "# The time series may be represented as a sequence of times like: t = 0, t = 1, t = 2, ..., t = N.\n",
        "# When preparing the dataset, we pick a given number of 'times' (indexes), and use them for\n",
        "# predicting a time in the future.\n",
        "# So, the INPUT_WIDTH represents how much times will be used for prediction. If INPUT_WIDTH = 6,\n",
        "# we use 6 values for prediction, e.g., t = 0, t = 1, ..., t = 5 will be a prediction window.\n",
        "# In turns, if INPUT_WIDTH = 3, 3 values are used: t = 0, t = 1, t = 2; if INPUT_WIDTH = N, N\n",
        "# consecutive values will be used: t = 0, t = 1, t = 2, ..., t = N. And so on.\n",
        "# LABEL_WIDTH, in turns, represent how much times will be predicted. If LABEL_WIDTH = 1, a single\n",
        "# value will be predicted. If LABEL_WIDTH = 2, two consecutive values are predicted; if LABEL_WIDTH =\n",
        "# N, N consecutive values are predicted; and so on.\n",
        "\n",
        "# SHIFT represents the offset, i.e., given the input values, which value in the time sequence will\n",
        "# be predicted. So, suppose INPUT_WIDTH = 6 and LABEL_WIDTH = 1\n",
        "# If SHIFT = 1, the label, i.e., the predicted value, will be the first after the sequence used for\n",
        "# prediction. So, if  t = 0, t = 1, ..., t = 5 will be a prediction window and t = 6 will be the\n",
        "# predicted value. Notice that the complete window has a total width = 7: t = 0, ..., t = 7.\n",
        "# If LABEL_WIDTH = 2, then t = 6 and t = 7 will be predicted (total width = 8).\n",
        "# Another example: suppose INPUT_WIDTH = 24. So the predicted window is: t = 0, t = 1, ..., t = 23.\n",
        "# If SHIFT = 24, the 24th element after the prediction sequence will be used as label, i.e., will\n",
        "# be predicted. So, t = 24 is the 1st after the sequence, t = 25 is the second, ... t = 47 is the\n",
        "# 24th after. If label_with = 1, then the sequence t = 0, t = 1, ..., t = 23 will be used for\n",
        "# predicting t = 47. Naturally, the total width of the window = 47 in this case.\n",
        "# Also, notice that the label is used by the model as the response (predicted) variable.\n",
        "\n",
        "# So for a given SHIFT: the sequence of timesteps i, i+1, ... will be used for predicting the\n",
        "# timestep i + SHIFT\n",
        "# If a sequence starts in index i, the next sequence will start from i + SEQUENCE_STRIDE.\n",
        "# The sequence will be formed by timesteps i, i + SAMPLING_RATE, i + 2* SAMPLING_RATE, ...\n",
        "# Example: Consider indices [0, 1, ... 99]. With sequence_length=10, SAMPLING_RATE=2,\n",
        "# SEQUENCE_STRIDE=3, the dataset will yield batches of sequences composed of the following indices:\n",
        "# First sequence:  [0  2  4  6  8 10 12 14 16 18]\n",
        "# Second sequence: [3  5  7  9 11 13 15 17 19 21]\n",
        "# Third sequence:  [6  8 10 12 14 16 18 20 22 24]\n",
        "# ...\n",
        "# Last sequence:   [78 80 82 84 86 88 90 92 94 96]\n",
        "\n",
        "USE_PAST_RESPONSES_FOR_PREDICTION = True\n",
        "# USE_PAST_RESPONSES_FOR_PREDICTION: True if the past responses will be used for predicting their\n",
        "# value in the future; False if you do not want to use them.\n",
        "\n",
        "PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 70\n",
        "# percent_of_data_used_for_model_training: float from 0 to 100,\n",
        "# representing the percent of data used for training the model\n",
        "\n",
        "PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 10\n",
        "# If you want to use cross-validation, separate a percent of the training data for validation.\n",
        "# Declare this percent as PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION (float from 0 to 100).\n",
        "\n",
        "# If PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 70, and\n",
        "# PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 10,\n",
        "# training dataset slice goes from 0 to 0.7 (70%) of the dataset;\n",
        "# testing slicing goes from 0.7 x dataset to ((1 - 0.1) = 0.9) x dataset\n",
        "# validation slicing goes from 0.9 x dataset to the end of the dataset.\n",
        "# Here, consider the time sequence t = 0, t = 1, ... , t = N, for a dataset with length N:\n",
        "# training: from t = 0 to t = (0.7 x N); testing: from t = ((0.7 x N) + 1) to (0.9 x N);\n",
        "# validation: from t = ((0.9 x N) + 1) to N (the fractions 0.7 x N and 0.9 x N are rounded to\n",
        "# the closest integer).\n",
        "\n",
        "\n",
        "# Dictionary with inputs and labels tensors returned as tensors_dict.\n",
        "# Simply modify this object on the left of equality:\n",
        "tensors_dict = multi_columns_time_series_tensors (df = DATASET, response_columns = RESPONSE_COLUMNS, sequence_stride = SEQUENCE_STRIDE, sampling_rate = SAMPLING_RATE, shift = SHIFT, use_past_responses_for_prediction = USE_PAST_RESPONSES_FOR_PREDICTION, percent_of_data_used_for_model_training = PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING, percent_of_training_data_used_for_model_validation = PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION)"
      ],
      "metadata": {
        "azdata_cell_guid": "d841ffeb-d5b7-47ac-9efc-bcc4f8d7c150",
        "language": "python",
        "id": "H7rS5DQ1f7es"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Union of several 1-dimensional tensors (obtained from single columns) into a single tensor\n",
        "- Each 1-dimensional tensor or array becomes a column from the new tensor."
      ],
      "metadata": {
        "azdata_cell_guid": "5e6c5fac-6d3d-4cef-bb74-a58b3bd138f2",
        "id": "TyDmaYeXf7et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LIST_OF_TENSORS_OR_ARRAYS = [tensor1, tensor2]\n",
        "# list of tensors: list containing the 1-dimensional tensors or arrays that the function will union.\n",
        "# the operation will be performed in the order that the tensors are declared.\n",
        "# Substitue tensor1, tensor2, tensor3,... by the tensor objects, in the correct sequence.\n",
        "# If the resulting tensor will contain the responses for a multi-response tensor, declare them in the\n",
        "# orders of the responses (tensor 1 corresponding to response 1, tensor 2 to response 2, etc.)\n",
        "\n",
        "# One-dimensional tensors have shape (X,), where X is the number of elements. Example: a column\n",
        "# of the dataframe with elements 1, 2, 3 in this order may result in an array like array([1, 2, 3])\n",
        "# and a Tensor with shape (3,). With we union it with the tensor from the column with elements\n",
        "# 4, 5, 6, the output will be array([[1,4], [2,5], [3,6]]). Alternatively, this new array could\n",
        "# be converted into a Pandas dataframe where each column would be correspondent to one individual\n",
        "# tensor.\n",
        "\n",
        "# Tensor resulting from the union of multiple single-dimension tensor returned as tensors_union.\n",
        "# Simply modify this object on the left of equality:\n",
        "tensors_union = union_1_dim_tensors (list_of_tensors_or_arrays = LIST_OF_TENSORS_OR_ARRAYS)"
      ],
      "metadata": {
        "azdata_cell_guid": "8b903857-9b24-408d-b9d0-6d0d3c42d5bd",
        "language": "python",
        "id": "j6otjcocf7et"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ordinary Least Squares (OLS) Linear Regression**\n",
        "- Fits a linear model with coefficients w = (w1, …, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation."
      ],
      "metadata": {
        "azdata_cell_guid": "a32adeb1-a938-4bd7-baff-11692eb32738",
        "id": "lhRttY6Ff7eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split_dictionary.keys() == dict_keys(['X_train', 'y_train', 'X_test', 'y_test', 'X_valid', 'y_valid'])\n",
        "\n",
        "X_TRAIN = split_dictionary['X_train']\n",
        "# X_TRAIN = tensor of predictive variables.\n",
        "# Alternatively, modify X_train, not X_TRAIN\n",
        "Y_TRAIN = split_dictionary['y_train']\n",
        "# Y_TRAIN = tensor of response variables.\n",
        "# Alternatively, modify y_train, not Y_TRAIN\n",
        "\n",
        "# Tensors of data separated for model testing:\n",
        "X_TEST = None\n",
        "Y_TEST = None\n",
        "#X_TEST = split_dictionary['X_test']\n",
        "#Y_TEST = split_dictionary['y_test']\n",
        "\n",
        "# Tensors of data separated for model validation:\n",
        "X_VALID = None\n",
        "Y_VALID = None\n",
        "#X_VALID = split_dictionary['X_valid']\n",
        "#Y_VALID = split_dictionary['y_valid']\n",
        "\n",
        "COLUMN_MAP_DICT = column_map_dict\n",
        "#COLUMN_MAP_DICT = None\n",
        "# COLUMN_MAP_DICT: Mapping dictionary correlating the position in array or tensor to the original\n",
        "# column name.\n",
        "\n",
        "ORIENTATION = 'vertical'\n",
        "# ORIENTATION = 'vertical' is the standard, and plots vertical bars\n",
        "# (perpendicular to the X axis). In this case, the categories are shown\n",
        "# in the X axis, and the correspondent responses are in Y axis.\n",
        "# Alternatively, ORIENTATION = 'horizontal' results in horizontal bars.\n",
        "# In this case, categories are in Y axis, and responses in X axis.\n",
        "# If None or invalid values are provided, orientation is set as 'vertical'.\n",
        "X_AXIS_ROTATION = 70\n",
        "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "Y_AXIS_ROTATION = 0\n",
        "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "GRID = True #Alternatively: True or False\n",
        "# If GRID = False, no grid lines are shown in the graphic.\n",
        "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
        "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
        "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
        "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
        "EXPORT_PNG = False\n",
        "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
        "# Set EXPORT_PNG = True to export the obtained image.\n",
        "DIRECTORY_TO_SAVE = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\"\n",
        "# or DIRECTORY_TO_SAVE = \"folder\"\n",
        "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
        "# path, DIRECTORY_TO_SAVE = \"\"\n",
        "FILE_NAME = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the name you want for the file without the\n",
        "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png'\n",
        "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
        "# 'feature_importance_ranking.png'\n",
        "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
        "# the file will be overwritten.\n",
        "PNG_RESOLUTION_DPI = 330\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
        "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
        "\n",
        "\n",
        "# Model object returned as ols_linear_reg_model;\n",
        "# Calculated metrics returned as metrics_dict; and feature importance dataframe returned as\n",
        "# feature_importance_df. Simply modify these objects on the left of equality:\n",
        "ols_linear_reg_model, metrics_dict, feature_importance_df = ols_linear_reg (X_train = X_TRAIN, y_train = Y_TRAIN, X_test = X_TEST, y_test = Y_TEST, X_valid = X_VALID, y_valid = Y_VALID, column_map_dict = COLUMN_MAP_DICT, orientation = ORIENTATION, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
      ],
      "metadata": {
        "azdata_cell_guid": "647e6a2e-400f-43ac-a558-628d67d4a958",
        "language": "python",
        "id": "6k2SmKD4f7eu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ridge Linear Regression**\n",
        "- Linear least squares with l2 regularization.\n",
        "- Minimizes the objective function: `||y - Xw||^2_2 + alpha * ||w||^2_2`\n",
        "- This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm.\n",
        "- Also known as Ridge Regression or Tikhonov regularization.\n",
        "- This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape (n_samples, n_targets)).\n",
        "\n",
        "#### The regularizer tends to bring the coefficients to zero: the model will behave as a constant line for higher regularization terms.\n",
        "- The Regularization term can be either:\n",
        "    - Lasso: absolute value of the coefficients.\n",
        "        - Force the coefficients of the regression to zero.\n",
        "    - Ridge: square of the coefficients.\n",
        "        - Bring the coefficients of the regression closer to zero.\n",
        "    - Elastic net: combination of Ridge and Lasso.\n",
        "\n",
        "    - Both shrink the coefficients related to unimportant predictors.\n",
        "\n",
        "- Regularization term `alpha`:\n",
        "    - `alpha = 0`: no regularization (standard regression);\n",
        "    - `alpha tending to infinite`: complete regularization (all coefficients to zero).\n",
        "        - Regression becomes a constant line.\n",
        "    \n",
        "    - If all coefficients are different from zero, all variables are being considered important for the prediction.\n",
        "    - The regularizer may bring coefficients to zero, selecting those which are effectively the most important parameters."
      ],
      "metadata": {
        "azdata_cell_guid": "80a86cbb-f97a-4665-a618-053987c6db11",
        "id": "4r9HOLqKf7eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split_dictionary.keys() == dict_keys(['X_train', 'y_train', 'X_test', 'y_test', 'X_valid', 'y_valid'])\n",
        "\n",
        "X_TRAIN = split_dictionary['X_train']\n",
        "# X_TRAIN = tensor of predictive variables.\n",
        "# Alternatively, modify X_train, not X_TRAIN\n",
        "Y_TRAIN = split_dictionary['y_train']\n",
        "# Y_TRAIN = tensor of response variables.\n",
        "# Alternatively, modify y_train, not Y_TRAIN\n",
        "\n",
        "ALPHA_HYPERPARAMETER = 0.001\n",
        "# The regularizer tends to bring all coefficients of the regression to zero, i.e., with higher\n",
        "# regularization terms, the model can become a constant line. On the other hand, it reduces the\n",
        "# impact of high-coefficient features like X^4, reducing overfitting (high variance problem).\n",
        "# So, apply low regularizers, like 0.001, specially if the data was previously normalized. alpha=1\n",
        "# may bring the equivalence to a constant line (underfitting, high bias problem).\n",
        "\n",
        "MAXIMUM_OF_ALLOWED_ITERATIONS = 20000\n",
        "# hyperparameters: alpha = ALPHA_HYPERPARAMETER and MAXIMUM_OF_ALLOWED_ITERATIONS = max_iter\n",
        "\n",
        "# MAXIMUM_OF_ALLOWED_ITERATIONS = integer representing the maximum number of iterations\n",
        "# that the optimization algorithm can perform. Depending on data, convergence may not be\n",
        "# reached within this limit, so you may need to increase this hyperparameter.\n",
        "\n",
        "# ALPHA_HYPERPARAMETER is the regularization strength and must be a positive float value.\n",
        "# Regularization improves the conditioning of the problem and reduces the variance\n",
        "# of the estimates. Larger values specify stronger regularization.\n",
        "# ALPHA_HYPERPARAMETER = 0 is equivalent to an ordinary least square, solved by the\n",
        "# LinearRegression object. For numerical reasons, using ALPHA_HYPERPARAMETER = 0\n",
        "# is not advised. Given this, you should use the ols_linear_reg function instead.\n",
        "\n",
        "\n",
        "# Tensors of data separated for model testing:\n",
        "X_TEST = None\n",
        "Y_TEST = None\n",
        "#X_TEST = split_dictionary['X_test']\n",
        "#Y_TEST = split_dictionary['y_test']\n",
        "\n",
        "# Tensors of data separated for model validation:\n",
        "X_VALID = None\n",
        "Y_VALID = None\n",
        "#X_VALID = split_dictionary['X_valid']\n",
        "#Y_VALID = split_dictionary['y_valid']\n",
        "\n",
        "COLUMN_MAP_DICT = column_map_dict\n",
        "#COLUMN_MAP_DICT = None\n",
        "# COLUMN_MAP_DICT: Mapping dictionary correlating the position in array or tensor to the original\n",
        "# column name.\n",
        "\n",
        "ORIENTATION = 'vertical'\n",
        "# ORIENTATION = 'vertical' is the standard, and plots vertical bars\n",
        "# (perpendicular to the X axis). In this case, the categories are shown\n",
        "# in the X axis, and the correspondent responses are in Y axis.\n",
        "# Alternatively, ORIENTATION = 'horizontal' results in horizontal bars.\n",
        "# In this case, categories are in Y axis, and responses in X axis.\n",
        "# If None or invalid values are provided, orientation is set as 'vertical'.\n",
        "X_AXIS_ROTATION = 70\n",
        "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "Y_AXIS_ROTATION = 0\n",
        "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "GRID = True #Alternatively: True or False\n",
        "# If GRID = False, no grid lines are shown in the graphic.\n",
        "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
        "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
        "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
        "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
        "EXPORT_PNG = False\n",
        "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
        "# Set EXPORT_PNG = True to export the obtained image.\n",
        "DIRECTORY_TO_SAVE = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\"\n",
        "# or DIRECTORY_TO_SAVE = \"folder\"\n",
        "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
        "# path, DIRECTORY_TO_SAVE = \"\"\n",
        "FILE_NAME = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the name you want for the file without the\n",
        "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png'\n",
        "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
        "# 'feature_importance_ranking.png'\n",
        "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
        "# the file will be overwritten.\n",
        "PNG_RESOLUTION_DPI = 330\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
        "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
        "\n",
        "\n",
        "# Model object returned as ridge_linear_reg_model;\n",
        "# Calculated metrics returned as metrics_dict; and feature importance dataframe returned as\n",
        "# feature_importance_df. Simply modify these objects on the left of equality:\n",
        "ridge_linear_reg_model, metrics_dict, feature_importance_df = ridge_linear_reg (X_train = X_TRAIN, y_train = Y_TRAIN, alpha_hyperparameter = ALPHA_HYPERPARAMETER, maximum_of_allowed_iterations = MAXIMUM_OF_ALLOWED_ITERATIONS, X_test = X_TEST, y_test = Y_TEST, X_valid = X_VALID, y_valid = Y_VALID, column_map_dict = COLUMN_MAP_DICT, orientation = ORIENTATION, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
      ],
      "metadata": {
        "azdata_cell_guid": "1cde7f4e-97cd-4c45-a74b-cb4b19b301e0",
        "language": "python",
        "id": "Siy1MusWf7ev"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lasso Linear Regression**\n",
        "- Linear Model trained with L1 prior as regularizer (aka the Lasso).\n",
        "- The optimization objective for Lasso is: `(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1`\n",
        "- Technically the Lasso model is optimizing the same objective function as the Elastic Net with l1_ratio=1.0 (no L2 penalty).\n",
        "\n",
        "#### The regularizer tends to bring the coefficients to zero: the model will behave as a constant line for higher regularization terms.\n",
        "- The Regularization term can be either:\n",
        "    - Lasso: absolute value of the coefficients.\n",
        "        - Force the coefficients of the regression to zero.\n",
        "    - Ridge: square of the coefficients.\n",
        "        - Bring the coefficients of the regression closer to zero.\n",
        "    - Elastic net: combination of Ridge and Lasso.\n",
        "\n",
        "    - Both shrink the coefficients related to unimportant predictors.\n",
        "\n",
        "- Regularization term `alpha`:\n",
        "    - `alpha = 0`: no regularization (standard regression);\n",
        "    - `alpha tending to infinite`: complete regularization (all coefficients to zero).\n",
        "        - Regression becomes a constant line.\n",
        "    \n",
        "    - If all coefficients are different from zero, all variables are being considered important for the prediction.\n",
        "    - The regularizer may bring coefficients to zero, selecting those which are effectively the most important parameters."
      ],
      "metadata": {
        "azdata_cell_guid": "127cda56-9acb-405b-8aaf-83589aee8fb7",
        "id": "8IkMdaemf7ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split_dictionary.keys() == dict_keys(['X_train', 'y_train', 'X_test', 'y_test', 'X_valid', 'y_valid'])\n",
        "\n",
        "X_TRAIN = split_dictionary['X_train']\n",
        "# X_TRAIN = tensor of predictive variables.\n",
        "# Alternatively, modify X_train, not X_TRAIN\n",
        "Y_TRAIN = split_dictionary['y_train']\n",
        "# Y_TRAIN = tensor of response variables.\n",
        "# Alternatively, modify y_train, not Y_TRAIN\n",
        "\n",
        "ALPHA_HYPERPARAMETER = 0.001\n",
        "# The regularizer tends to bring all coefficients of the regression to zero, i.e., with higher\n",
        "# regularization terms, the model can become a constant line. On the other hand, it reduces the\n",
        "# impact of high-coefficient features like X^4, reducing overfitting (high variance problem).\n",
        "# So, apply low regularizers, like 0.001, specially if the data was previously normalized. alpha=1\n",
        "# may bring the equivalence to a constant line (underfitting, high bias problem).\n",
        "MAXIMUM_OF_ALLOWED_ITERATIONS = 20000\n",
        "# hyperparameters: alpha = ALPHA_HYPERPARAMETER and MAXIMUM_OF_ALLOWED_ITERATIONS = max_iter\n",
        "\n",
        "# MAXIMUM_OF_ALLOWED_ITERATIONS = integer representing the maximum number of iterations\n",
        "# that the optimization algorithm can perform. Depending on data, convergence may not be\n",
        "# reached within this limit, so you may need to increase this hyperparameter.\n",
        "\n",
        "# ALPHA_HYPERPARAMETER is the regularization strength and must be a positive float value.\n",
        "# Regularization improves the conditioning of the problem and reduces the variance\n",
        "# of the estimates. Larger values specify stronger regularization.\n",
        "# ALPHA_HYPERPARAMETER = 0 is equivalent to an ordinary least square, solved by the\n",
        "# LinearRegression object. For numerical reasons, using ALPHA_HYPERPARAMETER = 0\n",
        "# is not advised. Given this, you should use the ols_linear_reg function instead.\n",
        "\n",
        "# Tensors of data separated for model testing:\n",
        "X_TEST = None\n",
        "Y_TEST = None\n",
        "#X_TEST = split_dictionary['X_test']\n",
        "#Y_TEST = split_dictionary['y_test']\n",
        "\n",
        "# Tensors of data separated for model validation:\n",
        "X_VALID = None\n",
        "Y_VALID = None\n",
        "#X_VALID = split_dictionary['X_valid']\n",
        "#Y_VALID = split_dictionary['y_valid']\n",
        "\n",
        "COLUMN_MAP_DICT = column_map_dict\n",
        "#COLUMN_MAP_DICT = None\n",
        "# COLUMN_MAP_DICT: Mapping dictionary correlating the position in array or tensor to the original\n",
        "# column name.\n",
        "\n",
        "ORIENTATION = 'vertical'\n",
        "# ORIENTATION = 'vertical' is the standard, and plots vertical bars\n",
        "# (perpendicular to the X axis). In this case, the categories are shown\n",
        "# in the X axis, and the correspondent responses are in Y axis.\n",
        "# Alternatively, ORIENTATION = 'horizontal' results in horizontal bars.\n",
        "# In this case, categories are in Y axis, and responses in X axis.\n",
        "# If None or invalid values are provided, orientation is set as 'vertical'.\n",
        "X_AXIS_ROTATION = 70\n",
        "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "Y_AXIS_ROTATION = 0\n",
        "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "GRID = True #Alternatively: True or False\n",
        "# If GRID = False, no grid lines are shown in the graphic.\n",
        "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
        "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
        "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
        "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
        "EXPORT_PNG = False\n",
        "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
        "# Set EXPORT_PNG = True to export the obtained image.\n",
        "DIRECTORY_TO_SAVE = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\"\n",
        "# or DIRECTORY_TO_SAVE = \"folder\"\n",
        "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
        "# path, DIRECTORY_TO_SAVE = \"\"\n",
        "FILE_NAME = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the name you want for the file without the\n",
        "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png'\n",
        "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
        "# 'feature_importance_ranking.png'\n",
        "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
        "# the file will be overwritten.\n",
        "PNG_RESOLUTION_DPI = 330\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
        "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
        "\n",
        "\n",
        "# Model object returned as lasso_linear_reg_model;\n",
        "# Calculated metrics returned as metrics_dict; and feature importance dataframe returned as\n",
        "# feature_importance_df. Simply modify these objects on the left of equality:\n",
        "lasso_linear_reg_model, metrics_dict, feature_importance_df = lasso_linear_reg (X_train = X_TRAIN, y_train = Y_TRAIN, alpha_hyperparameter = ALPHA_HYPERPARAMETER, maximum_of_allowed_iterations = MAXIMUM_OF_ALLOWED_ITERATIONS, X_test = X_TEST, y_test = Y_TEST, X_valid = X_VALID, y_valid = Y_VALID, column_map_dict = COLUMN_MAP_DICT, orientation = ORIENTATION, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
      ],
      "metadata": {
        "azdata_cell_guid": "ddff9c48-3010-4ea2-b3c3-9cbfbe8b691e",
        "language": "python",
        "id": "gzinLBXOf7ew"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Elastic Net Linear Regression**\n",
        "- Linear Model trained with combined L1 and L2 priors as regularizer.\n",
        "- Minimizes the objective function: `1 / (2 * n_samples) * ||y - Xw||^2_2 + alpha * l1_ratio * ||w||_1 + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2`\n",
        "- If you are interested in controlling the L1 and L2 penalty separately, keep in mind that this is equivalent to: `a * ||w||_1 + 0.5 * b * ||w||_2^2`\n",
        "- where: `alpha = a + b and l1_ratio = a / (a + b)`\n",
        "- The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable, unless you supply your own sequence of alpha.\n",
        "\n",
        "#### The regularizer tends to bring the coefficients to zero: the model will behave as a constant line for higher regularization terms.\n",
        "- The Regularization term can be either:\n",
        "    - Lasso: absolute value of the coefficients.\n",
        "        - Force the coefficients of the regression to zero.\n",
        "    - Ridge: square of the coefficients.\n",
        "        - Bring the coefficients of the regression closer to zero.\n",
        "    - Elastic net: combination of Ridge and Lasso.\n",
        "\n",
        "    - Both shrink the coefficients related to unimportant predictors.\n",
        "\n",
        "- Regularization term `alpha`:\n",
        "    - `alpha = 0`: no regularization (standard regression);\n",
        "    - `alpha tending to infinite`: complete regularization (all coefficients to zero).\n",
        "        - Regression becomes a constant line.\n",
        "    \n",
        "    - If all coefficients are different from zero, all variables are being considered important for the prediction.\n",
        "    - The regularizer may bring coefficients to zero, selecting those which are effectively the most important parameters."
      ],
      "metadata": {
        "azdata_cell_guid": "75eaa5bd-cfb3-4986-a92a-0e39fb82cc30",
        "id": "RmNA6ooxf7ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split_dictionary.keys() == dict_keys(['X_train', 'y_train', 'X_test', 'y_test', 'X_valid', 'y_valid'])\n",
        "\n",
        "X_TRAIN = split_dictionary['X_train']\n",
        "# X_TRAIN = tensor of predictive variables.\n",
        "# Alternatively, modify X_train, not X_TRAIN\n",
        "Y_TRAIN = split_dictionary['y_train']\n",
        "# Y_TRAIN = tensor of response variables.\n",
        "# Alternatively, modify y_train, not Y_TRAIN\n",
        "\n",
        "ALPHA_HYPERPARAMETER = 0.001\n",
        "L1_RATIO_HYPERPARAMETER = 0.02\n",
        "# The regularizer tends to bring all coefficients of the regression to zero, i.e., with higher\n",
        "# regularization terms, the model can become a constant line. On the other hand, it reduces the\n",
        "# impact of high-coefficient features like X^4, reducing overfitting (high variance problem).\n",
        "# So, apply low regularizers, like 0.001, specially if the data was previously normalized. alpha=1\n",
        "# may bring the equivalence to a constant line (underfitting, high bias problem).\n",
        "# Currently, l1_ratio <= 0.01 is not reliable, unless you supply your own sequence of alpha.\n",
        "MAXIMUM_OF_ALLOWED_ITERATIONS = 20000\n",
        "# hyperparameters: alpha = ALPHA_HYPERPARAMETER; MAXIMUM_OF_ALLOWED_ITERATIONS = max_iter\n",
        "# and L1_RATIO_HYPERPARAMETER = l1_ratio\n",
        "\n",
        "# MAXIMUM_OF_ALLOWED_ITERATIONS = integer representing the maximum number of iterations\n",
        "# that the optimization algorithm can perform. Depending on data, convergence may not be\n",
        "# reached within this limit, so you may need to increase this hyperparameter.\n",
        "\n",
        "# ALPHA_HYPERPARAMETER is the regularization strength and must be a positive float value.\n",
        "# Regularization improves the conditioning of the problem and reduces the variance\n",
        "# of the estimates. Larger values specify stronger regularization.\n",
        "\n",
        "# L1_RATIO_HYPERPARAMETER is The ElasticNet mixing parameter (float), with 0 <= l1_ratio <= 1.\n",
        "# For L1_RATIO_HYPERPARAMETER = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is an L1 penalty.\n",
        "# For 0 < L1_RATIO_HYPERPARAMETER < 1, the penalty is a combination of L1 and L2.\n",
        "\n",
        "# ALPHA_HYPERPARAMETER = 0 and L1_RATIO_HYPERPARAMETER = 0 is equivalent to an ordinary\n",
        "# least square, solved by the LinearRegression object. For numerical reasons,\n",
        "# using ALPHA_HYPERPARAMETER = 0 and L1_RATIO_HYPERPARAMETER = 0 is not advised.\n",
        "# Given this, you should use the ols_linear_reg function instead.\n",
        "\n",
        "# Tensors of data separated for model testing:\n",
        "X_TEST = None\n",
        "Y_TEST = None\n",
        "#X_TEST = split_dictionary['X_test']\n",
        "#Y_TEST = split_dictionary['y_test']\n",
        "\n",
        "# Tensors of data separated for model validation:\n",
        "X_VALID = None\n",
        "Y_VALID = None\n",
        "#X_VALID = split_dictionary['X_valid']\n",
        "#Y_VALID = split_dictionary['y_valid']\n",
        "\n",
        "COLUMN_MAP_DICT = column_map_dict\n",
        "#COLUMN_MAP_DICT = None\n",
        "# COLUMN_MAP_DICT: Mapping dictionary correlating the position in array or tensor to the original\n",
        "# column name.\n",
        "\n",
        "ORIENTATION = 'vertical'\n",
        "# ORIENTATION = 'vertical' is the standard, and plots vertical bars\n",
        "# (perpendicular to the X axis). In this case, the categories are shown\n",
        "# in the X axis, and the correspondent responses are in Y axis.\n",
        "# Alternatively, ORIENTATION = 'horizontal' results in horizontal bars.\n",
        "# In this case, categories are in Y axis, and responses in X axis.\n",
        "# If None or invalid values are provided, orientation is set as 'vertical'.\n",
        "X_AXIS_ROTATION = 70\n",
        "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "Y_AXIS_ROTATION = 0\n",
        "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "GRID = True #Alternatively: True or False\n",
        "# If GRID = False, no grid lines are shown in the graphic.\n",
        "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
        "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
        "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
        "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
        "EXPORT_PNG = False\n",
        "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
        "# Set EXPORT_PNG = True to export the obtained image.\n",
        "DIRECTORY_TO_SAVE = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\"\n",
        "# or DIRECTORY_TO_SAVE = \"folder\"\n",
        "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
        "# path, DIRECTORY_TO_SAVE = \"\"\n",
        "FILE_NAME = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the name you want for the file without the\n",
        "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png'\n",
        "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
        "# 'feature_importance_ranking.png'\n",
        "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
        "# the file will be overwritten.\n",
        "PNG_RESOLUTION_DPI = 330\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
        "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
        "\n",
        "\n",
        "# Model object returned as elastic_net_linear_reg_model;\n",
        "# Calculated metrics returned as metrics_dict; and feature importance dataframe returned as\n",
        "# feature_importance_df. Simply modify these objects on the left of equality:\n",
        "elastic_net_linear_reg_model, metrics_dict, feature_importance_df = elastic_net_linear_reg (X_train = X_TRAIN, y_train = Y_TRAIN, alpha_hyperparameter = ALPHA_HYPERPARAMETER, l1_ratio_hyperparameter = L1_RATIO_HYPERPARAMETER, maximum_of_allowed_iterations = MAXIMUM_OF_ALLOWED_ITERATIONS, X_test = X_TEST, y_test = Y_TEST, X_valid = X_VALID, y_valid = Y_VALID, column_map_dict = COLUMN_MAP_DICT, orientation = ORIENTATION, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
      ],
      "metadata": {
        "azdata_cell_guid": "75196dad-3037-42c2-9dec-20866df7c76b",
        "language": "python",
        "id": "jmbyqq_Ff7ex"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Logistic Regression (binary classification)**\n",
        "- This linear Model may be trained with combined L1 and L2 priors as regularizer.\n",
        "- The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable, unless you supply your own sequence of alpha.\n",
        "\n",
        "#### The regularizer tends to bring the coefficients to zero: the model will behave as a constant line for higher regularization terms.\n",
        "- The Regularization term can be either:\n",
        "    - Lasso: absolute value of the coefficients.\n",
        "        - Force the coefficients of the regression to zero.\n",
        "    - Ridge: square of the coefficients.\n",
        "        - Bring the coefficients of the regression closer to zero.\n",
        "    - Elastic net: combination of Ridge and Lasso.\n",
        "\n",
        "    - Both shrink the coefficients related to unimportant predictors.\n",
        "\n",
        "- Regularization term `alpha`:\n",
        "    - `alpha = 0`: no regularization (standard regression);\n",
        "    - `alpha tending to infinite`: complete regularization (all coefficients to zero).\n",
        "        - Regression becomes a constant line.\n",
        "    \n",
        "    - If all coefficients are different from zero, all variables are being considered important for the prediction.\n",
        "    - The regularizer may bring coefficients to zero, selecting those which are effectively the most important parameters."
      ],
      "metadata": {
        "azdata_cell_guid": "ee9195c8-c0d7-4950-b1ad-2be301715fd7",
        "id": "ksjFeN5Af7ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split_dictionary.keys() == dict_keys(['X_train', 'y_train', 'X_test', 'y_test', 'X_valid', 'y_valid'])\n",
        "\n",
        "X_TRAIN = split_dictionary['X_train']\n",
        "# X_TRAIN = tensor of predictive variables.\n",
        "# Alternatively, modify X_train, not X_TRAIN\n",
        "Y_TRAIN = split_dictionary['y_train']\n",
        "# Y_TRAIN = tensor of response variables.\n",
        "# Alternatively, modify y_train, not Y_TRAIN\n",
        "\n",
        "REGULARIZATION = 'l2'\n",
        "# REGULARIZATION is the norm of the penalty:\n",
        "# REGULARIZATION = None: no penalty is added; REGULARIZATION = 'l2': add a L2 penalty term and\n",
        "# it is the default choice; REGULARIZATION = 'l1': add a L1 penalty term;\n",
        "# REGULARIZATION = 'elasticnet': both L1 and L2 penalty terms are added.\n",
        "L1_RATIO_HYPERPARAMETER = 0.02\n",
        "# The regularizer tends to bring all coefficients of the regression to zero, i.e., with higher\n",
        "# regularization terms, the model can become a constant line. On the other hand, it reduces the\n",
        "# impact of high-coefficient features like X^4, reducing overfitting (high variance problem).\n",
        "# So, apply low regularizers, like 0.001, specially if the data was previously normalized. alpha=1\n",
        "# may bring the equivalence to a constant line (underfitting, high bias problem).\n",
        "# L1_RATIO_HYPERPARAMETER is The ElasticNet mixing parameter (float), with 0 <= l1_ratio <= 1.\n",
        "# For L1_RATIO_HYPERPARAMETER = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is an L1 penalty.\n",
        "# For 0 < L1_RATIO_HYPERPARAMETER < 1, the penalty is a combination of L1 and L2.\n",
        "# Currently, l1_ratio <= 0.01 is not reliable, unless you supply your own sequence of alpha.\n",
        "\n",
        "# THIS PARAMETER IS ONLY VALID when using 'elasticnet' regularization.\n",
        "\n",
        "MAXIMUM_OF_ALLOWED_ITERATIONS = 20000\n",
        "\n",
        "# MAXIMUM_OF_ALLOWED_ITERATIONS = integer representing the maximum number of iterations\n",
        "# that the optimization algorithm can perform. Depending on data, convergence may not be\n",
        "# reached within this limit, so you may need to increase this hyperparameter.\n",
        "\n",
        "# Tensors of data separated for model testing:\n",
        "X_TEST = None\n",
        "Y_TEST = None\n",
        "#X_TEST = split_dictionary['X_test']\n",
        "#Y_TEST = split_dictionary['y_test']\n",
        "\n",
        "# Tensors of data separated for model validation:\n",
        "X_VALID = None\n",
        "Y_VALID = None\n",
        "#X_VALID = split_dictionary['X_valid']\n",
        "#Y_VALID = split_dictionary['y_valid']\n",
        "\n",
        "COLUMN_MAP_DICT = column_map_dict\n",
        "#COLUMN_MAP_DICT = None\n",
        "# COLUMN_MAP_DICT: Mapping dictionary correlating the position in array or tensor to the original\n",
        "# column name.\n",
        "\n",
        "ORIENTATION = 'vertical'\n",
        "# ORIENTATION = 'vertical' is the standard, and plots vertical bars\n",
        "# (perpendicular to the X axis). In this case, the categories are shown\n",
        "# in the X axis, and the correspondent responses are in Y axis.\n",
        "# Alternatively, ORIENTATION = 'horizontal' results in horizontal bars.\n",
        "# In this case, categories are in Y axis, and responses in X axis.\n",
        "# If None or invalid values are provided, orientation is set as 'vertical'.\n",
        "X_AXIS_ROTATION = 70\n",
        "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "Y_AXIS_ROTATION = 0\n",
        "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "GRID = True #Alternatively: True or False\n",
        "# If GRID = False, no grid lines are shown in the graphic.\n",
        "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
        "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
        "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
        "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
        "EXPORT_PNG = False\n",
        "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
        "# Set EXPORT_PNG = True to export the obtained image.\n",
        "DIRECTORY_TO_SAVE = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\"\n",
        "# or DIRECTORY_TO_SAVE = \"folder\"\n",
        "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
        "# path, DIRECTORY_TO_SAVE = \"\"\n",
        "FILE_NAME = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the name you want for the file without the\n",
        "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png'\n",
        "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
        "# 'feature_importance_ranking.png'\n",
        "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
        "# the file will be overwritten.\n",
        "PNG_RESOLUTION_DPI = 330\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
        "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
        "\n",
        "\n",
        "# Model object returned as elastic_net_linear_reg_model;\n",
        "# Calculated metrics returned as metrics_dict; feature importance dataframe returned as\n",
        "# feature_importance_df; and dictionary containing total of classes and list of classes in the\n",
        "# training tensor y_train returned as classes_dict.\n",
        "# Simply modify these objects on the left of equality:\n",
        "logistic_reg_model, metrics_dict, feature_importance_df, classes_dict = logistic_reg (X_train = X_TRAIN, y_train = Y_TRAIN, regularization = REGULARIZATION, l1_ratio_hyperparameter = L1_RATIO_HYPERPARAMETER, maximum_of_allowed_iterations = MAXIMUM_OF_ALLOWED_ITERATIONS, X_test = X_TEST, y_test = Y_TEST, X_valid = X_VALID, y_valid = Y_VALID, column_map_dict = COLUMN_MAP_DICT, orientation = ORIENTATION, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
      ],
      "metadata": {
        "azdata_cell_guid": "49ff43cd-0847-4939-bc13-f74202c37325",
        "language": "python",
        "id": "CuoF5V1Pf7ey"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Making predictions with the models**"
      ],
      "metadata": {
        "azdata_cell_guid": "f8f03824-7810-406d-aa56-5b3653435f66",
        "id": "z0CSoILhf7ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_OBJECT = model # Alternatively: object storing another model\n",
        "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
        "# MODEL_OBJECT = elastic_net_linear_reg_model\n",
        "\n",
        "X_tensor = X\n",
        "# predict_for = 'subset' or predict_for = 'single_entry'\n",
        "# The function will automatically detect if it is dealing with lists, NumPy arrays\n",
        "# or Pandas dataframes. If X_df is a list or a single-dimension array, predict_for\n",
        "# will be set as 'single_entry'. If X is a multi-dimension NumPy array (as the\n",
        "# outputs for preparing data - even single_entry - for deep learning models), or if\n",
        "# it is a Pandas dataframe, the function will set predict_for = 'subset'\n",
        "\n",
        "# X_df = subset of predictive variables (dataframe, NumPy array, or list).\n",
        "# If PREDICT_FOR = 'single_entry', X_df should be a list of parameters values.\n",
        "# e.g. X_df = [1.2, 3, 4] (dot is the decimal case separator, comma separate values).\n",
        "# Notice that the list should contain only the numeric values, in the same order of the\n",
        "# correspondent columns.\n",
        "# If PREDICT_FOR = 'subset' (prediction for multiple entries), X_df should be a dataframe\n",
        "# (subset) or a multi-dimensional NumPy array of the parameters values, as usual.\n",
        "\n",
        "DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset\n",
        "# DATAFRAME_FOR_CONCATENATING_PREDICTIONS: if you want to concatenate the predictions\n",
        "# to a dataframe, pass it here:\n",
        "# e.g. DATAFRAME_FOR_CONCATENATING_PREDICTIONS = df\n",
        "# If the dataframe must be the same one passed as X, repeat the dataframe object here:\n",
        "# X_df = dataset, DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset.\n",
        "# Alternatively, if DATAFRAME_FOR_CONCATENATING_PREDICTIONS = None,\n",
        "# the prediction will be returned as a series or NumPy array, depending on the input format.\n",
        "# Notice that the concatenated predictions will be added as a new column.\n",
        "\n",
        "COLUMN_WITH_PREDICTIONS_SUFFIX = None\n",
        "# COLUMN_WITH_PREDICTIONS_SUFFIX = None. If the predictions are added as a new column\n",
        "# of the dataframe DATAFRAME_FOR_CONCATENATING_PREDICTIONS, you can declare this\n",
        "# parameter as string with a suffix for identifying the model. If no suffix is added, the new\n",
        "# column will be named 'y_pred'.\n",
        "# e.g. COLUMN_WITH_PREDICTIONS_SUFFIX = '_keras' will create a column named \"y_pred_keras\". This\n",
        "# parameter is useful when working with multiple models. Always start the suffix with underscore\n",
        "# \"_\" so that no blank spaces are added; the suffix will not be merged to the column; and there\n",
        "# will be no confusion with the dot (.) notation for methods, JSON attributes, etc.\n",
        "\n",
        "FUNCTION_USED_FOR_FITTING_DL_MODEL = 'get_deep_learning_tf_model'\n",
        "# FUNCTION_USED_FOR_FITTING_DL_MODEL: the function you used for obtaining the deep learning model.\n",
        "# Example: 'get_deep_learning_tf_model' or 'get_siamese_networks_model'\n",
        "\n",
        "ARCHITECTURE = None\n",
        "# ARCHITECTURE: some models require inputs in a proper format. Declare here if you are using\n",
        "# one of these architectures. Example: ARCHITECTURE = 'cnn_lstm' from class tf_models require\n",
        "# a special reshape before getting predictions. You can keep None or put the name of the\n",
        "# architecture, if no special reshape is needed.\n",
        "\n",
        "### ATTENTION: ALL MODELS WITH LSTM, CNN or OTHER SPECIAL LAYERS REQUIRE THIS ARGUMENT TO BE\n",
        "# DECLARED\n",
        "\n",
        "LIST_OF_RESPONSES = []\n",
        "# You may declare the list RESPONSE_COLUMNS previously used for separating into features and responses tensors.\n",
        "# LIST_OF_RESPONSES = []. This parameter is obbligatory for multi-response models, such as the ones obtained from\n",
        "# function 'get_siamese_networks_model'. It must contain a list with the same order of the output responses.\n",
        "# Example: suppose your siamese model outputs 4 responses: 'temperature', 'pressure', 'flow_rate', and 'ph', in\n",
        "# this order. The list of responses must be declared as:\n",
        "# LIST_OF_RESPONSES = ['temperature', 'pressure', 'flow_rate', 'ph']\n",
        "# tuples and numpy arrays are also acceptable: LIST_OF_RESPONSES = ('temperature', 'pressure', 'flow_rate', 'ph')\n",
        "# Attention: the number of responses must be exactly the number of elements in list_of_responses, or an error will\n",
        "# be raised.\n",
        "\n",
        "\n",
        "# Predictions returned as prediction_output\n",
        "# Simply modify this object (or variable) on the left of equality:\n",
        "prediction_output = make_model_predictions (model_object = MODEL_OBJECT, X = X_tensor, dataframe_for_concatenating_predictions = DATAFRAME_FOR_CONCATENATING_PREDICTIONS, column_with_predictions_suffix = COLUMN_WITH_PREDICTIONS_SUFFIX, function_used_for_fitting_dl_model = FUNCTION_USED_FOR_FITTING_DL_MODEL, architecture = ARCHITECTURE, list_of_responses = LIST_OF_RESPONSES)"
      ],
      "metadata": {
        "azdata_cell_guid": "852230d5-19c6-4d0d-bad9-0e6889e1b432",
        "language": "python",
        "id": "cehwfRJ7f7ez"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Calculating probabilities associated to each class**\n",
        "- Set the list_of_classes as the input of this function.\n",
        "- The predictions (outputs) from deep learning models (e.g. Keras/TensorFlow models) are themselves the probabilities associated to each possible class.\n",
        "    - For Scikit-learn and XGBoost, we must use a specific method for retrieving the probabilities."
      ],
      "metadata": {
        "azdata_cell_guid": "0c7e772d-d345-4529-b808-3b1fc2462db4",
        "id": "34oZBo2jf7ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_OBJECT = logistic_reg_model # Alternatively: object storing another model\n",
        "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
        "# MODEL_OBJECT = mlp_model\n",
        "\n",
        "X_tensor = X\n",
        "# predict_for = 'subset' or predict_for = 'single_entry'\n",
        "# The function will automatically detect if it is dealing with lists, NumPy arrays\n",
        "# or Pandas dataframes. If X_df is a list or a single-dimension array, predict_for\n",
        "# will be set as 'single_entry'. If X is a multi-dimension NumPy array (as the\n",
        "# outputs for preparing data - even single_entry - for deep learning models), or if\n",
        "# it is a Pandas dataframe, the function will set predict_for = 'subset'\n",
        "\n",
        "# X_df = subset of predictive variables (dataframe, NumPy array, or list).\n",
        "# If PREDICT_FOR = 'single_entry', X_df should be a list of parameters values.\n",
        "# e.g. X_df = [1.2, 3, 4] (dot is the decimal case separator, comma separate values).\n",
        "# Notice that the list should contain only the numeric values, in the same order of the\n",
        "# correspondent columns.\n",
        "# If PREDICT_FOR = 'subset' (prediction for multiple entries), X_df should be a dataframe\n",
        "# (subset) or a multi-dimensional NumPy array of the parameters values, as usual.\n",
        "\n",
        "LIST_OF_CLASSES = [0, 1] # Logistic regression: classes are 0 and 1\n",
        "# LIST_OF_CLASSES is the list of classes effectively used for training\n",
        "# the model. Set this parameter as the object returned from function\n",
        "# retrieve_classes_used_to_train\n",
        "\n",
        "TYPE_OF_MODEL = 'other'\n",
        "# TYPE_OF_MODEL = 'deep_learning' if Keras/TensorFlow or other deep learning\n",
        "# framework was used to obtain the model;\n",
        "# TYPE_OF_MODEL = 'other' for Scikit-learn or XGBoost models.\n",
        "\n",
        "DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset\n",
        "# DATAFRAME_FOR_CONCATENATING_PREDICTIONS: if you want to concatenate the predictions\n",
        "# to a dataframe, pass it here:\n",
        "# e.g. DATAFRAME_FOR_CONCATENATING_PREDICTIONS = df\n",
        "# If the dataframe must be the same one passed as X, repeat the dataframe object here:\n",
        "# X_df = dataset, DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset.\n",
        "# Alternatively, if DATAFRAME_FOR_CONCATENATING_PREDICTIONS = None,\n",
        "# the prediction will be returned as a series or NumPy array, depending on the input format.\n",
        "# Notice that the concatenated predictions will be added as a new column.\n",
        "# All of the new columns (appended or not) will have the prefix \"prob_class_\" followed\n",
        "# by the correspondent class name to identify them.\n",
        "\n",
        "ARCHITECTURE = None\n",
        "# ARCHITECTURE: some models require inputs in a proper format. Declare here if you are using\n",
        "# one of these architectures. Example: ARCHITECTURE = 'cnn_lstm' from class tf_models require\n",
        "# a special reshape before getting predictions. You can keep None or put the name of the\n",
        "# architecture, if no special reshape is needed.\n",
        "\n",
        "\n",
        "# Probabilities returned as calculated_probability\n",
        "# Simply modify this object (or variable) on the left of equality:\n",
        "calculated_probability = calculate_class_probability (model_object = MODEL_OBJECT, X = X_tensor, list_of_classes = LIST_OF_CLASSES, type_of_model = TYPE_OF_MODEL, dataframe_for_concatenating_predictions = DATAFRAME_FOR_CONCATENATING_PREDICTIONS, architecture = ARCHITECTURE)"
      ],
      "metadata": {
        "azdata_cell_guid": "62df7d8b-d31a-43eb-bd9d-c5b7c8b0615a",
        "language": "python",
        "id": "dUWpO_BAf7e0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Performing the SHAP feature importance analysis**\n",
        "- SHAP was developed by a mathematician from Washington University.\n",
        "- It combines the obtained machine learning model with Game Theory algorithms to analyze the relative importance of each variable, as well as the **interactions between variables**.\n",
        "- SHAP returns us a SHAP value that represents the relative importance."
      ],
      "metadata": {
        "azdata_cell_guid": "728b47c2-3aa3-4b7e-bedb-6a442635369b",
        "id": "NZHOyaKMf7e0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_OBJECT = ols_linear_reg_model # Alternatively: object storing another model\n",
        "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
        "# MODEL_OBJECT = xgb_model\n",
        "\n",
        "X_TRAIN = split_dictionary['X_train']\n",
        "# X_TRAIN = subset of predictive variables (dataframe).\n",
        "# Alternatively, modify X_train, not X_TRAIN\n",
        "\n",
        "MODEL_TYPE = 'general'\n",
        "# MODEL_TYPE = 'general' for the general case, including artificial neural networks.\n",
        "# MODEL_TYPE = 'linear' for Sklearn linear models (OLS, Ridge, Lasso, ElasticNet,\n",
        "# Logistic Regression).\n",
        "# MODEL_TYPE = 'tree' for tree-based models (Random Forest and XGBoost).\n",
        "# MODEL_TYPE = 'deep' for Deep Learning TensorFlow model.\n",
        "# Actually, any string different from 'linear', 'tree', or 'deep' (including blank string)\n",
        "# will apply the general case.\n",
        "\n",
        "TOTAL_OF_SHAP_POINTS = 40\n",
        "# TOTAL_OF_SHAP_POINTS (integer): number of points from the\n",
        "# subset X_train that will be randomly selected for the SHAP\n",
        "# analysis. If the kernel is taking too long, reduce this value.\n",
        "\n",
        "\n",
        "# Dictionary containing calculated metrics returned as shap_dict;\n",
        "# Simply modify this object on the left of equality:\n",
        "shap_dict = shap_feature_analysis (model_object = MODEL_OBJECT, X_train = X_TRAIN, model_type = MODEL_TYPE, total_of_shap_points = TOTAL_OF_SHAP_POINTS)"
      ],
      "metadata": {
        "azdata_cell_guid": "600649c1-e734-460b-993c-218b952dbed7",
        "language": "python",
        "id": "mQU8lgDmf7e0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualizing time series**"
      ],
      "metadata": {
        "azdata_cell_guid": "cd0ac91a-71b0-42a8-bc30-6073d0aa0c5b",
        "id": "A-uyORSUf7e1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_IN_SAME_COLUMN = False\n",
        "\n",
        "# Parameters to input when DATA_IN_SAME_COLUMN = True:\n",
        "DATASET = None #Alternatively: object containing the dataset to be analyzed (e.g. DATASET = dataset)\n",
        "COLUMN_WITH_PREDICT_VAR_X = 'X' # Alternatively: correct name for X-column\n",
        "COLUMN_WITH_RESPONSE_VAR_Y = 'Y' # Alternatively: correct name for Y-column\n",
        "COLUMN_WITH_LABELS = 'label_column' # Alternatively: correct name for column with the labels or groups\n",
        "\n",
        "# DATA_IN_SAME_COLUMN = False: set as True if all the values to plot are in a same column.\n",
        "# If DATA_IN_SAME_COLUMN = True, you must specify the dataframe containing the data as DATASET;\n",
        "# the column containing the predict variable (X) as COLUMN_WITH_PREDICT_VAR_X; the column\n",
        "# containing the responses to plot (Y) as COLUMN_WITH_RESPONSE_VAR_Y; and the column\n",
        "# containing the labels (subgroup) indication as COLUMN_WITH_LABELS.\n",
        "# DATASET is an object, so do not declare it in quotes. The other three arguments (columns' names)\n",
        "# are strings, so declare in quotes.\n",
        "\n",
        "# Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare.\n",
        "# All the results for both groups are in a column named 'results', wich will be plot against\n",
        "# the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
        "# an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
        "# column 'group' shows the value 'B'. In this example:\n",
        "# DATA_IN_SAME_COLUMN = True,\n",
        "# DATASET = dataset,\n",
        "# COLUMN_WITH_PREDICT_VAR_X = 'time',\n",
        "# COLUMN_WITH_RESPONSE_VAR_Y = 'results',\n",
        "# COLUMN_WITH_LABELS = 'group'\n",
        "# If you want to declare a list of dictionaries, keep DATA_IN_SAME_COLUMN = False and keep\n",
        "# DATASET = None (the other arguments may be set as None, but it is not mandatory:\n",
        "# COLUMN_WITH_PREDICT_VAR_X = None, COLUMN_WITH_RESPONSE_VAR_Y = None, COLUMN_WITH_LABELS = None).\n",
        "\n",
        "\n",
        "# Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
        "LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = [\n",
        "\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None}\n",
        "\n",
        "]\n",
        "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE: if data is already converted to series, lists\n",
        "# or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
        "# even if there is a single dictionary.\n",
        "# Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
        "# (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
        "# keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
        "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
        "# and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
        "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
        "# same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
        "# represents the series and label of the added dictionary (you can pass 'lab': None, but if\n",
        "# 'x' or 'y' are None, the new dictionary will be ignored).\n",
        "\n",
        "# Examples:\n",
        "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE =\n",
        "# [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
        "# will plot a single variable. In turns:\n",
        "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE =\n",
        "# [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
        "# will plot two series, Y1 x X and Y2 x X.\n",
        "# Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
        "# If None is provided to 'lab', an automatic label will be generated.\n",
        "\n",
        "\n",
        "X_AXIS_ROTATION = 70\n",
        "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "Y_AXIS_ROTATION = 0\n",
        "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "GRID = True #Alternatively: True or False\n",
        "# If GRID = False, no grid lines are shown in the graphic.\n",
        "ADD_SPLINE_LINES = True #Alternatively: True or False\n",
        "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
        "# Since we are obtaining a scatter plot, there is no meaning in omitting the dots,\n",
        "# as we can do for the time series visualization function.\n",
        "ADD_SCATTER_DOTS = False\n",
        "# If ADD_SCATTER_DOTS = False, no dots representing the data points are shown.\n",
        "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
        "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
        "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
        "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
        "\n",
        "EXPORT_PNG = False\n",
        "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
        "# Set EXPORT_PNG = True to export the obtained image.\n",
        "DIRECTORY_TO_SAVE = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\"\n",
        "# or DIRECTORY_TO_SAVE = \"folder\"\n",
        "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
        "# path, DIRECTORY_TO_SAVE = \"\"\n",
        "FILE_NAME = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the name you want for the file without the\n",
        "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png'\n",
        "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
        "# 'time_series_vis.png'\n",
        "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
        "# the file will be overwritten.\n",
        "PNG_RESOLUTION_DPI = 330\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
        "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
        "\n",
        "\n",
        "time_series_vis (data_in_same_column = DATA_IN_SAME_COLUMN, df = DATASET, column_with_predict_var_x = COLUMN_WITH_PREDICT_VAR_X, column_with_response_var_y = COLUMN_WITH_RESPONSE_VAR_Y, column_with_labels = COLUMN_WITH_LABELS, list_of_dictionaries_with_series_to_analyze = LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, add_scatter_dots = ADD_SCATTER_DOTS, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
      ],
      "metadata": {
        "azdata_cell_guid": "e419054a-a29f-42ed-8f53-0a7633697bfe",
        "language": "python",
        "id": "7dx1tyNdf7e1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing or exporting models and dictionaries (or lists)**"
      ],
      "metadata": {
        "azdata_cell_guid": "cf36a716-7e15-413a-83e7-58fc2fcedf6a",
        "id": "BnAex8gff7e2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case 1: import only a model"
      ],
      "metadata": {
        "azdata_cell_guid": "21bff921-c44c-4b60-9555-5a3bf6460524",
        "id": "0KLDbwuGf7e2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTION = 'import'\n",
        "# ACTION = 'import' for importing a model and/or a dictionary;\n",
        "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
        "\n",
        "OBJECTS_MANIPULATED = 'model_only'\n",
        "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will\n",
        "#  be manipulated.\n",
        "\n",
        "MODEL_FILE_NAME = None\n",
        "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. MODEL_FILE_NAME = 'model'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
        "\n",
        "DICTIONARY_OR_LIST_FILE_NAME = None\n",
        "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary\n",
        "# (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary\n",
        "# or list will be manipulated.\n",
        "\n",
        "DIRECTORY_PATH = ''\n",
        "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
        "# or from which the model will be retrieved. If no value is provided,\n",
        "# the DIRECTORY_PATH will be the root: \"\"\n",
        "# Notice that the model and the dictionary must be stored in the same path.\n",
        "# If a model and a dictionary will be exported, they will be stored in the same\n",
        "# DIRECTORY_PATH.\n",
        "\n",
        "MODEL_TYPE = 'sklearn'\n",
        "# This parameter has effect only when a model will be manipulated.\n",
        "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .keras\n",
        "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing\n",
        "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
        "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
        "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
        "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
        "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
        "\n",
        "DICT_OR_LIST_TO_EXPORT = None\n",
        "MODEL_TO_EXPORT = None\n",
        "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
        "# must be declared. If ACTION == 'export', keep:\n",
        "# DICT_OR_LIST_TO_EXPORT = None,\n",
        "# MODEL_TO_EXPORT = None\n",
        "# If one of these objects will be exported, substitute None by the name of the object\n",
        "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
        "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
        "# it is not a string, but an object.\n",
        "# For exporting a dictionary named as 'dict':\n",
        "# DICT_OR_LIST_TO_EXPORT = dict\n",
        "\n",
        "USE_COLAB_MEMORY = False\n",
        "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
        "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
        "# from Google Colaboratory: you will update or download the file and it will be available\n",
        "# only during the time when the kernel is running. It will be excluded when the kernel\n",
        "# dies, for instance, when you close the notebook.\n",
        "\n",
        "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
        "# to your computer (running the cell will start the download).\n",
        "\n",
        "# Model object saved as model.\n",
        "# Simply modify this object on the left of equality:\n",
        "model = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)"
      ],
      "metadata": {
        "azdata_cell_guid": "7a3a61de-c910-44ea-a3c5-53b05bcbe2f0",
        "language": "python",
        "id": "YnSZScw_f7e2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case 2: import only a dictionary or a list"
      ],
      "metadata": {
        "azdata_cell_guid": "fbe729dc-0ab6-423c-9c59-e93b17d33d06",
        "id": "JlPMDwzif7e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTION = 'import'\n",
        "# ACTION = 'import' for importing a model and/or a dictionary;\n",
        "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
        "\n",
        "OBJECTS_MANIPULATED = 'dict_or_list_only'\n",
        "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will\n",
        "#  be manipulated.\n",
        "\n",
        "MODEL_FILE_NAME = None\n",
        "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. MODEL_FILE_NAME = 'model'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
        "\n",
        "DICTIONARY_OR_LIST_FILE_NAME = None\n",
        "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary\n",
        "# (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary\n",
        "# or list will be manipulated.\n",
        "\n",
        "DIRECTORY_PATH = ''\n",
        "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
        "# or from which the model will be retrieved. If no value is provided,\n",
        "# the DIRECTORY_PATH will be the root: \"\"\n",
        "# Notice that the model and the dictionary must be stored in the same path.\n",
        "# If a model and a dictionary will be exported, they will be stored in the same\n",
        "# DIRECTORY_PATH.\n",
        "\n",
        "MODEL_TYPE = 'sklearn'\n",
        "# This parameter has effect only when a model will be manipulated.\n",
        "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .keras\n",
        "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing\n",
        "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
        "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
        "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
        "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
        "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
        "\n",
        "DICT_OR_LIST_TO_EXPORT = None\n",
        "MODEL_TO_EXPORT = None\n",
        "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
        "# must be declared. If ACTION == 'export', keep:\n",
        "# DICT_OR_LIST_TO_EXPORT = None,\n",
        "# MODEL_TO_EXPORT = None\n",
        "# If one of these objects will be exported, substitute None by the name of the object\n",
        "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
        "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
        "# it is not a string, but an object.\n",
        "# For exporting a dictionary named as 'dict':\n",
        "# DICT_OR_LIST_TO_EXPORT = dict\n",
        "\n",
        "USE_COLAB_MEMORY = False\n",
        "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
        "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
        "# from Google Colaboratory: you will update or download the file and it will be available\n",
        "# only during the time when the kernel is running. It will be excluded when the kernel\n",
        "# dies, for instance, when you close the notebook.\n",
        "\n",
        "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
        "# to your computer (running the cell will start the download).\n",
        "\n",
        "# Dictionary or list saved as imported_dict_or_list.\n",
        "# Simply modify this object on the left of equality:\n",
        "imported_dict_or_list = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)"
      ],
      "metadata": {
        "azdata_cell_guid": "d8fdf34d-661e-4425-8012-b95abac69d2d",
        "language": "python",
        "id": "A9aSUHflf7e3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case 3: import a model and a dictionary (or a list)"
      ],
      "metadata": {
        "azdata_cell_guid": "d88cc6a7-e0d9-493b-a9ad-8a6e8963069c",
        "id": "sXylo6-lf7e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTION = 'import'\n",
        "# ACTION = 'import' for importing a model and/or a dictionary;\n",
        "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
        "\n",
        "OBJECTS_MANIPULATED = 'model_and_dict'\n",
        "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will\n",
        "#  be manipulated.\n",
        "\n",
        "MODEL_FILE_NAME = None\n",
        "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. MODEL_FILE_NAME = 'model'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
        "\n",
        "DICTIONARY_OR_LIST_FILE_NAME = None\n",
        "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary\n",
        "# (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary\n",
        "# or list will be manipulated.\n",
        "\n",
        "DIRECTORY_PATH = ''\n",
        "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
        "# or from which the model will be retrieved. If no value is provided,\n",
        "# the DIRECTORY_PATH will be the root: \"\"\n",
        "# Notice that the model and the dictionary must be stored in the same path.\n",
        "# If a model and a dictionary will be exported, they will be stored in the same\n",
        "# DIRECTORY_PATH.\n",
        "\n",
        "MODEL_TYPE = 'sklearn'\n",
        "# This parameter has effect only when a model will be manipulated.\n",
        "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .keras\n",
        "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing\n",
        "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
        "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
        "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
        "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
        "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
        "\n",
        "DICT_OR_LIST_TO_EXPORT = None\n",
        "MODEL_TO_EXPORT = None\n",
        "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
        "# must be declared. If ACTION == 'export', keep:\n",
        "# DICT_OR_LIST_TO_EXPORT = None,\n",
        "# MODEL_TO_EXPORT = None\n",
        "# If one of these objects will be exported, substitute None by the name of the object\n",
        "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
        "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
        "# it is not a string, but an object.\n",
        "# For exporting a dictionary named as 'dict':\n",
        "# DICT_OR_LIST_TO_EXPORT = dict\n",
        "\n",
        "USE_COLAB_MEMORY = False\n",
        "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
        "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
        "# from Google Colaboratory: you will update or download the file and it will be available\n",
        "# only during the time when the kernel is running. It will be excluded when the kernel\n",
        "# dies, for instance, when you close the notebook.\n",
        "\n",
        "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
        "# to your computer (running the cell will start the download).\n",
        "\n",
        "# Model object saved as model. Dictionary or list saved as imported_dict_or_list.\n",
        "# Simply modify these objects on the left of equality:\n",
        "model, imported_dict_or_list = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)"
      ],
      "metadata": {
        "azdata_cell_guid": "969b061e-dc94-496d-a438-c05893624c66",
        "language": "python",
        "id": "qVcXxObef7e4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case 4: export a model and/or a dictionary (or a list)"
      ],
      "metadata": {
        "azdata_cell_guid": "6ecddb6d-f6c3-4556-a4a7-95d2089a743a",
        "id": "DgCQlrATf7e4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTION = 'export'\n",
        "# ACTION = 'import' for importing a model and/or a dictionary;\n",
        "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
        "\n",
        "OBJECTS_MANIPULATED = 'model_only'\n",
        "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will\n",
        "#  be manipulated.\n",
        "\n",
        "MODEL_FILE_NAME = None\n",
        "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. MODEL_FILE_NAME = 'model'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
        "\n",
        "DICTIONARY_OR_LIST_FILE_NAME = None\n",
        "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary\n",
        "# (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary\n",
        "# or list will be manipulated.\n",
        "\n",
        "DIRECTORY_PATH = ''\n",
        "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
        "# or from which the model will be retrieved. If no value is provided,\n",
        "# the DIRECTORY_PATH will be the root: \"\"\n",
        "# Notice that the model and the dictionary must be stored in the same path.\n",
        "# If a model and a dictionary will be exported, they will be stored in the same\n",
        "# DIRECTORY_PATH.\n",
        "\n",
        "MODEL_TYPE = 'sklearn'\n",
        "# This parameter has effect only when a model will be manipulated.\n",
        "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .keras\n",
        "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing\n",
        "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
        "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
        "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
        "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
        "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
        "\n",
        "DICT_OR_LIST_TO_EXPORT = None\n",
        "MODEL_TO_EXPORT = None\n",
        "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
        "# must be declared. If ACTION == 'export', keep:\n",
        "# DICT_OR_LIST_TO_EXPORT = None,\n",
        "# MODEL_TO_EXPORT = None\n",
        "# If one of these objects will be exported, substitute None by the name of the object\n",
        "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
        "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
        "# it is not a string, but an object.\n",
        "# For exporting a dictionary named as 'dict':\n",
        "# DICT_OR_LIST_TO_EXPORT = dict\n",
        "\n",
        "USE_COLAB_MEMORY = False\n",
        "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
        "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
        "# from Google Colaboratory: you will update or download the file and it will be available\n",
        "# only during the time when the kernel is running. It will be excluded when the kernel\n",
        "# dies, for instance, when you close the notebook.\n",
        "\n",
        "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
        "# to your computer (running the cell will start the download).\n",
        "\n",
        "import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)"
      ],
      "metadata": {
        "azdata_cell_guid": "d0a58924-cc24-4325-8571-f4a9ce33f318",
        "language": "python",
        "id": "M9bIV3x8f7e4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exporting the dataframe as CSV file (to notebook's workspace)**"
      ],
      "metadata": {
        "azdata_cell_guid": "9b2ed8e6-18a7-42f6-bac3-3bf72b6ae383",
        "id": "4iofhC7kf7e5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## WARNING: all files exported from this function are .csv (comma separated values)\n",
        "\n",
        "DATAFRAME_OBJ_TO_BE_EXPORTED = dataset\n",
        "# Alternatively: object containing the dataset to be exported.\n",
        "# DATAFRAME_OBJ_TO_BE_EXPORTED: dataframe object that is going to be exported from the\n",
        "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
        "# example: DATAFRAME_OBJ_TO_BE_EXPORTED = dataset will export the dataset object.\n",
        "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
        "\n",
        "FILE_DIRECTORY_PATH = \"\"\n",
        "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\"\n",
        "# or FILE_DIRECTORY_PATH = \"folder\"\n",
        "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
        "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
        "\n",
        "NEW_FILE_NAME_WITHOUT_EXTENSION = \"dataset\"\n",
        "# NEW_FILE_NAME_WITHOUT_EXTENSION - (string, in quotes): input the name of the\n",
        "# file without the extension. e.g. set NEW_FILE_NAME_WITHOUT_EXTENSION = \"my_file\"\n",
        "# to export the CSV file 'my_file.csv' to notebook's workspace.\n",
        "\n",
        "export_pd_dataframe_as_csv (dataframe_obj_to_be_exported = DATAFRAME_OBJ_TO_BE_EXPORTED, new_file_name_without_extension = NEW_FILE_NAME_WITHOUT_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH)"
      ],
      "metadata": {
        "azdata_cell_guid": "a9084294-475c-4399-8cae-14dfb382c252",
        "language": "python",
        "id": "CK_91Mf3f7e5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exporting dataframes as Excel file tables**"
      ],
      "metadata": {
        "azdata_cell_guid": "cd09c260-56ab-4a0a-81f0-6ad26cdc6330",
        "id": "bEDquKjEf7e5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## WARNING: all files exported from this function are .xlsx\n",
        "\n",
        "FILE_NAME_WITHOUT_EXTENSION = \"datasets\"\n",
        "# (string, in quotes): input the name of the\n",
        "# file without the extension. e.g. new_file_name_without_extension = \"my_file\"\n",
        "# will export a file 'my_file.xlsx' to notebook's workspace.\n",
        "\n",
        "EXPORTED_TABLES = [{'dataframe_obj_to_be_exported': None,\n",
        "                    'excel_sheet_name': None},]\n",
        "\n",
        "# exported_tables is a list of dictionaries. User may declare several dictionaries,\n",
        "# as long as the keys are always the same, and if the values stored in keys are not None.\n",
        "\n",
        "# key 'dataframe_obj_to_be_exported': dataframe object that is going to be exported from the\n",
        "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
        "# example: dataframe_obj_to_be_exported = dataset will export the dataset object.\n",
        "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
        "\n",
        "# key 'excel_sheet_name': string containing the name of the sheet to be written on the\n",
        "# exported Excel file. Example: excel_sheet_name = 'tab_1' will save the dataframe in the\n",
        "# sheet 'tab_1' from the file named as file_name_without_extension.\n",
        "\n",
        "# examples: exported_tables = [{'dataframe_obj_to_be_exported': dataset1,\n",
        "# 'excel_sheet_name': 'sheet1'},]\n",
        "# will export only dataset1 as 'sheet1';\n",
        "# exported_tables = [{'dataframe_obj_to_be_exported': dataset1, 'excel_sheet_name': 'sheet1'},\n",
        "# {'dataframe_obj_to_be_exported': dataset2, 'excel_sheet_name': 'sheet2']\n",
        "# will export dataset1 as 'sheet1' and dataset2 as 'sheet2'.\n",
        "\n",
        "# Notice that if the file does not contain the exported sheets, they will be created. If it has,\n",
        "# the sheets will be replaced.\n",
        "\n",
        "FILE_DIRECTORY_PATH = \"\"\n",
        "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\"\n",
        "# or FILE_DIRECTORY_PATH = \"folder\"\n",
        "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
        "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
        "\n",
        "\n",
        "export_pd_dataframe_as_excel (file_name_without_extension = FILE_NAME_WITHOUT_EXTENSION, exported_tables = EXPORTED_TABLES, file_directory_path = FILE_DIRECTORY_PATH)"
      ],
      "metadata": {
        "azdata_cell_guid": "8415e3fd-d5ef-4660-98da-52acc222364a",
        "language": "python",
        "id": "ZZ2j_Jxzf7e5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1",
        "id": "dmdKSFY7f7e6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Windowed datasets - Background**\n",
        "\n",
        "`WindowGenerator` class:\n",
        "1. Handle the indexes and offsets.\n",
        "1. Split windows of features into `(features, labels)` pairs.\n",
        "2. Plot the content of the resulting windows.\n",
        "3. Efficiently generate batches of these windows from the training, evaluation, and test data, using `tf.data.Dataset`s.\n",
        "\n",
        "#### 1. Indexes and offsets\n",
        "\n",
        "Start by creating the `WindowGenerator` class. The `__init__` method includes all the necessary logic for the input and label indices.\n",
        "\n",
        "It also takes the training, evaluation, and test DataFrames as input. These will be converted to `tf.data.Dataset`s of windows later.\n",
        "\n",
        "Depending on the task and type of model you may want to generate a variety of data windows. Here are some examples:\n",
        "\n",
        "1. For example, to make a single prediction 24 hours into the future, given 24 hours of history, you might define a window like this:\n",
        "\n",
        "  ![One prediction 24 hours into the future.](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/images/raw_window_24h.png?raw=1)\n",
        "\n",
        "2. A model that makes a prediction one hour into the future, given six hours of history, would need a window like this:\n",
        "\n",
        "  ![One prediction one hour into the future.](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/images/raw_window_1h.png?raw=1)\n",
        "\n",
        "### 2. Split\n",
        "\n",
        "Given a list of consecutive inputs, the `split_window` method will convert them to a window of inputs and a window of labels.\n",
        "\n",
        "The example `w2` you define earlier will be split like this:\n",
        "\n",
        "![The initial window is all consecutive samples, this splits it into an (inputs, labels) pairs](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/structured_data/images/split_window.png?raw=1)\n",
        "\n",
        "This diagram doesn't show the `features` axis of the data, but this `split_window` function also handles the `label_columns` so it can be used for both the single output and multi-output examples."
      ],
      "metadata": {
        "azdata_cell_guid": "dc7c97cb-9672-4f96-9759-2fb508f2b32f",
        "id": "9Ddm19d9f7e6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Classification metrics - Background**"
      ],
      "metadata": {
        "azdata_cell_guid": "460e154f-86dc-4dd3-8e2b-d7f4be80c7e1",
        "id": "YDOm1NP_f7e6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explaining graphic:\n",
        "\n",
        "![TP%20x%20FP%201-2.png](attachment:TP%20x%20FP%201-2.png)\n",
        "\n",
        "https://towardsdatascience.com/how-to-evaluate-your-machine-learning-models-with-python-code-5f8d2d8d945b"
      ],
      "metadata": {
        "azdata_cell_guid": "ed9cbb59-7e8c-4588-97b8-df9dda33e053",
        "tags": [
          "CELL_23"
        ],
        "id": "FcUYwFKkf7e7"
      }
    }
  ]
}