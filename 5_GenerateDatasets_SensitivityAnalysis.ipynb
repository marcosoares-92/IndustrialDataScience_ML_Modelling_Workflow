{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "instance_type": "ml.t3.medium",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcosoares-92/IndustrialDataScience_ML_Modelling_Workflow/blob/main/5_GenerateDatasets_SensitivityAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generate Test Datasets for Sensitivity Analysis**"
      ],
      "metadata": {
        "azdata_cell_guid": "1d4fe9bd-1ec2-400d-99c0-cb17cd588327",
        "id": "fMXsFu0jTn3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _Machine Learning Modelling Workflow Notebook 5_"
      ],
      "metadata": {
        "azdata_cell_guid": "5e348ade-bbd9-48a8-943e-0fc6c2af6dd6",
        "id": "tmRefTZQTn3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content:\n",
        "1. Generate test datasets for sensitivity analysis"
      ],
      "metadata": {
        "azdata_cell_guid": "aab784e5-8907-4b47-8f8b-a3b9d534126a",
        "id": "1O_AoVYJTn3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
        "- marcosoares.feq@gmail.com\n",
        "- marco.soares@bayer.com"
      ],
      "metadata": {
        "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267",
        "id": "VixjNRAKTn3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Python Libraries in Global Context**"
      ],
      "metadata": {
        "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea",
        "id": "umH8z4MYTn3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import load\n",
        "from idsw import *"
      ],
      "metadata": {
        "azdata_cell_guid": "14e13e18-ed87-4518-b6b1-622b0d9efe2e",
        "id": "bzZgOvXCyHHl",
        "language": "python",
        "tags": [
          "CELL_4"
        ]
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Call the functions**"
      ],
      "metadata": {
        "azdata_cell_guid": "b3257c28-4505-4def-8d7c-38d3dc86fe78",
        "id": "zHUhoX1XyHHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing the dataset**"
      ],
      "metadata": {
        "azdata_cell_guid": "a8f0245c-bc90-404d-9923-b448ad5f2a75",
        "id": "SfmYPqiETn4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt),\n",
        "## JSON, txt, or CSV (comma separated values) files. Tables in webpages or html files can also be read.\n",
        "\n",
        "FILE_DIRECTORY_PATH = \"\"\n",
        "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\"\n",
        "# or FILE_DIRECTORY_PATH = \"folder\"\n",
        "\n",
        "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
        "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the\n",
        "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or,\n",
        "# FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
        "# Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
        "# Also, html files and webpages may be also read.\n",
        "\n",
        "# You may input the path for an HTML file containing a table to be read; or\n",
        "# a string containing the address for a webpage containing the table. The address must start\n",
        "# with www or htpp. If a website is input, the full address can be input as FILE_DIRECTORY_PATH\n",
        "# or as FILE_NAME_WITH_EXTENSION.\n",
        "\n",
        "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
        "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True\n",
        "# if you want to read a file with txt extension containing a text formatted as JSON\n",
        "# (but not saved as JSON).\n",
        "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the\n",
        "# function (below) must be set. If not, an error message will be raised.\n",
        "\n",
        "HOW_MISSING_VALUES_ARE_REGISTERED = None\n",
        "# HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
        "# empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
        "# This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
        "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’,\n",
        "#‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’,\n",
        "# ‘n/a’, ‘nan’, ‘null’.\n",
        "\n",
        "# If a different denomination is used, indicate it as a string. e.g.\n",
        "# HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
        "# HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
        "\n",
        "# If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
        "# only in column 'numeric_col', you can specify the following dictionary:\n",
        "# how_missing_values_are_registered = {'numeric-col': 0}\n",
        "\n",
        "\n",
        "HAS_HEADER = True\n",
        "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
        "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
        "\n",
        "DECIMAL_SEPARATOR = '.'\n",
        "# DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
        "# the decimal separator. Alternatively, specify here the separator.\n",
        "# e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
        "# It manipulates the argument 'decimal' from Pandas functions.\n",
        "\n",
        "TXT_CSV_COL_SEP = \"comma\"\n",
        "# txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
        "# or 'csv'. It informs how the different columns are separated.\n",
        "# Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\"\n",
        "# for columns separated by comma;\n",
        "# txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \"\n",
        "# for columns separated by simple spaces.\n",
        "# You can also set a specific separator as string. For example:\n",
        "# txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
        "# is used as separator for the columns - '\\t' represents the tab character).\n",
        "\n",
        "## Parameters for loading Excel files:\n",
        "\n",
        "LOAD_ALL_SHEETS_AT_ONCE = False\n",
        "# LOAD_ALL_SHEETS_AT_ONCE = False - This parameter has effect only when for Excel files.\n",
        "# If LOAD_ALL_SHEETS_AT_ONCE = True, the function will return a list of dictionaries, each\n",
        "# dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
        "# value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
        "# and its value will be the pandas dataframe object obtained from that sheet.\n",
        "# This argument has preference over SHEET_TO_LOAD. If it is True, all sheets will be loaded.\n",
        "\n",
        "SHEET_TO_LOAD = None\n",
        "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
        "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
        "# will be loaded.\n",
        "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
        "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
        "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
        "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
        "# name to load the sheet with that name.\n",
        "\n",
        "## Parameters for loading JSON files:\n",
        "\n",
        "JSON_RECORD_PATH = None\n",
        "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
        "# Path in each object to list of records. If not passed, data will be assumed to\n",
        "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
        "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
        "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
        "\n",
        "JSON_FIELD_SEPARATOR = \"_\"\n",
        "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
        "# Nested records will generate names separated by sep.\n",
        "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
        "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
        "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
        "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
        "\n",
        "JSON_METADATA_PREFIX_LIST = None\n",
        "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter\n",
        "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting\n",
        "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
        "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
        "\n",
        "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
        "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
        "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
        "# are 'name' and 'last'.\n",
        "# Then, JSON_RECORD_PATH = 'books'\n",
        "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
        "\n",
        "\n",
        "# The dataframe will be stored in the object named 'dataset':\n",
        "# Simply modify this object on the left of equality:\n",
        "dataset = load_pandas_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, how_missing_values_are_registered = HOW_MISSING_VALUES_ARE_REGISTERED, has_header = HAS_HEADER, decimal_separator = DECIMAL_SEPARATOR, txt_csv_col_sep = TXT_CSV_COL_SEP, load_all_sheets_at_once = LOAD_ALL_SHEETS_AT_ONCE, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)\n",
        "\n",
        "# OBS: If an Excel file is loaded and LOAD_ALL_SHEETS_AT_ONCE = True, then the object\n",
        "# dataset will be a list of dictionaries, with 'sheet' as key containing the sheet name; and 'df'\n",
        "# as key correspondent to the Pandas dataframe. So, to access the 3rd dataframe (index 2, since\n",
        "# indexing starts from zero): df = dataframe[2]['df'], where dataframe is the list returned."
      ],
      "metadata": {
        "azdata_cell_guid": "c80b5ea6-2d21-47ab-a90c-798700e2cb31",
        "language": "python",
        "id": "7eXDOtisTn4w"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Generating the test datasets for sensitivity analysis**\n",
        "1. The datasets will be generated by: selecting a test variable; dividing its (maximum value) - (minimum value) range into a given number of bins to find a step;\n",
        "2. Filling this range with values separated by this constant step;\n",
        "3. Keeping all other variables equal to the correspondent mean value (numeric variables); or equal to the mode (categorical variables)."
      ],
      "metadata": {
        "azdata_cell_guid": "47e4e6d5-8c05-421d-a5ce-c26d41ed6bc9",
        "id": "ne0KGwoaTn46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = dataset #Alternatively: object containing the original dataset\n",
        "# DATASET: dataset containing historical data, from which the\n",
        "# analysis will be generated.\n",
        "\n",
        "SIMULATED_VARIABLES = \"analyzed_column_name\"\n",
        "# SIMULATED_VARIABLES: name (string) or list of names of the variables that will be tested.\n",
        "# In the generated dataset, the variable SIMULATED_VARIABLEs will be ranged from its\n",
        "# minimum to its maximum value in the original dataset. In turns, the\n",
        "# other variables will be kept constant, and with value set as the\n",
        "# respective mean value (mean values calculated on the original dataset).\n",
        "# e.g. SIMULATED_VARIABLES = \"feature1\" or SIMULATED_VARIABLES = ['col1', 'col2', 'col3']\n",
        "\n",
        "# It allows us to simulate situations where the effects of each\n",
        "# feature are isolated from the variation of the other variables.\n",
        "\n",
        "# Notice that it may be impossible in real scenarios: different constraints\n",
        "# and even the need for keeping the operation ongoing may require the\n",
        "# parameters to be defined in given levels. Also, it is possible that\n",
        "# the variables in the original dataset are all modified simultaneously\n",
        "# and with different rules. Finally, all the variables have their own\n",
        "# sources of variability interacting in the real data, making it\n",
        "# difficult or impossible to observe the correlations present.\n",
        "\n",
        "# Applying the generated dataframes to the obtained models allows us to\n",
        "# understand how each variable influences the responses (isolately) and\n",
        "# how to optimize them.\n",
        "\n",
        "TOTAL_BINS = 50\n",
        "# TOTAL_BINS: amount of divisions of the tested range, i.e, into how much\n",
        "# bins we will split the variables, from their minimum to their maximum\n",
        "# values in the original dataset.\n",
        "# The range (max - min) of the variable will be divided into this number\n",
        "# of bins.\n",
        "# So, TOTAL_BINS will be the number of rows of the generated dataset\n",
        "# (in fact, since the division may not result into an integer, the number\n",
        "# of rows may be total_bins +- 1).\n",
        "\n",
        "# For instance: if a variable Y ranges from 0 to 10, and TOTAL_BINS = 11,\n",
        "# we will create a dataset with the following values of Y:\n",
        "# Y = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
        "# Each generated value will be stored as a different row (an entry)\n",
        "# of the generated dataset.\n",
        "\n",
        "\n",
        "# dictionary containing datasets for sensitivity analysis returned as simulation_dfs_dict.\n",
        "# The dataframes are stored in the key 'sensitivity_analysis_df'. The keys to access the nested\n",
        "# dictionaries are integers starting from zero, representing the position (order) of the generated\n",
        "# dataframe. For example, simulation_dfs_dict[0]['sensitivity_analysis_df'] access the 1st dataframe,\n",
        "# simulation_dfs_dict[1]['sensitivity_analysis_df'] access the 2nd dataframe, and so on.\n",
        "# Simply modify this object on the left of equality:\n",
        "simulation_dfs_dict = generateSensitivityAnalysis_dataset (df = DATASET, simulated_variables = SIMULATED_VARIABLES, total_bins = TOTAL_BINS)"
      ],
      "metadata": {
        "azdata_cell_guid": "93f8a83f-c72f-4ec9-b01c-6d1175e20b17",
        "language": "python",
        "id": "CcSvZQ37Tn4-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exporting the dataframe as CSV file (to notebook's workspace)**"
      ],
      "metadata": {
        "azdata_cell_guid": "b837bbf8-4d88-4bb7-bb6e-ac02bb9b3ac9",
        "id": "GfGhrrlTTn5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## WARNING: all files exported from this function are .csv (comma separated values)\n",
        "\n",
        "DATAFRAME_OBJ_TO_BE_EXPORTED = dataset\n",
        "# Alternatively: object containing the dataset to be exported.\n",
        "# DATAFRAME_OBJ_TO_BE_EXPORTED: dataframe object that is going to be exported from the\n",
        "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
        "# example: DATAFRAME_OBJ_TO_BE_EXPORTED = dataset will export the dataset object.\n",
        "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
        "\n",
        "FILE_DIRECTORY_PATH = \"\"\n",
        "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\"\n",
        "# or FILE_DIRECTORY_PATH = \"folder\"\n",
        "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
        "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
        "\n",
        "NEW_FILE_NAME_WITHOUT_EXTENSION = \"dataset\"\n",
        "# NEW_FILE_NAME_WITHOUT_EXTENSION - (string, in quotes): input the name of the\n",
        "# file without the extension. e.g. set NEW_FILE_NAME_WITHOUT_EXTENSION = \"my_file\"\n",
        "# to export the CSV file 'my_file.csv' to notebook's workspace.\n",
        "\n",
        "export_pd_dataframe_as_csv (dataframe_obj_to_be_exported = DATAFRAME_OBJ_TO_BE_EXPORTED, new_file_name_without_extension = NEW_FILE_NAME_WITHOUT_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH)"
      ],
      "metadata": {
        "azdata_cell_guid": "7d3abc95-7bb5-4270-baa7-c8b285dbf3d8",
        "language": "python",
        "id": "ICROQvzRTn5F"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exporting dataframes as Excel file tables**"
      ],
      "metadata": {
        "azdata_cell_guid": "b3d9dc8f-53ef-4f46-b3ab-80378ff816e3",
        "id": "2iaIXvGtTn5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## WARNING: all files exported from this function are .xlsx\n",
        "\n",
        "FILE_NAME_WITHOUT_EXTENSION = \"datasets\"\n",
        "# (string, in quotes): input the name of the\n",
        "# file without the extension. e.g. new_file_name_without_extension = \"my_file\"\n",
        "# will export a file 'my_file.xlsx' to notebook's workspace.\n",
        "\n",
        "EXPORTED_TABLES = [{'dataframe_obj_to_be_exported': None,\n",
        "                    'excel_sheet_name': None},]\n",
        "\n",
        "# exported_tables is a list of dictionaries. User may declare several dictionaries,\n",
        "# as long as the keys are always the same, and if the values stored in keys are not None.\n",
        "\n",
        "# key 'dataframe_obj_to_be_exported': dataframe object that is going to be exported from the\n",
        "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
        "# example: dataframe_obj_to_be_exported = dataset will export the dataset object.\n",
        "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
        "\n",
        "# key 'excel_sheet_name': string containing the name of the sheet to be written on the\n",
        "# exported Excel file. Example: excel_sheet_name = 'tab_1' will save the dataframe in the\n",
        "# sheet 'tab_1' from the file named as file_name_without_extension.\n",
        "\n",
        "# examples: exported_tables = [{'dataframe_obj_to_be_exported': dataset1,\n",
        "# 'excel_sheet_name': 'sheet1'},]\n",
        "# will export only dataset1 as 'sheet1';\n",
        "# exported_tables = [{'dataframe_obj_to_be_exported': dataset1, 'excel_sheet_name': 'sheet1'},\n",
        "# {'dataframe_obj_to_be_exported': dataset2, 'excel_sheet_name': 'sheet2']\n",
        "# will export dataset1 as 'sheet1' and dataset2 as 'sheet2'.\n",
        "\n",
        "# Notice that if the file does not contain the exported sheets, they will be created. If it has,\n",
        "# the sheets will be replaced.\n",
        "\n",
        "FILE_DIRECTORY_PATH = \"\"\n",
        "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\"\n",
        "# or FILE_DIRECTORY_PATH = \"folder\"\n",
        "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
        "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
        "\n",
        "\n",
        "export_pd_dataframe_as_excel (file_name_without_extension = FILE_NAME_WITHOUT_EXTENSION, exported_tables = EXPORTED_TABLES, file_directory_path = FILE_DIRECTORY_PATH)"
      ],
      "metadata": {
        "azdata_cell_guid": "f2801390-9521-4610-8138-9f1990972f8e",
        "language": "python",
        "id": "ttTsG7JATn5I"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1",
        "id": "V_GDTCO4Tn5K"
      }
    }
  ]
}