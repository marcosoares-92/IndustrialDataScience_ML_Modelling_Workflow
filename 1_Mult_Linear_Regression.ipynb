{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Multiple Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Machine Learning Modelling Workflow Notebook 1_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content:\n",
    "1. Splitting the dataframe into train and test subsets;\n",
    "2. Ordinary Least Squares (OLS) Linear Regression;\n",
    "3. Ridge Linear Regression;\n",
    "4. Lasso Linear Regression;\n",
    "5. Elastic Net Linear Regression;\n",
    "6. Getting a general feature ranking;\n",
    "7. Calculating metrics for regression models;\n",
    "8. Making predictions with the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267"
   },
   "source": [
    "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
    "- marcosoares.feq@gmail.com\n",
    "- marco.soares@bayer.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5dbf6196-9599-414f-a337-ec52f7f2122c",
    "id": "zGDOhNQbGzrr"
   },
   "source": [
    "Install statsmodels library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "11e929c6-8980-4298-8ded-b40a2080913e",
    "id": "7q1Bfti3Gzrs"
   },
   "outputs": [],
   "source": [
    "! pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5dbf6196-9599-414f-a337-ec52f7f2122c",
    "id": "zGDOhNQbGzrr"
   },
   "source": [
    "Install tensorflow library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "11e929c6-8980-4298-8ded-b40a2080913e",
    "id": "7q1Bfti3Gzrs"
   },
   "outputs": [],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1a5c1713-e549-4b18-bbaf-114c019c976d",
    "id": "wXnEEdHuGzru"
   },
   "source": [
    "Install Keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "dbd37f46-51f0-4a30-a812-4b27679ff1a1",
    "id": "N1OlfernGzrw"
   },
   "outputs": [],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e691d1ee-cb58-482b-9edb-d6baca45fdb3",
    "id": "Qowk3bTaGzrx"
   },
   "source": [
    "Install SHAP library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "245331e6-1f20-48ce-8ab9-00d47c128616",
    "id": "nC2bqVfxGzry"
   },
   "outputs": [],
   "source": [
    "! pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "7ec04518-6f8a-43a0-868e-3932f998886e",
    "id": "7ldt-mnjGzrz",
    "outputId": "b70c28af-c67c-4c61-a7b3-2a21bf4c6d64"
   },
   "outputs": [],
   "source": [
    "#check the version of the package\n",
    "! pip show shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e9286147-d907-490e-8596-ec38fc6c21c8",
    "id": "H9YCpVctGzr1"
   },
   "outputs": [],
   "source": [
    "# Upgrade to the most recent library versions, if a given module is not present and analysis cannot be\n",
    "# executed.\n",
    "! pip install pip --upgrade\n",
    "! pip install tensorflow --upgrade\n",
    "! pip install keras --upgrade\n",
    "! pip install shap --upgrade\n",
    "! pip install sklearn --upgrade\n",
    "! pip install pandas --upgrade\n",
    "! pip install numpy --upgrade\n",
    "! pip install matplotlib --upgrade\n",
    "! pip install seaborn --upgrade\n",
    "! pip install scipy --upgrade\n",
    "! pip install statsmodels --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea"
   },
   "source": [
    "## **Load Python Libraries in Global Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzZgOvXCyHHl",
    "tags": [
     "CELL_4"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels as sm\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mount_storage_system (source = 'aws', path_to_store_imported_s3_bucket = '/', s3_bucket_name = None, s3_obj_key_preffix = None):\n",
    "    \n",
    "    import sagemaker\n",
    "    # sagemaker is AWS SageMaker Python SDK\n",
    "    from sagemaker.session import Session\n",
    "    from google.colab import drive\n",
    "    \n",
    "    # source = 'google' for mounting the google drive;\n",
    "    # source = 'aws' for mounting an AWS S3 bucket.\n",
    "    \n",
    "    # THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN source == 'aws'\n",
    "    \n",
    "    # path_to_store_imported_s3_bucket: path of the Python environment to which the\n",
    "    # S3 bucket contents will be imported. If it is None, or if \n",
    "    # path_to_store_imported_s3_bucket = '/', bucket will be imported to the root path. \n",
    "    # Alternatively, input the path as a string (in quotes). e.g. \n",
    "    # path_to_store_imported_s3_bucket = '/copied_s3_bucket'\n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # s3_obj_key_preffix = None. Keep it None or as an empty string (s3_obj_key_preffix = '')\n",
    "    # to import the whole bucket content, instead of a single object from it.\n",
    "    # Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "    # Suppose that your bucket (admin-created) has four objects with the following object \n",
    "    # keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "    # s3-dg.pdf. The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "    # at the root level of the bucket. If you open the Development/ folder, you see \n",
    "    # the Projects.xlsx object in it.\n",
    "    # Check Amazon documentation:\n",
    "    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\n",
    "    \n",
    "    # In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "    # where 'bucket' is the bucket's name, key_preffix = 'my_path/.../', without the\n",
    "    # 'file.csv' (file name with extension) last part.\n",
    "    \n",
    "    if (source == 'google'):\n",
    "        \n",
    "        print(\"Associate the Python environment to your Google Drive account, and authorize the access in the opened window.\")\n",
    "        \n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        print(\"Now your Python environment is connected to your Google Drive: the root directory of your environment is now the root of your Google Drive.\")\n",
    "        print(\"In Google Colab, navigate to the folder icon (\\'Files\\') of the left navigation menu to find a specific folder or file in your Google Drive.\")\n",
    "        print(\"Click on the folder or file name and select the elipsis (...) icon on the right of the name to reveal the option \\'Copy path\\', which will give you the path to use as input for loading objects and files on your Python environment.\")\n",
    "        print(\"Caution: save your files into different directories of the Google Drive. If files are all saved in a same folder or directory, like the root path, they may not be accessible from your Python environment.\")\n",
    "        print(\"If you still cannot see the file after moving it to a different folder, reload the environment.\")\n",
    "    \n",
    "    elif (source == 'aws'):\n",
    "        \n",
    "        # Notice: if you wanted to authenticate directly from Python code, you could use\n",
    "        # the following code, instead, to start the S3 client. boto3 is AWS S3 Python SDK:\n",
    "        \n",
    "        # import boto3\n",
    "        # ACCESS_KEY = 'access_key_ID'\n",
    "        # PASSWORD_KEY = 'password_key'\n",
    "        # s3_client = boto3.client('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = PASSWORD_KEY)\n",
    "        # ... [here, use the same following code until line new_session = Session()]\n",
    "        # [keep the line for session start. Substitute the line with the .download_data\n",
    "        # method by the following line:]\n",
    "        # s3_client.download_file(s3_bucket_name, s3_file_name_with_extension, path_to_store_imported_s3_bucket)\n",
    "        \n",
    "        # Check if the whole bucket will be downloaded (s3_obj_key_preffix = None):\n",
    "        if (s3_obj_key_preffix is None):\n",
    "            \n",
    "            s3_obj_key_preffix = ''\n",
    "        \n",
    "        # If the path to store is None, also import the bucket to the root path:\n",
    "        if (path_to_store_imported_s3_bucket is None):\n",
    "            \n",
    "            path_to_store_imported_s3_bucket = '/'\n",
    "        \n",
    "        # If the bucket name was provided, start the session. If not, print an error\n",
    "        # message:\n",
    "        \n",
    "        if (s3_bucket_name is None):\n",
    "            \n",
    "            print(\"Please, provide a valid S3 Bucket name to download from.\")\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            # start a new sagemaker session:\n",
    "\n",
    "            print(\"Starting a SageMaker session to be associated with the S3 bucket.\")\n",
    "\n",
    "            new_session = Session()\n",
    "            # Check sagemaker session class documentation:\n",
    "            # https://sagemaker.readthedocs.io/en/stable/api/utility/session.html\n",
    "            session.download_data(path = path_to_store_imported_s3_bucket, bucket = s3_bucket_name, key_prefix = s3_obj_key_preffix)\n",
    "\n",
    "            print(f\"S3 bucket contents successfully imported to path \\'{path_to_store_imported_s3_bucket}\\'.\")\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        print(\"Select a valid source: \\'google\\' for mounting Google Drive; or \\'aws\\' for accessing AWS S3 Bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for loading the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe (file_directory_path, file_name_with_extension, load_txt_file_with_json_format = False, has_header = True, txt_csv_col_sep = \"comma\", sheet_to_load = None, json_record_path = None, json_field_separator = \"_\", json_metadata_preffix_list = None):\n",
    "    \n",
    "    import os\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    \n",
    "    ## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, etc), \n",
    "    ## JSON, txt, or CSV (comma separated values) files.\n",
    "    \n",
    "    # file_directory_path - (string, in quotes): input the path of the directory (e.g. folder path) \n",
    "    # where the file is stored. e.g. file_directory_path = \"/\" or file_directory_path = \"/folder\"\n",
    "    \n",
    "    # file_name_with_extension - (string, in quotes): input the name of the file with the extension\n",
    "    # e.g. file_name_with_extension = \"file.xlsx\", or, file_name_with_extension = \"file.csv\"\n",
    "    \n",
    "    # load_txt_file_with_json_format = False. Set load_txt_file_with_json_format = True \n",
    "    # if you want to read a file with txt extension containing a text formatted as JSON \n",
    "    # (but not saved as JSON).\n",
    "    # WARNING: if load_txt_file_with_json_format = True, all the JSON file parameters of the \n",
    "    # function (below) must be set. If not, an error message will be raised.\n",
    "    \n",
    "    # has_header = True if the the imported table has headers (row with columns names).\n",
    "    # Alternatively, has_header = False if the dataframe does not have header.\n",
    "    \n",
    "    ## Parameters for loading txt or CSV files\n",
    "    \n",
    "    # txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "    # or 'csv'. It informs how the different columns are separated.\n",
    "    # Alternatively, txt_csv_col_sep = \"comma\" for columns separated by comma (\",\")\n",
    "    # txt_csv_col_sep = \"whitespace\" for columns separated by simple spaces (\" \").\n",
    "    \n",
    "    # sheet_to_load - This parameter has effect only when for Excel files.\n",
    "    # keep sheet_to_load = None not to specify a sheet of the file, so that the first sheet\n",
    "    # will be loaded.\n",
    "    # sheet_to_load may be an integer or an string (inside quotes). sheet_to_load = 0\n",
    "    # loads the first sheet (sheet with index 0); sheet_to_load = 1 loads the second sheet\n",
    "    # of the file (index 1); sheet_to_load = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "    # Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "    # name to load the sheet with that name.\n",
    "    \n",
    "    ## Parameters for loading JSON files:\n",
    "    \n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html#pandas.json_normalize\n",
    "    \n",
    "    # json_record_path (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "    # Path in each object to list of records. If not passed, data will be assumed to \n",
    "    # be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "    # dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "    # 'books' stores a nested JSON, declare, json_record_path = 'books'\n",
    "    \n",
    "    # json_field_separator = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "    # Nested records will generate names separated by sep. \n",
    "    # e.g., for json_field_separator = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "    # Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "    # the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "    # separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "    \n",
    "    # json_metadata_preffix_list: list of strings (in quotes). Manipulates the parameter \n",
    "    # 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "    # table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "    # will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "    \n",
    "    # e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "    # 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "    # Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "    # are 'name' and 'last'.\n",
    "    # Then, json_record_path = 'books'\n",
    "    # json_metadata_preffix_list = ['name', 'last']\n",
    "    \n",
    "    # Create the complete file path:\n",
    "    file_path = os.path.join(file_directory_path, file_name_with_extension)\n",
    "    # Extract the file extension\n",
    "    file_extension = os.path.splitext(file_path)[1][1:]\n",
    "    # os.path.splitext(file_path) is a tuple of strings: the first is the complete file\n",
    "    # root with no extension; the second is the extension starting with a point: '.txt'\n",
    "    # When we set os.path.splitext(file_path)[1], we are selecting the second element of\n",
    "    # the tuple. By selecting os.path.splitext(file_path)[1][1:], we are taking this string\n",
    "    # from the second character (index 1), eliminating the dot: 'txt'\n",
    "    \n",
    "    if ((file_extension == 'txt') | (file_extension == 'csv')): \n",
    "        # The operator & is equivalent to 'And' (intersection).\n",
    "        # The operator | is equivalent to 'Or' (union).\n",
    "        # pandas.read_csv method must be used.\n",
    "        if (load_txt_file_with_json_format == True):\n",
    "            \n",
    "            print(\"Reading a txt file containing JSON parsed data. A reading error will be raised if you did not set the JSON parameters.\")\n",
    "            \n",
    "            with open(file_path, 'r') as opened_file:\n",
    "                # 'r' stands for read mode; 'w' stands for write mode\n",
    "                # read the whole file as a string named 'file_full_text'\n",
    "                file_full_text = opened_file.read()\n",
    "                # if we used the readlines() method, we would be reading the\n",
    "                # file by line, not the whole text at once.\n",
    "                # https://stackoverflow.com/questions/8369219/how-to-read-a-text-file-into-a-string-variable-and-strip-newlines?msclkid=a772c37bbfe811ec9a314e3629df4e1e\n",
    "                # https://www.tutorialkart.com/python/python-read-file-as-string/#:~:text=example.py%20%E2%80%93%20Python%20Program.%20%23open%20text%20file%20in,and%20prints%20it%20to%20the%20standard%20output.%20Output.?msclkid=a7723a1abfe811ecb68bba01a2b85bd8\n",
    "                \n",
    "            #Now, file_full_text is a string containing the full content of the txt file.\n",
    "            json_file = json.loads(file_full_text)\n",
    "            # json.load() : This method is used to parse JSON from URL or file.\n",
    "            # json.loads(): This method is used to parse string with JSON content.\n",
    "            # e.g. .json.loads() must be used to read a string with JSON and convert it to a flat file\n",
    "            # like a dataframe.\n",
    "            # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.\n",
    "            dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_preffix_list)\n",
    "        \n",
    "        else:\n",
    "            # Not a JSON txt\n",
    "        \n",
    "            if (has_header == True):\n",
    "\n",
    "                if (txt_csv_col_sep == \"comma\"):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path)\n",
    "\n",
    "                elif (txt_csv_col_sep == \"whitespace\"):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, delim_whitespace = True)\n",
    "\n",
    "                else:\n",
    "                    print(f\"Enter a valid column separator for the {file_extension} file: \\'comma\\' or \\'whitespace\\'.\")\n",
    "\n",
    "            else:\n",
    "                # has_header == False\n",
    "\n",
    "                if (txt_csv_col_sep == \"comma\"):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, header = None)\n",
    "\n",
    "                elif (txt_csv_col_sep == \"whitespace\"):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, delim_whitespace = True, header = None)\n",
    "\n",
    "                else:\n",
    "                    print(f\"Enter a valid column separator for the {file_extension} file: \\'comma\\' or \\'whitespace\\'.\")\n",
    "\n",
    "    elif (file_extension == 'json'):\n",
    "        \n",
    "        with open(file_path) as opened_file:\n",
    "            \n",
    "            json_file = json.load(opened_file)\n",
    "            # The structure json_file = json.load(open(file_path)) relies on the GC to close the file. That's not a \n",
    "            # good idea: If someone doesn't use CPython the garbage collector might not be using refcounting (which \n",
    "            # collects unreferenced objects immediately) but e.g. collect garbage only after some time.\n",
    "            # Since file handles are closed when the associated object is garbage collected or closed \n",
    "            # explicitly (.close() or .__exit__() from a context manager) the file will remain open until \n",
    "            # the GC kicks in.\n",
    "            # Using 'with' ensures the file is closed as soon as the block is left - even if an exception \n",
    "            # happens inside that block, so it should always be preferred for any real application.\n",
    "            # source: https://stackoverflow.com/questions/39447362/equivalent-ways-to-json-load-a-file-in-python\n",
    "            \n",
    "        # json.load() : This method is used to parse JSON from URL or file.\n",
    "        # json.loads(): This method is used to parse string with JSON content.\n",
    "        # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.   \n",
    "        dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_preffix_list)\n",
    "    \n",
    "    else:\n",
    "        # If it is not neither a csv nor a txt file, let's assume it is one of different\n",
    "        # possible Excel files.\n",
    "        print(\"Excel file inferred. If an error message is shown, check if a valid file extension was used: \\'xlsx\\', \\'xls\\', etc.\")\n",
    "            \n",
    "        if (sheet_to_load is not None):        \n",
    "        #Case where the user specifies which sheet of the Excel file should be loaded.\n",
    "            \n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load)\n",
    "            \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load, header = None)\n",
    "        \n",
    "        else:\n",
    "            #No sheet specified\n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path)\n",
    "            \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, header = None)\n",
    "    \n",
    "    print(f\"Dataset extracted from {file_path}. Check the 10 first rows of this dataframe:\\n\")\n",
    "    print(dataset.head(10))\n",
    "    \n",
    "    return dataset   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for converting JSON object to dataframe**\n",
    "- Objects may be:\n",
    "    - String with JSON formatted text;\n",
    "    - List with nested dictionaries (JSON formatted);\n",
    "    - Dictionaries, possibly with nested dictionaries (JSON formatted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_obj_to_dataframe (json_obj_to_convert, json_record_path = None, json_field_separator = \"_\", json_metadata_preffix_list = None):\n",
    "    \n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    \n",
    "    # json_obj_to_convert: object containing JSON, or string with JSON content to parse.\n",
    "    # Objects may be: string with JSON formatted text;\n",
    "    # list with nested dictionaries (JSON formatted);\n",
    "    # dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "    \n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html#pandas.json_normalize\n",
    "    \n",
    "    # json_record_path (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "    # Path in each object to list of records. If not passed, data will be assumed to \n",
    "    # be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "    # dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "    # 'books' stores a nested JSON, declare, json_record_path = 'books'\n",
    "    \n",
    "    # json_field_separator = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "    # Nested records will generate names separated by sep. \n",
    "    # e.g., for json_field_separator = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "    # Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "    # the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "    # separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "    \n",
    "    # json_metadata_preffix_list: list of strings (in quotes). Manipulates the parameter \n",
    "    # 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "    # table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "    # will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "    \n",
    "    # e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "    # 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "    # Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "    # are 'name' and 'last'.\n",
    "    # Then, json_record_path = 'books'\n",
    "    # json_metadata_preffix_list = ['name', 'last']\n",
    "    \n",
    "    json_file = json.loads(json_obj_to_convert)\n",
    "    # json.load() : This method is used to parse JSON from URL or file.\n",
    "    # json.loads(): This method is used to parse string with JSON content.\n",
    "    # e.g. .json.loads() must be used to read a string with JSON and convert it to a flat file\n",
    "    # like a dataframe.\n",
    "    # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.\n",
    "    dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_preffix_list)\n",
    "    \n",
    "    print(f\"JSON object {json_obj_to_convert} converted to a flat dataframe object. Check the 10 first rows of this dataframe:\\n\")\n",
    "    print(dataset.head(10))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for concatenating (SQL UNION) multiple dataframes**\n",
    "- Vertical concatenation of the dataframes.\n",
    "- Equivalent to SQL Union: vertical stack/append of the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNION_DATAFRAMES (list_of_dataframes, what_to_append = 'rows', ignore_index_on_union = True, sort_values_on_union = True, union_join_type = None):\n",
    "    \n",
    "    import pandas as pd\n",
    "    #JOIN can be 'inner' to perform an inner join, eliminating the missing values\n",
    "    #The default (None) is 'outer': the dataframes will be stacked on the columns with\n",
    "    #same names but, in case there is no correspondence, the row will present a missing\n",
    "    #value for the columns which are not present in one of the dataframes.\n",
    "    #When using the 'inner' method, only the common columns will remain\n",
    "    \n",
    "    #list_of_dataframes must be a list containing the dataframe objects\n",
    "    # example: list_of_dataframes = [df1, df2, df3, df4]\n",
    "    #Notice that the dataframes are objects, not strings. Therefore, they should not\n",
    "    # be declared inside quotes.\n",
    "    # There is no limit of dataframes. In this example, we will concatenate 4 dataframes.\n",
    "    # If list_of_dataframes = [df1, df2, df3] we would concatenate 3, and if\n",
    "    # list_of_dataframes = [df1, df2, df3, df4, df5] we would concatenate 5 dataframes.\n",
    "    \n",
    "    # what_to_append = 'rows' for appending the rows from one dataframe\n",
    "    # into the other; what_to_append = 'columns' for appending the columns\n",
    "    # from one dataframe into the other (horizontal or lateral append).\n",
    "    \n",
    "    # When what_to_append = 'rows', Pandas .concat method is defined as\n",
    "    # axis = 0, i.e., the operation occurs in the row level, so the rows\n",
    "    # of the second dataframe are added to the bottom of the first one.\n",
    "    # It is the SQL union, and creates a dataframe with more rows, and\n",
    "    # total of columns equals to the total of columns of the first dataframe\n",
    "    # plus the columns of the second one that were not in the first dataframe.\n",
    "    # When what_to_append = 'columns', Pandas .concat method is defined as\n",
    "    # axis = 1, i.e., the operation occurs in the column level: the two\n",
    "    # dataframes are laterally merged using the index as the key, \n",
    "    # preserving all columns from both dataframes. Therefore, the number of\n",
    "    # rows will be the total of rows of the dataframe with more entries,\n",
    "    # and the total of columns will be the sum of the total of columns of\n",
    "    # the first dataframe with the total of columns of the second dataframe.\n",
    "    \n",
    "    #The other parameters are the same from Pandas .concat method.\n",
    "    # ignore_index_on_union = ignore_index;\n",
    "    # sort_values_on_union = sort\n",
    "    # union_join_type = join\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\n",
    "    \n",
    "    #Check Datacamp course Joining Data with pandas, Chap.3, \n",
    "    # Advanced Merging and Concatenating\n",
    "    \n",
    "    # Check axis:\n",
    "    if (what_to_append == 'rows'):\n",
    "        \n",
    "        AXIS = 0\n",
    "    \n",
    "    elif (what_to_append == 'columns'):\n",
    "        \n",
    "        AXIS = 1\n",
    "    \n",
    "    else:\n",
    "        print(\"No valid string was input to what_to_append, so appending rows (vertical append, equivalent to SQL UNION).\")\n",
    "        AXIS = 0\n",
    "    \n",
    "    if (union_join_type == 'inner'):\n",
    "        \n",
    "        print(\"Warning: concatenating dataframes using the \\'inner\\' join method, that removes missing values.\")\n",
    "        concat_df = pd.concat(list_of_dataframes, axis = AXIS, ignore_index = ignore_index_on_union, sort = sort_values_on_union, join = union_join_type)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #In case None or an invalid value is provided, use the default 'outer', by simply\n",
    "        # not declaring the 'join':\n",
    "        concat_df = pd.concat(list_of_dataframes, axis = AXIS, ignore_index = ignore_index_on_union, sort = sort_values_on_union)\n",
    "    \n",
    "    # Pandas .head(Y) method results in a dataframe containing the first Y rows of the \n",
    "    # original dataframe. The default .head() is Y = 5. Print first 10 rows of the \n",
    "    # new dataframe:\n",
    "    print(\"Dataframes successfully concatenated. Check the 10 first rows of new dataframe:\\n\")\n",
    "    print(concat_df.head(10))\n",
    "    \n",
    "    #Now return the concatenated dataframe:\n",
    "    \n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for column filtering (selecting); or column renaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_filter_rename (df, cols_list, mode = 'filter'):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    #mode = 'filter' for filtering only the list of columns passed as cols_list;\n",
    "    #mode = 'rename' for renaming the columns with the names passed as cols_list.\n",
    "    \n",
    "    #cols_list = list of strings containing the names (headers) of the columns to select\n",
    "    # (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "    # For instance: cols_list = ['col1', 'col2', 'col3'] will \n",
    "    # select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "    # Declare the names inside quotes.\n",
    "    \n",
    "    print(f\"Original columns in the dataframe:\\n{df.columns}\")\n",
    "    \n",
    "    if (mode == 'filter'):\n",
    "        \n",
    "        #filter the dataframe so that it will contain only the cols_list.\n",
    "        df = df[cols_list]\n",
    "        print(\"Dataframe filtered according to the list provided.\")\n",
    "        \n",
    "    elif (mode == 'rename'):\n",
    "        \n",
    "        #Check if the number of columns of the dataset is equal to the number of elements\n",
    "        # of the new list. It will avoid raising an exception error.\n",
    "        boolean_filter = (len(cols_list) == len(df.columns))\n",
    "        \n",
    "        if (boolean_filter == False):\n",
    "            #Impossible to rename, number of elements are different.\n",
    "            print(\"The number of columns of the dataframe is different from the number of elements of the list. Please, provide a list with number of elements equals to the number of columns.\")\n",
    "        \n",
    "        else:\n",
    "            #Same number of elements, so that we can update the columns' names.\n",
    "            df.columns = cols_list\n",
    "            print(\"Dataframe columns renamed according to the list provided.\")\n",
    "            print(\"Warning: the substitution is element-wise: the first element of the list is now the name of the first column, and so on, ..., so that the last element is the name of the last column.\")\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"Enter a valid mode: \\'filter\\' or \\'rename\\'.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for plotting the bar chart**\n",
    "- Bars may be vertically or horizontally oriented.\n",
    "- Bar charts are plotted after selecting an aggregation function, and the cumulative percent curve may be obtained and plotted with the bars (in secondary axis).\n",
    "- To obtain a **Pareto chart**, keep `aggregate_function = 'sum'`, `plot_cumulative_percent = True`, and `orientation = 'vertical'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "CELL_7"
    ]
   },
   "outputs": [],
   "source": [
    "def bar_chart (df, categorical_var_name, response_var_name, aggregate_function = 'sum', add_suffix_to_aggregated_col = True, suffix = None, calculate_and_plot_cumulative_percent = True, orientation = 'vertical', limit_of_plotted_categories = None, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # df: dataframe being analyzed\n",
    "    \n",
    "    # categorical_var_name: string (inside quotes) containing the name \n",
    "    # of the column to be analyzed. e.g. \n",
    "    # categorical_var_name = \"column1\"\n",
    "    \n",
    "    # response_var_name: string (inside quotes) containing the name \n",
    "    # of the column that stores the response correspondent to the\n",
    "    # categories. e.g. response_var_name = \"response_feature\" \n",
    "    \n",
    "    # aggregate_function = 'sum': String defining the aggregation \n",
    "    # method that will be applied. Possible values:\n",
    "    # 'median', 'mean', 'mode', 'sum', 'min', 'max', 'variance',\n",
    "    # 'standard_deviation','10_percent_quantile', '20_percent_quantile',\n",
    "    # '25_percent_quantile', '30_percent_quantile', '40_percent_quantile',\n",
    "    # '50_percent_quantile', '60_percent_quantile', '70_percent_quantile',\n",
    "    # '75_percent_quantile', '80_percent_quantile', '90_percent_quantile',\n",
    "    # and '95_percent_quantile'.\n",
    "    # To use another aggregate function, the method must be added to the\n",
    "    # dictionary of methods agg_methods_dict, defined in the function.\n",
    "    # If None or an invalid function is input, 'sum' will be used.\n",
    "    \n",
    "    # add_suffix_to_aggregated_col = True will add a suffix to the\n",
    "    # aggregated column. e.g. 'responseVar_mean'. If add_suffix_to_aggregated_col \n",
    "    # = False, the aggregated column will have the original column name.\n",
    "    \n",
    "    # suffix = None. Keep it None if no suffix should be added, or if\n",
    "    # the name of the aggregate function should be used as suffix, after\n",
    "    # \"_\". Alternatively, set it as a string. As recommendation, put the\n",
    "    # \"_\" sign in the beginning of this string to separate the suffix from\n",
    "    # the original column name. e.g. if the response variable is 'Y' and\n",
    "    # suffix = '_agg', the new aggregated column will be named as 'Y_agg'\n",
    "    \n",
    "    # calculate_and_plot_cumulative_percent = True to calculate and plot\n",
    "    # the line of cumulative percent, or \n",
    "    # calculate_and_plot_cumulative_percent = False to omit it.\n",
    "    # This feature is only shown when aggregate_function = 'sum', 'median',\n",
    "    # 'mean', or 'mode'. So, it will be automatically set as False if \n",
    "    # another aggregate is selected.\n",
    "    \n",
    "    # orientation = 'vertical' is the standard, and plots vertical bars\n",
    "    # (perpendicular to the X axis). In this case, the categories are shown\n",
    "    # in the X axis, and the correspondent responses are in Y axis.\n",
    "    # Alternatively, orientation = 'horizontal' results in horizontal bars.\n",
    "    # In this case, categories are in Y axis, and responses in X axis.\n",
    "    # If None or invalid values are provided, orientation is set as 'vertical'.\n",
    "    \n",
    "    # Note: to obtain a Pareto chart, keep aggregate_function = 'sum',\n",
    "    # plot_cumulative_percent = True, and orientation = 'vertical'.\n",
    "    \n",
    "    # limit_of_plotted_categories: integer value that represents\n",
    "    # the maximum of categories that will be plot. Keep it None to plot\n",
    "    # all categories. Alternatively, set an integer value. e.g.: if\n",
    "    # limit_of_plotted_categories = 4, but there are more categories,\n",
    "    # the dataset will be sorted in descending order and: 1) The remaining\n",
    "    # categories will be sum in a new category named 'others' if the\n",
    "    # aggregate function is 'sum'; 2) Or the other categories will be simply\n",
    "    # omitted from the plot, for other aggregate functions. Notice that\n",
    "    # it limits only the variables in the plot: all of them will be\n",
    "    # returned in the dataframe.\n",
    "    # Use this parameter to obtain a cleaner plot. Notice that the remaining\n",
    "    # columns will be aggregated as 'others' even if there is a single column\n",
    "    # beyond the limit.\n",
    "    \n",
    "    \n",
    "    # Create a local copy of the dataframe to manipulate:\n",
    "    \n",
    "    DATASET = df\n",
    "    \n",
    "    # Create the dictionary of possible aggregates, to define the\n",
    "    # aggregation method, according to the set by the user:\n",
    "    agg_methods_dict = {\n",
    "        \n",
    "        'median': DATASET.groupby(categorical_var_name)[response_var_name].median(),\n",
    "        'mean': DATASET.groupby(categorical_var_name)[response_var_name].mean(),\n",
    "        'mode': DATASET.groupby(categorical_var_name)[response_var_name].mode(),\n",
    "        'sum': DATASET.groupby(categorical_var_name)[response_var_name].sum(),\n",
    "        'min': DATASET.groupby(categorical_var_name)[response_var_name].min(),\n",
    "        'max': DATASET.groupby(categorical_var_name)[response_var_name].max(),\n",
    "        'variance': DATASET.groupby(categorical_var_name)[response_var_name].var(),\n",
    "        'standard_deviation': DATASET.groupby(categorical_var_name)[response_var_name].std(),\n",
    "        '10_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.10),\n",
    "        '20_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.20),\n",
    "        '25_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.25),\n",
    "        '30_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.30),\n",
    "        '40_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.40),\n",
    "        '50_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.50),\n",
    "        '60_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.60),\n",
    "        '70_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.70),\n",
    "        '75_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.75),\n",
    "        '80_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.80),\n",
    "        '90_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.90),\n",
    "        '95_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.95)\n",
    "    }\n",
    "    \n",
    "    # check if the function was not set in the dictionary. If not,\n",
    "    # use 'sum'\n",
    "    if (aggregate_function not in (agg_methods_dict.keys())):\n",
    "        \n",
    "        aggregate_function = 'sum'\n",
    "        print(\"Invalid or no aggregation function input, so using the default \\'sum\\'.\")\n",
    "    \n",
    "    # Select the method in the dictionary and apply it. To access a value\n",
    "    # 'val' correspondent to the key 'key' from a dictionary dict, we\n",
    "    # declare: dict['key'], just as accessing a column from a dataframe.\n",
    "    \n",
    "    # The value will be the application of the method itself, i.e., the\n",
    "    # dataset will be aggregated:\n",
    "    DATASET = agg_methods_dict[aggregate_function]\n",
    "    \n",
    "    # If an aggregate function different from 'sum', 'mean', 'median' or 'mode' \n",
    "    # is used with plot_cumulative_percent = True, \n",
    "    # set plot_cumulative_percent = False:\n",
    "    # (check if aggregate function is not in the list of allowed values):\n",
    "    if ((aggregate_function not in ['sum', 'mean', 'median', 'mode']) & (calculate_and_plot_cumulative_percent == True)):\n",
    "        \n",
    "        calculate_and_plot_cumulative_percent = False\n",
    "        print(\"The cumulative percent is only calculated when aggregate_function = \\'sum\\', \\'mean\\', \\'median\\', or \\'mode\\'. So, plot_cumulative_percent was set as False.\")\n",
    "    \n",
    "    # Guarantee that the columns from the aggregated dataset have the correct\n",
    "    \n",
    "    # Let's create a list of the new column names\n",
    "    # The first element is categorical_var_name, which is not modified:\n",
    "    list_of_cols = [categorical_var_name]\n",
    "    \n",
    "    # Check if add_suffix_to_aggregated_col is False. If it is, simply\n",
    "    # repeat the original response_var_name:\n",
    "    if (add_suffix_to_aggregated_col == False):\n",
    "        \n",
    "        list_of_cols.append(response_var_name)\n",
    "    \n",
    "    else:\n",
    "        # Let's add a suffix. Check if suffix is None. If it is,\n",
    "        # set \"_\" + aggregate_function as suffix:\n",
    "        \n",
    "        if (suffix is None):\n",
    "            suffix = \"_\" + aggregate_function\n",
    "        \n",
    "        # Now, append response_var_name + suffix to the list to\n",
    "        # create the name of the new aggregated column:\n",
    "        response_var_name = response_var_name + suffix\n",
    "        list_of_cols.append(response_var_name)\n",
    "    \n",
    "    # Now, rename the columns of the aggregated dataset as the list\n",
    "    # list_of_cols:\n",
    "    DATASET.columns = list_of_cols\n",
    "    \n",
    "    # Let's sort the dataframe.\n",
    "    \n",
    "    # Order the dataframe in descending order by the response.\n",
    "    # If there are equal responses, order them by category, in\n",
    "    # ascending order; put the missing values in the first position\n",
    "    # To pass multiple columns and multiple types of ordering, we use\n",
    "    # lists. If there was a single column to order by, we would declare\n",
    "    # it as a string. If only one order of ascending was used, we would\n",
    "    # declare it as a simple boolean\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html\n",
    "    \n",
    "    DATASET = DATASET.sort_values(by = [response_var_name, categorical_var_name], ascending = [False, True], na_position = 'first')\n",
    "    \n",
    "    # Now, reset index positions:\n",
    "    DATASET = DATASET.reset_index(drop = True)\n",
    "    \n",
    "    # plot_cumulative_percent = True, create a column to store the\n",
    "    # cumulative percent:\n",
    "    if (calculate_and_plot_cumulative_percent): \n",
    "        # Run the following code if the boolean value is True (implicity)\n",
    "        \n",
    "        # Calculate the total sum of the array correspondent to\n",
    "        # the column (series) response_var_name\n",
    "        total_sum = np.sum(np.array(DATASET[response_var_name]))\n",
    "        \n",
    "        # Create a column series for the cumulative sum:\n",
    "        cumsum_col = response_var_name + \"_cumsum\"\n",
    "        DATASET[cumsum_col] = DATASET[response_var_name].cumsum()\n",
    "        \n",
    "        # Now, create a column for the accumulated percent\n",
    "        # by dividing cumsum_col by total_sum and multiplying it by\n",
    "        # 100 (%):\n",
    "        cum_pct_col = response_var_name + \"_cum_pct\"\n",
    "        DATASET[cum_pct_col] = (DATASET[cumsum_col])/(total_sum)*100\n",
    "        print(f\"Successfully calculated cumulative sum and cumulative percent correspondent to the response variable {response_var_name}.\")\n",
    "    \n",
    "    print(\"Successfully aggregated and ordered the dataset to plot. Check the 10 first rows of this returned dataset:\\n\")\n",
    "    print(DATASET.head(10))\n",
    "    \n",
    "    # Check if the total of plotted categories is limited:\n",
    "    if not (limit_of_plotted_categories is None):\n",
    "        \n",
    "        # Since the value is not None, we have to limit it\n",
    "        # Check if the limit is lower than or equal to the length of the dataframe.\n",
    "        # If it is, we simply copy the columns to the series (there is no need of\n",
    "        # a memory-consuming loop or of applying the head method to a local copy\n",
    "        # of the dataframe):\n",
    "        df_length = len(DATASET)\n",
    "            \n",
    "        if (limit_of_plotted_categories <= df_length):\n",
    "            # Simply copy the columns to the graphic series:\n",
    "            categories = DATASET[categorical_var_name]\n",
    "            responses = DATASET[response_var_name]\n",
    "            # If there is a cum_pct column, copy it to a series too:\n",
    "            if (calculate_and_plot_cumulative_percent):\n",
    "                cum_pct = plotted_df[cum_pct_col]\n",
    "        \n",
    "        else:\n",
    "            # The limit is lower than the total of categories,\n",
    "            # so we actually have to limit the size of plotted df:\n",
    "        \n",
    "            # If aggregate_function is not 'sum', we simply apply\n",
    "            # the head method to obtain the first rows (number of\n",
    "            # rows input as parameter; if no parameter is input, the\n",
    "            # number of 5 rows is used):\n",
    "            if (aggregate_function != 'sum'):\n",
    "                # Limit to the number limit_of_plotted_categories:\n",
    "                # create another local copy of the dataframe not to\n",
    "                # modify the returned dataframe object:\n",
    "                plotted_df = DATASET.head(limit_of_plotted_categories)\n",
    "\n",
    "                # Create the series of elements to plot:\n",
    "                categories = plotted_df[categorical_var_name]\n",
    "                responses = plotted_df[response_var_name]\n",
    "                # If the cumulative percent was obtained, create the series for it:\n",
    "                if (calculate_and_plot_cumulative_percent):\n",
    "                    cum_pct = plotted_df[cum_pct_col]\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Firstly, copy the elements that will be kept to x, y and (possibly) cum_pct\n",
    "                # lists.\n",
    "                # Start the lists:\n",
    "                categories = []\n",
    "                responses = []\n",
    "                if (calculate_and_plot_cumulative_percent):\n",
    "                    cum_pct = [] # start this list only if its needed to save memory\n",
    "\n",
    "                for i in range (0, limit_of_plotted_categories):\n",
    "                    # i goes from 0 (first index) to limit_of_plotted_categories - 1\n",
    "                    # (index of the last category to be kept):\n",
    "                    # copy the elements from the DATASET to the list\n",
    "                    # category is the 1st column (column 0); response is the 2nd (col 1);\n",
    "                    # and cumulative percent is the 4th (col 3):\n",
    "                    categories.append(DATASET.iloc[i, 0])\n",
    "                    responses.append(DATASET.iloc[i, 1])\n",
    "                    \n",
    "                    if (calculate_and_plot_cumulative_percent):\n",
    "                        cum_pct.append(DATASET.iloc[i, 3]) # only if there is something to iloc\n",
    "                    \n",
    "                # Now, i = limit_of_plotted_categories - 1\n",
    "                # Create a variable to store the sum of other responses\n",
    "                other_responses = 0\n",
    "                # loop from i = limit_of_plotted_categories to i = df_length-1, index\n",
    "                # of the last element. Notice that this loop may have a single call, if there\n",
    "                # is only one element above the limit:\n",
    "                for i in range (limit_of_plotted_categories, (df_length - 1)):\n",
    "                    \n",
    "                    other_responses = other_responses + (DATASET.iloc[i, 1])\n",
    "                \n",
    "                # Now, add the last elements to the series:\n",
    "                # The last category is named 'others':\n",
    "                categories.append('others')\n",
    "                # The correspondent aggregated response is the value \n",
    "                # stored in other_responses:\n",
    "                responses.append(other_responses)\n",
    "                # The cumulative percent is 100%, since this must be the sum of all\n",
    "                # elements (the previous ones plus the ones aggregated as 'others'\n",
    "                # must totalize 100%).\n",
    "                # On the other hand, the cumulative percent is stored only if needed:\n",
    "                cum_pct.append(100)\n",
    "    \n",
    "    else:\n",
    "        # This is the situation where there is no limit of plotted categories. So, we\n",
    "        # simply copy the columns to the plotted series (it is equivalent to the \n",
    "        # situation where there is a limit, but the limit is equal or inferior to the\n",
    "        # size of the dataframe):\n",
    "        categories = DATASET[categorical_var_name]\n",
    "        responses = DATASET[response_var_name]\n",
    "        # If there is a cum_pct column, copy it to a series too:\n",
    "        if (calculate_and_plot_cumulative_percent):\n",
    "            cum_pct = plotted_df[cum_pct_col]\n",
    "    \n",
    "    \n",
    "    # Now the data is prepared and we only have to plot \n",
    "    # categories, responses, and cum_pct:\n",
    "    \n",
    "    # Set labels and titles for the case they are None\n",
    "    if (plot_title is None):\n",
    "        plot_title = f\"Bar_chart_for_{response_var_name}_by_{categorical_var_name}\"\n",
    "    \n",
    "    if (horizontal_axis_title is None):\n",
    "\n",
    "        horizontal_axis_title = categorical_var_name\n",
    "\n",
    "    if (vertical_axis_title is None):\n",
    "        # Notice that response_var_name already has the suffix indicating the\n",
    "        # aggregation function\n",
    "        vertical_axis_title = response_var_name\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 70 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    plt.title(plot_title)\n",
    "    ax1.set_xlabel(horizontal_axis_title)\n",
    "    ax1.set_ylabel(vertical_axis_title, color = 'blue')\n",
    "    \n",
    "    if (orientation == 'horizontal'):\n",
    "        \n",
    "        # Horizontal bars used - barh method (bar horizontal):\n",
    "        # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.barh.html\n",
    "        # Now, the categorical variables stored in series categories must be\n",
    "        # positioned as the vertical axis Y, whereas the correspondent responses\n",
    "        # must be in the horizontal axis X.\n",
    "        ax1.barh(categories, responses, color = 'blue', label = categorical_var_name)\n",
    "        #.barh(y, x, ...)\n",
    "        \n",
    "        if (calculate_and_plot_cumulative_percent):\n",
    "            # Let's plot the line for the cumulative percent\n",
    "            # Set the grid for the bar chart as False. If it is True, there will\n",
    "            # be to grids, one for the bars and other for the percents, making \n",
    "            # the image difficult to interpretate:\n",
    "            ax1.grid(False)\n",
    "            \n",
    "            # Create the twin plot for the cumulative percent:\n",
    "            ax2 = ax1.twinx()\n",
    "            # Here, the x axis must be the cum_pct value, and the Y\n",
    "            # axis must be categories (it must be correspondent to the\n",
    "            # bar chart)\n",
    "            ax2.plot(cum_pct, categories, '-ro', color = 'red', label = \"cumulative\\npercent\")\n",
    "            #.plot(x, y, ...)\n",
    "            ax2.tick_params('x', color = 'red')\n",
    "            ax2.set_ylabel(\"Cumulative Percent (\\%)\", color = 'red')\n",
    "            ax2.legend()\n",
    "            ax2.grid(grid) # shown if user set grid = True\n",
    "            # If user wants to see the grid, it is shown only for the cumulative line.\n",
    "        \n",
    "        else:\n",
    "            # There is no cumulative line, so the parameter grid must control \n",
    "            # the bar chart's grid\n",
    "            ax1.legend()\n",
    "            ax1.grid(grid)\n",
    "        \n",
    "    else: \n",
    "        # If None or an invalid orientation was used, set it as vertical\n",
    "        # Use Matplotlib standard bar method (vertical bar):\n",
    "        # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html#matplotlib.pyplot.bar\n",
    "        \n",
    "        # In this standard case, the categorical variables (categories) are positioned\n",
    "        # as X, and the responses as Y:\n",
    "        ax1.bar(categories, responses, color = 'blue', label = categorical_var_name)\n",
    "        #.bar(x, y, ...)\n",
    "        \n",
    "        if (calculate_and_plot_cumulative_percent):\n",
    "            # Let's plot the line for the cumulative percent\n",
    "            # Set the grid for the bar chart as False. If it is True, there will\n",
    "            # be to grids, one for the bars and other for the percents, making \n",
    "            # the image difficult to interpretate:\n",
    "            ax1.grid(False)\n",
    "            \n",
    "            # Create the twin plot for the cumulative percent:\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.plot(categories, cum_pct, '-ro', color = 'red', label = \"cumulative\\npercent\")\n",
    "            #.plot(x, y, ...)\n",
    "            ax2.tick_params('y', color = 'red')\n",
    "            ax2.set_ylabel(\"Cumulative Percent (\\%)\", color = 'red')\n",
    "            ax2.legend()\n",
    "            ax2.grid(grid) # shown if user set grid = True\n",
    "            # If user wants to see the grid, it is shown only for the cumulative line.\n",
    "        \n",
    "        else:\n",
    "            # There is no cumulative line, so the parameter grid must control \n",
    "            # the bar chart's grid\n",
    "            ax1.legend()\n",
    "            ax1.grid(grid)\n",
    "    \n",
    "    # Notice that the .plot method is used for generating the plot for both orientations.\n",
    "    # It is different from .bar and .barh, which specify the orientation of a bar; or\n",
    "    # .hline (creation of an horizontal constant line); or .vline (creation of a vertical\n",
    "    # constant line).\n",
    "    \n",
    "    # Now the parameters specific to the configurations are finished, so we can go back\n",
    "    # to the general code:\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "        \n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"/\"\n",
    "        \n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"bar_chart\"\n",
    "        \n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 110 dpi\n",
    "            png_resolution_dpi = 110\n",
    "        \n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "        \n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "    \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for splitting the dataframe into train and test subsets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_into_train_and_test (X, y, percent_of_data_used_for_model_training = 75):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # X = subset of predictive variables (dataframe).\n",
    "    # y = subset of response variable (series).\n",
    "    \n",
    "    # percent_of_data_used_for_model_training: float from 0 to 100,\n",
    "    # representing the percent of data used for training the model\n",
    "    \n",
    "    # Convert the percent to fraction.\n",
    "    train_fraction = (percent_of_data_used_for_model_training / 100)\n",
    "    # Calculate the test fraction:\n",
    "    test_fraction = (1 - train_fraction)\n",
    "    \n",
    "    #Funcao para dividir os dados (split em treino e teste)\n",
    "    X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size = test_fraction, random_state = 0)\n",
    "    #test_size: proportion: 0.25 used for test\n",
    "    #test_size = 0.25 = 25% of data used for tests \n",
    "    #-> then, 0.75 = 75% of data used for training the Machine Learning model\n",
    "    \n",
    "    print(f\"X and y successfully splitted into train: X_train, y_train ({percent_of_data_used_for_model_training}\\% of data); and test subsets: X_test, y_test ({100 - percent_of_data_used_for_model_training}\\% of data).\")\n",
    "    # the slash is used to pass a prohibited character % to the string: it is ignorated, so it is printed.\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for Ordinary Least Squares (OLS) Linear Regression**\n",
    "    - This function runs the 'bar_chart' function. Certify that this function was properly loaded.\n",
    "- Fits a linear model with coefficients w = (w1, …, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_linear_reg (X_train, y_train):\n",
    "    \n",
    "    # check Scikit-learn documentation: \n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html?msclkid=636b4046c01b11ec973dee34641f67b0\n",
    "    # This function runs the 'bar_chart' function. Certify that this function was properly loaded.\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    # X_train = subset of predictive variables (dataframe).\n",
    "    # y_train = subset of response variable (series).\n",
    "    \n",
    "    # Create an instance (object) from the class LinearRegression:\n",
    "    ols_linear_reg_model = LinearRegression()\n",
    "    \n",
    "    # Fit the model:\n",
    "    ols_linear_reg_model = ols_linear_reg_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Set the list of the predictors:\n",
    "    # Use the list attribute to guarantee that it is a list:\n",
    "    predictive_features = list(X_train.columns)\n",
    "    # Append the 'intercept' to this list:\n",
    "    predictive_features.append('intercept')\n",
    "    \n",
    "    # Get the list of coefficients. Apply the list method to convert the\n",
    "    # array from .coef_ to a list:\n",
    "    reg_coefficients = list(ols_linear_reg_model.coef_)\n",
    "    \n",
    "    # Append the intercept coefficient to this list:\n",
    "    reg_coefficients.append(ols_linear_reg_model.intercept_)\n",
    "    \n",
    "    # Create the regression dictionary:\n",
    "    reg_dict = {'predictive_features': predictive_features,\n",
    "               'regression_coefficients': reg_coefficients}\n",
    "    \n",
    "    # Convert it to a Pandas dataframe:\n",
    "    ols_feature_importance_df = pd.DataFrame(data = reg_dict)\n",
    "    \n",
    "    # Now sort the dataframe in descending order of coefficient, and ascending order of\n",
    "    # feature (when sorting by multiple columns, we pass a list of columns to by and a \n",
    "    # list of booleans to ascending, instead of passing a simple string to by and a boolean\n",
    "    # to ascending. The element on a given index from the list by corresponds to the boolean\n",
    "    # with the same index in ascending):\n",
    "    ols_feature_importance_df = ols_feature_importance_df.sort_values(by = ['regression_coefficients', 'predictive_features'], ascending = [False, True])\n",
    "    \n",
    "    # Now that the dataframe is sorted in descending order, it represents the feature\n",
    "    # importance ranking.\n",
    "    \n",
    "    # Restart the indices:\n",
    "    ols_feature_importance_df = ols_feature_importance_df.reset_index(drop = True)\n",
    "    \n",
    "    print(\"Successfully obtained the linear regression.\")\n",
    "    print(f\"R² = {ols_linear_reg_model.score(X_train, y_train)}\\n\")\n",
    "    print(\"Check the parameters of the estimator:\")\n",
    "    print(ols_linear_reg_model.get_params(deep = True))\n",
    "    print(\"Returning the model object \\'ols_linear_reg_model\\' and the dataframe \\'ols_feature_importance_df\\' with the feature importance ranking (regression coefficients in descending order). Check the ranking below (first 20 features):\\n\")\n",
    "    print(ols_feature_importance_df.head(20))\n",
    "    \n",
    "    print(\"\\n\") #line break\n",
    "    print(\"To predict the model output y_pred for a dataframe X, declare: y_pred = ols_linear_reg_model.predict(X)\\n\")\n",
    "    print(\"For a one-dimensional correlation, the one-dimension array or list with format X_train = [x1, x2, ...] must be converted into a dataframe subset, X_train = [[x1, x2, ...]] before the prediction. To do so, create a list with X_train as its element: X_train = [X_train], or use the numpy.reshape(-1,1):\")\n",
    "    print(\"X_train = np.reshape(np.array(X_train), (-1, 1))\")\n",
    "    # numpy reshape: https://numpy.org/doc/1.21/reference/generated/numpy.reshape.html?msclkid=5de33f8bc02c11ec803224a6bd588362\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Check the feature importance bar chart:\\n\")\n",
    "    \n",
    "    # Obtain the bar chart. Set the local variables for using as bar_chart function parameters:\n",
    "    DATASET = ols_feature_importance_df\n",
    "    CATEGORICAL_VAR_NAME = 'predictive_features'\n",
    "    RESPONSE_VAR_NAME = 'regression_coefficients'\n",
    "    AGGREGATE_FUNCTION = 'sum'\n",
    "    ADD_SUFFIX_TO_AGGREGATED_COL = True\n",
    "    SUFFIX = None\n",
    "    CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = False\n",
    "    ORIENTATION = 'vertical'\n",
    "    X_AXIS_ROTATION = 70\n",
    "    Y_AXIS_ROTATION = 0\n",
    "    GRID = True\n",
    "    HORIZONTAL_AXIS_TITLE = 'Feature'\n",
    "    VERTICAL_AXIS_TITLE = 'Regression_coefficients'\n",
    "    PLOT_TITLE = 'Feature_ranking'\n",
    "    EXPORT_PNG = False\n",
    "    DIRECTORY_TO_SAVE = None\n",
    "    FILE_NAME = None\n",
    "    PNG_RESOLUTION_DPI = 110\n",
    "    \n",
    "    # use underscore to ignore the dataframe, and simply obtain the plot:\n",
    "    _ = bar_chart (df = DATASET, categorical_var_name = CATEGORICAL_VAR_NAME, response_var_name = RESPONSE_VAR_NAME, aggregate_function = AGGREGATE_FUNCTION, add_suffix_to_aggregated_col = ADD_SUFFIX_TO_AGGREGATED_COL, suffix = SUFFIX, calculate_and_plot_cumulative_percent = CALCULATE_AND_PLOT_CUMULATIVE_PERCENT, orientation = ORIENTATION, limit_of_plotted_categories = LIMIT_OF_PLOTTED_CATEGORIES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)\n",
    "\n",
    "    return ols_linear_reg_model, ols_feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for Ridge Linear Regression**\n",
    "    - This function runs the 'bar_chart' function. Certify that this function was properly loaded.\n",
    "- Linear least squares with l2 regularization.\n",
    "- Minimizes the objective function: `||y - Xw||^2_2 + alpha * ||w||^2_2`\n",
    "- This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. \n",
    "- Also known as Ridge Regression or Tikhonov regularization.\n",
    "- This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape (n_samples, n_targets))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_linear_reg (X_train, y_train, alpha_hyperparameter = 1.0, maximum_of_allowed_iterations = 20000):\n",
    "    \n",
    "    # check Scikit-learn documentation: \n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge\n",
    "    # This function runs the 'bar_chart' function. Certify that this function was properly loaded.\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import Ridge\n",
    "    \n",
    "    # X_train = subset of predictive variables (dataframe).\n",
    "    # y_train = subset of response variable (series).\n",
    "    \n",
    "    # hyperparameters: alpha = ALPHA_HYPERPARAMETER and MAXIMUM_OF_ALLOWED_ITERATIONS = max_iter\n",
    "\n",
    "    # MAXIMUM_OF_ALLOWED_ITERATIONS = integer representing the maximum number of iterations\n",
    "    # that the optimization algorithm can perform. Depending on data, convergence may not be\n",
    "    # reached within this limit, so you may need to increase this hyperparameter.\n",
    "\n",
    "    # alpha is the regularization strength and must be a positive float value. \n",
    "    # Regularization improves the conditioning of the problem and reduces the variance \n",
    "    # of the estimates. Larger values specify stronger regularization.\n",
    "    \n",
    "    # alpha = 0 is equivalent to an ordinary least square, solved by the LinearRegression \n",
    "    # object. For numerical reasons, using alpha = 0 is not advised. \n",
    "    # Given this, you should use the ols_linear_reg function instead.\n",
    "    \n",
    "    # Create an instance (object) from the class Ridge:\n",
    "    ridge_linear_reg_model = Ridge(alpha = alpha_hyperparameter, max_iter = maximum_of_allowed_iterations)\n",
    "    \n",
    "    # Fit the model:\n",
    "    ridge_linear_reg_model = ridge_linear_reg_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Set the list of the predictors:\n",
    "    # Use the list attribute to guarantee that it is a list:\n",
    "    predictive_features = list(X_train.columns)\n",
    "    # Append the 'intercept' to this list:\n",
    "    predictive_features.append('intercept')\n",
    "    \n",
    "    # Get the list of coefficients. Apply the list method to convert the\n",
    "    # array from .coef_ to a list:\n",
    "    reg_coefficients = list(ridge_linear_reg_model.coef_)\n",
    "    \n",
    "    # Append the intercept coefficient to this list:\n",
    "    reg_coefficients.append(ridge_linear_reg_model.intercept_)\n",
    "    \n",
    "    # Create the regression dictionary:\n",
    "    reg_dict = {'predictive_features': predictive_features,\n",
    "               'regression_coefficients': reg_coefficients}\n",
    "    \n",
    "    # Convert it to a Pandas dataframe:\n",
    "    ridge_feature_importance_df = pd.DataFrame(data = reg_dict)\n",
    "    \n",
    "    # Now sort the dataframe in descending order of coefficient, and ascending order of\n",
    "    # feature (when sorting by multiple columns, we pass a list of columns to by and a \n",
    "    # list of booleans to ascending, instead of passing a simple string to by and a boolean\n",
    "    # to ascending. The element on a given index from the list by corresponds to the boolean\n",
    "    # with the same index in ascending):\n",
    "    ridge_feature_importance_df = ridge_feature_importance_df.sort_values(by = ['regression_coefficients', 'predictive_features'], ascending = [False, True])\n",
    "    \n",
    "    # Now that the dataframe is sorted in descending order, it represents the feature\n",
    "    # importance ranking.\n",
    "    \n",
    "    # Restart the indices:\n",
    "    ridge_feature_importance_df = ridge_feature_importance_df.reset_index(drop = True)\n",
    "    \n",
    "    print(\"Successfully obtained the linear regression.\")\n",
    "    print(f\"R² = {ridge_linear_reg_model.score(X_train, y_train)}\\n\")\n",
    "    print(f\"Total of iterations to fit the model = {ridge_linear_reg_model.n_iter_}\")\n",
    "    \n",
    "    if (ridge_linear_reg_model.n_iter_ == maximum_of_allowed_iterations):\n",
    "        print(\"Warning! Total of iterations equals to the maximum allowed. It indicates that the convergence was not reached yet. Try to increase the maximum number of allowed iterations.\")\n",
    "    \n",
    "    print(\"Check the parameters of the estimator:\")\n",
    "    print(ridge_linear_reg_model.get_params(deep = True))\n",
    "    print(\"Returning the model object \\'ridge_linear_reg_model\\' and the dataframe \\'ridge_feature_importance_df\\' with the feature importance ranking (regression coefficients in descending order). Check the ranking below (first 20 features):\\n\")\n",
    "    print(ridge_feature_importance_df.head(20))\n",
    "    \n",
    "    print(\"\\n\") #line break\n",
    "    print(\"To predict the model output y_pred for a dataframe X, declare: y_pred = ridge_linear_reg_model.predict(X)\\n\")\n",
    "    print(\"For a one-dimensional correlation, the one-dimension array or list with format X_train = [x1, x2, ...] must be converted into a dataframe subset, X_train = [[x1, x2, ...]] before the prediction. To do so, create a list with X_train as its element: X_train = [X_train], or use the numpy.reshape(-1,1):\")\n",
    "    print(\"X_train = np.reshape(np.array(X_train), (-1, 1))\")\n",
    "    # numpy reshape: https://numpy.org/doc/1.21/reference/generated/numpy.reshape.html?msclkid=5de33f8bc02c11ec803224a6bd588362\n",
    "      \n",
    "    print(\"\\n\")\n",
    "    print(\"Check the feature importance bar chart:\\n\")\n",
    "    \n",
    "    # Obtain the bar chart. Set the local variables for using as bar_chart function parameters:\n",
    "    DATASET = ridge_feature_importance_df\n",
    "    CATEGORICAL_VAR_NAME = 'predictive_features'\n",
    "    RESPONSE_VAR_NAME = 'regression_coefficients'\n",
    "    AGGREGATE_FUNCTION = 'sum'\n",
    "    ADD_SUFFIX_TO_AGGREGATED_COL = True\n",
    "    SUFFIX = None\n",
    "    CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = False\n",
    "    ORIENTATION = 'vertical'\n",
    "    X_AXIS_ROTATION = 70\n",
    "    Y_AXIS_ROTATION = 0\n",
    "    GRID = True\n",
    "    HORIZONTAL_AXIS_TITLE = 'Feature'\n",
    "    VERTICAL_AXIS_TITLE = 'Regression_coefficients'\n",
    "    PLOT_TITLE = 'Feature_ranking'\n",
    "    EXPORT_PNG = False\n",
    "    DIRECTORY_TO_SAVE = None\n",
    "    FILE_NAME = None\n",
    "    PNG_RESOLUTION_DPI = 110\n",
    "    \n",
    "    # use underscore to ignore the dataframe, and simply obtain the plot:\n",
    "    _ = bar_chart (df = DATASET, categorical_var_name = CATEGORICAL_VAR_NAME, response_var_name = RESPONSE_VAR_NAME, aggregate_function = AGGREGATE_FUNCTION, add_suffix_to_aggregated_col = ADD_SUFFIX_TO_AGGREGATED_COL, suffix = SUFFIX, calculate_and_plot_cumulative_percent = CALCULATE_AND_PLOT_CUMULATIVE_PERCENT, orientation = ORIENTATION, limit_of_plotted_categories = LIMIT_OF_PLOTTED_CATEGORIES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)\n",
    "\n",
    "    return ridge_linear_reg_model, ridge_feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for Lasso Linear Regression**\n",
    "    - This function runs the 'bar_chart' function. Certify that this function was properly loaded.\n",
    "- Linear Model trained with L1 prior as regularizer (aka the Lasso).\n",
    "- The optimization objective for Lasso is: `(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1`\n",
    "- Technically the Lasso model is optimizing the same objective function as the Elastic Net with l1_ratio=1.0 (no L2 penalty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_linear_reg (X_train, y_train, alpha_hyperparameter = 1.0, maximum_of_allowed_iterations = 20000):\n",
    "    \n",
    "    # check Scikit-learn documentation: \n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso\n",
    "    # This function runs the 'bar_chart' function. Certify that this function was properly loaded.\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import Lasso\n",
    "    \n",
    "    # X_train = subset of predictive variables (dataframe).\n",
    "    # y_train = subset of response variable (series).\n",
    "    \n",
    "    # hyperparameters: alpha = ALPHA_HYPERPARAMETER and MAXIMUM_OF_ALLOWED_ITERATIONS = max_iter\n",
    "\n",
    "    # MAXIMUM_OF_ALLOWED_ITERATIONS = integer representing the maximum number of iterations\n",
    "    # that the optimization algorithm can perform. Depending on data, convergence may not be\n",
    "    # reached within this limit, so you may need to increase this hyperparameter.\n",
    "\n",
    "    # alpha is the regularization strength and must be a positive float value. \n",
    "    # Regularization improves the conditioning of the problem and reduces the variance \n",
    "    # of the estimates. Larger values specify stronger regularization.\n",
    "    \n",
    "    # alpha = 0 is equivalent to an ordinary least square, solved by the LinearRegression \n",
    "    # object. For numerical reasons, using alpha = 0 is not advised. \n",
    "    # Given this, you should use the ols_linear_reg function instead.\n",
    "    \n",
    "    # Create an instance (object) from the class Lasso:\n",
    "    lasso_linear_reg_model = Lasso(alpha = alpha_hyperparameter, max_iter = maximum_of_allowed_iterations)\n",
    "    \n",
    "    # Fit the model:\n",
    "    lasso_linear_reg_model = lasso_linear_reg_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Set the list of the predictors:\n",
    "    # Use the list attribute to guarantee that it is a list:\n",
    "    predictive_features = list(X_train.columns)\n",
    "    # Append the 'intercept' to this list:\n",
    "    predictive_features.append('intercept')\n",
    "    \n",
    "    # Get the list of coefficients. Apply the list method to convert the\n",
    "    # array from .coef_ to a list:\n",
    "    reg_coefficients = list(lasso_linear_reg_model.coef_)\n",
    "    \n",
    "    # Append the intercept coefficient to this list:\n",
    "    reg_coefficients.append(lasso_linear_reg_model.intercept_)\n",
    "    \n",
    "    # Create the regression dictionary:\n",
    "    reg_dict = {'predictive_features': predictive_features,\n",
    "               'regression_coefficients': reg_coefficients}\n",
    "    \n",
    "    # Convert it to a Pandas dataframe:\n",
    "    lasso_feature_importance_df = pd.DataFrame(data = reg_dict)\n",
    "    \n",
    "    # Now sort the dataframe in descending order of coefficient, and ascending order of\n",
    "    # feature (when sorting by multiple columns, we pass a list of columns to by and a \n",
    "    # list of booleans to ascending, instead of passing a simple string to by and a boolean\n",
    "    # to ascending. The element on a given index from the list by corresponds to the boolean\n",
    "    # with the same index in ascending):\n",
    "    lasso_feature_importance_df = lasso_feature_importance_df.sort_values(by = ['regression_coefficients', 'predictive_features'], ascending = [False, True])\n",
    "    \n",
    "    # Now that the dataframe is sorted in descending order, it represents the feature\n",
    "    # importance ranking.\n",
    "    \n",
    "    # Restart the indices:\n",
    "    lasso_feature_importance_df = lasso_feature_importance_df.reset_index(drop = True)\n",
    "    \n",
    "    print(\"Successfully obtained the linear regression.\")\n",
    "    print(f\"R² = {lasso_linear_reg_model.score(X_train, y_train)}\\n\")\n",
    "    print(f\"Total of iterations to fit the model = {lasso_linear_reg_model.n_iter_}\")\n",
    "    \n",
    "    if (lasso_linear_reg_model.n_iter_ == maximum_of_allowed_iterations):\n",
    "        print(\"Warning! Total of iterations equals to the maximum allowed. It indicates that the convergence was not reached yet. Try to increase the maximum number of allowed iterations.\")\n",
    "    \n",
    "    print(\"Check the parameters of the estimator:\")\n",
    "    print(lasso_linear_reg_model.get_params(deep = True))\n",
    "    print(\"Returning the model object \\'lasso_linear_reg_model\\' and the dataframe \\'lasso_feature_importance_df\\' with the feature importance ranking (regression coefficients in descending order). Check the ranking below (first 20 features):\\n\")\n",
    "    print(lasso_feature_importance_df.head(20))\n",
    "    \n",
    "    print(\"\\n\") #line break\n",
    "    print(\"To predict the model output y_pred for a dataframe X, declare: y_pred = lasso_linear_reg_model.predict(X)\\n\")\n",
    "    print(\"For a one-dimensional correlation, the one-dimension array or list with format X_train = [x1, x2, ...] must be converted into a dataframe subset, X_train = [[x1, x2, ...]] before the prediction. To do so, create a list with X_train as its element: X_train = [X_train], or use the numpy.reshape(-1,1):\")\n",
    "    print(\"X_train = np.reshape(np.array(X_train), (-1, 1))\")\n",
    "    # numpy reshape: https://numpy.org/doc/1.21/reference/generated/numpy.reshape.html?msclkid=5de33f8bc02c11ec803224a6bd588362\n",
    "      \n",
    "    print(\"\\n\")\n",
    "    print(\"Check the feature importance bar chart:\\n\")\n",
    "    \n",
    "    # Obtain the bar chart. Set the local variables for using as bar_chart function parameters:\n",
    "    DATASET = lasso_feature_importance_df\n",
    "    CATEGORICAL_VAR_NAME = 'predictive_features'\n",
    "    RESPONSE_VAR_NAME = 'regression_coefficients'\n",
    "    AGGREGATE_FUNCTION = 'sum'\n",
    "    ADD_SUFFIX_TO_AGGREGATED_COL = True\n",
    "    SUFFIX = None\n",
    "    CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = False\n",
    "    ORIENTATION = 'vertical'\n",
    "    X_AXIS_ROTATION = 70\n",
    "    Y_AXIS_ROTATION = 0\n",
    "    GRID = True\n",
    "    HORIZONTAL_AXIS_TITLE = 'Feature'\n",
    "    VERTICAL_AXIS_TITLE = 'Regression_coefficients'\n",
    "    PLOT_TITLE = 'Feature_ranking'\n",
    "    EXPORT_PNG = False\n",
    "    DIRECTORY_TO_SAVE = None\n",
    "    FILE_NAME = None\n",
    "    PNG_RESOLUTION_DPI = 110\n",
    "    \n",
    "    # use underscore to ignore the dataframe, and simply obtain the plot:\n",
    "    _ = bar_chart (df = DATASET, categorical_var_name = CATEGORICAL_VAR_NAME, response_var_name = RESPONSE_VAR_NAME, aggregate_function = AGGREGATE_FUNCTION, add_suffix_to_aggregated_col = ADD_SUFFIX_TO_AGGREGATED_COL, suffix = SUFFIX, calculate_and_plot_cumulative_percent = CALCULATE_AND_PLOT_CUMULATIVE_PERCENT, orientation = ORIENTATION, limit_of_plotted_categories = LIMIT_OF_PLOTTED_CATEGORIES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)\n",
    "\n",
    "    return lasso_linear_reg_model, lasso_feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for Elastic Net Linear Regression**\n",
    "    - This function runs the 'bar_chart' function. Certify that this function was properly loaded.\n",
    "- Linear Model trained with combined L1 and L2 priors as regularizer.\n",
    "- Minimizes the objective function: `1 / (2 * n_samples) * ||y - Xw||^2_2 + alpha * l1_ratio * ||w||_1 + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2`\n",
    "- If you are interested in controlling the L1 and L2 penalty separately, keep in mind that this is equivalent to: `a * ||w||_1 + 0.5 * b * ||w||_2^2`\n",
    "- where: `alpha = a + b and l1_ratio = a / (a + b)`\n",
    "- The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable, unless you supply your own sequence of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_net_linear_reg (X_train, y_train, alpha_hyperparameter = 1.0, l1_ratio_hyperparameter = 0.5, maximum_of_allowed_iterations = 20000):\n",
    "    \n",
    "    # check Scikit-learn documentation: \n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet\n",
    "    # This function runs the 'bar_chart' function. Certify that this function was properly loaded.\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    \n",
    "    # X_train = subset of predictive variables (dataframe).\n",
    "    # y_train = subset of response variable (series).\n",
    "    \n",
    "    # hyperparameters: alpha = alpha_hyperparameter; maximum_of_allowed_iterations = max_iter;\n",
    "    # and l1_ratio_hyperparameter = l1_ratio\n",
    "\n",
    "    # MAXIMUM_OF_ALLOWED_ITERATIONS = integer representing the maximum number of iterations\n",
    "    # that the optimization algorithm can perform. Depending on data, convergence may not be\n",
    "    # reached within this limit, so you may need to increase this hyperparameter.\n",
    "\n",
    "    # alpha is the regularization strength and must be a positive float value. \n",
    "    # Regularization improves the conditioning of the problem and reduces the variance \n",
    "    # of the estimates. Larger values specify stronger regularization.\n",
    "    \n",
    "    # l1_ratio is The ElasticNet mixing parameter (float), with 0 <= l1_ratio <= 1. \n",
    "    # For l1_ratio = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is an L1 penalty. \n",
    "    # For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.\n",
    "    \n",
    "    # alpha = 0 and l1_ratio = 0 is equivalent to an ordinary least square, solved by \n",
    "    # the LinearRegression object. For numerical reasons, using alpha = 0 and \n",
    "    # l1_ratio = 0 is not advised. Given this, you should use the ols_linear_reg function instead.\n",
    "    \n",
    "    # Create an instance (object) from the class ElasticNet:\n",
    "    elastic_net_linear_reg_model = ElasticNet(alpha = alpha_hyperparameter, l1_ratio = l1_ratio_hyperparameter, max_iter = maximum_of_allowed_iterations)\n",
    "    \n",
    "    # Fit the model:\n",
    "    elastic_net_linear_reg_model = elastic_net_linear_reg_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Set the list of the predictors:\n",
    "    # Use the list attribute to guarantee that it is a list:\n",
    "    predictive_features = list(X_train.columns)\n",
    "    # Append the 'intercept' to this list:\n",
    "    predictive_features.append('intercept')\n",
    "    \n",
    "    # Get the list of coefficients. Apply the list method to convert the\n",
    "    # array from .coef_ to a list:\n",
    "    reg_coefficients = list(elastic_net_linear_reg_model.coef_)\n",
    "    \n",
    "    # Append the intercept coefficient to this list:\n",
    "    reg_coefficients.append(elastic_net_linear_reg_model.intercept_)\n",
    "    \n",
    "    # Create the regression dictionary:\n",
    "    reg_dict = {'predictive_features': predictive_features,\n",
    "               'regression_coefficients': reg_coefficients}\n",
    "    \n",
    "    # Convert it to a Pandas dataframe:\n",
    "    elastic_net_feature_importance_df = pd.DataFrame(data = reg_dict)\n",
    "    \n",
    "    # Now sort the dataframe in descending order of coefficient, and ascending order of\n",
    "    # feature (when sorting by multiple columns, we pass a list of columns to by and a \n",
    "    # list of booleans to ascending, instead of passing a simple string to by and a boolean\n",
    "    # to ascending. The element on a given index from the list by corresponds to the boolean\n",
    "    # with the same index in ascending):\n",
    "    elastic_net_feature_importance_df = elastic_net_feature_importance_df.sort_values(by = ['regression_coefficients', 'predictive_features'], ascending = [False, True])\n",
    "    \n",
    "    # Now that the dataframe is sorted in descending order, it represents the feature\n",
    "    # importance ranking.\n",
    "    \n",
    "    # Restart the indices:\n",
    "    elastic_net_feature_importance_df = elastic_net_feature_importance_df.reset_index(drop = True)\n",
    "    \n",
    "    print(\"Successfully obtained the linear regression.\")\n",
    "    print(f\"R² = {elastic_net_linear_reg_model.score(X_train, y_train)}\\n\")\n",
    "    print(f\"Total of iterations to fit the model = {elastic_net_linear_reg_model.n_iter_}\")\n",
    "    \n",
    "    if (elastic_net_linear_reg_model.n_iter_ == maximum_of_allowed_iterations):\n",
    "        print(\"Warning! Total of iterations equals to the maximum allowed. It indicates that the convergence was not reached yet. Try to increase the maximum number of allowed iterations.\")\n",
    "    \n",
    "    print(\"Check the parameters of the estimator:\")\n",
    "    print(elastic_net_linear_reg_model.get_params(deep = True))\n",
    "    print(\"Returning the model object \\'elastic_net_linear_reg_model\\' and the dataframe \\'elastic_net_feature_importance_df\\' with the feature importance ranking (regression coefficients in descending order). Check the ranking below (first 20 features):\\n\")\n",
    "    print(elastic_net_feature_importance_df.head(20))\n",
    "    \n",
    "    print(\"\\n\") #line break\n",
    "    print(\"To predict the model output y_pred for a dataframe X, declare: y_pred = elastic_net_linear_reg_model.predict(X)\\n\")\n",
    "    print(\"For a one-dimensional correlation, the one-dimension array or list with format X_train = [x1, x2, ...] must be converted into a dataframe subset, X_train = [[x1, x2, ...]] before the prediction. To do so, create a list with X_train as its element: X_train = [X_train], or use the numpy.reshape(-1,1):\")\n",
    "    print(\"X_train = np.reshape(np.array(X_train), (-1, 1))\")\n",
    "    # numpy reshape: https://numpy.org/doc/1.21/reference/generated/numpy.reshape.html?msclkid=5de33f8bc02c11ec803224a6bd588362\n",
    "     \n",
    "    print(\"\\n\")\n",
    "    print(\"Check the feature importance bar chart:\\n\")\n",
    "    \n",
    "    # Obtain the bar chart. Set the local variables for using as bar_chart function parameters:\n",
    "    DATASET = elastic_net_feature_importance_df\n",
    "    CATEGORICAL_VAR_NAME = 'predictive_features'\n",
    "    RESPONSE_VAR_NAME = 'regression_coefficients'\n",
    "    AGGREGATE_FUNCTION = 'sum'\n",
    "    ADD_SUFFIX_TO_AGGREGATED_COL = True\n",
    "    SUFFIX = None\n",
    "    CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = False\n",
    "    ORIENTATION = 'vertical'\n",
    "    X_AXIS_ROTATION = 70\n",
    "    Y_AXIS_ROTATION = 0\n",
    "    GRID = True\n",
    "    HORIZONTAL_AXIS_TITLE = 'Feature'\n",
    "    VERTICAL_AXIS_TITLE = 'Regression_coefficients'\n",
    "    PLOT_TITLE = 'Feature_ranking'\n",
    "    EXPORT_PNG = False\n",
    "    DIRECTORY_TO_SAVE = None\n",
    "    FILE_NAME = None\n",
    "    PNG_RESOLUTION_DPI = 110\n",
    "    \n",
    "    # use underscore to ignore the dataframe, and simply obtain the plot:\n",
    "    _ = bar_chart (df = DATASET, categorical_var_name = CATEGORICAL_VAR_NAME, response_var_name = RESPONSE_VAR_NAME, aggregate_function = AGGREGATE_FUNCTION, add_suffix_to_aggregated_col = ADD_SUFFIX_TO_AGGREGATED_COL, suffix = SUFFIX, calculate_and_plot_cumulative_percent = CALCULATE_AND_PLOT_CUMULATIVE_PERCENT, orientation = ORIENTATION, limit_of_plotted_categories = LIMIT_OF_PLOTTED_CATEGORIES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)\n",
    "\n",
    "    return elastic_net_linear_reg_model, elastic_net_feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for getting a general feature ranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_feature_ranking (dictionary_of_feature_rankings_dataframes, eliminate_non_correspondence = False, limit_of_ranked_features = None):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # dictionary_of_feature_rankings_dataframes\n",
    "    # The key of this dictionary must be the model name or the name of the ranking.\n",
    "    # This key will be used to identify the column on the new dataframe (it will be\n",
    "    # used as suffix). The correspondent value must be the feature importance ranking\n",
    "    # dataframe, with configuration similar to reg_dict: it must have a column\n",
    "    # named, 'predictive_features', which will be used as key for merging.\n",
    "    \n",
    "    # For instance, for a dictionary = {'ols_linear_regression': ols_feature_df,\n",
    "    # 'ridge_linear_regression': ridge_feature_df}, the columns 'regression_coefficients'\n",
    "    # will be identified as: 'regression_coefficients_ols_linear_regression' and\n",
    "    # 'regression_coefficients_ridge_linear_regression'. Notice that the underscore (\"_\")\n",
    "    # is used as suffix separator.\n",
    "    \n",
    "    # eliminate_non_correspondence = False. Since the dataframes will be merged using an\n",
    "    # \"outer\" join, all entries from all dataframes will be added, possibly resulting in\n",
    "    # missing values (pandas \"outer\" is a full outer join). \n",
    "    # Then, set eliminate_non_correspondence = True to eliminate all missing values\n",
    "    # (rows with entries without correspondence).\n",
    "    \n",
    "    # limit_of_ranked_features = None. Alternatively, set as an integer to limit the number\n",
    "    # of ranked features. e.g. limit_of_ranked_features = 20 will return a dataset with only\n",
    "    # 20 features. Notice that the features are sorted in accordance to their order in the\n",
    "    # input dictionary. Then, the most important ranking will be the one from the first dataframe.\n",
    "    \n",
    "    \n",
    "    # Get the list of keys from the dictionary. This will generate an array dict_keys([])\n",
    "    # that cannot be referenced throgh indexing. So, use the list attribute to convert it to\n",
    "    # an indexable list:\n",
    "    list_of_keys = list(dictionary_of_feature_rankings_dataframes.keys())\n",
    "    \n",
    "    # Get the total of dataframes that will be merged. It is the length of the list_of_keys\n",
    "    total_dfs = len(list_of_keys)\n",
    "    \n",
    "    # Get the first key. It is the first element of the list.\n",
    "    suffix_left = list_of_keys[0]\n",
    "    \n",
    "    # Get the correspondent dataframe by accessing the dictionary value with this key:\n",
    "    df_left = dictionary_of_feature_rankings_dataframes[suffix_left]\n",
    "    \n",
    "    # Now let's convert suffix_left to an appropriate suffix for merging.\n",
    "    # Use the str attribute to guarantee that the key was properly read as a string instead\n",
    "    # of other type. Then, there will be no concatenation errors:\n",
    "    suffix_left = str(suffix_left)\n",
    "    # Concatenate an underscore on the left of suffix_left to obtain the suffix for merging.\n",
    "    suffix_left = \"_\" + suffix_left\n",
    "    \n",
    "    # Now we have the first dataframe and the first suffix. We must loop through the rest of\n",
    "    # list_of_keys list starting from index i = 1 (second dataframe) to merge all with this\n",
    "    # first one:\n",
    "    \n",
    "    for i in range (1, total_dfs):\n",
    "        \n",
    "        # goes from i = 1 to i = (total_dfs - 1), index of the last dictionary key.\n",
    "        # Get the new dataframe to merge. It is the i-th key from list of keys:\n",
    "        suffix_right = list_of_keys[i]\n",
    "        \n",
    "        # Access this dataframe on the dictionary:\n",
    "        df_right = dictionary_of_feature_rankings_dataframes[suffix_right]\n",
    "        \n",
    "        # Now let's convert suffix_right to an appropriate suffix for merging.\n",
    "        # Use the str attribute to guarantee that the key was properly read as a string instead\n",
    "        # of other type. Then, there will be no concatenation errors:\n",
    "        suffix_right = str(suffix_right)\n",
    "        # Concatenate an underscore on the left of suffix_left to obtain the suffix for merging.\n",
    "        suffix_right = \"_\" + suffix_right\n",
    "        \n",
    "        # Create the tuple of suffixes:\n",
    "        SUFFIXES = (suffix_left, suffix_right)\n",
    "        \n",
    "        # Apply the merge method to update the left_df my merging it to right_df.\n",
    "        # The merged left_df will continue to be update on the following iterations:\n",
    "        # notice that we could specify the arguments left_on = 'predictive_features',\n",
    "        # and right_on = 'predictive_features'. Since the columns have the same name,\n",
    "        # we omit this parameter and simply specify on = 'predictive_features'\n",
    "        # We also modify the standard 'inner' to 'outer' join, so that no entries are\n",
    "        # removed at first (pandas \"outer\" is a full outer join: it keeps all rows from\n",
    "        # both dataframes)\n",
    "        df_left = df_left.merge(df_right, on = 'predictive_features', how = \"outer\", suffixes = SUFFIXES)\n",
    "        \n",
    "        # Update the left_suffix as _merge_i. Since i is an integer, we again use the\n",
    "        # str attribute to convert it to a string. So the string concatenation is allowed:\n",
    "        suffix_left = \"_merge_\" + str(i) \n",
    "    \n",
    "    # Now we must order the dataframe df_left in descending order by all of its columns except \n",
    "    # 'predictive_features'. This one will be in ascending (alphabetic) order.\n",
    "    # The order of importance will be the same order of the dictionary, i.e., the order of the\n",
    "    # dataframes passed. The last important column for sorting will be 'predictive_features'.\n",
    "    # Then, we must create a list of columns in their order of importance, and a corresponding\n",
    "    # list of booleans for the sorting order. This list will have value False for the columns\n",
    "    # sorted in descending order; and value True for the column sorted in ascending order.\n",
    "    \n",
    "    # Start the list of columns importance and the list of booleans for ascending or descending\n",
    "    # order:\n",
    "    cols_importance = []\n",
    "    asc_booleans = []\n",
    "    \n",
    "    # loop through all new columns of the dataframe:\n",
    "    \n",
    "    for column in df_left.columns:\n",
    "        # loop through each element, named 'column' from the list df_left.columns (list of columns\n",
    "        # of the merged dataframe):\n",
    "        \n",
    "        # check if the column is different from 'predictive_features'. \n",
    "        # If it is, add it to col_importance list and add a False value to asc_booleans list:\n",
    "        if (column != 'predictive_features'):\n",
    "            \n",
    "            cols_importance.append(column)\n",
    "            asc_booleans.append(False)\n",
    "    \n",
    "    # Now, we finished adding the columns to sort in descending order.\n",
    "    # Then, we simply append 'predictive_features' to the cols_importance list, and append True\n",
    "    # to the asc_booleans list:\n",
    "    cols_importance.append('predictive_features')\n",
    "    asc_booleans.append(True)\n",
    "    \n",
    "    # Now we can sort the df_left dataframe using the pandas sort_values method:\n",
    "    # Since we are sorting by a subset, we pass the list of columns to by instead of passing a simple\n",
    "    # string (case when we are sorting by a single column); and instead of passing a simple boolean\n",
    "    # to ascending (case for single column)\n",
    "    df_left = df_left.sort_values(by = cols_importance, ascending = asc_booleans)\n",
    "    #sort by the cols_importance list; ascending is set on asc_booleans: if its True, column is \n",
    "    # sorted in ascending order; if it is False, sorting is performed in descending order.\n",
    "    \n",
    "    # Check if missing values should be dropped:\n",
    "    if (eliminate_non_correspondence): # runs if the boolean is True:\n",
    "        # drop rows (axis = 0) with missing values:\n",
    "        # axis = 1 would drop columns containing missing values\n",
    "        df_cleaned = df_left.dropna(axis = 0)\n",
    "    \n",
    "    # Now, reset index positions:\n",
    "    df_left = df_left.reset_index(drop = True)\n",
    "    \n",
    "    # If limit_of_ranked_features is not None, use the head method to limit the output\n",
    "    # from the dataframe:\n",
    "    \n",
    "    if not (limit_of_ranked_features is None):\n",
    "        \n",
    "        df_left = df_left.head(limit_of_ranked_features)\n",
    "        # The dataframe will have only the 'limit_of_ranked_features' first entries\n",
    "        print(f\"General feature ranking successfully returned. It was limited to {limit_of_ranked_features}. Check the new dataframe:\\n\")\n",
    "        print(df_left)\n",
    "        \n",
    "        return df_left\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print(\"General feature ranking successfully returned. Check the 20 first features:\\n\")\n",
    "        print(df_left.head(20))\n",
    "        \n",
    "        return df_left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for calculating metrics for regression models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_models_metrics (model_object, X_train, y_train, X_test = None, y_test = None):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from keras.layers import Dense\n",
    "    from keras.models import Sequential\n",
    "    from xgboost import XGBRegressor\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import r2_score\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    \n",
    "    # model_object: object containing the model that will be analyzed. e.g.\n",
    "    # model_object = elastic_net_linear_reg_model\n",
    "    # X_train = subset of predictive variables (dataframe).\n",
    "    # y_train = subset of response variable (series).\n",
    "    # X_test = subset of predictive variables (test dataframe, in case the \n",
    "    # original one was split into train and test).\n",
    "    # y_test = subset of response variable (test series, in case the \n",
    "    # original one was split into train and test).\n",
    "    \n",
    "    y_pred = model_object.predict(X_train)\n",
    "    \n",
    "    # Start a dictionary for storing the metrics:\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    #METRICS FOR TRAINING\n",
    "    \n",
    "    #mean squared error- MSE\n",
    "    mse_train = mean_squared_error(y_train, y_pred)\n",
    "    # Save this metric in the dictionary:\n",
    "    metrics_dict.update({'MSE_train': mse_train})\n",
    "    print(f\"Mean squared error (MSE) for training = {mse_train}\")\n",
    "    \n",
    "    #Root mean squared error - RMSE\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    metrics_dict.update({'RMSE_train': rmse_train})\n",
    "    print(f\"Root mean squared error (RMSE) for training = {rmse_train}\")\n",
    "    \n",
    "    #R²\n",
    "    val_r2 = r2_score(y_train, y_pred)\n",
    "    metrics_dict.update({'R2_train': val_r2})\n",
    "    print(f\"Correlation coefficient R2 for training = {val_r2}\")\n",
    "\n",
    "    # n_size_train = number of sample size\n",
    "    # k_model = number of independent variables of the defined model\n",
    "    \n",
    "    # the number of predictive features is the length of the list of the predictors:\n",
    "    # Use the list attribute to guarantee that it is a list:\n",
    "    k_model = len(list(X_train.columns)) # numer of X columns (variables)\n",
    "\n",
    "    n_size_train = len(y_train)\n",
    "    #numer of rows\n",
    "\n",
    "    Adj_r2_train = 1 - (1 - val_r2)*(n_size_train - 1)/(n_size_train - k_model - 1)\n",
    "    metrics_dict.update({'Adj_R2_train': Adj_r2_train})\n",
    "    print(f\"Adjusted coefficient of correlation Adj R2 for training = {Adj_r2_train}\")\n",
    "    \n",
    "    if not ((X_test is None) & (y_test is None)):\n",
    "        \n",
    "        # calculate the metrics for the tests:\n",
    "        y_pred_test = model_object.predict(X_test)\n",
    "        \n",
    "        #METRICS FOR TESTING\n",
    "        #mean squared error- MSE\n",
    "        mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "        # Save this metric in the dictionary:\n",
    "        metrics_dict.update({'MSE_test': mse_test})\n",
    "        print(f\"Mean squared error (MSE) for testing = {mse_test}\")\n",
    "        #Root mean squared error - RMSE\n",
    "        rmse_test = np.sqrt(mse_test)\n",
    "        metrics_dict.update({'RMSE_test': rmse_test})\n",
    "        print(f\"Root mean squared error (RMSE) for testing = {rmse_test}\")\n",
    "        #R²\n",
    "        val_r2_test = r2_score(y_test, y_pred_test)\n",
    "        metrics_dict.update({'R2_test': val_r2_test})\n",
    "        print(f\"Correlation coefficient R2 for testing = {val_r2_test}\")\n",
    "        \n",
    "        n_size_test = len(y_test)\n",
    "        \n",
    "        Adj_r2_test = 1 - (1 - val_r2_test)*(n_size_test - 1)/(n_size_test - k_model - 1)\n",
    "        metrics_dict.update({'Adj_R2_test': Adj_r2_test})\n",
    "        print(f\"Adjusted coefficient of correlation Adj R2 for testing = {Adj_r2_test}\")\n",
    "    \n",
    "    print(\"Dictionary with metrics returned as metrics_dict\")\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for making predictions with the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_predictions (model_object, X, predict_for = 'subset', concatenate_predictions_with_X = True, col_with_predictions_suffix = None):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from keras.layers import Dense\n",
    "    from keras.models import Sequential\n",
    "    from xgboost import XGBRegressor\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    from sklearn.metrics import r2_score\n",
    "    from sklearn.metrics import mean_absolute_error\n",
    "    \n",
    "    # predict_for = 'subset' or predict_for = 'single_entry'\n",
    "    \n",
    "    # X = subset of predictive variables (dataframe).\n",
    "    # If PREDICT_FOR = 'single_entry', X should be a list of parameters values.\n",
    "    # e.g. X = [1.2, 3, 4] (dot is the decimal case separator, comma separate values). \n",
    "    # Notice that the list should contain only the numeric values, in the same order of the\n",
    "    # correspondent columns.\n",
    "    # If PREDICT_FOR = 'subset' (prediction for multiple entries), X should be a dataframe \n",
    "    # (subset) of the parameters values, as usual.\n",
    "    \n",
    "    # model_object: object containing the model that will be analyzed. e.g.\n",
    "    # model_object = elastic_net_linear_reg_model\n",
    "    \n",
    "    # concatenate_predictions_with_X = True to concatenate the predicted values with X. \n",
    "    # For a single entry, it will be appended as an element from the list. For multiple entries, \n",
    "    # it will be added as a column.\n",
    "    # If concatenate_predictions_with_X = False, the prediction will be returned as a single variable value\n",
    "    # (single entry), or as a series (prediction for a subset).\n",
    "    \n",
    "    # col_with_predictions_suffix = None. If you want to add a new column, you can declare this\n",
    "    # parameter as string with a suffix for identifying the model. If no suffix is added, the new\n",
    "    # column will be named 'y_pred'.\n",
    "    # e.g. col_with_predictions_suffix = '_keras' will create a column named \"y_pred_keras\". This\n",
    "    # parameter is useful when working with multiple models. Always start the suffix with underscore\n",
    "    # \"_\" so that no blank spaces are added; the suffix will not be merged to the column; and there\n",
    "    # will be no confusion with the dot (.) notation for methods, JSON attributes, etc.\n",
    "    \n",
    "    if (predict_for == 'single_entry'):\n",
    "        \n",
    "        print(\"Making prediction for a single entry. X must be a list with values in the order of the correspondent columns of the dataset.\")\n",
    "        \n",
    "        # Get reshaped list for making the prediction:\n",
    "        X_reshaped = np.reshape(np.array(X), (-1, 1))\n",
    "        \n",
    "        y_pred = model_object.predict(X_reshaped)\n",
    "        print(f\"Output value predicted for the entry parameters = {y_pred}\\n\")\n",
    "        print(\"Attention: for classification, this output may be either a class or a probability. Check the model documentation for verifying it.\")\n",
    "        \n",
    "        if (concatenate_predictions_with_X == True):\n",
    "            \n",
    "            # concatenate the predicted values with X. For a single entry, it will be appended as a new\n",
    "            # element from the list. For multiple entries, it will be added as a column\n",
    "            appended_list = X.append(y_pred)\n",
    "            \n",
    "            print(\"The prediction was added as a new element of the list X, and the appended list was returned.\")\n",
    "            \n",
    "            return appended_list\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            print(\"Returning only the predicted value.\")\n",
    "            \n",
    "            return y_pred\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # prediction for a subset\n",
    "        y_pred = model_object.predict(X)\n",
    "        \n",
    "        if (concatenate_predictions_with_X == True):\n",
    "            \n",
    "            # concatenate the predicted values with X. For a single entry, it will be appended as a new\n",
    "            # element from the list. For multiple entries, it will be added as a column\n",
    "            \n",
    "            # check if there is a suffix:\n",
    "            if not (col_with_predictions_suffix is None):\n",
    "                \n",
    "                # if there is a suffix, concatenate it to 'y_pred':\n",
    "                col_name = \"y_pred\" + col_with_predictions_suffix\n",
    "            \n",
    "            else:\n",
    "                # the name of the new column is simply 'y_pred'\n",
    "                col_name = \"y_pred\"\n",
    "            \n",
    "            # Set a local copy of the dataframe to manipulate:\n",
    "            X_copy = X\n",
    "            \n",
    "            # Add the predictions as the new column named col_name:\n",
    "            X_copy[col_name] = y_pred\n",
    "            \n",
    "            print(f\"The prediction was added as the new column {col_name} of the dataframe X, and this dataframe was returned.\")\n",
    "            \n",
    "            return X_copy\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            print(\"Returning only the predicted values.\")\n",
    "            \n",
    "            return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for importing or exporting models and dictionaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_export_model_or_dict (action = 'import', objects_manipulated = 'model_only', model_file_name = None, dictionary_file_name = None, directory_path = '/', model_type = 'keras', dict_to_export = None, model_to_export = None, use_colab_memory = False):\n",
    "    \n",
    "    import os\n",
    "    import pickel as pkl\n",
    "    import dill\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from keras.layers import Dense\n",
    "    from keras.models import Sequential\n",
    "    from xgboost import XGBRegressor\n",
    "    from xgboost import XGBClassifier\n",
    "    from statsmodels.tsa.arima.model import ARIMA\n",
    "    from statsmodels.tsa.arima.model import ARIMAResults\n",
    "    from keras.models import load_model\n",
    "    from google.colab import files\n",
    "    \n",
    "    # action = 'import' for importing a model and/or a dictionary;\n",
    "    # action = 'export' for exporting a model and/or a dictionary.\n",
    "    \n",
    "    # objects_manipulated = 'model_only' if only a model will be manipulated.\n",
    "    # objects_manipulated = 'dict_only' if only a dictionary will be manipulated.\n",
    "    # objects_manipulated = 'model_and_dict' if both a model and a dictionary will be\n",
    "    # manipulated.\n",
    "    \n",
    "    #model_file_name: string with the name of the file containing the model (for 'import');\n",
    "    # or of the name that the exported file will have (for 'export')\n",
    "    # e.g. model_file_name = 'model'\n",
    "    # WARNING: Do not add the file extension.\n",
    "    # Keep it in quotes. Keep model_file_name = None if no model will be manipulated.\n",
    "    \n",
    "    # dictionary_file_name: string with the name of the file containing the dictionary \n",
    "    # (for 'import');\n",
    "    # or of the name that the exported file will have (for 'export')\n",
    "    # e.g. dictionary_file_name = 'history_dict'\n",
    "    # WARNING: Do not add the file extension.\n",
    "    # Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no \n",
    "    # dictionary will be manipulated.\n",
    "    \n",
    "    # DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "    # or from which the model will be retrieved. If no value is provided,\n",
    "    # the DIRECTORY_PATH will be the root: \"/\"\n",
    "    # Notice that the model and the dictionary must be stored in the same path.\n",
    "    # If a model and a dictionary will be exported, they will be stored in the same\n",
    "    # DIRECTORY_PATH.\n",
    "    \n",
    "    # model_type: This parameter has effect only when a model will be manipulated.\n",
    "    # model_type: 'keras' for deep learning keras/ tensorflow models with extension .h5\n",
    "    # model_type = 'sklearn' for models from scikit-learn (non-deep learning)\n",
    "    # model_type = 'xgb' for XGBoost models (non-deep learning)\n",
    "    # model_type = 'arima' for ARIMA model (Statsmodels)\n",
    "    \n",
    "    # dict_to_export and model_to_export: \n",
    "    # These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "    # must be declared. If ACTION == 'export', keep:\n",
    "    # dict_to_export = None, \n",
    "    # model_to_export = None\n",
    "    # If one of these objects will be exported, substitute None by the name of the object\n",
    "    # e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "    # model_to_export = keras_model. Notice that it must be declared without quotes, since\n",
    "    # it is not a string, but an object.\n",
    "    # For exporting a dictionary named as 'dict':\n",
    "    # dict_to_export = dict\n",
    "    \n",
    "    # use_colab_memory: this parameter has only effect when using Google Colab (or it will\n",
    "    # raise an error). Set as use_colab_memory = True if you want to use the instant memory\n",
    "    # from Google Colaboratory: you will update or download the file and it will be available\n",
    "    # only during the time when the kernel is running. It will be excluded when the kernel\n",
    "    # dies, for instance, when you close the notebook.\n",
    "    \n",
    "    # If action == 'export' and use_colab_memory == True, then the file will be downloaded\n",
    "    # to your computer (running the cell will start the download).\n",
    "    \n",
    "    # Check the directory path\n",
    "    if (directory_path is None):\n",
    "        # set as the root:\n",
    "        directory_path = \"/\"\n",
    "        \n",
    "        \n",
    "    bool_check1 = (objects_manipulated != 'model_only')\n",
    "    # bool_check1 == True if a dictionary will be manipulated\n",
    "    \n",
    "    bool_check2 = (objects_manipulated != 'dict_only')\n",
    "    # bool_check1 == True if a dictionary will be manipulated\n",
    "    \n",
    "    if (bool_check1 == True):\n",
    "        #manipulate a dictionary\n",
    "        \n",
    "        if (dictionary_file_name is None):\n",
    "            print(\"Please, enter a name for the dictionary.\")\n",
    "            return \"error1\"\n",
    "        \n",
    "        else:\n",
    "            # Create the file path for the dictionary:\n",
    "            dict_path = os.path.join(directory_path, dictionary_file_name)\n",
    "            # Extract the file extension\n",
    "            dict_extension = 'pkl'\n",
    "            #concatenate:\n",
    "            dict_path = dict_path + \".\" + dict_extension\n",
    "            \n",
    "    \n",
    "    if (bool_check2 == True):\n",
    "        #manipulate a model\n",
    "        \n",
    "        if (model_file_name is None):\n",
    "            print(\"Please, enter a name for the model.\")\n",
    "            return \"error1\"\n",
    "        \n",
    "        else:\n",
    "            # Create the file path for the dictionary:\n",
    "            model_path = os.path.join(directory_path, model_file_name)\n",
    "            # Extract the file extension\n",
    "            \n",
    "            #check model_type:\n",
    "            if (model_type == 'keras'):\n",
    "                model_extension = 'h5'\n",
    "            \n",
    "            elif (model_type == 'sklearn'):\n",
    "                model_extension = 'dill'\n",
    "                #it could be 'pkl', though\n",
    "            \n",
    "            elif (model_type == 'xgb'):\n",
    "                model_extension = 'json'\n",
    "                #it could be 'ubj', though\n",
    "            \n",
    "            elif (model_tyoe == 'arima'):\n",
    "                model_extension = 'pkl'\n",
    "            \n",
    "            else:\n",
    "                print(\"Enter a valid model_type: keras, sklearn_xgb, or arima.\")\n",
    "                return \"error2\"\n",
    "            \n",
    "            #concatenate:\n",
    "            model_path = model_path +  \".\" + model_extension\n",
    "            \n",
    "    # Now we have the full paths for the dictionary and for the model.\n",
    "    \n",
    "    if (action == 'import'):\n",
    "        \n",
    "        if (use_colab_memory == True):\n",
    "            \n",
    "            print(\"Click on the button for file selection and select the files from your machine that will be uploaded in the Colab environment.\")\n",
    "            print(\"Warning: the files will be removed from Colab memory after the Kernel dies or after the notebook is closed.\")\n",
    "            # this functionality requires the previous declaration:\n",
    "            ## from google.colab import files\n",
    "            colab_files_dict = files.upload()\n",
    "            # The files are stored into a dictionary called colab_files_dict where the keys\n",
    "            # are the names of the files and the values are the files themselves.\n",
    "            ## e.g. if you upload a single file named \"dictionary.pkl\", the dictionary will be\n",
    "            ## colab_files_dict = {'dictionary.pkl': file}, where file is actually a big string\n",
    "            ## representing the contents of the file. The length of this value is the size of the\n",
    "            ## uploaded file, in bytes.\n",
    "            ## To access the file is like accessing a value from a dictionary: \n",
    "            ## d = {'key1': 'val1'}, d['key1'] == 'val1'\n",
    "            ## we simply declare the key inside brackets and quotes, the same way we would do for\n",
    "            ## accessing the column of a dataframe.\n",
    "            ## In this example, colab_files_dict['dictionary.pkl'] access the content of the \n",
    "            ## .pkl file, and len(colab_files_dict['dictionary.pkl']) is the size of the .pkl\n",
    "            ## file in bytes.\n",
    "            ## To check the dictionary keys, apply the method .keys() to the dictionary (with empty\n",
    "            ## parentheses): colab_files_dict.keys()\n",
    "            \n",
    "            for key in colab_files_dict.keys():\n",
    "                #loop through each element of the list of keys of the dictionary\n",
    "                # (list colab_files_dict.keys()). Each element is named 'key'\n",
    "                print(f\"User uploaded file {key} with length {len(colab_files_dict[key])} bytes.\")\n",
    "                # The key is the name of the file, and the length of the value\n",
    "                ## correspondent to the key is the file's size in bytes.\n",
    "                ## Notice that the content of the uploaded object must be passed \n",
    "                ## as argument for a proper function to be interpreted. \n",
    "                ## For instance, the content of a xlsx file should be passed as\n",
    "                ## argument for Pandas .read_excel function; the pkl file must be passed as\n",
    "                ## argument for pickle.\n",
    "                ## e.g., if you uploaded 'table.xlsx' and stored it into colab_files_dict you should\n",
    "                ## declare df = pd.read_excel(colab_files_dict['table.xlsx']) to obtain a dataframe\n",
    "                ## df from the uploaded table. Notice that is the value, not the key, that is the\n",
    "                ## argument.\n",
    "        \n",
    "        if (bool_check1 == True):\n",
    "            #manipulate a dictionary\n",
    "            if (use_colab_memory == True):\n",
    "                key = dictionary_file_name + \".\" + dict_extension\n",
    "                #Use the key to access the file content, and pass the file content\n",
    "                # to pickle:\n",
    "                with open(colab_files_dict[key], 'rb') as opened_file:\n",
    "            \n",
    "                    imported_dict = pkl.load(opened_file)\n",
    "                    # The structure imported_dict = pkl.load(open(colab_files_dict[key], 'rb')) relies \n",
    "                    # on the GC to close the file. That's not a good idea: If someone doesn't use \n",
    "                    # CPython the garbage collector might not be using refcounting (which collects \n",
    "                    # unreferenced objects immediately) but e.g. collect garbage only after some time.\n",
    "                    # Since file handles are closed when the associated object is garbage collected or \n",
    "                    # closed explicitly (.close() or .__exit__() from a context manager) the file \n",
    "                    # will remain open until the GC kicks in.\n",
    "                    # Using 'with' ensures the file is closed as soon as the block is left - even if \n",
    "                    # an exception happens inside that block, so it should always be preferred for any \n",
    "                    # real application.\n",
    "                    # source: https://stackoverflow.com/questions/39447362/equivalent-ways-to-json-load-a-file-in-python\n",
    "\n",
    "                print(f\"Dictionary {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "            else:\n",
    "                #standard method\n",
    "                with open(dict_path, 'rb') as opened_file:\n",
    "            \n",
    "                    imported_dict = pkl.load(opened_file)\n",
    "                \n",
    "                # 'rb' stands for read binary (read mode). For writing mode, 'wb', 'write binary'\n",
    "                print(f\"Dictionary successfully imported from {dict_path}.\")\n",
    "                \n",
    "        if (bool_chek2 == True):\n",
    "            #manipulate a model\n",
    "            # select the proper model\n",
    "        \n",
    "            if (model_type == 'keras'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = load_model(colab_files_dict[key])\n",
    "                    print(f\"Keras/TensorFlow model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # We previously declared:\n",
    "                    # from keras.models import load_model\n",
    "                    model = load_model(model_path)\n",
    "                    print(f\"Keras/TensorFlow model successfully imported from {model_path}.\")\n",
    "\n",
    "            elif (model_type == 'sklearn'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    \n",
    "                    with open(colab_files_dict[key], 'rb') as opened_file:\n",
    "            \n",
    "                        model = dill.load(opened_file)\n",
    "                    \n",
    "                    print(f\"Scikit-learn model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    with open(model_path, 'rb') as opened_file:\n",
    "            \n",
    "                        model = dill.load(opened_file)\n",
    "                \n",
    "                    print(f\"Scikit-learn model successfully imported from {model_path}.\")\n",
    "                    # For loading a pickle model:\n",
    "                    ## model = pkl.load(open(model_path, 'rb'))\n",
    "                    # 'rb' stands for read binary (read mode). For writing mode, 'wb', 'write binary'\n",
    "\n",
    "            elif (model_type == 'xgb'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = load_model(colab_files_dict[key])\n",
    "                    print(f\"XGBoost model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model = load_model(model_path)\n",
    "                    print(f\"XGBoost model successfully imported from {model_path}.\")\n",
    "                    # model.load_model(\"model.json\") or model.load_model(\"model.ubj\")\n",
    "                    # .load_model is a method from xgboost object\n",
    "\n",
    "            elif (model_type == 'arima'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = ARIMAResults.load(colab_files_dict[key])\n",
    "                    print(f\"ARIMA model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # We previously declared:\n",
    "                    # from statsmodels.tsa.arima.model import ARIMAResults\n",
    "                    model = ARIMAResults.load(model_path)\n",
    "                    print(f\"ARIMA model successfully imported from {model_path}.\")\n",
    "            \n",
    "            if (objects_manipulated == 'model_only'):\n",
    "                # only the model should be returned\n",
    "                return model\n",
    "            \n",
    "            elif (objects_manipulated == 'dict_only'):\n",
    "                # only the dictionary should be returned:\n",
    "                return imported_dict\n",
    "            \n",
    "            else:\n",
    "                # Both objects are returned:\n",
    "                return model, imported_dict\n",
    "\n",
    "    \n",
    "    elif (action == 'export'):\n",
    "        \n",
    "        #Let's export the models or dictionary:\n",
    "        if (use_colab_memory == True):\n",
    "            print(\"The files will be downloaded to your computer.\")\n",
    "        \n",
    "        if (bool_check1 == True):\n",
    "            #manipulate a dictionary\n",
    "            if (use_colab_memory == True):\n",
    "                ## Download the dictionary\n",
    "                key = dictionary_file_name + \".\" + dict_extension\n",
    "                \n",
    "                with open(key, 'wb') as opened_file:\n",
    "            \n",
    "                    pkl.dump(dict_to_export, opened_file)\n",
    "                \n",
    "                # this functionality requires the previous declaration:\n",
    "                ## from google.colab import files\n",
    "                files.download(key)\n",
    "                \n",
    "                print(f\"Dictionary {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "            else:\n",
    "                #standard method \n",
    "                with open(dict_path, 'wb') as opened_file:\n",
    "            \n",
    "                    pkl.dump(dict_to_export, opened_file)\n",
    "                \n",
    "                #to save the file, the mode must be set as 'wb' (write binary)\n",
    "                print(f\"Dictionary successfully exported as {dict_path}.\")\n",
    "                \n",
    "        if (bool_chek2 == True):\n",
    "            #manipulate a model\n",
    "            # select the proper model\n",
    "        \n",
    "            if (model_type == 'keras'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"Keras/TensorFlow model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save(model_path)\n",
    "                    print(f\"Keras/TensorFlow model successfully exported as {model_path}.\")\n",
    "\n",
    "            elif (model_type == 'sklearn'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    \n",
    "                    with open(key, 'wb') as opened_file:\n",
    "\n",
    "                        dill.dump(model_to_export, opened_file)\n",
    "                    \n",
    "                    #to save the file, the mode must be set as 'wb' (write binary)\n",
    "                    files.download(key)\n",
    "                    print(f\"Scikit-learn model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    with open(model_path, 'wb') as opened_file:\n",
    "\n",
    "                        dill.dump(model_to_export, opened_file)\n",
    "                    \n",
    "                    print(f\"Scikit-learn model successfully exported as {model_path}.\")\n",
    "                    # For exporting a pickle model:\n",
    "                    ## pkl.dump(model_to_export, open(model_path, 'wb'))\n",
    "            \n",
    "            elif (model_type == 'xgb'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save_model(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"XGBoost model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save_model(model_path)\n",
    "                    print(f\"XGBoost model successfully exported as {model_path}.\")\n",
    "                    # For exporting a pickle model:\n",
    "                    ## pkl.dump(model_to_export, open(model_path, 'wb'))\n",
    "            \n",
    "            elif (model_type == 'arima'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"ARIMA model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save(model_path)\n",
    "                    print(f\"ARIMA model successfully exported as {model_path}.\")\n",
    "        \n",
    "        print(\"Export of files completed.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Enter a valid action, import or export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for exporting the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_dataframe (dataframe_to_be_exported, new_file_name_with_csv_extension, file_directory_path = None, export_to_s3_bucket = False, s3_bucket_name = None, desired_s3_file_name_with_csv_extension = None):\n",
    "    \n",
    "    import os\n",
    "    import boto3\n",
    "    #boto3 is AWS S3 Python SDK\n",
    "    import pandas as pd\n",
    "    \n",
    "    ## WARNING: all file extensions should be .csv for this function\n",
    "    \n",
    "    # FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "    # (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"/\" \n",
    "    # or FILE_DIRECTORY_PATH = \"/folder\"\n",
    "    # If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "    # In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "    # NEW_FILE_NAME_WITH_CSV_EXTENSION - (string, in quotes): input the name of the \n",
    "    # file with the  extension. e.g. FILE_NAME_WITH_CSV_EXTENSION = \"file.csv\"\n",
    "    \n",
    "    # export_to_s3_bucket = False. Alternatively, set as True to export the file to an\n",
    "    # AWS S3 Bucket.\n",
    "\n",
    "    ## The following parameters have effect only when export_to_s3_bucket == True:\n",
    "\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. S3_BUCKET_NAME = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "\n",
    "    # The name desired for the object stored in S3 (string, in quotes). \n",
    "    # Keep it None to set it equals to new_file_name_with_csv_extension. \n",
    "    # Alternatively, set it as a string analogous to new_file_name_with_csv_extension. \n",
    "    # e.g. desired_s3_file_name_with_csv_extension = \"S3_file.csv\"\n",
    "    \n",
    "    if (export_to_s3_bucket == True):\n",
    "        \n",
    "        if (desired_s3_file_name_with_csv_extension is None):\n",
    "            #Repeat new_file_name_with_extension\n",
    "            desired_s3_file_name_with_csv_extension = new_file_name_with_csv_extension\n",
    "        \n",
    "        # If the bucket name was provided, start the session. If not, print an error\n",
    "        # message:\n",
    "        \n",
    "        if (s3_bucket_name is None):\n",
    "            \n",
    "            print(\"Please, provide a valid S3 Bucket name to download from.\")\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            # start S3 client:\n",
    "            print(\"Starting AWS S3 client.\")\n",
    "        \n",
    "            # Let's export the file to a AWS S3 (simple storage service) bucket\n",
    "            # instantiate S3 client and upload to s3\n",
    "            s3_client = boto3.resource('s3')\n",
    "            \n",
    "            # Create a local copy of the file on the root.\n",
    "            local_copy_path = os.path.join(\"/\", new_file_name_with_csv_extension)\n",
    "            dataframe_to_be_exported.to_csv(local_copy_path, index = False)\n",
    "            \n",
    "            print(\"Local copy of the dataframe created on the root path to export to S3.\")\n",
    "            print(\"Simply delete this file from the root path if you only want to keep the S3 version.\")\n",
    "            \n",
    "            # Upload this local copy to S3:\n",
    "            try:\n",
    "                response = s3_client.meta.client.upload_file(local_copy_path, s3_bucket_name, desired_s3_file_name_with_extension)\n",
    "            \n",
    "            except ClientError as e:\n",
    "                logging.error(e)\n",
    "                return False\n",
    "            \n",
    "            print(f\"{desired_s3_file_name_with_csv_extension} successfully exported to {s3_bucket_name} AWS S3 bucket.\")\n",
    "            return True\n",
    "            # Check AWS Documentation:\n",
    "            # https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html\n",
    "            \n",
    "            # Notice: if you wanted to authenticate directly from Python code, you could use\n",
    "            # the following code, instead:        \n",
    "            # ACCESS_KEY = 'access_key_ID'\n",
    "            # PASSWORD_KEY = 'password_key'\n",
    "            # s3_client = boto3.client('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = PASSWORD_KEY)\n",
    "            # s3_client.upload_file(local_copy_path, s3_bucket_name, desired_s3_file_name_with_extension)\n",
    "            \n",
    "    else :\n",
    "        # Do not export to AWS S3. Export to other path.\n",
    "        # Create the complete file path:\n",
    "        file_path = os.path.join(file_directory_path, new_file_name_with_csv_extension)\n",
    "\n",
    "        dataframe_to_be_exported.to_csv(file_path, index = False)\n",
    "\n",
    "        print(f\"Dataframe {new_file_name_with_csv_extension} exported as \\'{file_path}\\'.\")\n",
    "        print(\"Warning: if there was a file in this file path, it was replaced by the exported dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for downloading a file from Google Colab or AWS S3 to the local machine or uploading a file from the machine to S3 or to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_or_upload_file (source = 'aws', action = 'download', object_to_download_from_colab = None, s3_bucket_name = None, local_path_of_storage = '/', file_name_with_extension = None):\n",
    "    \n",
    "    import os\n",
    "    import boto3\n",
    "    # boto3 is AWS S3 Python SDK\n",
    "    from google.colab import files\n",
    "    \n",
    "    # source = 'google' for downloading from (or uploading to) Google Colab's instant memory;\n",
    "    # source = 'aws' for downloading from (or uploading to) an AWS S3 bucket.\n",
    "    \n",
    "    # action = 'download' to download the file to the local machine\n",
    "    # action = 'upload' to upload a file from local machine to AWS S3 or to\n",
    "    # Google Colab's instant memory\n",
    "    \n",
    "    # object_to_download_from_colab = None. This option has effect only when\n",
    "    # source == 'google'. In this case, this parameter is obbligatory. \n",
    "    # Declare as object_to_download_from_colab the object that you want to download.\n",
    "    # Since it is an object and not a string, it should not be declared in quotes.\n",
    "    # e.g. to download a dictionary named dict, object_to_download_from_colab = dict.\n",
    "    # To download a dataframe named df, declare object_to_download_from_colab = df.\n",
    "    # To export a model named keras_model, declare object_to_download_from_colab = keras_model\n",
    "    \n",
    "    ## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN source == 'aws'\n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # LOCAL_PATH_OF_STORAGE: path of the local computer environment \n",
    "    # to which the S3 bucket contents will be downloaded (ACTION == 'download'); or\n",
    "    # path of the folder containing the file that will be uploaded in S3 (ACTION = 'upload'). \n",
    "    # If it is None, or if LOCAL_PATH_OF_STORAGE = '/', files \n",
    "    # will be imported to the root path. Alternatively, input the path as a string \n",
    "    # (in quotes).\n",
    "    # Examples: LOCAL_PATH_OF_STORAGE = '/copied_s3_bucket'; \n",
    "    # LOCAL_PATH_OF_STORAGE = \"/My_folder\"; LOCAL_PATH_OF_STORAGE = \"/Users/Me/Documents/\"\n",
    "    # Notice that only the directories should be declared: do not include the file name and\n",
    "    # its extension.\n",
    "    \n",
    "    # file_name_with_extension: string, in quotes, containing the file name which will be\n",
    "    # downloaded from S3; or uploaded from S3, followed by its extension. \n",
    "    ## This parameter is obbligatory when source == 'aws'\n",
    "    # Examples:\n",
    "    # file_name_with_extension = 'Screen_Shot.png'; file_name_with_extension = 'dataset.csv',\n",
    "    # file_name_with_extension = \"dictionary.pkl\", file_name_with_extension = \"model.h5\",\n",
    "    # file_name_with_extension = 'doc.pdf', file_name_with_extension = 'model.dill'\n",
    "\n",
    "    if (source == 'google'):\n",
    "        \n",
    "        if (action == 'upload'):\n",
    "            \n",
    "            print(\"Click on the button for file selection and select the files from your machine that will be uploaded in the Colab environment.\")\n",
    "            print(\"Warning: the files will be removed from Colab memory after the Kernel dies or after the notebook is closed.\")\n",
    "            # this functionality requires the previous declaration:\n",
    "            ## from google.colab import files\n",
    "            \n",
    "            colab_files_dict = files.upload()\n",
    "            \n",
    "            # The files are stored into a dictionary called colab_files_dict where the keys\n",
    "            # are the names of the files and the values are the files themselves.\n",
    "            ## e.g. if you upload a single file named \"dictionary.pkl\", the dictionary will be\n",
    "            ## colab_files_dict = {'dictionary.pkl': file}, where file is actually a big string\n",
    "            ## representing the contents of the file. The length of this value is the size of the\n",
    "            ## uploaded file, in bytes.\n",
    "            ## To access the file is like accessing a value from a dictionary: \n",
    "            ## d = {'key1': 'val1'}, d['key1'] == 'val1'\n",
    "            ## we simply declare the key inside brackets and quotes, the same way we would do for\n",
    "            ## accessing the column of a dataframe.\n",
    "            ## In this example, colab_files_dict['dictionary.pkl'] access the content of the \n",
    "            ## .pkl file, and len(colab_files_dict['dictionary.pkl']) is the size of the .pkl\n",
    "            ## file in bytes.\n",
    "            ## To check the dictionary keys, apply the method .keys() to the dictionary (with empty\n",
    "            ## parentheses): colab_files_dict.keys()\n",
    "            \n",
    "            for key in colab_files_dict.keys():\n",
    "                #loop through each element of the list of keys of the dictionary\n",
    "                # (list colab_files_dict.keys()). Each element is named 'key'\n",
    "                print(f\"User uploaded file {key} with length {len(colab_files_dict[key])} bytes.\")\n",
    "                # The key is the name of the file, and the length of the value\n",
    "                ## correspondent to the key is the file's size in bytes.\n",
    "                ## Notice that the content of the uploaded object must be passed \n",
    "                ## as argument for a proper function to be interpreted. \n",
    "                ## For instance, the content of a xlsx file should be passed as\n",
    "                ## argument for Pandas .read_excel function; the pkl file must be passed as\n",
    "                ## argument for pickle.\n",
    "                ## e.g., if you uploaded 'table.xlsx' and stored it into colab_files_dict you should\n",
    "                ## declare df = pd.read_excel(colab_files_dict['table.xlsx']) to obtain a dataframe\n",
    "                ## df from the uploaded table. Notice that is the value, not the key, that is the\n",
    "                ## argument.\n",
    "                \n",
    "                print(\"The uploaded files are stored into a dictionary object named as colab_files_dict.\")\n",
    "                print(\"Each key from this dictionary is the name of an uploaded file. The value correspondent to that key is the file itself.\")\n",
    "                print(\"The structure of a general Python dictionary is dict = {\\'key1\\': value1}. To access value1, declare file = dict[\\'key1\\'], as if you were accessing a column from a dataframe.\")\n",
    "                print(\"Then, if you uploaded a file named \\'table.xlsx\\', you can access this file as:\")\n",
    "                print(\"uploaded_file = colab_files_dict[\\'table.xlsx\\']\")\n",
    "                print(\"Notice, though, that the object uploaded_file is the whole file content, not a Python object already converted. To convert to a Python object, pass this element as argument for a proper function or method.\")\n",
    "                print(\"In this example, to convert the object uploaded_file to a dataframe, Pandas pd.read_excel function could be used. In the following line, a df dataframe object is obtained from the uploaded file:\")\n",
    "                print(\"df = pd.read_excel(uploaded_file)\")\n",
    "        \n",
    "        elif (action == 'download'):\n",
    "            \n",
    "            if (object_to_download_from_colab is None):\n",
    "                \n",
    "                #No object was declared\n",
    "                print(\"Please, inform an object to download. Since it is an object, not a string, it should not be declared in quotes.\")\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                print(\"The file will be downloaded to your computer.\")\n",
    "\n",
    "                files.download(object_to_download_from_colab)\n",
    "\n",
    "                print(f\"File {object_to_download_from_colab} successfully downloaded from Colab environment.\")\n",
    "\n",
    "        else:\n",
    "            \n",
    "            print(\"Please, select a valid action, download or upload.\")\n",
    "          \n",
    "    elif (source == 'aws'):\n",
    "        \n",
    "        # Notice: if you wanted to authenticate directly from Python code, you could use\n",
    "        # the following code, instead for starting the client:\n",
    "        \n",
    "        # ACCESS_KEY = 'access_key_ID'\n",
    "        # PASSWORD_KEY = 'password_key'\n",
    "        # s3_client = boto3.client('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = PASSWORD_KEY)\n",
    "        # Nextly, the code is the same.\n",
    "        \n",
    "        \n",
    "        # If the path to store is None, also import the bucket content to root path;\n",
    "        # or upload the file from root path to the bucket\n",
    "        if (local_path_of_storage is None):\n",
    "            \n",
    "            local_path_of_storage = '/'\n",
    "        \n",
    "        # If the bucket name was provided, start the session. If not, print an error\n",
    "        # message. The same for the file name with extension:\n",
    "        \n",
    "        if (s3_bucket_name is None):\n",
    "            \n",
    "            print(\"Please, provide a valid S3 Bucket name.\")\n",
    "        \n",
    "        elif (file_name_with_extension is None):\n",
    "            \n",
    "            print(\"Please, provide a valid file name with its extension. e.g. \\'dataset.csv\\'.\")\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # Obtain the full file path from which the file will be uploaded to S3; or to\n",
    "            # which the file will be downloaded from S3:\n",
    "            file_path = os.path.join(local_path_of_storage, file_name_with_extension)\n",
    "            \n",
    "            # Start S3 client:\n",
    "            s3_client = boto3.resource('s3')\n",
    "            \n",
    "            print(\"Starting AWS S3 client.\")\n",
    "            \n",
    "            if (action == 'upload'):\n",
    "                \n",
    "                s3_client.Object(s3_bucket_name, file_name_with_extension).\\\n",
    "                    upload_file(Filename = file_path)\n",
    "                \n",
    "                print(f\"File {file_name_with_extension} successfully uploaded to AWS S3 {s3_bucket_name} bucket.\")\n",
    "            \n",
    "            elif (action == 'download'):\n",
    "\n",
    "                print(\"The file will be downloaded to your computer.\")\n",
    "                \n",
    "                s3_client.Object(s3_bucket_name, file_name_with_extension).download_file(file_path)\n",
    "                \n",
    "                print(f\"File {file_name_with_extension} successfully downloaded from AWS S3 {s3_bucket_name} bucket.\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(\"Please, select a valid action, download or upload.\")\n",
    "\n",
    "    else:\n",
    "        \n",
    "        print(\"Select a valid source: \\'google\\' for mounting Google Drive; or \\'aws\\' for accessing AWS S3 Bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "## **Call the functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = 'aws'\n",
    "# SOURCE = 'google' for mounting the google drive;\n",
    "# SOURCE = 'aws' for accessing an AWS S3 bucket\n",
    "\n",
    "## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN SOURCE == 'aws':\n",
    "\n",
    "PATH_TO_STORE_IMPORTED_S3_BUCKET = '/'\n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET: path of the Python environment to which the\n",
    "# S3 bucket contents will be imported. If it is None, or if \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = '/', bucket will be imported to the root path. \n",
    "# Alternatively, input the path as a string (in quotes). e.g. \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = '/copied_s3_bucket'\n",
    "\n",
    "S3_BUCKET_NAME = 'name_of_aws_s3_bucket_to_be_accessed'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_KEY_PREFFIX_FOLDER = None\n",
    "# S3_OBJECT_KEY_PREFFIX_FOLDER = None. Keep it None or as an empty string \n",
    "# (S3_OBJECT_KEY_PREFFIX_FOLDER = '') to import the whole bucket content, instead of a \n",
    "# single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, key_preffix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "mount_storage_system (source = SOURCE, path_to_store_imported_s3_bucket = PATH_TO_STORE_IMPORTED_S3_BUCKET, s3_bucket_name = S3_BUCKET_NAME, s3_obj_key_preffix = S3_OBJECT_KEY_PREFFIX_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "azdata_cell_guid": "0d4292a3-e6c6-41b2-bbda-0743ee04195e",
    "id": "g3zcdInJyHHm"
   },
   "outputs": [],
   "source": [
    "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, etc), \n",
    "## JSON, txt, or CSV (comma separated values) files.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"/\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"/\" \n",
    "# or FILE_DIRECTORY_PATH = \"/folder\"\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
    "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "# FILE_NAME_WITH_EXTENSION = \"file.csv\"\n",
    "\n",
    "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
    "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True \n",
    "# if you want to read a file with txt extension containing a text formatted as JSON \n",
    "# (but not saved as JSON).\n",
    "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the \n",
    "# function (below) must be set. If not, an error message will be raised.\n",
    "    \n",
    "HAS_HEADER = True\n",
    "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
    "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
    "\n",
    "## Parameters for loading txt or CSV files:\n",
    "\n",
    "TXT_CSV_COL_SEP = \"comma\"\n",
    "# TXT_CSV_COL_SEP = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "# or 'csv'. It informs how the different columns are separated.\n",
    "# Alternatively, TXT_CSV_COL_SEP = \"comma\" for columns separated by comma (\",\")\n",
    "# TXT_CSV_COL_SEP = \"whitespace\" for columns separated by simple spaces (\" \").\n",
    "\n",
    "SHEET_TO_LOAD = None\n",
    "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
    "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
    "# will be loaded.\n",
    "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
    "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
    "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "# name to load the sheet with that name.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFFIX_LIST = None\n",
    "# JSON_METADATA_PREFFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFFIX_LIST = ['name', 'last']\n",
    "\n",
    "#The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = load_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, has_header = HAS_HEADER, txt_csv_col_sep = TXT_CSV_COL_SEP, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_preffix_list = JSON_METADATA_PREFFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Converting JSON object to dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "azdata_cell_guid": "0d4292a3-e6c6-41b2-bbda-0743ee04195e",
    "id": "g3zcdInJyHHm"
   },
   "outputs": [],
   "source": [
    "JSON_OBJ_TO_CONVERT = json_object #Alternatively: object containing the JSON to be converted\n",
    "\n",
    "# JSON_OBJ_TO_CONVERT: object containing JSON, or string with JSON content to parse.\n",
    "# Objects may be: string with JSON formatted text;\n",
    "# list with nested dictionaries (JSON formatted);\n",
    "# dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFFIX_LIST = None\n",
    "# JSON_METADATA_PREFFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFFIX_LIST = ['name', 'last']\n",
    "\n",
    "#The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = json_obj_to_dataframe (json_obj_to_convert = JSON_OBJ_TO_CONVERT, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_preffix_list = JSON_METADATA_PREFFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Concatenating (SQL UNION) multiple dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_DATAFRAMES = [dataset1, dataset2]\n",
    "# LIST_OF_DATAFRAMES must be a list containing the dataframe objects\n",
    "# example: list_of_dataframes = [df1, df2, df3, df4]\n",
    "# Notice that the dataframes are objects, not strings. Therefore, they should not\n",
    "# be declared inside quotes.\n",
    "# There is no limit of dataframes. In this example, we will concatenate 4 dataframes.\n",
    "# If LIST_OF_DATAFRAMES = [df1, df2, df3] we would concatenate 3, and if\n",
    "# LIST_OF_DATAFRAMES = [df1, df2, df3, df4, df5] we would concatenate 5 dataframes.\n",
    "\n",
    "WHAT_TO_APPEND = 'rows'\n",
    "# WHAT_TO_APPEND = 'rows' for appending the rows from one dataframe\n",
    "# into the other; WHAT_TO_APPEND = 'columns' for appending the columns\n",
    "# from one dataframe into the other (horizontal or lateral append).\n",
    "\n",
    "IGNORE_INDEX_ON_UNION = True # Alternatively: True or False\n",
    "\n",
    "SORT_VALUES_ON_UNION = True # Alternatively: True or False\n",
    "\n",
    "UNION_JOIN_TYPE = None\n",
    "# JOIN can be 'inner' to perform an inner join, eliminating the missing values\n",
    "# The default (None) is 'outer': the dataframes will be stacked on the columns with\n",
    "# same names but, in case there is no correspondence, the row will present a missing\n",
    "# value for the columns which are not present in one of the dataframes.\n",
    "# When using the 'inner' method, only the common columns will remain.\n",
    "# Alternatively, keep UNION_JOIN_TYPE = None for the standard outer join; or set\n",
    "# UNION_JOIN_TYPE = \"inner\" (inside quotes) for using the inner join.\n",
    "    \n",
    "#These 3 last parameters are the same from Pandas .concat method:\n",
    "# IGNORE_INDEX_ON_UNION = ignore_index;\n",
    "# SORT_VALUES_ON_UNION = sort\n",
    "# UNION_JOIN_TYPE = join\n",
    "# Check Datacamp course Joining Data with pandas, Chap.3, \n",
    "# Advanced Merging and Concatenating\n",
    "    \n",
    "\n",
    "#New dataframe saved as concat_df. Simply modify this object on the left of equality:\n",
    "concat_df = UNION_DATAFRAMES (list_of_dataframes = LIST_OF_DATAFRAMES, what_to_append = WHAT_TO_APPEND, ignore_index_on_union = IGNORE_INDEX_ON_UNION, sort_values_on_union = SORT_VALUES_ON_UNION, union_join_type = UNION_JOIN_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Filtering (selecting); or renaming columns of the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "MODE = 'filter'\n",
    "# MODE = 'filter' for filtering only the list of columns passed as cols_list;\n",
    "# MODE = 'rename' for renaming the columns with the names passed as cols_list.\n",
    "\n",
    "COLS_LIST = ['column1', 'column2', 'column3']\n",
    "# COLS_LIST = list of strings containing the names (headers) of the columns to select\n",
    "# (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "# For instance: COLS_LIST = ['col1', 'col2', 'col3'] will \n",
    "# select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "# Declare the names inside quotes.\n",
    "# Simply substitute the list by the list of columns that you want to select; or the\n",
    "# list of the new names you want to give to the dataset columns.\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = col_filter_rename (df = DATASET, cols_list = COLS_LIST, mode = MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Splitting the dataframe into train and test subsets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = X\n",
    "# X_df = subset of predictive variables (dataframe). Alternatively, modify X, not X_df\n",
    "Y = y\n",
    "# Y = subset of response variable (series). Alternatively, modify y, not Y\n",
    "\n",
    "PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 75   \n",
    "# percent_of_data_used_for_model_training: float from 0 to 100,\n",
    "# representing the percent of data used for training the model\n",
    "\n",
    "# Subset and series destined to training returned as X_train, y_train;\n",
    "# Subset and series separated for testing returned as X_test, y_test;\n",
    "# Simply modify these objects on the left of equality:\n",
    "X_train, X_test, y_train, y_test = split_data_into_train_and_test (X = X_df, y = Y, percent_of_data_used_for_model_training = PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ordinary Least Squares (OLS) Linear Regression**\n",
    "- This function runs the 'bar_chart' function. Certify that this function was properly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN = X_train\n",
    "# X_TRAIN = subset of predictive variables (dataframe).\n",
    "# Alternatively, modify X_train, not X_TRAIN\n",
    "Y_TRAIN = y_train\n",
    "# Y_TRAIN = subset of response variable (series).\n",
    "# Alternatively, modify y_train, not Y_TRAIN\n",
    "\n",
    "# Model object returned as ols_linear_reg_model;\n",
    "# Feature ranking dataframe returned as ols_feature_importance_df;\n",
    "# Simply modify these objects on the left of equality:\n",
    "ols_linear_reg_model, ols_feature_importance_df = ols_linear_reg (X_train = X_TRAIN, y_train = Y_TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ridge Linear Regression**\n",
    "- This function runs the 'bar_chart' function. Certify that this function was properly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN = X_train\n",
    "# X_TRAIN = subset of predictive variables (dataframe).\n",
    "# Alternatively, modify X_train, not X_TRAIN\n",
    "Y_TRAIN = y_train\n",
    "# Y_TRAIN = subset of response variable (series).\n",
    "# Alternatively, modify y_train, not Y_TRAIN\n",
    "\n",
    "ALPHA_HYPERPARAMETER = 1.0\n",
    "MAXIMUM_OF_ALLOWED_ITERATIONS = 20000\n",
    "# hyperparameters: alpha = ALPHA_HYPERPARAMETER and MAXIMUM_OF_ALLOWED_ITERATIONS = max_iter\n",
    "\n",
    "# MAXIMUM_OF_ALLOWED_ITERATIONS = integer representing the maximum number of iterations\n",
    "# that the optimization algorithm can perform. Depending on data, convergence may not be\n",
    "# reached within this limit, so you may need to increase this hyperparameter.\n",
    "\n",
    "# ALPHA_HYPERPARAMETER is the regularization strength and must be a positive float value. \n",
    "# Regularization improves the conditioning of the problem and reduces the variance \n",
    "# of the estimates. Larger values specify stronger regularization.\n",
    "# ALPHA_HYPERPARAMETER = 0 is equivalent to an ordinary least square, solved by the \n",
    "# LinearRegression object. For numerical reasons, using ALPHA_HYPERPARAMETER = 0 \n",
    "# is not advised. Given this, you should use the ols_linear_reg function instead.\n",
    "\n",
    "# Model object returned as ridge_linear_reg_model;\n",
    "# Feature ranking dataframe returned as ridge_feature_importance_df;\n",
    "# Simply modify these objects on the left of equality:\n",
    "ridge_linear_reg_model, ridge_feature_importance_df = ridge_linear_reg (X_train = X_TRAIN, y_train = Y_TRAIN, alpha_hyperparameter = ALPHA_HYPERPARAMETER, maximum_of_allowed_iterations = MAXIMUM_OF_ALLOWED_ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Lasso Linear Regression**\n",
    "- This function runs the 'bar_chart' function. Certify that this function was properly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN = X_train\n",
    "# X_TRAIN = subset of predictive variables (dataframe).\n",
    "# Alternatively, modify X_train, not X_TRAIN\n",
    "Y_TRAIN = y_train\n",
    "# Y_TRAIN = subset of response variable (series).\n",
    "# Alternatively, modify y_train, not Y_TRAIN\n",
    "\n",
    "ALPHA_HYPERPARAMETER = 1.0\n",
    "MAXIMUM_OF_ALLOWED_ITERATIONS = 20000\n",
    "# hyperparameters: alpha = ALPHA_HYPERPARAMETER and MAXIMUM_OF_ALLOWED_ITERATIONS = max_iter\n",
    "\n",
    "# MAXIMUM_OF_ALLOWED_ITERATIONS = integer representing the maximum number of iterations\n",
    "# that the optimization algorithm can perform. Depending on data, convergence may not be\n",
    "# reached within this limit, so you may need to increase this hyperparameter.\n",
    "\n",
    "# ALPHA_HYPERPARAMETER is the regularization strength and must be a positive float value. \n",
    "# Regularization improves the conditioning of the problem and reduces the variance \n",
    "# of the estimates. Larger values specify stronger regularization.\n",
    "# ALPHA_HYPERPARAMETER = 0 is equivalent to an ordinary least square, solved by the \n",
    "# LinearRegression object. For numerical reasons, using ALPHA_HYPERPARAMETER = 0 \n",
    "# is not advised. Given this, you should use the ols_linear_reg function instead.\n",
    "\n",
    "# Model object returned as lasso_linear_reg_model;\n",
    "# Feature ranking dataframe returned as lasso_feature_importance_df;\n",
    "# Simply modify these objects on the left of equality:\n",
    "lasso_linear_reg_model, lasso_feature_importance_df = lasso_linear_reg (X_train = X_TRAIN, y_train = Y_TRAIN, alpha_hyperparameter = ALPHA_HYPERPARAMETER, maximum_of_allowed_iterations = MAXIMUM_OF_ALLOWED_ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Elastic Net Linear Regression**\n",
    "- This function runs the 'bar_chart' function. Certify that this function was properly loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN = X_train\n",
    "# X_TRAIN = subset of predictive variables (dataframe).\n",
    "# Alternatively, modify X_train, not X_TRAIN\n",
    "Y_TRAIN = y_train\n",
    "# Y_TRAIN = subset of response variable (series).\n",
    "# Alternatively, modify y_train, not Y_TRAIN\n",
    "\n",
    "ALPHA_HYPERPARAMETER = 1.0\n",
    "MAXIMUM_OF_ALLOWED_ITERATIONS = 20000\n",
    "L1_RATIO_HYPERPARAMETER = 0.5\n",
    "# hyperparameters: alpha = ALPHA_HYPERPARAMETER; MAXIMUM_OF_ALLOWED_ITERATIONS = max_iter\n",
    "# and L1_RATIO_HYPERPARAMETER = l1_ratio\n",
    "\n",
    "# MAXIMUM_OF_ALLOWED_ITERATIONS = integer representing the maximum number of iterations\n",
    "# that the optimization algorithm can perform. Depending on data, convergence may not be\n",
    "# reached within this limit, so you may need to increase this hyperparameter.\n",
    "\n",
    "# ALPHA_HYPERPARAMETER is the regularization strength and must be a positive float value. \n",
    "# Regularization improves the conditioning of the problem and reduces the variance \n",
    "# of the estimates. Larger values specify stronger regularization.\n",
    "\n",
    "# L1_RATIO_HYPERPARAMETER is The ElasticNet mixing parameter (float), with 0 <= l1_ratio <= 1. \n",
    "# For L1_RATIO_HYPERPARAMETER = 0 the penalty is an L2 penalty. For l1_ratio = 1 it is an L1 penalty. \n",
    "# For 0 < L1_RATIO_HYPERPARAMETER < 1, the penalty is a combination of L1 and L2.\n",
    "\n",
    "# ALPHA_HYPERPARAMETER = 0 and L1_RATIO_HYPERPARAMETER = 0 is equivalent to an ordinary \n",
    "# least square, solved by the LinearRegression object. For numerical reasons, \n",
    "# using ALPHA_HYPERPARAMETER = 0 and L1_RATIO_HYPERPARAMETER = 0 is not advised. \n",
    "# Given this, you should use the ols_linear_reg function instead.\n",
    "\n",
    "# Model object returned as elastic_net_linear_reg_model;\n",
    "# Feature ranking dataframe returned as elastic_net_feature_importance_df;\n",
    "# Simply modify these objects on the left of equality:\n",
    "elastic_net_linear_reg_model, elastic_net_feature_importance_df = elastic_net_linear_reg (X_train = X_TRAIN, y_train = Y_TRAIN, alpha_hyperparameter = ALPHA_HYPERPARAMETER, l1_ratio_hyperparameter = L1_RATIO_HYPERPARAMETER, maximum_of_allowed_iterations = MAXIMUM_OF_ALLOWED_ITERATIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Getting a general feature ranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICTIONARY_OF_FEATURE_RANKINGS_DATAFRAMES = {\n",
    "    \n",
    "    'ols_linear_reg': ols_feature_importance_df,\n",
    "    'ridge_linear_reg': ridge_feature_importance_df,\n",
    "    'lasso_linear_reg': lasso_feature_importance_df,\n",
    "    'elastic_net_linear_reg': elastic_net_feature_importance_df\n",
    "    \n",
    "}\n",
    "# DICTIONARY_OF_FEATURE_RANKINGS_DATAFRAMES\n",
    "# The key of this dictionary must be the model name or the name of the ranking.\n",
    "# This key will be used to identify the column on the new dataframe (it will be\n",
    "# used as suffix). The correspondent value must be the feature importance ranking\n",
    "# dataframe, with configuration similar to reg_dict: it must have a column\n",
    "# named, 'predictive_features', which will be used as key for merging.\n",
    "# For instance, for a dictionary = {'ols_linear_regression': ols_feature_df,\n",
    "# 'ridge_linear_regression': ridge_feature_df}, the columns 'regression_coefficients'\n",
    "# will be identified as: 'regression_coefficients_ols_linear_regression' and\n",
    "# 'regression_coefficients_ridge_linear_regression'. Notice that the underscore (\"_\")\n",
    "# is used as suffix separator.\n",
    "\n",
    "ELIMINATE_NON_CORRESPONDENCE = False\n",
    "# ELIMINATE_NON_CORRESPONDENCE = False. Since the dataframes will be merged using an\n",
    "# \"outer\" join, all entries from all dataframes will be added, possibly resulting in\n",
    "# missing values (pandas \"outer\" is a full outer join). \n",
    "# Then, set ELIMINATE_NON_CORRESPONDENCE = True to eliminate all missing values\n",
    "# (rows with entries without correspondence).\n",
    "LIMIT_OF_RANKED_FEATURES = None\n",
    "# LIMIT_OF_RANKED_FEATURES = None. Alternatively, set as an integer to limit the number\n",
    "# of ranked features. e.g. LIMIT_OF_RANKED_FEATURES = 20 will return a dataset with only\n",
    "# 20 features. Notice that the features are sorted in accordance to their order in the\n",
    "# input dictionary. Then, the most important ranking will be the one from the first dataframe.\n",
    "\n",
    "# General feature ranking dataframe returned as general_feature_ranking_df;\n",
    "# Simply modify this object on the left of equality:\n",
    "general_feature_ranking_df = (dictionary_of_feature_rankings_dataframes = DICTIONARY_OF_FEATURE_RANKINGS_DATAFRAMES, eliminate_non_correspondence = ELIMINATE_NON_CORRESPONDENCE, limit_of_ranked_features = LIMIT_OF_RANKED_FEATURES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculating metrics for regression models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OBJECT = ols_linear_reg_model # Alternatively: object storing another model\n",
    "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
    "# MODEL_OBJECT = elastic_net_linear_reg_model\n",
    "\n",
    "X_TRAIN = X_train\n",
    "# X_TRAIN = subset of predictive variables (dataframe).\n",
    "# Alternatively, modify X_train, not X_TRAIN\n",
    "Y_TRAIN = y_train\n",
    "# Y_TRAIN = subset of response variable (series).\n",
    "# Alternatively, modify y_train, not Y_TRAIN\n",
    "\n",
    "X_TEST = None\n",
    "# X_TEST = subset of predictive variables (test dataframe, in case the original one was \n",
    "# split into train and test). Alternatively, modify X_test (or None), not X_TEST\n",
    "# e.g. X_TEST = X_test\n",
    "Y_TEST = None\n",
    "# Y_TEST = subset of response variable (test series, in case the original one was \n",
    "# split into train and test). Alternatively, modify y_test (or None), not Y_TEST\n",
    "# e.g. Y_TEST = y_test\n",
    "\n",
    "# Dictionary containing calculated metrics returned as metrics_dict;\n",
    "# Simply modify this object on the left of equality:\n",
    "metrics_dict = regression_models_metrics (model_object = MODEL_OBJECT, X_train = X_TRAIN, y_train = Y_TRAIN, X_test = X_TEST, y_test = Y_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Making predictions with the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OBJECT = ols_linear_reg_model # Alternatively: object storing another model\n",
    "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
    "# MODEL_OBJECT = elastic_net_linear_reg_model\n",
    "\n",
    "PREDICT_FOR = 'subset'\n",
    "# Alternatively: PREDICT_FOR = 'subset' or PREDICT_FOR = 'single_entry'\n",
    "\n",
    "X_df = X\n",
    "# X_df = subset of predictive variables (dataframe). Alternatively, modify X, not X_df\n",
    "# If PREDICT_FOR = 'single_entry', X should be a list of parameters values.\n",
    "# e.g. X = [1.2, 3, 4] (dot is the decimal case separator, comma separate values). \n",
    "# Notice that the list should contain only the numeric values, in the same order of the\n",
    "# correspondent columns.\n",
    "# If PREDICT_FOR = 'subset' (prediction for multiple entries), X should be a dataframe \n",
    "# (subset) of the parameters values, as usual.\n",
    "\n",
    "CONCATENATE_PREDICTIONS_WITH_X = True\n",
    "# CONCATENATE_PREDICTIONS_WITH_X = True to concatenate the predicted values with X. \n",
    "# For a single entry, it will be appended as an element from the list. For multiple entries, \n",
    "# it will be added as a column.\n",
    "# If CONCATENATE_PREDICTIONS_WITH_X = False, the prediction will be returned as a single variable value\n",
    "# (single entry), or as a series (prediction for a subset).\n",
    "\n",
    "COLUMN_WITH_PREDICTIONS_SUFFIX = None\n",
    "# COLUMN_WITH_PREDICTIONS_SUFFIX = None. If you want to add a new column, you can declare this\n",
    "# parameter as string with a suffix for identifying the model. If no suffix is added, the new\n",
    "# column will be named 'y_pred'.\n",
    "# e.g. COLUMN_WITH_PREDICTIONS_SUFFIX = '_keras' will create a column named \"y_pred_keras\". This\n",
    "# parameter is useful when working with multiple models. Always start the suffix with underscore\n",
    "# \"_\" so that no blank spaces are added; the suffix will not be merged to the column; and there\n",
    "# will be no confusion with the dot (.) notation for methods, JSON attributes, etc.\n",
    "\n",
    "# Prediction (single float value or series of values if CONCATENATE_PREDICTIONS_WITH_X = False);\n",
    "# (list appended with the prediction or subset with a new column with predictions if \n",
    "# CONCATENATE_PREDICTIONS_WITH_X = True) returned as prediction_output\n",
    "# Simply modify this object (or variable) on the left of equality:\n",
    "prediction_output = make_model_predictions (model_object = MODEL_OBJECT, X = X_df, predict_for = PREDICT_FOR, concatenate_predictions_with_X = CONCATENATE_PREDICTIONS_WITH_X, col_with_predictions_suffix = COLUMN_WITH_PREDICTIONS_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing or exporting models and dictionaries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: import only a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_FILE_NAME = None\n",
    "# DICTIONARY_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no dictionary will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = '/'\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"/\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'sklearn'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning keras/ tensorflow models with extension .h5\n",
    "# MODEL_TYPE = 'sklearn' for models from scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb' for XGBoost models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "#Model object saved as model.\n",
    "# Simply modify this object on the left of equality:\n",
    "model = import_export_model_or_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_file_name = DICTIONARY_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_to_export = DICT_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2: import only a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'dict_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_FILE_NAME = None\n",
    "# DICTIONARY_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no dictionary will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = '/'\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"/\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'sklearn'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning keras/ tensorflow models with extension .h5\n",
    "# MODEL_TYPE = 'sklearn' for models from scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb' for XGBoost models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Dictionary saved as imported_dict.\n",
    "# Simply modify this object on the left of equality:\n",
    "imported_dict = import_export_model_or_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_file_name = DICTIONARY_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_to_export = DICT_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 3: import a model and a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_and_dict'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_FILE_NAME = None\n",
    "# DICTIONARY_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no dictionary will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = '/'\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"/\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'sklearn'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning keras/ tensorflow models with extension .h5\n",
    "# MODEL_TYPE = 'sklearn' for models from scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb' for XGBoost models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model. Dictionary saved as imported_dict.\n",
    "# Simply modify these objects on the left of equality:\n",
    "model, imported_dict = import_export_model_or_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_file_name = DICTIONARY_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_to_export = DICT_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 4: export a model and/or a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'export'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_FILE_NAME = None\n",
    "# DICTIONARY_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no dictionary will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = '/'\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"/\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'sklearn'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning keras/ tensorflow models with extension .h5\n",
    "# MODEL_TYPE = 'sklearn' for models from scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb' for XGBoost models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "import_export_model_or_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_file_name = DICTIONARY_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_to_export = DICT_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exporting the dataframe as CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING: all file extensions should be .csv for this function\n",
    "\n",
    "DATAFRAME_TO_BE_EXPORTED = dataset\n",
    "#Alternatively: object containing the dataset to be exported.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"/\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"/\" \n",
    "# or FILE_DIRECTORY_PATH = \"/folder\"\n",
    "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "NEW_FILE_NAME_WITH_CSV_EXTENSION = \"dataset.csv\"\n",
    "# NEW_FILE_NAME_WITH_CSV_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_CSV_EXTENSION = \"file.csv\"\n",
    "\n",
    "EXPORT_TO_S3_BUCKET = False\n",
    "# export_to_s3_bucket = False. Alternatively, set as True to export the file to an\n",
    "# AWS S3 Bucket.\n",
    "    \n",
    "## The following parameters have effect only when export_to_s3_bucket == True:\n",
    "\n",
    "S3_BUCKET_NAME = None    \n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. S3_BUCKET_NAME = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "DESIRED_S3_FILE_NAME_WITH_CSV_EXTENSION = None\n",
    "# The name desired for the object stored in S3 (string, in quotes). \n",
    "# Keep it None to set it equals to NEW_FILE_NAME_WITH_CSV_EXTENSION. \n",
    "# Alternatively, set it as a string analogous to NEW_FILE_NAME_WITH_CSV_EXTENSION.\n",
    "# e.g. DESIRED_S3_FILE_NAME_WITH_CSV_EXTENSION = \"S3_file.csv\"\n",
    "\n",
    "export_dataframe(dataframe_to_be_exported = DATAFRAME_TO_BE_EXPORTED, new_file_name_with_csv_extension = NEW_FILE_NAME_WITH_CSV_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH, export_to_s3_bucket = EXPORT_TO_S3_BUCKET, s3_bucket_name = S3_BUCKET_NAME, desired_s3_file_name_with_csv_extension = DESIRED_S3_FILE_NAME_WITH_CSV_EXTENSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Downloading a file from Google Colab or AWS S3 to the local machine or uploading a file from the machine to S3 or to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = 'aws'\n",
    "# SOURCE = 'google' for downloading from (or uploading to) Google Colab's instant memory;\n",
    "# SOURCE = 'aws' for downloading from (or uploading to) an AWS S3 bucket.\n",
    "\n",
    "ACTION = 'download'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to AWS S3 or to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "OBJECT_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# OBJECT_TO_DOWNLOAD_FROM_COLAB = None. This option has effect only when\n",
    "# SOURCE == 'google'. In this case, this parameter is obbligatory. \n",
    "# Declare as OBJECT_TO_DOWNLOAD_FROM_COLAB the object that you want to download.\n",
    "# Since it is an object and not a string, it should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, OBJECT_TO_DOWNLOAD_FROM_COLAB = dict.\n",
    "# To download a dataframe named df, declare OBJECT_TO_DOWNLOAD_FROM_COLAB = df.\n",
    "# To export a model named keras_model, declare OBJECT_TO_DOWNLOAD_FROM_COLAB = keras_model\n",
    "    \n",
    "## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN source == 'aws'\n",
    "\n",
    "S3_BUCKET_NAME = None\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. S3_BUCKET_NAME = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "LOCAL_PATH_OF_STORAGE = '/'\n",
    "# LOCAL_PATH_OF_STORAGE: path of the local computer environment \n",
    "# to which the S3 bucket contents will be downloaded (ACTION == 'download'); or\n",
    "# path of the folder containing the file that will be uploaded in S3 (ACTION = 'upload'). \n",
    "# If it is None, or if LOCAL_PATH_OF_STORAGE = '/', files \n",
    "# will be imported to the root path. Alternatively, input the path as a string (in quotes). \n",
    "# Examples: LOCAL_PATH_OF_STORAGE = '/copied_s3_bucket'; \n",
    "# LOCAL_PATH_OF_STORAGE = \"/My_folder\"; LOCAL_PATH_OF_STORAGE = \"/Users/Me/Documents/\"\n",
    "# Notice that only the directories should be declared: do not include the file name and\n",
    "# its extension.\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = None\n",
    "# FILE_NAME_WITH_EXTENSION: string, in quotes, containing the file name which will be\n",
    "# downloaded from S3; or uploaded from S3, followed by its extension. \n",
    "## This parameter is obbligatory when SOURCE == 'aws'\n",
    "# Examples:\n",
    "# FILE_NAME_WITH_EXTENSION = 'Screen_Shot.png'; FILE_NAME_WITH_EXTENSION = 'dataset.csv',\n",
    "# FILE_NAME_WITH_EXTENSION = \"dictionary.pkl\", FILE_NAME_WITH_EXTENSION = \"model.h5\",\n",
    "# FILE_NAME_WITH_EXTENSION = 'doc.pdf', FILE_NAME_WITH_EXTENSION = 'model.dill'\n",
    "\n",
    "download_or_upload_file (source = SOURCE, action = ACTION, object_to_download_from_colab = OBJECT_TO_DOWNLOAD_FROM_COLAB, s3_bucket_name = S3_BUCKET_NAME, local_path_of_storage = LOCAL_PATH_OF_STORAGE, file_name_with_extension = FILE_NAME_WITH_EXTENSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plotting a bar chart**\n",
    "- Bars may be vertically or horizontally oriented.\n",
    "- Bar charts are plotted after selecting an aggregation function, and the cumulative percent curve may be obtained and plotted with the bars (in secondary axis).\n",
    "- To obtain a Pareto chart, keep aggregate_function = 'sum', plot_cumulative_percent = True, and orientation = 'vertical'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "CATEGORICAL_VAR_NAME = 'categorical_column_name'\n",
    "# CATEGORICAL_VAR_NAME: string (inside quotes) containing the name \n",
    "# of the column to be analyzed. e.g. \n",
    "# CATEGORICAL_VAR_NAME = \"column1\"\n",
    "\n",
    "RESPONSE_VAR_NAME = \"response_column_name\"\n",
    "# RESPONSE_VAR_NAME: string (inside quotes) containing the name \n",
    "# of the column that stores the response correspondent to the\n",
    "# categories. e.g. RESPONSE_VAR_NAME = \"response_feature\"\n",
    "\n",
    "AGGREGATE_FUNCTION = 'sum'\n",
    "# AGGREGATE_FUNCTION = 'sum': String defining the aggregation \n",
    "# method that will be applied. Possible values:\n",
    "# 'median', 'mean', 'mode', 'sum', 'min', 'max', 'variance',\n",
    "# 'standard_deviation','10_percent_quantile', '20_percent_quantile',\n",
    "# '25_percent_quantile', '30_percent_quantile', '40_percent_quantile',\n",
    "# '50_percent_quantile', '60_percent_quantile', '70_percent_quantile',\n",
    "# '75_percent_quantile', '80_percent_quantile', '90_percent_quantile',\n",
    "# and '95_percent_quantile'.\n",
    "# To use another aggregate function, the method must be added to the\n",
    "# dictionary of methods agg_methods_dict, defined in the function.\n",
    "# If None or an invalid function is input, 'sum' will be used.\n",
    "\n",
    "ADD_SUFFIX_TO_AGGREGATED_COL = True\n",
    "# ADD_SUFFIX_TO_AGGREGATED_COL = True will add a suffix to the\n",
    "# aggregated column. e.g. 'responseVar_mean'. If ADD_SUFFIX_TO_AGGREGATED_COL\n",
    "# = False, the aggregated column will have the original column name.\n",
    "SUFFIX = None\n",
    "# suffix = None. Keep it None if no suffix should be added, or if\n",
    "# the name of the aggregate function should be used as suffix, after\n",
    "# \"_\". Alternatively, set it as a string. As recommendation, put the\n",
    "# \"_\" sign in the beginning of this string to separate the suffix from\n",
    "# the original column name. e.g. if the response variable is 'Y' and\n",
    "# suffix = '_agg', the new aggregated column will be named as 'Y_agg'\n",
    "CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = True\n",
    "# CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = True to calculate and plot\n",
    "# the line of cumulative percent, or \n",
    "# CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = False to omit it.\n",
    "# This feature is only shown when AGGREGATE_FUNCTION = 'sum', 'median',\n",
    "# 'mean', or 'mode'. So, it will be automatically set as False if \n",
    "# another aggregate is selected.\n",
    "ORIENTATION = 'vertical'\n",
    "# ORIENTATION = 'vertical' is the standard, and plots vertical bars\n",
    "# (perpendicular to the X axis). In this case, the categories are shown\n",
    "# in the X axis, and the correspondent responses are in Y axis.\n",
    "# Alternatively, ORIENTATION = 'horizontal' results in horizontal bars.\n",
    "# In this case, categories are in Y axis, and responses in X axis.\n",
    "# If None or invalid values are provided, orientation is set as 'vertical'.\n",
    "LIMIT_OF_PLOTTED_CATEGORIES = None\n",
    "# LIMIT_OF_PLOTTED_CATEGORIES: integer value that represents\n",
    "# the maximum of categories that will be plot. Keep it None to plot\n",
    "# all categories. Alternatively, set an integer value. e.g.: if\n",
    "# LIMIT_OF_PLOTTED_CATEGORIES = 4, but there are more categories,\n",
    "# the dataset will be sorted in descending order and: 1) The remaining\n",
    "# categories will be sum in a new category named 'others' if the\n",
    "# aggregate function is 'sum'; 2) Or the other categories will be simply\n",
    "# omitted from the plot, for other aggregate functions. Notice that\n",
    "# it limits only the variables in the plot: all of them will be\n",
    "# returned in the dataframe.\n",
    "# Use this parameter to obtain a cleaner plot. Notice that the remaining\n",
    "# columns will be aggregated as 'others' even if there is a single column\n",
    "# beyond the limit.\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'scatter_plot_lin_reg.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "# New dataframe saved as aggregated_sorted_df. \n",
    "# Simply modify this object on the left of equality:\n",
    "aggregated_sorted_df = bar_chart (df = DATASET, categorical_var_name = CATEGORICAL_VAR_NAME, response_var_name = RESPONSE_VAR_NAME, aggregate_function = AGGREGATE_FUNCTION, add_suffix_to_aggregated_col = ADD_SUFFIX_TO_AGGREGATED_COL, suffix = SUFFIX, calculate_and_plot_cumulative_percent = CALCULATE_AND_PLOT_CUMULATIVE_PERCENT, orientation = ORIENTATION, limit_of_plotted_categories = LIMIT_OF_PLOTTED_CATEGORIES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1"
   },
   "source": [
    "****"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
