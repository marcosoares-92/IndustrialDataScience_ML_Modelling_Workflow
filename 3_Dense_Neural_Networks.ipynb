{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "instance_type": "ml.t3.medium",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcosoares-92/IndustrialDataScience_ML_Modelling_Workflow/blob/main/3_Dense_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dense Artificial Neural Networks**"
      ],
      "metadata": {
        "azdata_cell_guid": "5edcdce4-06a8-4dc6-947d-20eee44ae7ba",
        "id": "xGHmgXrigC_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _Machine Learning Modelling Workflow Notebook 3_"
      ],
      "metadata": {
        "azdata_cell_guid": "d0c4fe20-ff25-4875-8113-3c16a46bc83f",
        "id": "13yemc0WgC_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content:\n",
        "1. Splitting the dataframe into train and test subsets;\n",
        "2. Retrieving the list of classes used for training the classification models;\n",
        "3. Scikit-learn Multi-Layer Perceptron;\n",
        "4. Calculating metrics for regression models;\n",
        "5. Calculating metrics for classification models;\n",
        "6. Making predictions with the models;\n",
        "7. Calculating probabilities associated to each class;\n",
        "8. Performing the SHAP feature importance analysis;\n",
        "9. Time series visualization."
      ],
      "metadata": {
        "azdata_cell_guid": "1721cd2c-7d13-49f3-a74e-0faf52df7b5c",
        "id": "SUDAo9h2gC_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
        "- marcosoares.feq@gmail.com\n",
        "- marco.soares@bayer.com"
      ],
      "metadata": {
        "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267",
        "id": "OrGHXKUmgC_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Python Libraries in Global Context**"
      ],
      "metadata": {
        "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea",
        "id": "oD2tNreQgC_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import load\n",
        "from idsw import *"
      ],
      "metadata": {
        "azdata_cell_guid": "694895ef-4275-46c6-82ad-a08addf9ed8e",
        "id": "bzZgOvXCyHHl",
        "language": "python",
        "tags": [
          "CELL_4"
        ]
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Call the functions**"
      ],
      "metadata": {
        "azdata_cell_guid": "eee74d56-8e4c-4851-be92-759a8182901c",
        "id": "zHUhoX1XyHHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing the dataset**"
      ],
      "metadata": {
        "azdata_cell_guid": "5fe1661a-23b2-42b6-b996-144eb951fd50",
        "id": "1QWnflW7gC_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt),\n",
        "## JSON, txt, or CSV (comma separated values) files. Tables in webpages or html files can also be read.\n",
        "\n",
        "FILE_DIRECTORY_PATH = \"\"\n",
        "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\"\n",
        "# or FILE_DIRECTORY_PATH = \"folder\"\n",
        "\n",
        "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
        "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the\n",
        "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or,\n",
        "# FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
        "# Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
        "# Also, html files and webpages may be also read.\n",
        "\n",
        "# You may input the path for an HTML file containing a table to be read; or\n",
        "# a string containing the address for a webpage containing the table. The address must start\n",
        "# with www or htpp. If a website is input, the full address can be input as FILE_DIRECTORY_PATH\n",
        "# or as FILE_NAME_WITH_EXTENSION.\n",
        "\n",
        "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
        "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True\n",
        "# if you want to read a file with txt extension containing a text formatted as JSON\n",
        "# (but not saved as JSON).\n",
        "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the\n",
        "# function (below) must be set. If not, an error message will be raised.\n",
        "\n",
        "HOW_MISSING_VALUES_ARE_REGISTERED = None\n",
        "# HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
        "# empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
        "# This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
        "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’,\n",
        "#‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’,\n",
        "# ‘n/a’, ‘nan’, ‘null’.\n",
        "\n",
        "# If a different denomination is used, indicate it as a string. e.g.\n",
        "# HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
        "# HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
        "\n",
        "# If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
        "# only in column 'numeric_col', you can specify the following dictionary:\n",
        "# how_missing_values_are_registered = {'numeric-col': 0}\n",
        "\n",
        "\n",
        "HAS_HEADER = True\n",
        "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
        "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
        "\n",
        "DECIMAL_SEPARATOR = '.'\n",
        "# DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
        "# the decimal separator. Alternatively, specify here the separator.\n",
        "# e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
        "# It manipulates the argument 'decimal' from Pandas functions.\n",
        "\n",
        "TXT_CSV_COL_SEP = \"comma\"\n",
        "# txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
        "# or 'csv'. It informs how the different columns are separated.\n",
        "# Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\"\n",
        "# for columns separated by comma;\n",
        "# txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \"\n",
        "# for columns separated by simple spaces.\n",
        "# You can also set a specific separator as string. For example:\n",
        "# txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
        "# is used as separator for the columns - '\\t' represents the tab character).\n",
        "\n",
        "## Parameters for loading Excel files:\n",
        "\n",
        "LOAD_ALL_SHEETS_AT_ONCE = False\n",
        "# LOAD_ALL_SHEETS_AT_ONCE = False - This parameter has effect only when for Excel files.\n",
        "# If LOAD_ALL_SHEETS_AT_ONCE = True, the function will return a list of dictionaries, each\n",
        "# dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
        "# value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
        "# and its value will be the pandas dataframe object obtained from that sheet.\n",
        "# This argument has preference over SHEET_TO_LOAD. If it is True, all sheets will be loaded.\n",
        "\n",
        "SHEET_TO_LOAD = None\n",
        "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
        "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
        "# will be loaded.\n",
        "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
        "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
        "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
        "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
        "# name to load the sheet with that name.\n",
        "\n",
        "## Parameters for loading JSON files:\n",
        "\n",
        "JSON_RECORD_PATH = None\n",
        "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
        "# Path in each object to list of records. If not passed, data will be assumed to\n",
        "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
        "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
        "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
        "\n",
        "JSON_FIELD_SEPARATOR = \"_\"\n",
        "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
        "# Nested records will generate names separated by sep.\n",
        "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
        "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
        "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
        "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
        "\n",
        "JSON_METADATA_PREFIX_LIST = None\n",
        "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter\n",
        "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting\n",
        "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
        "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
        "\n",
        "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
        "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
        "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
        "# are 'name' and 'last'.\n",
        "# Then, JSON_RECORD_PATH = 'books'\n",
        "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
        "\n",
        "\n",
        "# The dataframe will be stored in the object named 'dataset':\n",
        "# Simply modify this object on the left of equality:\n",
        "dataset = load_pandas_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, how_missing_values_are_registered = HOW_MISSING_VALUES_ARE_REGISTERED, has_header = HAS_HEADER, decimal_separator = DECIMAL_SEPARATOR, txt_csv_col_sep = TXT_CSV_COL_SEP, load_all_sheets_at_once = LOAD_ALL_SHEETS_AT_ONCE, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)\n",
        "\n",
        "# OBS: If an Excel file is loaded and LOAD_ALL_SHEETS_AT_ONCE = True, then the object\n",
        "# dataset will be a list of dictionaries, with 'sheet' as key containing the sheet name; and 'df'\n",
        "# as key correspondent to the Pandas dataframe. So, to access the 3rd dataframe (index 2, since\n",
        "# indexing starts from zero): df = dataframe[2]['df'], where dataframe is the list returned."
      ],
      "metadata": {
        "azdata_cell_guid": "94eec662-5468-46fe-8098-501725b687ec",
        "language": "python",
        "id": "wIC4G576gC_s"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Separating and preparing features and responses tensors**"
      ],
      "metadata": {
        "azdata_cell_guid": "fe6ecfda-9620-47c9-91b5-f68c298c159d",
        "id": "r7eq3riFgC_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = dataset  #Alternatively: object containing the dataset to be analyzed\n",
        "\n",
        "FEATURES_COLUMNS = ['col1', 'col2']\n",
        "# FEATURES_COLUMNS: list of strings or string containing the names of columns\n",
        "# with predictive variables in the original dataframe.\n",
        "# Example: FEATURES_COLUMNS = ['col1', 'col2']; FEATURES_COLUMNS = 'predictor';\n",
        "# FEATURES_COLUMNS = ['predictor'].\n",
        "\n",
        "RESPONSE_COLUMNS = \"response\"\n",
        "# RESPONSE_COLUMNS: list of strings or string containing the names of columns\n",
        "# with response variables in the original dataframe.\n",
        "# Example: RESPONSE_COLUMNS= ['col3', 'col4']; RESPONSE_COLUMNS = 'response';\n",
        "# RESPONSE_COLUMNS = ['response']\n",
        "\n",
        "# Arrays or tensors containing features and responses returned as X and y, respectively.\n",
        "# Mapping dictionary correlating the position in array or tensor to the original column name\n",
        "# returned as column_map_dict.\n",
        "# Simply modify these objects on the left of equality:\n",
        "X, y, column_map_dict = separate_and_prepare_features_and_responses (df = DATASET, features_columns = FEATURES_COLUMNS, response_columns = RESPONSE_COLUMNS)"
      ],
      "metadata": {
        "azdata_cell_guid": "302dcfa9-30d9-436d-b97c-fa31972336fb",
        "language": "python",
        "id": "Q6L5BCl4gC_v"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Converting a whole dataframe or array-like object to tensor**"
      ],
      "metadata": {
        "azdata_cell_guid": "b0ec8b20-ce3e-4f36-8ab7-24730702e392",
        "id": "y2X6oN68gC_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_OR_ARRAY_TO_CONVERT = dataset\n",
        "# Alternatively: object containing the dataset or array-like object to be converted and reshaped.\n",
        "\n",
        "COLUMNS_TO_CONVERT = None\n",
        "# ATTENTION: This argument only works for Pandas dataframes.\n",
        "# COLUMNS_TO_CONVERT: list of strings or string containing the names of columns\n",
        "# that you want to convert. Use this if you want to convert only a subset of the dataframe.\n",
        "# Example: COLUMNS_TO_CONVERT = ['col1', 'col2']; COLUMNS_TO_CONVERT = 'predictor';\n",
        "# COLUMNS_TO_CONVERT = ['predictor'] will create a tensor with only the specified columns;\n",
        "# If None, the whole dataframe will be converted.\n",
        "\n",
        "COLUMNS_TO_EXCLUDE = None\n",
        "# ATTENTION: This argument only works for Pandas dataframes.\n",
        "# COLUMNS_TO_EXCLUDE: Alternative parameter.\n",
        "# list of strings or string containing the names of columns that you want to exclude from the\n",
        "# returned tensor. Use this if you want to convert only a subset of the dataframe.\n",
        "# Example: COLUMNS_TO_EXCLUDE = ['col1', 'col2']; COLUMNS_TO_EXCLUDE = 'predictor';\n",
        "# COLUMNS_TO_EXCLUDE = ['predictor'] will create a tensor with all columns from the dataframe\n",
        "# except the specified ones. This argument will only be used if the previous one was not.\n",
        "\n",
        "\n",
        "# Array or tensor returned as X. Mapping dictionary correlating the position in array or tensor\n",
        "# to the original column name returned as column_map_dict.\n",
        "# Simply modify these objects on the left of equality:\n",
        "X, column_map_dict = convert_to_tensor (df_or_array_to_convert = DATASET_OR_ARRAY_TO_CONVERT, columns_to_convert = COLUMNS_TO_CONVERT, columns_to_exclude = COLUMNS_TO_EXCLUDE)"
      ],
      "metadata": {
        "azdata_cell_guid": "40232256-4dc5-4eca-935f-9bdc47c127e9",
        "language": "python",
        "id": "Q4A1Zh1NgC_w"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Splitting features and responses into train and test tensors**"
      ],
      "metadata": {
        "azdata_cell_guid": "b975176a-0e1d-47cc-9151-456955980d1f",
        "id": "rg-sNg72gC_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_tensor = X\n",
        "# X_df = tensor or array of predictive variables. Alternatively, modify X, not X_tensor.\n",
        "Y_tensor = y\n",
        "# Y = tensor or array of response variables. Alternatively, modify y, not Y_tensor.\n",
        "\n",
        "PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 75\n",
        "# percent_of_data_used_for_model_training: float from 0 to 100,\n",
        "# representing the percent of data used for training the model\n",
        "\n",
        "PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 0\n",
        "# If you want to use cross-validation, separate a percent of the training data for validation.\n",
        "# Declare this percent as PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION (float from 0 to 100).\n",
        "\n",
        "# Subset and series destined to training, testing and/or validation returned in the dictionary split_dictionary;\n",
        "# Simply modify this object on the left of equality:\n",
        "split_dictionary = split_data_into_train_and_test (X = X_tensor, y = Y_tensor, percent_of_data_used_for_model_training = PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING, percent_of_training_data_used_for_model_validation = PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION)"
      ],
      "metadata": {
        "azdata_cell_guid": "57e8b5f4-453d-4342-a584-82b24f335f2b",
        "language": "python",
        "id": "-m4NRqpYgC_x"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Splitting time series into train and test tensors**"
      ],
      "metadata": {
        "azdata_cell_guid": "ae327068-9685-495f-97ef-aebd96777b74",
        "id": "0hxPUaj1gC_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_tensor = X\n",
        "# X_df = tensor or array of predictive variables. Alternatively, modify X, not X_tensor.\n",
        "Y_tensor = y\n",
        "# Y = tensor or array of response variables. Alternatively, modify y, not Y_tensor.\n",
        "\n",
        "PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 75\n",
        "# percent_of_data_used_for_model_training: float from 0 to 100,\n",
        "# representing the percent of data used for training the model\n",
        "\n",
        "PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 0\n",
        "# If you want to use cross-validation, separate a percent of the training data for validation.\n",
        "# Declare this percent as PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION (float from 0 to 100).\n",
        "\n",
        "# Subset and series destined to training, testing and/or validation returned in the dictionary split_dictionary;\n",
        "# Simply modify this object on the left of equality:\n",
        "split_dictionary = time_series_train_test_split (X = X_tensor, y = Y_tensor, percent_of_data_used_for_model_training = PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING, percent_of_training_data_used_for_model_validation = PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION)"
      ],
      "metadata": {
        "azdata_cell_guid": "c4375c3f-c21f-4620-8701-ca22951fa55f",
        "language": "python",
        "id": "k4tlajIRgC_y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creating a TensorFlow windowed dataset from a time series**"
      ],
      "metadata": {
        "azdata_cell_guid": "ffb76259-dfef-4409-8491-e961968e8eb6",
        "id": "aMisjb5YgC_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_tensor = y\n",
        "# Y = tensor or array of response variables. Alternatively, modify y, not Y_tensor.\n",
        "\n",
        "WINDOW_SIZE = 20\n",
        "# WINDOW_SIZE (integer): number of rows/ size of the time window used.\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "# BATCH_SIZE (integer): number of rows/ size of the batches used for training.\n",
        "\n",
        "SHUFFLE_BUFFER_SIZE = 100\n",
        "# SHUFFLE_BUFFER_SIZE (integer): number of rows/ size used for shuffling the entries.\n",
        "\n",
        "# TensorFlow Dataset obtained from the time series returned as dataset_from_time_series.\n",
        "# Simply modify this object on the left of equality:\n",
        "dataset_from_time_series = windowed_dataset_from_time_series (y = Y_tensor, window_size = WINDOW_SIZE, batch_size = BATCH_SIZE, shuffle_buffer_size = SHUFFLE_BUFFER_SIZE)"
      ],
      "metadata": {
        "azdata_cell_guid": "c43031a7-4d55-47a8-b18b-31233b5fe688",
        "language": "python",
        "id": "YgkOHBX4gC_y"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creating a TensorFlow windowed dataset from multiple-feature time series**"
      ],
      "metadata": {
        "azdata_cell_guid": "a8327966-b8b8-4be9-993d-fe07b5354310",
        "id": "vJ4PqwWsgC_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = dataset\n",
        "# Alternatively: object containing the Pandas dataframe to be converted and reshaped.\n",
        "\n",
        "RESPONSE_COLUMNS = 'response_variable'\n",
        "# RESPONSE_COLUMNS: string or list of strings with the response columns\n",
        "\n",
        "SEQUENCE_STRIDE = 1\n",
        "SAMPLING_RATE = 1\n",
        "SHIFT = 1\n",
        "# SHIFT, SAMPLING_RATE, and SEQUENCE_STRIDE: integers\n",
        "\n",
        "# The time series may be represented as a sequence of times like: t = 0, t = 1, t = 2, ..., t = N.\n",
        "# When preparing the dataset, we pick a given number of 'times' (indexes), and use them for\n",
        "# predicting a time in the future.\n",
        "# So, the INPUT_WIDTH represents how much times will be used for prediction. If INPUT_WIDTH = 6,\n",
        "# we use 6 values for prediction, e.g., t = 0, t = 1, ..., t = 5 will be a prediction window.\n",
        "# In turns, if INPUT_WIDTH = 3, 3 values are used: t = 0, t = 1, t = 2; if INPUT_WIDTH = N, N\n",
        "# consecutive values will be used: t = 0, t = 1, t = 2, ..., t = N. And so on.\n",
        "# LABEL_WIDTH, in turns, represent how much times will be predicted. If LABEL_WIDTH = 1, a single\n",
        "# value will be predicted. If LABEL_WIDTH = 2, two consecutive values are predicted; if LABEL_WIDTH =\n",
        "# N, N consecutive values are predicted; and so on.\n",
        "\n",
        "# SHIFT represents the offset, i.e., given the input values, which value in the time sequence will\n",
        "# be predicted. So, suppose INPUT_WIDTH = 6 and LABEL_WIDTH = 1\n",
        "# If SHIFT = 1, the label, i.e., the predicted value, will be the first after the sequence used for\n",
        "# prediction. So, if  t = 0, t = 1, ..., t = 5 will be a prediction window and t = 6 will be the\n",
        "# predicted value. Notice that the complete window has a total width = 7: t = 0, ..., t = 7.\n",
        "# If LABEL_WIDTH = 2, then t = 6 and t = 7 will be predicted (total width = 8).\n",
        "# Another example: suppose INPUT_WIDTH = 24. So the predicted window is: t = 0, t = 1, ..., t = 23.\n",
        "# If SHIFT = 24, the 24th element after the prediction sequence will be used as label, i.e., will\n",
        "# be predicted. So, t = 24 is the 1st after the sequence, t = 25 is the second, ... t = 47 is the\n",
        "# 24th after. If label_with = 1, then the sequence t = 0, t = 1, ..., t = 23 will be used for\n",
        "# predicting t = 47. Naturally, the total width of the window = 47 in this case.\n",
        "# Also, notice that the label is used by the model as the response (predicted) variable.\n",
        "\n",
        "# So for a given SHIFT: the sequence of timesteps i, i+1, ... will be used for predicting the\n",
        "# timestep i + SHIFT\n",
        "# If a sequence starts in index i, the next sequence will start from i + SEQUENCE_STRIDE.\n",
        "# The sequence will be formed by timesteps i, i + SAMPLING_RATE, i + 2* SAMPLING_RATE, ...\n",
        "# Example: Consider indices [0, 1, ... 99]. With sequence_length=10, SAMPLING_RATE=2,\n",
        "# SEQUENCE_STRIDE=3, the dataset will yield batches of sequences composed of the following indices:\n",
        "# First sequence:  [0  2  4  6  8 10 12 14 16 18]\n",
        "# Second sequence: [3  5  7  9 11 13 15 17 19 21]\n",
        "# Third sequence:  [6  8 10 12 14 16 18 20 22 24]\n",
        "# ...\n",
        "# Last sequence:   [78 80 82 84 86 88 90 92 94 96]\n",
        "\n",
        "USE_PAST_RESPONSES_FOR_PREDICTION = True\n",
        "# USE_PAST_RESPONSES_FOR_PREDICTION: True if the past responses will be used for predicting their\n",
        "# value in the future; False if you do not want to use them.\n",
        "\n",
        "PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 70\n",
        "# percent_of_data_used_for_model_training: float from 0 to 100,\n",
        "# representing the percent of data used for training the model\n",
        "\n",
        "PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 10\n",
        "# If you want to use cross-validation, separate a percent of the training data for validation.\n",
        "# Declare this percent as PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION (float from 0 to 100).\n",
        "\n",
        "# If PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 70, and\n",
        "# PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 10,\n",
        "# training dataset slice goes from 0 to 0.7 (70%) of the dataset;\n",
        "# testing slicing goes from 0.7 x dataset to ((1 - 0.1) = 0.9) x dataset\n",
        "# validation slicing goes from 0.9 x dataset to the end of the dataset.\n",
        "# Here, consider the time sequence t = 0, t = 1, ... , t = N, for a dataset with length N:\n",
        "# training: from t = 0 to t = (0.7 x N); testing: from t = ((0.7 x N) + 1) to (0.9 x N);\n",
        "# validation: from t = ((0.9 x N) + 1) to N (the fractions 0.7 x N and 0.9 x N are rounded to\n",
        "# the closest integer).\n",
        "\n",
        "\n",
        "# Dictionary with inputs and labels tensors returned as tensors_dict.\n",
        "# Simply modify this object on the left of equality:\n",
        "tensors_dict = multi_columns_time_series_tensors (df = DATASET, response_columns = RESPONSE_COLUMNS, sequence_stride = SEQUENCE_STRIDE, sampling_rate = SAMPLING_RATE, shift = SHIFT, use_past_responses_for_prediction = USE_PAST_RESPONSES_FOR_PREDICTION, percent_of_data_used_for_model_training = PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING, percent_of_training_data_used_for_model_validation = PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION)"
      ],
      "metadata": {
        "azdata_cell_guid": "1a801821-a9e9-43dd-b380-a8aaf47ee3bd",
        "language": "python",
        "id": "DWkU4-CAgC_z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Union of several 1-dimensional tensors (obtained from single columns) into a single tensor\n",
        "- Each 1-dimensional tensor or array becomes a column from the new tensor."
      ],
      "metadata": {
        "azdata_cell_guid": "54605bef-420d-4f53-b7f8-1d5bdf9691d6",
        "id": "063rR2JygC_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LIST_OF_TENSORS_OR_ARRAYS = [tensor1, tensor2]\n",
        "# list of tensors: list containing the 1-dimensional tensors or arrays that the function will union.\n",
        "# the operation will be performed in the order that the tensors are declared.\n",
        "# Substitue tensor1, tensor2, tensor3,... by the tensor objects, in the correct sequence.\n",
        "# If the resulting tensor will contain the responses for a multi-response tensor, declare them in the\n",
        "# orders of the responses (tensor 1 corresponding to response 1, tensor 2 to response 2, etc.)\n",
        "\n",
        "# One-dimensional tensors have shape (X,), where X is the number of elements. Example: a column\n",
        "# of the dataframe with elements 1, 2, 3 in this order may result in an array like array([1, 2, 3])\n",
        "# and a Tensor with shape (3,). With we union it with the tensor from the column with elements\n",
        "# 4, 5, 6, the output will be array([[1,4], [2,5], [3,6]]). Alternatively, this new array could\n",
        "# be converted into a Pandas dataframe where each column would be correspondent to one individual\n",
        "# tensor.\n",
        "\n",
        "# Tensor resulting from the union of multiple single-dimension tensor returned as tensors_union.\n",
        "# Simply modify this object on the left of equality:\n",
        "tensors_union = union_1_dim_tensors (list_of_tensors_or_arrays = LIST_OF_TENSORS_OR_ARRAYS)"
      ],
      "metadata": {
        "azdata_cell_guid": "af3249a7-8a7d-42a0-a2e1-d6fdf3596cc6",
        "language": "python",
        "id": "fMn-JIxmgC_3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Scikit-learn Multi-Layer Perceptron**"
      ],
      "metadata": {
        "azdata_cell_guid": "c2452032-84b3-4cab-99f6-fc3f944917d1",
        "id": "B3RTQunhgC_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split_dictionary.keys() == dict_keys(['X_train', 'y_train', 'X_test', 'y_test', 'X_valid', 'y_valid'])\n",
        "\n",
        "X_TRAIN = split_dictionary['X_train']\n",
        "# X_TRAIN = tensor of predictive variables.\n",
        "# Alternatively, modify X_train, not X_TRAIN\n",
        "Y_TRAIN = split_dictionary['y_train']\n",
        "# Y_TRAIN = tensor of response variables.\n",
        "# Alternatively, modify y_train, not Y_TRAIN\n",
        "\n",
        "TYPE_OF_PROBLEM = \"regression\"\n",
        "# TYPE_OF_PROBLEM = 'regression'; or TYPE_OF_PROBLEM = 'classification'\n",
        "# The default is 'regression', which will be used if no type is\n",
        "# provided.\n",
        "\n",
        "NUMBER_OF_HIDDEN_LAYERS = 1\n",
        "# NUMBER_OF_HIDDEN_LAYERS = 1 - integer with the number of hidden\n",
        "# layers. This number must be higher or equal to 1.\n",
        "\n",
        "NUMBER_OF_NEURONS_PER_HIDDEN_LAYER = 64\n",
        "# NUMBER_OF_NEURONS_PER_HIDDEN_LAYER = 64 - integer containing the\n",
        "# number of neurons in each hidden layer. Even though sklearn.neural_network\n",
        "# accepts different layers sizes, passed as a list of integers where each\n",
        "# value represents the neurons for one layer, this function works with layers\n",
        "# of equal sizes.\n",
        "\n",
        "# Scikit-learn MLPClassifier adjusts the size of the output layer to have\n",
        "# a number of units (neurons) equals to the number of output classes. Then, we\n",
        "# do not have to manually set this parameter.\n",
        "\n",
        "SIZE_OF_TRAINING_BATCH = 200\n",
        "# SIZE_OF_TRAINING_BATCH (integer): amount of data used on each training cycle (epoch).\n",
        "# If we had 20000 data and SIZE_OF_TRAINING_BATCH = 200, then there would be 100\n",
        "# batches (cycles) using 200 data. Training is more efficient when dividing the data into\n",
        "# smaller subsets (batches) of ramdonly chosen data and separately training the model\n",
        "# for each batch (in training cycles called epochs). Also, this helps preventing\n",
        "# overfitting: if we use at once all data, the model is much more prone to overfit\n",
        "# (memorizing effect), selecting non-general features highly specific from the data\n",
        "# for the description of it. Therefore, it will have lower capability of predicting\n",
        "# data for values it already did not observe.\n",
        "# This is the parameter batch_size of most of the algorithms.\n",
        "\n",
        "MAXIMUM_OF_ALLOWED_ITERATIONS = 20000\n",
        "# MAXIMUM_OF_ALLOWED_ITERATIONS = integer representing the maximum number of iterations\n",
        "# that the optimization algorithm can perform. Depending on data, convergence may not be\n",
        "# reached within this limit, so you may need to increase this hyperparameter.\n",
        "\n",
        "# Tensors of data separated for model testing:\n",
        "X_TEST = None\n",
        "Y_TEST = None\n",
        "#X_TEST = split_dictionary['X_test']\n",
        "#Y_TEST = split_dictionary['y_test']\n",
        "\n",
        "# Tensors of data separated for model validation:\n",
        "X_VALID = None\n",
        "Y_VALID = None\n",
        "#X_VALID = split_dictionary['X_valid']\n",
        "#Y_VALID = split_dictionary['y_valid']\n",
        "\n",
        "COLUMN_MAP_DICT = column_map_dict\n",
        "#COLUMN_MAP_DICT = None\n",
        "# COLUMN_MAP_DICT: Mapping dictionary correlating the position in array or tensor to the original\n",
        "# column name.\n",
        "\n",
        "X_AXIS_ROTATION = 0\n",
        "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "Y_AXIS_ROTATION = 0\n",
        "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "GRID = True #Alternatively: True or False\n",
        "# If GRID = False, no grid lines are shown in the graphic.\n",
        "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
        "METRICS_VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for metrics plot title\n",
        "LOSS_VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for loss plot title\n",
        "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", METRICS_VERTICAL_AXIS_TITLE = \"training_metrics\",\n",
        "# LOSS_VERTICAL_AXIS_TITLE = \"training_loss\"\n",
        "EXPORT_PNG = False\n",
        "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
        "# Set EXPORT_PNG = True to export the obtained image.\n",
        "DIRECTORY_TO_SAVE = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\"\n",
        "# or DIRECTORY_TO_SAVE = \"folder\"\n",
        "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
        "# path, DIRECTORY_TO_SAVE = \"\"\n",
        "FILE_NAME = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the name you want for the file without the\n",
        "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png'\n",
        "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
        "# 'feature_importance_ranking.png'\n",
        "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
        "# the file will be overwritten.\n",
        "PNG_RESOLUTION_DPI = 330\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
        "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
        "\n",
        "\n",
        "# Model object returned as mlp_model;\n",
        "# Summary dictionary storing the metrics returned as metrics_dict;\n",
        "# training history object returned as history.\n",
        "# Simply modify these objects on the left of equality:\n",
        "mlp_model, metrics_dict, history = sklearn_ann (X_train = X_TRAIN, y_train = Y_TRAIN, type_of_problem = TYPE_OF_PROBLEM, number_of_hidden_layers = NUMBER_OF_HIDDEN_LAYERS, number_of_neurons_per_hidden_layer = NUMBER_OF_NEURONS_PER_HIDDEN_LAYER, size_of_training_batch = SIZE_OF_TRAINING_BATCH, maximum_of_allowed_iterations = MAXIMUM_OF_ALLOWED_ITERATIONS, X_test = X_TEST, y_test = Y_TEST, X_valid = X_VALID, y_valid = Y_VALID, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, metrics_vertical_axis_title = METRICS_VERTICAL_AXIS_TITLE, loss_vertical_axis_title = LOSS_VERTICAL_AXIS_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
      ],
      "metadata": {
        "azdata_cell_guid": "c0d29add-d3b2-494c-8ad4-c7e4f586d61a",
        "language": "python",
        "id": "Ikkyi2JxgC_4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Making predictions with the models**"
      ],
      "metadata": {
        "azdata_cell_guid": "0295f638-de93-4034-b7d5-42f6e69f40f9",
        "id": "y8UoG_PkgC_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_OBJECT = model # Alternatively: object storing another model\n",
        "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
        "# MODEL_OBJECT = elastic_net_linear_reg_model\n",
        "\n",
        "X_tensor = X\n",
        "# predict_for = 'subset' or predict_for = 'single_entry'\n",
        "# The function will automatically detect if it is dealing with lists, NumPy arrays\n",
        "# or Pandas dataframes. If X_df is a list or a single-dimension array, predict_for\n",
        "# will be set as 'single_entry'. If X is a multi-dimension NumPy array (as the\n",
        "# outputs for preparing data - even single_entry - for deep learning models), or if\n",
        "# it is a Pandas dataframe, the function will set predict_for = 'subset'\n",
        "\n",
        "# X_df = subset of predictive variables (dataframe, NumPy array, or list).\n",
        "# If PREDICT_FOR = 'single_entry', X_df should be a list of parameters values.\n",
        "# e.g. X_df = [1.2, 3, 4] (dot is the decimal case separator, comma separate values).\n",
        "# Notice that the list should contain only the numeric values, in the same order of the\n",
        "# correspondent columns.\n",
        "# If PREDICT_FOR = 'subset' (prediction for multiple entries), X_df should be a dataframe\n",
        "# (subset) or a multi-dimensional NumPy array of the parameters values, as usual.\n",
        "\n",
        "DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset\n",
        "# DATAFRAME_FOR_CONCATENATING_PREDICTIONS: if you want to concatenate the predictions\n",
        "# to a dataframe, pass it here:\n",
        "# e.g. DATAFRAME_FOR_CONCATENATING_PREDICTIONS = df\n",
        "# If the dataframe must be the same one passed as X, repeat the dataframe object here:\n",
        "# X_df = dataset, DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset.\n",
        "# Alternatively, if DATAFRAME_FOR_CONCATENATING_PREDICTIONS = None,\n",
        "# the prediction will be returned as a series or NumPy array, depending on the input format.\n",
        "# Notice that the concatenated predictions will be added as a new column.\n",
        "\n",
        "COLUMN_WITH_PREDICTIONS_SUFFIX = None\n",
        "# COLUMN_WITH_PREDICTIONS_SUFFIX = None. If the predictions are added as a new column\n",
        "# of the dataframe DATAFRAME_FOR_CONCATENATING_PREDICTIONS, you can declare this\n",
        "# parameter as string with a suffix for identifying the model. If no suffix is added, the new\n",
        "# column will be named 'y_pred'.\n",
        "# e.g. COLUMN_WITH_PREDICTIONS_SUFFIX = '_keras' will create a column named \"y_pred_keras\". This\n",
        "# parameter is useful when working with multiple models. Always start the suffix with underscore\n",
        "# \"_\" so that no blank spaces are added; the suffix will not be merged to the column; and there\n",
        "# will be no confusion with the dot (.) notation for methods, JSON attributes, etc.\n",
        "\n",
        "FUNCTION_USED_FOR_FITTING_DL_MODEL = 'get_deep_learning_tf_model'\n",
        "# FUNCTION_USED_FOR_FITTING_DL_MODEL: the function you used for obtaining the deep learning model.\n",
        "# Example: 'get_deep_learning_tf_model' or 'get_siamese_networks_model'\n",
        "\n",
        "ARCHITECTURE = None\n",
        "# ARCHITECTURE: some models require inputs in a proper format. Declare here if you are using\n",
        "# one of these architectures. Example: ARCHITECTURE = 'cnn_lstm' from class tf_models require\n",
        "# a special reshape before getting predictions. You can keep None or put the name of the\n",
        "# architecture, if no special reshape is needed.\n",
        "\n",
        "### ATTENTION: ALL MODELS WITH LSTM, CNN or OTHER SPECIAL LAYERS REQUIRE THIS ARGUMENT TO BE\n",
        "# DECLARED\n",
        "\n",
        "LIST_OF_RESPONSES = RESPONSE_COLUMNS\n",
        "# You may declare the list RESPONSE_COLUMNS previously used for separating into features and responses tensors.\n",
        "# LIST_OF_RESPONSES = []. This parameter is obbligatory for multi-response models, such as the ones obtained from\n",
        "# function 'get_siamese_networks_model'. It must contain a list with the same order of the output responses.\n",
        "# Example: suppose your siamese model outputs 4 responses: 'temperature', 'pressure', 'flow_rate', and 'ph', in\n",
        "# this order. The list of responses must be declared as:\n",
        "# LIST_OF_RESPONSES = ['temperature', 'pressure', 'flow_rate', 'ph']\n",
        "# tuples and numpy arrays are also acceptable: LIST_OF_RESPONSES = ('temperature', 'pressure', 'flow_rate', 'ph')\n",
        "# Attention: the number of responses must be exactly the number of elements in list_of_responses, or an error will\n",
        "# be raised.\n",
        "\n",
        "\n",
        "# Predictions returned as prediction_output\n",
        "# Simply modify this object (or variable) on the left of equality:\n",
        "prediction_output = make_model_predictions (model_object = MODEL_OBJECT, X = X_tensor, dataframe_for_concatenating_predictions = DATAFRAME_FOR_CONCATENATING_PREDICTIONS, column_with_predictions_suffix = COLUMN_WITH_PREDICTIONS_SUFFIX, function_used_for_fitting_dl_model = FUNCTION_USED_FOR_FITTING_DL_MODEL, architecture = ARCHITECTURE, list_of_responses = LIST_OF_RESPONSES)"
      ],
      "metadata": {
        "azdata_cell_guid": "4b24cd43-4cb4-4039-8969-b7b9953c6b7e",
        "language": "python",
        "id": "NtFU758BgC_5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Calculating probabilities associated to each class**\n",
        "- Set the list_of_classes as the input of this function.\n",
        "- The predictions (outputs) from deep learning models (e.g. Keras/TensorFlow models) are themselves the probabilities associated to each possible class.\n",
        "    - For Scikit-learn and XGBoost, we must use a specific method for retrieving the probabilities."
      ],
      "metadata": {
        "azdata_cell_guid": "e5e53556-e062-4e66-81c1-1584f5762f8d",
        "id": "m0XCXliHgC_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_OBJECT = mlp_model # Alternatively: object storing another model\n",
        "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
        "# MODEL_OBJECT = mlp_model\n",
        "\n",
        "X_tensor = X\n",
        "# predict_for = 'subset' or predict_for = 'single_entry'\n",
        "# The function will automatically detect if it is dealing with lists, NumPy arrays\n",
        "# or Pandas dataframes. If X_df is a list or a single-dimension array, predict_for\n",
        "# will be set as 'single_entry'. If X is a multi-dimension NumPy array (as the\n",
        "# outputs for preparing data - even single_entry - for deep learning models), or if\n",
        "# it is a Pandas dataframe, the function will set predict_for = 'subset'\n",
        "\n",
        "# X_df = subset of predictive variables (dataframe, NumPy array, or list).\n",
        "# If PREDICT_FOR = 'single_entry', X_df should be a list of parameters values.\n",
        "# e.g. X_df = [1.2, 3, 4] (dot is the decimal case separator, comma separate values).\n",
        "# Notice that the list should contain only the numeric values, in the same order of the\n",
        "# correspondent columns.\n",
        "# If PREDICT_FOR = 'subset' (prediction for multiple entries), X_df should be a dataframe\n",
        "# (subset) or a multi-dimensional NumPy array of the parameters values, as usual.\n",
        "\n",
        "LIST_OF_CLASSES = list_of_classes\n",
        "# LIST_OF_CLASSES is the list of classes effectively used for training\n",
        "# the model. Set this parameter as the object returned from function\n",
        "# retrieve_classes_used_to_train\n",
        "\n",
        "TYPE_OF_MODEL = 'other'\n",
        "# TYPE_OF_MODEL = 'deep_learning' if Keras/TensorFlow or other deep learning\n",
        "# framework was used to obtain the model;\n",
        "# TYPE_OF_MODEL = 'other' for Scikit-learn or XGBoost models.\n",
        "\n",
        "DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset\n",
        "# DATAFRAME_FOR_CONCATENATING_PREDICTIONS: if you want to concatenate the predictions\n",
        "# to a dataframe, pass it here:\n",
        "# e.g. DATAFRAME_FOR_CONCATENATING_PREDICTIONS = df\n",
        "# If the dataframe must be the same one passed as X, repeat the dataframe object here:\n",
        "# X_df = dataset, DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset.\n",
        "# Alternatively, if DATAFRAME_FOR_CONCATENATING_PREDICTIONS = None,\n",
        "# the prediction will be returned as a series or NumPy array, depending on the input format.\n",
        "# Notice that the concatenated predictions will be added as a new column.\n",
        "# All of the new columns (appended or not) will have the prefix \"prob_class_\" followed\n",
        "# by the correspondent class name to identify them.\n",
        "\n",
        "ARCHITECTURE = None\n",
        "# ARCHITECTURE: some models require inputs in a proper format. Declare here if you are using\n",
        "# one of these architectures. Example: ARCHITECTURE = 'cnn_lstm' from class tf_models require\n",
        "# a special reshape before getting predictions. You can keep None or put the name of the\n",
        "# architecture, if no special reshape is needed.\n",
        "\n",
        "\n",
        "# Probabilities returned as calculated_probability\n",
        "# Simply modify this object (or variable) on the left of equality:\n",
        "calculated_probability = calculate_class_probability (model_object = MODEL_OBJECT, X = X_tensor, list_of_classes = LIST_OF_CLASSES, type_of_model = TYPE_OF_MODEL, dataframe_for_concatenating_predictions = DATAFRAME_FOR_CONCATENATING_PREDICTIONS, architecture = ARCHITECTURE)"
      ],
      "metadata": {
        "azdata_cell_guid": "c6b3a31b-9412-47c0-b47f-9458a9764ce3",
        "language": "python",
        "id": "WGMsKhatgC_6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Performing the SHAP feature importance analysis**\n",
        "- SHAP was developed by a mathematician from Washington University.\n",
        "- It combines the obtained machine learning model with Game Theory algorithms to analyze the relative importance of each variable, as well as the **interactions between variables**.\n",
        "- SHAP returns us a SHAP value that represents the relative importance."
      ],
      "metadata": {
        "azdata_cell_guid": "3151c081-e849-4e84-a005-cb4d1bee5436",
        "id": "Svy3rhaWgC_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_OBJECT = mlp_model # Alternatively: object storing another model\n",
        "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
        "# MODEL_OBJECT = xgb_model\n",
        "\n",
        "X_TRAIN = split_dictionary['X_train']\n",
        "# X_TRAIN = subset of predictive variables (dataframe).\n",
        "# Alternatively, modify X_train, not X_TRAIN\n",
        "\n",
        "MODEL_TYPE = 'general'\n",
        "# MODEL_TYPE = 'general' for the general case, including artificial neural networks.\n",
        "# MODEL_TYPE = 'linear' for Sklearn linear models (OLS, Ridge, Lasso, ElasticNet,\n",
        "# Logistic Regression).\n",
        "# MODEL_TYPE = 'tree' for tree-based models (Random Forest and XGBoost).\n",
        "# MODEL_TYPE = 'deep' for Deep Learning TensorFlow model.\n",
        "# Actually, any string different from 'linear', 'tree', or 'deep' (including blank string)\n",
        "# will apply the general case.\n",
        "\n",
        "TOTAL_OF_SHAP_POINTS = 40\n",
        "# TOTAL_OF_SHAP_POINTS (integer): number of points from the\n",
        "# subset X_train that will be randomly selected for the SHAP\n",
        "# analysis. If the kernel is taking too long, reduce this value.\n",
        "\n",
        "\n",
        "# Dictionary containing calculated metrics returned as shap_dict;\n",
        "# Simply modify this object on the left of equality:\n",
        "shap_dict = shap_feature_analysis (model_object = MODEL_OBJECT, X_train = X_TRAIN, model_type = MODEL_TYPE, total_of_shap_points = TOTAL_OF_SHAP_POINTS)"
      ],
      "metadata": {
        "azdata_cell_guid": "523a488e-de01-49a1-8aa7-5772ab900173",
        "language": "python",
        "id": "AO1b9ehhgC_8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualizing time series**"
      ],
      "metadata": {
        "azdata_cell_guid": "df155b22-8cc4-46c4-8c67-de9fb6a73996",
        "id": "lTyxWDJ3gC_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_IN_SAME_COLUMN = False\n",
        "\n",
        "# Parameters to input when DATA_IN_SAME_COLUMN = True:\n",
        "DATASET = None #Alternatively: object containing the dataset to be analyzed (e.g. DATASET = dataset)\n",
        "COLUMN_WITH_PREDICT_VAR_X = 'X' # Alternatively: correct name for X-column\n",
        "COLUMN_WITH_RESPONSE_VAR_Y = 'Y' # Alternatively: correct name for Y-column\n",
        "COLUMN_WITH_LABELS = 'label_column' # Alternatively: correct name for column with the labels or groups\n",
        "\n",
        "# DATA_IN_SAME_COLUMN = False: set as True if all the values to plot are in a same column.\n",
        "# If DATA_IN_SAME_COLUMN = True, you must specify the dataframe containing the data as DATASET;\n",
        "# the column containing the predict variable (X) as COLUMN_WITH_PREDICT_VAR_X; the column\n",
        "# containing the responses to plot (Y) as COLUMN_WITH_RESPONSE_VAR_Y; and the column\n",
        "# containing the labels (subgroup) indication as COLUMN_WITH_LABELS.\n",
        "# DATASET is an object, so do not declare it in quotes. The other three arguments (columns' names)\n",
        "# are strings, so declare in quotes.\n",
        "\n",
        "# Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare.\n",
        "# All the results for both groups are in a column named 'results', wich will be plot against\n",
        "# the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
        "# an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
        "# column 'group' shows the value 'B'. In this example:\n",
        "# DATA_IN_SAME_COLUMN = True,\n",
        "# DATASET = dataset,\n",
        "# COLUMN_WITH_PREDICT_VAR_X = 'time',\n",
        "# COLUMN_WITH_RESPONSE_VAR_Y = 'results',\n",
        "# COLUMN_WITH_LABELS = 'group'\n",
        "# If you want to declare a list of dictionaries, keep DATA_IN_SAME_COLUMN = False and keep\n",
        "# DATASET = None (the other arguments may be set as None, but it is not mandatory:\n",
        "# COLUMN_WITH_PREDICT_VAR_X = None, COLUMN_WITH_RESPONSE_VAR_Y = None, COLUMN_WITH_LABELS = None).\n",
        "\n",
        "\n",
        "# Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
        "LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = [\n",
        "\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None}\n",
        "\n",
        "]\n",
        "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE: if data is already converted to series, lists\n",
        "# or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
        "# even if there is a single dictionary.\n",
        "# Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
        "# (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
        "# keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
        "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
        "# and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
        "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
        "# same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
        "# represents the series and label of the added dictionary (you can pass 'lab': None, but if\n",
        "# 'x' or 'y' are None, the new dictionary will be ignored).\n",
        "\n",
        "# Examples:\n",
        "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE =\n",
        "# [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
        "# will plot a single variable. In turns:\n",
        "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE =\n",
        "# [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
        "# will plot two series, Y1 x X and Y2 x X.\n",
        "# Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
        "# If None is provided to 'lab', an automatic label will be generated.\n",
        "\n",
        "\n",
        "X_AXIS_ROTATION = 70\n",
        "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "Y_AXIS_ROTATION = 0\n",
        "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "GRID = True #Alternatively: True or False\n",
        "# If GRID = False, no grid lines are shown in the graphic.\n",
        "ADD_SPLINE_LINES = True #Alternatively: True or False\n",
        "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
        "# Since we are obtaining a scatter plot, there is no meaning in omitting the dots,\n",
        "# as we can do for the time series visualization function.\n",
        "ADD_SCATTER_DOTS = False\n",
        "# If ADD_SCATTER_DOTS = False, no dots representing the data points are shown.\n",
        "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
        "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
        "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
        "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
        "\n",
        "EXPORT_PNG = False\n",
        "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
        "# Set EXPORT_PNG = True to export the obtained image.\n",
        "DIRECTORY_TO_SAVE = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\"\n",
        "# or DIRECTORY_TO_SAVE = \"folder\"\n",
        "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
        "# path, DIRECTORY_TO_SAVE = \"\"\n",
        "FILE_NAME = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the name you want for the file without the\n",
        "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png'\n",
        "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
        "# 'time_series_vis.png'\n",
        "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
        "# the file will be overwritten.\n",
        "PNG_RESOLUTION_DPI = 330\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
        "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
        "\n",
        "\n",
        "time_series_vis (data_in_same_column = DATA_IN_SAME_COLUMN, df = DATASET, column_with_predict_var_x = COLUMN_WITH_PREDICT_VAR_X, column_with_response_var_y = COLUMN_WITH_RESPONSE_VAR_Y, column_with_labels = COLUMN_WITH_LABELS, list_of_dictionaries_with_series_to_analyze = LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, add_scatter_dots = ADD_SCATTER_DOTS, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
      ],
      "metadata": {
        "azdata_cell_guid": "fcbb2ea3-1722-4b09-a1d6-8c3e6b79fd02",
        "language": "python",
        "id": "D2zAzkD-gC_8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing or exporting models and dictionaries (or lists)**"
      ],
      "metadata": {
        "azdata_cell_guid": "df90ed9f-4bbd-4a4f-9986-e706702217d9",
        "id": "xK1xtp3ogC_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case 1: import only a model"
      ],
      "metadata": {
        "azdata_cell_guid": "c83aac50-c444-4de1-a851-7c98b91ee1ce",
        "id": "XPpskvyEgC_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTION = 'import'\n",
        "# ACTION = 'import' for importing a model and/or a dictionary;\n",
        "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
        "\n",
        "OBJECTS_MANIPULATED = 'model_only'\n",
        "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will\n",
        "#  be manipulated.\n",
        "\n",
        "MODEL_FILE_NAME = None\n",
        "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. MODEL_FILE_NAME = 'model'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
        "\n",
        "DICTIONARY_OR_LIST_FILE_NAME = None\n",
        "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary\n",
        "# (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary\n",
        "# or list will be manipulated.\n",
        "\n",
        "DIRECTORY_PATH = ''\n",
        "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
        "# or from which the model will be retrieved. If no value is provided,\n",
        "# the DIRECTORY_PATH will be the root: \"\"\n",
        "# Notice that the model and the dictionary must be stored in the same path.\n",
        "# If a model and a dictionary will be exported, they will be stored in the same\n",
        "# DIRECTORY_PATH.\n",
        "\n",
        "MODEL_TYPE = 'sklearn'\n",
        "# This parameter has effect only when a model will be manipulated.\n",
        "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .keras\n",
        "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing\n",
        "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
        "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
        "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
        "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
        "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
        "\n",
        "DICT_OR_LIST_TO_EXPORT = None\n",
        "MODEL_TO_EXPORT = None\n",
        "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
        "# must be declared. If ACTION == 'export', keep:\n",
        "# DICT_OR_LIST_TO_EXPORT = None,\n",
        "# MODEL_TO_EXPORT = None\n",
        "# If one of these objects will be exported, substitute None by the name of the object\n",
        "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
        "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
        "# it is not a string, but an object.\n",
        "# For exporting a dictionary named as 'dict':\n",
        "# DICT_OR_LIST_TO_EXPORT = dict\n",
        "\n",
        "USE_COLAB_MEMORY = False\n",
        "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
        "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
        "# from Google Colaboratory: you will update or download the file and it will be available\n",
        "# only during the time when the kernel is running. It will be excluded when the kernel\n",
        "# dies, for instance, when you close the notebook.\n",
        "\n",
        "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
        "# to your computer (running the cell will start the download).\n",
        "\n",
        "# Model object saved as model.\n",
        "# Simply modify this object on the left of equality:\n",
        "model = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)"
      ],
      "metadata": {
        "azdata_cell_guid": "19ceeae7-121c-4e55-9d41-3e498b2d57ca",
        "language": "python",
        "id": "hUnY74WLgC_9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case 2: import only a dictionary or a list"
      ],
      "metadata": {
        "azdata_cell_guid": "98416876-845b-4632-8ed8-ec685b13abee",
        "id": "k-HJfGAWgC_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTION = 'import'\n",
        "# ACTION = 'import' for importing a model and/or a dictionary;\n",
        "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
        "\n",
        "OBJECTS_MANIPULATED = 'dict_or_list_only'\n",
        "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will\n",
        "#  be manipulated.\n",
        "\n",
        "MODEL_FILE_NAME = None\n",
        "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. MODEL_FILE_NAME = 'model'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
        "\n",
        "DICTIONARY_OR_LIST_FILE_NAME = None\n",
        "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary\n",
        "# (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary\n",
        "# or list will be manipulated.\n",
        "\n",
        "DIRECTORY_PATH = ''\n",
        "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
        "# or from which the model will be retrieved. If no value is provided,\n",
        "# the DIRECTORY_PATH will be the root: \"\"\n",
        "# Notice that the model and the dictionary must be stored in the same path.\n",
        "# If a model and a dictionary will be exported, they will be stored in the same\n",
        "# DIRECTORY_PATH.\n",
        "\n",
        "MODEL_TYPE = 'sklearn'\n",
        "# This parameter has effect only when a model will be manipulated.\n",
        "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .keras\n",
        "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing\n",
        "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
        "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
        "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
        "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
        "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
        "\n",
        "DICT_OR_LIST_TO_EXPORT = None\n",
        "MODEL_TO_EXPORT = None\n",
        "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
        "# must be declared. If ACTION == 'export', keep:\n",
        "# DICT_OR_LIST_TO_EXPORT = None,\n",
        "# MODEL_TO_EXPORT = None\n",
        "# If one of these objects will be exported, substitute None by the name of the object\n",
        "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
        "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
        "# it is not a string, but an object.\n",
        "# For exporting a dictionary named as 'dict':\n",
        "# DICT_OR_LIST_TO_EXPORT = dict\n",
        "\n",
        "USE_COLAB_MEMORY = False\n",
        "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
        "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
        "# from Google Colaboratory: you will update or download the file and it will be available\n",
        "# only during the time when the kernel is running. It will be excluded when the kernel\n",
        "# dies, for instance, when you close the notebook.\n",
        "\n",
        "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
        "# to your computer (running the cell will start the download).\n",
        "\n",
        "# Dictionary or list saved as imported_dict_or_list.\n",
        "# Simply modify this object on the left of equality:\n",
        "imported_dict_or_list = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)"
      ],
      "metadata": {
        "azdata_cell_guid": "ed0579c2-2dfc-4856-84b0-d2c6cd549efb",
        "language": "python",
        "id": "CsPgLqiJgC__"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case 3: import a model and a dictionary (or a list)"
      ],
      "metadata": {
        "azdata_cell_guid": "6a461900-d684-48e0-b09e-9bed23fb445d",
        "id": "NkHn-d2ugDAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTION = 'import'\n",
        "# ACTION = 'import' for importing a model and/or a dictionary;\n",
        "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
        "\n",
        "OBJECTS_MANIPULATED = 'model_and_dict'\n",
        "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will\n",
        "#  be manipulated.\n",
        "\n",
        "MODEL_FILE_NAME = None\n",
        "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. MODEL_FILE_NAME = 'model'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
        "\n",
        "DICTIONARY_OR_LIST_FILE_NAME = None\n",
        "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary\n",
        "# (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary\n",
        "# or list will be manipulated.\n",
        "\n",
        "DIRECTORY_PATH = ''\n",
        "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
        "# or from which the model will be retrieved. If no value is provided,\n",
        "# the DIRECTORY_PATH will be the root: \"\"\n",
        "# Notice that the model and the dictionary must be stored in the same path.\n",
        "# If a model and a dictionary will be exported, they will be stored in the same\n",
        "# DIRECTORY_PATH.\n",
        "\n",
        "MODEL_TYPE = 'sklearn'\n",
        "# This parameter has effect only when a model will be manipulated.\n",
        "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .keras\n",
        "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing\n",
        "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
        "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
        "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
        "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
        "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
        "\n",
        "DICT_OR_LIST_TO_EXPORT = None\n",
        "MODEL_TO_EXPORT = None\n",
        "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
        "# must be declared. If ACTION == 'export', keep:\n",
        "# DICT_OR_LIST_TO_EXPORT = None,\n",
        "# MODEL_TO_EXPORT = None\n",
        "# If one of these objects will be exported, substitute None by the name of the object\n",
        "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
        "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
        "# it is not a string, but an object.\n",
        "# For exporting a dictionary named as 'dict':\n",
        "# DICT_OR_LIST_TO_EXPORT = dict\n",
        "\n",
        "USE_COLAB_MEMORY = False\n",
        "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
        "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
        "# from Google Colaboratory: you will update or download the file and it will be available\n",
        "# only during the time when the kernel is running. It will be excluded when the kernel\n",
        "# dies, for instance, when you close the notebook.\n",
        "\n",
        "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
        "# to your computer (running the cell will start the download).\n",
        "\n",
        "# Model object saved as model. Dictionary or list saved as imported_dict_or_list.\n",
        "# Simply modify these objects on the left of equality:\n",
        "model, imported_dict_or_list = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)"
      ],
      "metadata": {
        "azdata_cell_guid": "68be2dfa-cb28-4134-a5f5-b325a1b29595",
        "language": "python",
        "id": "CFoJAPLggDAK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case 4: export a model and/or a dictionary (or a list)"
      ],
      "metadata": {
        "azdata_cell_guid": "4d0c9875-ec07-40dd-bbdd-63fb620c22e1",
        "id": "XRtOVJ0bgDAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTION = 'export'\n",
        "# ACTION = 'import' for importing a model and/or a dictionary;\n",
        "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
        "\n",
        "OBJECTS_MANIPULATED = 'model_only'\n",
        "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will\n",
        "#  be manipulated.\n",
        "\n",
        "MODEL_FILE_NAME = None\n",
        "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. MODEL_FILE_NAME = 'model'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
        "\n",
        "DICTIONARY_OR_LIST_FILE_NAME = None\n",
        "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary\n",
        "# (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary\n",
        "# or list will be manipulated.\n",
        "\n",
        "DIRECTORY_PATH = ''\n",
        "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
        "# or from which the model will be retrieved. If no value is provided,\n",
        "# the DIRECTORY_PATH will be the root: \"\"\n",
        "# Notice that the model and the dictionary must be stored in the same path.\n",
        "# If a model and a dictionary will be exported, they will be stored in the same\n",
        "# DIRECTORY_PATH.\n",
        "\n",
        "MODEL_TYPE = 'sklearn'\n",
        "# This parameter has effect only when a model will be manipulated.\n",
        "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .keras\n",
        "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing\n",
        "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
        "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
        "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
        "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
        "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
        "\n",
        "DICT_OR_LIST_TO_EXPORT = None\n",
        "MODEL_TO_EXPORT = None\n",
        "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
        "# must be declared. If ACTION == 'export', keep:\n",
        "# DICT_OR_LIST_TO_EXPORT = None,\n",
        "# MODEL_TO_EXPORT = None\n",
        "# If one of these objects will be exported, substitute None by the name of the object\n",
        "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
        "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
        "# it is not a string, but an object.\n",
        "# For exporting a dictionary named as 'dict':\n",
        "# DICT_OR_LIST_TO_EXPORT = dict\n",
        "\n",
        "USE_COLAB_MEMORY = False\n",
        "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
        "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
        "# from Google Colaboratory: you will update or download the file and it will be available\n",
        "# only during the time when the kernel is running. It will be excluded when the kernel\n",
        "# dies, for instance, when you close the notebook.\n",
        "\n",
        "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
        "# to your computer (running the cell will start the download).\n",
        "\n",
        "import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)"
      ],
      "metadata": {
        "azdata_cell_guid": "290ed3d6-656b-4ada-a931-55ee6ca8b5b1",
        "language": "python",
        "id": "-B8FBzYAgDAL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exporting the dataframe as CSV file (to notebook's workspace)**"
      ],
      "metadata": {
        "azdata_cell_guid": "717a95cc-68f7-478f-acc8-295187e25295",
        "id": "v6y0gV8sgDAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## WARNING: all files exported from this function are .csv (comma separated values)\n",
        "\n",
        "DATAFRAME_OBJ_TO_BE_EXPORTED = dataset\n",
        "# Alternatively: object containing the dataset to be exported.\n",
        "# DATAFRAME_OBJ_TO_BE_EXPORTED: dataframe object that is going to be exported from the\n",
        "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
        "# example: DATAFRAME_OBJ_TO_BE_EXPORTED = dataset will export the dataset object.\n",
        "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
        "\n",
        "FILE_DIRECTORY_PATH = \"\"\n",
        "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\"\n",
        "# or FILE_DIRECTORY_PATH = \"folder\"\n",
        "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
        "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
        "\n",
        "NEW_FILE_NAME_WITHOUT_EXTENSION = \"dataset\"\n",
        "# NEW_FILE_NAME_WITHOUT_EXTENSION - (string, in quotes): input the name of the\n",
        "# file without the extension. e.g. set NEW_FILE_NAME_WITHOUT_EXTENSION = \"my_file\"\n",
        "# to export the CSV file 'my_file.csv' to notebook's workspace.\n",
        "\n",
        "export_pd_dataframe_as_csv (dataframe_obj_to_be_exported = DATAFRAME_OBJ_TO_BE_EXPORTED, new_file_name_without_extension = NEW_FILE_NAME_WITHOUT_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH)"
      ],
      "metadata": {
        "azdata_cell_guid": "6bf2552b-6799-4c5f-89a7-0b626bacedc6",
        "language": "python",
        "id": "2kLfeAKsgDAO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exporting dataframes as Excel file tables**"
      ],
      "metadata": {
        "azdata_cell_guid": "308cd87b-3c23-4e93-888b-9fb72d419f4a",
        "id": "vXPn-fvogDAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## WARNING: all files exported from this function are .xlsx\n",
        "\n",
        "FILE_NAME_WITHOUT_EXTENSION = \"datasets\"\n",
        "# (string, in quotes): input the name of the\n",
        "# file without the extension. e.g. new_file_name_without_extension = \"my_file\"\n",
        "# will export a file 'my_file.xlsx' to notebook's workspace.\n",
        "\n",
        "EXPORTED_TABLES = [{'dataframe_obj_to_be_exported': None,\n",
        "                    'excel_sheet_name': None},]\n",
        "\n",
        "# exported_tables is a list of dictionaries. User may declare several dictionaries,\n",
        "# as long as the keys are always the same, and if the values stored in keys are not None.\n",
        "\n",
        "# key 'dataframe_obj_to_be_exported': dataframe object that is going to be exported from the\n",
        "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
        "# example: dataframe_obj_to_be_exported = dataset will export the dataset object.\n",
        "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
        "\n",
        "# key 'excel_sheet_name': string containing the name of the sheet to be written on the\n",
        "# exported Excel file. Example: excel_sheet_name = 'tab_1' will save the dataframe in the\n",
        "# sheet 'tab_1' from the file named as file_name_without_extension.\n",
        "\n",
        "# examples: exported_tables = [{'dataframe_obj_to_be_exported': dataset1,\n",
        "# 'excel_sheet_name': 'sheet1'},]\n",
        "# will export only dataset1 as 'sheet1';\n",
        "# exported_tables = [{'dataframe_obj_to_be_exported': dataset1, 'excel_sheet_name': 'sheet1'},\n",
        "# {'dataframe_obj_to_be_exported': dataset2, 'excel_sheet_name': 'sheet2']\n",
        "# will export dataset1 as 'sheet1' and dataset2 as 'sheet2'.\n",
        "\n",
        "# Notice that if the file does not contain the exported sheets, they will be created. If it has,\n",
        "# the sheets will be replaced.\n",
        "\n",
        "FILE_DIRECTORY_PATH = \"\"\n",
        "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\"\n",
        "# or FILE_DIRECTORY_PATH = \"folder\"\n",
        "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
        "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
        "\n",
        "\n",
        "export_pd_dataframe_as_excel (file_name_without_extension = FILE_NAME_WITHOUT_EXTENSION, exported_tables = EXPORTED_TABLES, file_directory_path = FILE_DIRECTORY_PATH)"
      ],
      "metadata": {
        "azdata_cell_guid": "b6909ac5-ae05-4b54-9aba-c54b6c9ad411",
        "language": "python",
        "id": "mBTMuHCPgDAR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1",
        "id": "FD8JTlUXgDAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chosen of activation function - Background**\n",
        "- by Jason Brownlee on January 18, 2021 in Deep Learning\n",
        "- In: https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/?msclkid=b2ff61a7c17811eca0396af8009afac9\n",
        "\n",
        "Activation Functions\n",
        "An activation function in a neural network defines how the weighted sum of the input is transformed into an output from a node or nodes in a layer of the network.\n",
        "\n",
        "Sometimes the activation function is called a “transfer function.” If the output range of the activation function is limited, then it may be called a “squashing function.” Many activation functions are nonlinear and may be referred to as the “nonlinearity” in the layer or the network design.\n",
        "\n",
        "The choice of activation function has a large impact on the capability and performance of the neural network, and different activation functions may be used in different parts of the model.\n",
        "\n",
        "Technically, the activation function is used within or after the internal processing of each node in the network, although networks are designed to use the same activation function for all nodes in a layer.\n",
        "\n",
        "A network may have three types of layers: input layers that take raw input from the domain, hidden layers that take input from another layer and pass output to another layer, and output layers that make a prediction.\n",
        "\n",
        "All hidden layers typically use the same activation function. The output layer will typically use a different activation function from the hidden layers and is dependent upon the type of prediction required by the model.\n",
        "\n",
        "Activation functions are also typically differentiable, meaning the first-order derivative can be calculated for a given input value. This is required given that neural networks are typically trained using the backpropagation of error algorithm that requires the derivative of prediction error in order to update the weights of the model.\n",
        "\n",
        "There are many different types of activation functions used in neural networks, although perhaps only a small number of functions used in practice for hidden and output layers.\n",
        "\n",
        "Let’s take a look at the activation functions used for each type of layer in turn.\n",
        "\n",
        "Activation for Hidden Layers\n",
        "A hidden layer in a neural network is a layer that receives input from another layer (such as another hidden layer or an input layer) and provides output to another layer (such as another hidden layer or an output layer).\n",
        "\n",
        "A hidden layer does not directly contact input data or produce outputs for a model, at least in general.\n",
        "\n",
        "A neural network may have zero or more hidden layers.\n",
        "\n",
        "Typically, a differentiable nonlinear activation function is used in the hidden layers of a neural network. This allows the model to learn more complex functions than a network trained using a linear activation function.\n",
        "\n",
        "In order to get access to a much richer hypothesis space that would benefit from deep representations, you need a non-linearity, or activation function.\n",
        "\n",
        "— Page 72, Deep Learning with Python, 2017.\n",
        "\n",
        "There are perhaps three activation functions you may want to consider for use in hidden layers; they are:\n",
        "\n",
        " - Rectified Linear Activation (ReLU)\n",
        " - Logistic (Sigmoid)\n",
        " - Hyperbolic Tangent (Tanh)\n",
        "\n",
        "This is not an exhaustive list of activation functions used for hidden layers, but they are the most commonly used.\n",
        "\n",
        "Let’s take a closer look at each in turn.\n",
        "\n",
        "## **ReLU Hidden Layer Activation Function**\n",
        "The rectified linear activation function, or ReLU activation function, is perhaps the most common function used for hidden layers.\n",
        "\n",
        "It is common because it is both simple to implement and effective at overcoming the limitations of other previously popular activation functions, such as Sigmoid and Tanh. Specifically, it is less susceptible to vanishing gradients that prevent deep models from being trained, although it can suffer from other problems like saturated or “dead” units.\n",
        "\n",
        "The ReLU function is calculated as follows:\n",
        "\n",
        "`max(0.0, x)`\n",
        "\n",
        "This means that if the input value (x) is negative, then a value 0.0 is returned, otherwise, the value is returned.\n",
        "\n",
        "When using the ReLU function for hidden layers, it is a good practice to use a “He Normal” or “He Uniform” weight initialization and scale input data to the range 0-1 (normalize) prior to training.\n",
        "\n",
        "## **Sigmoid Hidden Layer Activation Function**\n",
        "The sigmoid activation function is also called the logistic function.\n",
        "\n",
        "It is the same function used in the logistic regression classification algorithm.\n",
        "\n",
        "The function takes any real value as input and outputs values in the range 0 to 1. The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to 0.0.\n",
        "\n",
        "The sigmoid activation function is calculated as follows:\n",
        "\n",
        "`1.0 / (1.0 + e^-x)`\n",
        "\n",
        "Where e is a mathematical constant, which is the base of the natural logarithm.\n",
        "\n",
        "When using the Sigmoid function for hidden layers, it is a good practice to use a “Xavier Normal” or “Xavier Uniform” weight initialization (also referred to Glorot initialization, named for Xavier Glorot) and scale input data to the range 0-1 (e.g. the range of the activation function) prior to training.\n",
        "\n",
        "## **Tanh Hidden Layer Activation Function**\n",
        "The hyperbolic tangent activation function is also referred to simply as the Tanh (also “tanh” and “TanH“) function.\n",
        "\n",
        "It is very similar to the sigmoid activation function and even has the same S-shape.\n",
        "\n",
        "The function takes any real value as input and outputs values in the range -1 to 1. The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0.\n",
        "\n",
        "The Tanh activation function is calculated as follows:\n",
        "\n",
        "`(e^x – e^-x) / (e^x + e^-x)`\n",
        "\n",
        "Where e is a mathematical constant that is the base of the natural logarithm.\n",
        "\n",
        "When using the TanH function for hidden layers, it is a good practice to use a “Xavier Normal” or “Xavier Uniform” weight initialization (also referred to Glorot initialization, named for Xavier Glorot) and scale input data to the range -1 to 1 (e.g. the range of the activation function) prior to training.\n",
        "\n",
        "## **How to Choose a Hidden Layer Activation Function**\n",
        "A neural network will almost always have the same activation function in all hidden layers.\n",
        "\n",
        "It is most unusual to vary the activation function through a network model.\n",
        "\n",
        "Traditionally, the sigmoid activation function was the default activation function in the 1990s. Perhaps through the mid to late 1990s to 2010s, the Tanh function was the default activation function for hidden layers.\n",
        "\n",
        "… the hyperbolic tangent activation function typically performs better than the logistic sigmoid.\n",
        "\n",
        "— Page 195, Deep Learning, 2016.\n",
        "\n",
        "Both the sigmoid and Tanh functions can make the model more susceptible to problems during training, via the so-called vanishing gradients problem.\n",
        "\n",
        "You can learn more about this problem in this tutorial:\n",
        "\n",
        "A Gentle Introduction to the Rectified Linear Unit (ReLU)\n",
        "The activation function used in hidden layers is typically chosen based on the type of neural network architecture.\n",
        "\n",
        "Modern neural network models with common architectures, such as MLP and CNN, will make use of the ReLU activation function, or extensions.\n",
        "\n",
        "In modern neural networks, the default recommendation is to use the rectified linear unit or ReLU …\n",
        "\n",
        "— Page 174, Deep Learning, 2016.\n",
        "\n",
        "Recurrent networks still commonly use Tanh or sigmoid activation functions, or even both. For example, the LSTM commonly uses the Sigmoid activation for recurrent connections and the Tanh activation for output.\n",
        "\n",
        " - **Multilayer Perceptron (MLP)**: ReLU activation function.\n",
        " - **Convolutional Neural Network (CNN)**: ReLU activation function.\n",
        " - **Recurrent Neural Network**: Tanh and/or Sigmoid activation function.\n",
        "\n",
        "If you’re unsure which activation function to use for your network, try a few and compare the results.\n",
        "\n",
        "The figure below summarizes how to choose an activation function for the hidden layers of your neural network model.\n",
        "\n",
        "![image.png](attachment:image.png)"
      ],
      "metadata": {
        "azdata_cell_guid": "9a8007c0-f249-499f-aadd-1eaf0a0f0824",
        "id": "tOpaVTDjgDAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Activation for Output Layers**\n",
        "The output layer is the layer in a neural network model that directly outputs a prediction.\n",
        "\n",
        "All feed-forward neural network models have an output layer.\n",
        "\n",
        "There are perhaps three activation functions you may want to consider for use in the output layer; they are:\n",
        "\n",
        "- Linear\n",
        "- Logistic (Sigmoid)\n",
        "- Softmax\n",
        "\n",
        "This is not an exhaustive list of activation functions used for output layers, but they are the most commonly used.\n",
        "\n",
        "Let’s take a closer look at each in turn.\n",
        "\n",
        "## **Linear Output Activation Function**\n",
        "The linear activation function is also called **“identity” (multiplied by 1.0) or “no activation.”**\n",
        "\n",
        "This is because the linear activation function does not change the weighted sum of the input in any way and instead returns the value directly.\n",
        "\n",
        "`linear(x) = x`\n",
        "\n",
        "Target values used to train a model with a linear activation function in the output layer are **typically scaled prior to modeling using normalization or standardization transforms**.\n",
        "\n",
        "## **Sigmoid Output Activation Function**\n",
        "The sigmoid of logistic activation function was described in the previous section.\n",
        "\n",
        "Target labels used to train a model with a sigmoid activation function in the output layer will have the values 0 or 1.\n",
        "\n",
        "## **Softmax Output Activation Function**\n",
        "The softmax function outputs a vector of values that sum to 1.0 that can be **interpreted as probabilities of class membership.**\n",
        "\n",
        "It is related to the argmax function that outputs a 0 for all options and 1 for the chosen option. Softmax is a “softer” version of argmax that allows a probability-like output of a winner-take-all function.\n",
        "\n",
        "As such, the input to the function is a vector of real values and the output is a vector of the same length with values that sum to 1.0 like probabilities.\n",
        "\n",
        "The softmax function is calculated as follows:\n",
        "\n",
        "`e^x / sum(e^x)`\n",
        "\n",
        "Where x is a vector of outputs and e is a mathematical constant that is the base of the natural logarithm.\n",
        "\n",
        "Target labels used to train a model with the softmax activation function in the output layer will be vectors with 1 for the target class and 0 for all other classes.\n",
        "\n",
        "## **How to Choose an Output Activation Function**\n",
        "You must choose the activation function for your output layer based on the type of prediction problem that you are solving.\n",
        "\n",
        "Specifically, the type of variable that is being predicted.\n",
        "\n",
        "For example, you may divide prediction problems into two main groups, predicting a categorical variable (classification) and predicting a numerical variable (regression).\n",
        "\n",
        "If your problem is a regression problem, you should use a linear activation function.\n",
        "\n",
        " - **Regression**: One node, linear activation.\n",
        "\n",
        "If your problem is a classification problem, then there are three main types of classification problems and each may use a different activation function.\n",
        "\n",
        "Predicting a probability is not a regression problem; it is classification. In all cases of classification, your model will predict the probability of class membership (e.g. probability that an example belongs to each class) that you can convert to a crisp class label by rounding (for sigmoid) or argmax (for softmax).\n",
        "\n",
        "If there are two mutually exclusive classes (binary classification), then your output layer will have one node and a sigmoid activation function should be used. If there are more than two mutually exclusive classes (multiclass classification), then your output layer will have one node per class and a softmax activation should be used. If there are two or more mutually inclusive classes (multilabel classification), then your output layer will have one node for each class and a sigmoid activation function is used.\n",
        "\n",
        " - **Binary Classification**: One node, sigmoid activation.\n",
        " - **Multiclass Classification**: One node per class, softmax activation.\n",
        " - **Multilabel Classification**: One node per class, sigmoid activation.\n",
        "\n",
        "The figure below summarizes how to choose an activation function for the output layer of your neural network model.\n",
        "\n",
        "![image.png](attachment:image.png)"
      ],
      "metadata": {
        "azdata_cell_guid": "62cd2a3f-3daf-434c-b9bf-dd7a0702f2c0",
        "id": "EjJYDJFugDAU"
      }
    }
  ]
}