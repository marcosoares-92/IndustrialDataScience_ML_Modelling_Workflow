{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Process Simulation**\n",
    "- Input datasets for sensitivity analysis into the models; and reverse transformations to simulate and visualize different conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Machine Learning Modelling Workflow Notebook 6_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content:\n",
    "1. Loading the dataframes;\n",
    "2. Loading the models;\n",
    "3. Converting the datasets into NumPy arrays with correct format for CNN and RNN Architectures;\n",
    "4. Using the models to predict outputs;\n",
    "5. Using the classification models to predict probabilities;\n",
    "6. Merging (joining) dataframes on given keys; and sorting the merged table;\n",
    "7. Concatenating (SQL Union/Stacking/Appending) dataframes;\n",
    "8. Column filtering (selecting) or column renaming;\n",
    "9. Reversing transforms: log-transform (exponentially transforming variables); \n",
    "10. Box-Cox transform; \n",
    "11. One-Hot Encoding;\n",
    "12. Feature scaling;\n",
    "13. Bar chart visualization;\n",
    "14. Time series visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267"
   },
   "source": [
    "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
    "- marcosoares.feq@gmail.com\n",
    "- marco.soares@bayer.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5dbf6196-9599-414f-a337-ec52f7f2122c",
    "id": "zGDOhNQbGzrr"
   },
   "source": [
    "Install statsmodels library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "11e929c6-8980-4298-8ded-b40a2080913e",
    "id": "7q1Bfti3Gzrs"
   },
   "outputs": [],
   "source": [
    "! pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5dbf6196-9599-414f-a337-ec52f7f2122c",
    "id": "zGDOhNQbGzrr"
   },
   "source": [
    "Install tensorflow library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "11e929c6-8980-4298-8ded-b40a2080913e",
    "id": "7q1Bfti3Gzrs"
   },
   "outputs": [],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1a5c1713-e549-4b18-bbaf-114c019c976d",
    "id": "wXnEEdHuGzru"
   },
   "source": [
    "Install Keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "dbd37f46-51f0-4a30-a812-4b27679ff1a1",
    "id": "N1OlfernGzrw"
   },
   "outputs": [],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e691d1ee-cb58-482b-9edb-d6baca45fdb3",
    "id": "Qowk3bTaGzrx"
   },
   "source": [
    "Install SHAP library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "245331e6-1f20-48ce-8ab9-00d47c128616",
    "id": "nC2bqVfxGzry"
   },
   "outputs": [],
   "source": [
    "! pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "7ec04518-6f8a-43a0-868e-3932f998886e",
    "id": "7ldt-mnjGzrz",
    "outputId": "b70c28af-c67c-4c61-a7b3-2a21bf4c6d64"
   },
   "outputs": [],
   "source": [
    "#check the version of the package\n",
    "! pip show shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e9286147-d907-490e-8596-ec38fc6c21c8",
    "id": "H9YCpVctGzr1"
   },
   "outputs": [],
   "source": [
    "# Upgrade to the most recent library versions, if a given module is not present and analysis cannot be\n",
    "# executed.\n",
    "! pip install pip --upgrade\n",
    "! pip install tensorflow --upgrade\n",
    "! pip install keras --upgrade\n",
    "! pip install shap --upgrade\n",
    "! pip install sklearn --upgrade\n",
    "! pip install pandas --upgrade\n",
    "! pip install numpy --upgrade\n",
    "! pip install matplotlib --upgrade\n",
    "! pip install seaborn --upgrade\n",
    "! pip install scipy --upgrade\n",
    "! pip install statsmodels --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea"
   },
   "source": [
    "## **Load Python Libraries in Global Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzZgOvXCyHHl",
    "tags": [
     "CELL_4"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels as sm\n",
    "import tensorflow as tf\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from xgboost import XGBRegressor, XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mount_storage_system (source = 'aws', path_to_store_imported_s3_bucket = '/', s3_bucket_name = None, s3_obj_key_prefix = None):\n",
    "    \n",
    "    # source = 'google' for mounting the google drive;\n",
    "    # source = 'aws' for mounting an AWS S3 bucket.\n",
    "    \n",
    "    # THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN source == 'aws'\n",
    "    \n",
    "    # path_to_store_imported_s3_bucket: path of the Python environment to which the\n",
    "    # S3 bucket contents will be imported. If it is None, or if \n",
    "    # path_to_store_imported_s3_bucket = '/', bucket will be imported to the root path. \n",
    "    # Alternatively, input the path as a string (in quotes). e.g. \n",
    "    # path_to_store_imported_s3_bucket = '/copied_s3_bucket'\n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # s3_obj_key_prefix = None. Keep it None or as an empty string (s3_obj_key_prefix = '')\n",
    "    # to import the whole bucket content, instead of a single object from it.\n",
    "    # Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "    # Suppose that your bucket (admin-created) has four objects with the following object \n",
    "    # keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "    # s3-dg.pdf. The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "    # at the root level of the bucket. If you open the Development/ folder, you see \n",
    "    # the Projects.xlsx object in it.\n",
    "    # Check Amazon documentation:\n",
    "    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\n",
    "    \n",
    "    # In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "    # where 'bucket' is the bucket's name, key_prefix = 'my_path/.../', without the\n",
    "    # 'file.csv' (file name with extension) last part.\n",
    "    \n",
    "    if (source == 'google'):\n",
    "        \n",
    "        from google.colab import drive\n",
    "        # Google Colab library must be imported only in case it is\n",
    "        # going to be used, for avoiding AWS compatibility issues.\n",
    "        \n",
    "        print(\"Associate the Python environment to your Google Drive account, and authorize the access in the opened window.\")\n",
    "        \n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        print(\"Now your Python environment is connected to your Google Drive: the root directory of your environment is now the root of your Google Drive.\")\n",
    "        print(\"In Google Colab, navigate to the folder icon (\\'Files\\') of the left navigation menu to find a specific folder or file in your Google Drive.\")\n",
    "        print(\"Click on the folder or file name and select the elipsis (...) icon on the right of the name to reveal the option \\'Copy path\\', which will give you the path to use as input for loading objects and files on your Python environment.\")\n",
    "        print(\"Caution: save your files into different directories of the Google Drive. If files are all saved in a same folder or directory, like the root path, they may not be accessible from your Python environment.\")\n",
    "        print(\"If you still cannot see the file after moving it to a different folder, reload the environment.\")\n",
    "    \n",
    "    elif (source == 'aws'):\n",
    "        # sagemaker and boto3 libraries must be imported only in case \n",
    "        # they are going to be used, for avoiding \n",
    "        # Google Colab compatibility issues.\n",
    "        \n",
    "        import sagemaker\n",
    "        # sagemaker is AWS SageMaker Python SDK\n",
    "        from sagemaker.session import Session\n",
    "        \n",
    "        # Notice: if you wanted to authenticate directly from Python code, you could use\n",
    "        # the following code, instead, to start the S3 client. boto3 is AWS S3 Python SDK:\n",
    "        \n",
    "        # import boto3\n",
    "        # ACCESS_KEY = 'access_key_ID'\n",
    "        # PASSWORD_KEY = 'password_key'\n",
    "        # s3_client = boto3.client('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = PASSWORD_KEY)\n",
    "        # ... [here, use the same following code until line new_session = Session()]\n",
    "        # [keep the line for session start. Substitute the line with the .download_data\n",
    "        # method by the following line:]\n",
    "        # s3_client.download_file(s3_bucket_name, s3_file_name_with_extension, path_to_store_imported_s3_bucket)\n",
    "        \n",
    "        # Check if the whole bucket will be downloaded (s3_obj_key_prefix = None):\n",
    "        if (s3_obj_key_prefix is None):\n",
    "            \n",
    "            s3_obj_key_prefix = ''\n",
    "        \n",
    "        # If the path to store is None, also import the bucket to the root path:\n",
    "        if (path_to_store_imported_s3_bucket is None):\n",
    "            \n",
    "            path_to_store_imported_s3_bucket = '/'\n",
    "        \n",
    "        # If the bucket name was provided, start the session. If not, print an error\n",
    "        # message:\n",
    "        \n",
    "        if (s3_bucket_name is None):\n",
    "            \n",
    "            print(\"Please, provide a valid S3 Bucket name to download from.\")\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            # start a new sagemaker session:\n",
    "\n",
    "            print(\"Starting a SageMaker session to be associated with the S3 bucket.\")\n",
    "\n",
    "            new_session = Session()\n",
    "            # Check sagemaker session class documentation:\n",
    "            # https://sagemaker.readthedocs.io/en/stable/api/utility/session.html\n",
    "            session.download_data(path = path_to_store_imported_s3_bucket, bucket = s3_bucket_name, key_prefix = s3_obj_key_prefix)\n",
    "\n",
    "            print(f\"S3 bucket contents successfully imported to path \\'{path_to_store_imported_s3_bucket}\\'.\")\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        print(\"Select a valid source: \\'google\\' for mounting Google Drive; or \\'aws\\' for accessing AWS S3 Bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for downloading a file from Google Colab or AWS S3 to the local machine or uploading a file from the machine to S3 or to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_or_upload_file (source = 'aws', action = 'download', object_to_download_from_colab = None, s3_bucket_name = None, local_path_of_storage = '/', file_name_with_extension = None):\n",
    "    \n",
    "    import os\n",
    "    \n",
    "    # source = 'google' for downloading from (or uploading to) Google Colab's instant memory;\n",
    "    # source = 'aws' for downloading from (or uploading to) an AWS S3 bucket.\n",
    "    \n",
    "    # action = 'download' to download the file to the local machine\n",
    "    # action = 'upload' to upload a file from local machine to AWS S3 or to\n",
    "    # Google Colab's instant memory\n",
    "    \n",
    "    # object_to_download_from_colab = None. This option has effect only when\n",
    "    # source == 'google'. In this case, this parameter is obbligatory. \n",
    "    # Declare as object_to_download_from_colab the object that you want to download.\n",
    "    # Since it is an object and not a string, it should not be declared in quotes.\n",
    "    # e.g. to download a dictionary named dict, object_to_download_from_colab = dict.\n",
    "    # To download a dataframe named df, declare object_to_download_from_colab = df.\n",
    "    # To export a model named keras_model, declare object_to_download_from_colab = keras_model\n",
    "    \n",
    "    ## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN source == 'aws'\n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # LOCAL_PATH_OF_STORAGE: path of the local computer environment \n",
    "    # to which the S3 bucket contents will be downloaded (ACTION == 'download'); or\n",
    "    # path of the folder containing the file that will be uploaded in S3 (ACTION = 'upload'). \n",
    "    # If it is None, or if LOCAL_PATH_OF_STORAGE = '/', files \n",
    "    # will be imported to the root path. Alternatively, input the path as a string \n",
    "    # (in quotes).\n",
    "    # Examples: LOCAL_PATH_OF_STORAGE = '/copied_s3_bucket'; \n",
    "    # LOCAL_PATH_OF_STORAGE = \"/My_folder\"; LOCAL_PATH_OF_STORAGE = \"/Users/Me/Documents/\"\n",
    "    # Notice that only the directories should be declared: do not include the file name and\n",
    "    # its extension.\n",
    "    \n",
    "    # file_name_with_extension: string, in quotes, containing the file name which will be\n",
    "    # downloaded from S3; or uploaded from S3, followed by its extension. \n",
    "    ## This parameter is obbligatory when source == 'aws'\n",
    "    # Examples:\n",
    "    # file_name_with_extension = 'Screen_Shot.png'; file_name_with_extension = 'dataset.csv',\n",
    "    # file_name_with_extension = \"dictionary.pkl\", file_name_with_extension = \"model.h5\",\n",
    "    # file_name_with_extension = 'doc.pdf', file_name_with_extension = 'model.dill'\n",
    "\n",
    "    if (source == 'google'):\n",
    "        \n",
    "        from google.colab import files\n",
    "        # google.colab library must be imported only in case \n",
    "        # it is going to be used, for avoiding \n",
    "        # AWS compatibility issues.\n",
    "        \n",
    "        if (action == 'upload'):\n",
    "            \n",
    "            print(\"Click on the button for file selection and select the files from your machine that will be uploaded in the Colab environment.\")\n",
    "            print(\"Warning: the files will be removed from Colab memory after the Kernel dies or after the notebook is closed.\")\n",
    "            # this functionality requires the previous declaration:\n",
    "            ## from google.colab import files\n",
    "            \n",
    "            colab_files_dict = files.upload()\n",
    "            \n",
    "            # The files are stored into a dictionary called colab_files_dict where the keys\n",
    "            # are the names of the files and the values are the files themselves.\n",
    "            ## e.g. if you upload a single file named \"dictionary.pkl\", the dictionary will be\n",
    "            ## colab_files_dict = {'dictionary.pkl': file}, where file is actually a big string\n",
    "            ## representing the contents of the file. The length of this value is the size of the\n",
    "            ## uploaded file, in bytes.\n",
    "            ## To access the file is like accessing a value from a dictionary: \n",
    "            ## d = {'key1': 'val1'}, d['key1'] == 'val1'\n",
    "            ## we simply declare the key inside brackets and quotes, the same way we would do for\n",
    "            ## accessing the column of a dataframe.\n",
    "            ## In this example, colab_files_dict['dictionary.pkl'] access the content of the \n",
    "            ## .pkl file, and len(colab_files_dict['dictionary.pkl']) is the size of the .pkl\n",
    "            ## file in bytes.\n",
    "            ## To check the dictionary keys, apply the method .keys() to the dictionary (with empty\n",
    "            ## parentheses): colab_files_dict.keys()\n",
    "            \n",
    "            for key in colab_files_dict.keys():\n",
    "                #loop through each element of the list of keys of the dictionary\n",
    "                # (list colab_files_dict.keys()). Each element is named 'key'\n",
    "                print(f\"User uploaded file {key} with length {len(colab_files_dict[key])} bytes.\")\n",
    "                # The key is the name of the file, and the length of the value\n",
    "                ## correspondent to the key is the file's size in bytes.\n",
    "                ## Notice that the content of the uploaded object must be passed \n",
    "                ## as argument for a proper function to be interpreted. \n",
    "                ## For instance, the content of a xlsx file should be passed as\n",
    "                ## argument for Pandas .read_excel function; the pkl file must be passed as\n",
    "                ## argument for pickle.\n",
    "                ## e.g., if you uploaded 'table.xlsx' and stored it into colab_files_dict you should\n",
    "                ## declare df = pd.read_excel(colab_files_dict['table.xlsx']) to obtain a dataframe\n",
    "                ## df from the uploaded table. Notice that is the value, not the key, that is the\n",
    "                ## argument.\n",
    "                \n",
    "                print(\"The uploaded files are stored into a dictionary object named as colab_files_dict.\")\n",
    "                print(\"Each key from this dictionary is the name of an uploaded file. The value correspondent to that key is the file itself.\")\n",
    "                print(\"The structure of a general Python dictionary is dict = {\\'key1\\': value1}. To access value1, declare file = dict[\\'key1\\'], as if you were accessing a column from a dataframe.\")\n",
    "                print(\"Then, if you uploaded a file named \\'table.xlsx\\', you can access this file as:\")\n",
    "                print(\"uploaded_file = colab_files_dict[\\'table.xlsx\\']\")\n",
    "                print(\"Notice, though, that the object uploaded_file is the whole file content, not a Python object already converted. To convert to a Python object, pass this element as argument for a proper function or method.\")\n",
    "                print(\"In this example, to convert the object uploaded_file to a dataframe, Pandas pd.read_excel function could be used. In the following line, a df dataframe object is obtained from the uploaded file:\")\n",
    "                print(\"df = pd.read_excel(uploaded_file)\")\n",
    "        \n",
    "        elif (action == 'download'):\n",
    "            \n",
    "            if (object_to_download_from_colab is None):\n",
    "                \n",
    "                #No object was declared\n",
    "                print(\"Please, inform an object to download. Since it is an object, not a string, it should not be declared in quotes.\")\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                print(\"The file will be downloaded to your computer.\")\n",
    "\n",
    "                files.download(object_to_download_from_colab)\n",
    "\n",
    "                print(f\"File {object_to_download_from_colab} successfully downloaded from Colab environment.\")\n",
    "\n",
    "        else:\n",
    "            \n",
    "            print(\"Please, select a valid action, download or upload.\")\n",
    "          \n",
    "    elif (source == 'aws'):\n",
    "        \n",
    "        import boto3\n",
    "        # boto3 is AWS S3 Python SDK\n",
    "        # sagemaker and boto3 libraries must be imported only in case \n",
    "        # they are going to be used, for avoiding \n",
    "        # Google Colab compatibility issues.\n",
    "        \n",
    "        # Notice: if you wanted to authenticate directly from Python code, you could use\n",
    "        # the following code, instead for starting the client:\n",
    "        \n",
    "        # ACCESS_KEY = 'access_key_ID'\n",
    "        # PASSWORD_KEY = 'password_key'\n",
    "        # s3_client = boto3.client('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = PASSWORD_KEY)\n",
    "        # Nextly, the code is the same.\n",
    "        \n",
    "        \n",
    "        # If the path to store is None, also import the bucket content to root path;\n",
    "        # or upload the file from root path to the bucket\n",
    "        if (local_path_of_storage is None):\n",
    "            \n",
    "            local_path_of_storage = '/'\n",
    "        \n",
    "        # If the bucket name was provided, start the session. If not, print an error\n",
    "        # message. The same for the file name with extension:\n",
    "        \n",
    "        if (s3_bucket_name is None):\n",
    "            \n",
    "            print(\"Please, provide a valid S3 Bucket name.\")\n",
    "        \n",
    "        elif (file_name_with_extension is None):\n",
    "            \n",
    "            print(\"Please, provide a valid file name with its extension. e.g. \\'dataset.csv\\'.\")\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # Obtain the full file path from which the file will be uploaded to S3; or to\n",
    "            # which the file will be downloaded from S3:\n",
    "            file_path = os.path.join(local_path_of_storage, file_name_with_extension)\n",
    "            \n",
    "            # Start S3 client:\n",
    "            s3_client = boto3.resource('s3')\n",
    "            \n",
    "            print(\"Starting AWS S3 client.\")\n",
    "            \n",
    "            if (action == 'upload'):\n",
    "                \n",
    "                s3_client.Object(s3_bucket_name, file_name_with_extension).\\\n",
    "                    upload_file(Filename = file_path)\n",
    "                \n",
    "                print(f\"File {file_name_with_extension} successfully uploaded to AWS S3 {s3_bucket_name} bucket.\")\n",
    "            \n",
    "            elif (action == 'download'):\n",
    "\n",
    "                print(\"The file will be downloaded to your computer.\")\n",
    "                \n",
    "                s3_client.Object(s3_bucket_name, file_name_with_extension).download_file(file_path)\n",
    "                \n",
    "                print(f\"File {file_name_with_extension} successfully downloaded from AWS S3 {s3_bucket_name} bucket.\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(\"Please, select a valid action, download or upload.\")\n",
    "\n",
    "    else:\n",
    "        \n",
    "        print(\"Select a valid source: \\'google\\' for mounting Google Drive; or \\'aws\\' for accessing AWS S3 Bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for loading the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe (file_directory_path, file_name_with_extension, load_txt_file_with_json_format = False, has_header = True, txt_csv_col_sep = \"comma\", sheet_to_load = None, json_record_path = None, json_field_separator = \"_\", json_metadata_prefix_list = None):\n",
    "    \n",
    "    import os\n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    \n",
    "    ## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, etc), \n",
    "    ## JSON, txt, or CSV (comma separated values) files.\n",
    "    \n",
    "    # file_directory_path - (string, in quotes): input the path of the directory (e.g. folder path) \n",
    "    # where the file is stored. e.g. file_directory_path = \"/\" or file_directory_path = \"/folder\"\n",
    "    \n",
    "    # file_name_with_extension - (string, in quotes): input the name of the file with the extension\n",
    "    # e.g. file_name_with_extension = \"file.xlsx\", or, file_name_with_extension = \"file.csv\"\n",
    "    \n",
    "    # load_txt_file_with_json_format = False. Set load_txt_file_with_json_format = True \n",
    "    # if you want to read a file with txt extension containing a text formatted as JSON \n",
    "    # (but not saved as JSON).\n",
    "    # WARNING: if load_txt_file_with_json_format = True, all the JSON file parameters of the \n",
    "    # function (below) must be set. If not, an error message will be raised.\n",
    "    \n",
    "    # has_header = True if the the imported table has headers (row with columns names).\n",
    "    # Alternatively, has_header = False if the dataframe does not have header.\n",
    "    \n",
    "    ## Parameters for loading txt or CSV files\n",
    "    \n",
    "    # txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "    # or 'csv'. It informs how the different columns are separated.\n",
    "    # Alternatively, txt_csv_col_sep = \"comma\" for columns separated by comma (\",\")\n",
    "    # txt_csv_col_sep = \"whitespace\" for columns separated by simple spaces (\" \").\n",
    "    \n",
    "    # sheet_to_load - This parameter has effect only when for Excel files.\n",
    "    # keep sheet_to_load = None not to specify a sheet of the file, so that the first sheet\n",
    "    # will be loaded.\n",
    "    # sheet_to_load may be an integer or an string (inside quotes). sheet_to_load = 0\n",
    "    # loads the first sheet (sheet with index 0); sheet_to_load = 1 loads the second sheet\n",
    "    # of the file (index 1); sheet_to_load = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "    # Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "    # name to load the sheet with that name.\n",
    "    \n",
    "    ## Parameters for loading JSON files:\n",
    "    \n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html#pandas.json_normalize\n",
    "    \n",
    "    # json_record_path (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "    # Path in each object to list of records. If not passed, data will be assumed to \n",
    "    # be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "    # dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "    # 'books' stores a nested JSON, declare, json_record_path = 'books'\n",
    "    \n",
    "    # json_field_separator = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "    # Nested records will generate names separated by sep. \n",
    "    # e.g., for json_field_separator = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "    # Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "    # the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "    # separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "    \n",
    "    # json_metadata_prefix_list: list of strings (in quotes). Manipulates the parameter \n",
    "    # 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "    # table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "    # will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "    \n",
    "    # e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "    # 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "    # Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "    # are 'name' and 'last'.\n",
    "    # Then, json_record_path = 'books'\n",
    "    # json_metadata_prefix_list = ['name', 'last']\n",
    "    \n",
    "    # Create the complete file path:\n",
    "    file_path = os.path.join(file_directory_path, file_name_with_extension)\n",
    "    # Extract the file extension\n",
    "    file_extension = os.path.splitext(file_path)[1][1:]\n",
    "    # os.path.splitext(file_path) is a tuple of strings: the first is the complete file\n",
    "    # root with no extension; the second is the extension starting with a point: '.txt'\n",
    "    # When we set os.path.splitext(file_path)[1], we are selecting the second element of\n",
    "    # the tuple. By selecting os.path.splitext(file_path)[1][1:], we are taking this string\n",
    "    # from the second character (index 1), eliminating the dot: 'txt'\n",
    "    \n",
    "    if ((file_extension == 'txt') | (file_extension == 'csv')): \n",
    "        # The operator & is equivalent to 'And' (intersection).\n",
    "        # The operator | is equivalent to 'Or' (union).\n",
    "        # pandas.read_csv method must be used.\n",
    "        if (load_txt_file_with_json_format == True):\n",
    "            \n",
    "            print(\"Reading a txt file containing JSON parsed data. A reading error will be raised if you did not set the JSON parameters.\")\n",
    "            \n",
    "            with open(file_path, 'r') as opened_file:\n",
    "                # 'r' stands for read mode; 'w' stands for write mode\n",
    "                # read the whole file as a string named 'file_full_text'\n",
    "                file_full_text = opened_file.read()\n",
    "                # if we used the readlines() method, we would be reading the\n",
    "                # file by line, not the whole text at once.\n",
    "                # https://stackoverflow.com/questions/8369219/how-to-read-a-text-file-into-a-string-variable-and-strip-newlines?msclkid=a772c37bbfe811ec9a314e3629df4e1e\n",
    "                # https://www.tutorialkart.com/python/python-read-file-as-string/#:~:text=example.py%20%E2%80%93%20Python%20Program.%20%23open%20text%20file%20in,and%20prints%20it%20to%20the%20standard%20output.%20Output.?msclkid=a7723a1abfe811ecb68bba01a2b85bd8\n",
    "                \n",
    "            #Now, file_full_text is a string containing the full content of the txt file.\n",
    "            json_file = json.loads(file_full_text)\n",
    "            # json.load() : This method is used to parse JSON from URL or file.\n",
    "            # json.loads(): This method is used to parse string with JSON content.\n",
    "            # e.g. .json.loads() must be used to read a string with JSON and convert it to a flat file\n",
    "            # like a dataframe.\n",
    "            # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.\n",
    "            dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "        \n",
    "        else:\n",
    "            # Not a JSON txt\n",
    "        \n",
    "            if (has_header == True):\n",
    "\n",
    "                if (txt_csv_col_sep == \"comma\"):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path)\n",
    "\n",
    "                elif (txt_csv_col_sep == \"whitespace\"):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, delim_whitespace = True)\n",
    "\n",
    "                else:\n",
    "                    print(f\"Enter a valid column separator for the {file_extension} file: \\'comma\\' or \\'whitespace\\'.\")\n",
    "\n",
    "            else:\n",
    "                # has_header == False\n",
    "\n",
    "                if (txt_csv_col_sep == \"comma\"):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, header = None)\n",
    "\n",
    "                elif (txt_csv_col_sep == \"whitespace\"):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, delim_whitespace = True, header = None)\n",
    "\n",
    "                else:\n",
    "                    print(f\"Enter a valid column separator for the {file_extension} file: \\'comma\\' or \\'whitespace\\'.\")\n",
    "\n",
    "    elif (file_extension == 'json'):\n",
    "        \n",
    "        with open(file_path) as opened_file:\n",
    "            \n",
    "            json_file = json.load(opened_file)\n",
    "            # The structure json_file = json.load(open(file_path)) relies on the GC to close the file. That's not a \n",
    "            # good idea: If someone doesn't use CPython the garbage collector might not be using refcounting (which \n",
    "            # collects unreferenced objects immediately) but e.g. collect garbage only after some time.\n",
    "            # Since file handles are closed when the associated object is garbage collected or closed \n",
    "            # explicitly (.close() or .__exit__() from a context manager) the file will remain open until \n",
    "            # the GC kicks in.\n",
    "            # Using 'with' ensures the file is closed as soon as the block is left - even if an exception \n",
    "            # happens inside that block, so it should always be preferred for any real application.\n",
    "            # source: https://stackoverflow.com/questions/39447362/equivalent-ways-to-json-load-a-file-in-python\n",
    "            \n",
    "        # json.load() : This method is used to parse JSON from URL or file.\n",
    "        # json.loads(): This method is used to parse string with JSON content.\n",
    "        # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.   \n",
    "        dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "    \n",
    "    else:\n",
    "        # If it is not neither a csv nor a txt file, let's assume it is one of different\n",
    "        # possible Excel files.\n",
    "        print(\"Excel file inferred. If an error message is shown, check if a valid file extension was used: \\'xlsx\\', \\'xls\\', etc.\")\n",
    "            \n",
    "        if (sheet_to_load is not None):        \n",
    "        #Case where the user specifies which sheet of the Excel file should be loaded.\n",
    "            \n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load)\n",
    "            \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load, header = None)\n",
    "        \n",
    "        else:\n",
    "            #No sheet specified\n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path)\n",
    "            \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, header = None)\n",
    "    \n",
    "    print(f\"Dataset extracted from {file_path}. Check the 10 first rows of this dataframe:\\n\")\n",
    "    print(dataset.head(10))\n",
    "    \n",
    "    return dataset   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for converting JSON object to dataframe**\n",
    "- Objects may be:\n",
    "    - String with JSON formatted text;\n",
    "    - List with nested dictionaries (JSON formatted);\n",
    "    - Dictionaries, possibly with nested dictionaries (JSON formatted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_obj_to_dataframe (json_obj_to_convert, json_record_path = None, json_field_separator = \"_\", json_metadata_prefix_list = None):\n",
    "    \n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    \n",
    "    # json_obj_to_convert: object containing JSON, or string with JSON content to parse.\n",
    "    # Objects may be: string with JSON formatted text;\n",
    "    # list with nested dictionaries (JSON formatted);\n",
    "    # dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "    \n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html#pandas.json_normalize\n",
    "    \n",
    "    # json_record_path (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "    # Path in each object to list of records. If not passed, data will be assumed to \n",
    "    # be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "    # dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "    # 'books' stores a nested JSON, declare, json_record_path = 'books'\n",
    "    \n",
    "    # json_field_separator = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "    # Nested records will generate names separated by sep. \n",
    "    # e.g., for json_field_separator = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "    # Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "    # the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "    # separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "    \n",
    "    # json_metadata_prefix_list: list of strings (in quotes). Manipulates the parameter \n",
    "    # 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "    # table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "    # will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "    \n",
    "    # e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "    # 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "    # Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "    # are 'name' and 'last'.\n",
    "    # Then, json_record_path = 'books'\n",
    "    # json_metadata_prefix_list = ['name', 'last']\n",
    "    \n",
    "    json_file = json.loads(json_obj_to_convert)\n",
    "    # json.load() : This method is used to parse JSON from URL or file.\n",
    "    # json.loads(): This method is used to parse string with JSON content.\n",
    "    # e.g. .json.loads() must be used to read a string with JSON and convert it to a flat file\n",
    "    # like a dataframe.\n",
    "    # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.\n",
    "    dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "    \n",
    "    print(f\"JSON object {json_obj_to_convert} converted to a flat dataframe object. Check the 10 first rows of this dataframe:\\n\")\n",
    "    print(dataset.head(10))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for importing or exporting models and dictionaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_export_model_or_dict (action = 'import', objects_manipulated = 'model_only', model_file_name = None, dictionary_file_name = None, directory_path = '/', model_type = 'keras', dict_to_export = None, model_to_export = None, use_colab_memory = False):\n",
    "    \n",
    "    import os\n",
    "    import pickel as pkl\n",
    "    import dill\n",
    "    import tensorflow as tf\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "    from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "    from xgboost import XGBRegressor, XGBClassifier\n",
    "    from statsmodels.tsa.arima.model import ARIMA, ARIMAResults\n",
    "    \n",
    "    # action = 'import' for importing a model and/or a dictionary;\n",
    "    # action = 'export' for exporting a model and/or a dictionary.\n",
    "    \n",
    "    # objects_manipulated = 'model_only' if only a model will be manipulated.\n",
    "    # objects_manipulated = 'dict_only' if only a dictionary will be manipulated.\n",
    "    # objects_manipulated = 'model_and_dict' if both a model and a dictionary will be\n",
    "    # manipulated.\n",
    "    \n",
    "    #model_file_name: string with the name of the file containing the model (for 'import');\n",
    "    # or of the name that the exported file will have (for 'export')\n",
    "    # e.g. model_file_name = 'model'\n",
    "    # WARNING: Do not add the file extension.\n",
    "    # Keep it in quotes. Keep model_file_name = None if no model will be manipulated.\n",
    "    \n",
    "    # dictionary_file_name: string with the name of the file containing the dictionary \n",
    "    # (for 'import');\n",
    "    # or of the name that the exported file will have (for 'export')\n",
    "    # e.g. dictionary_file_name = 'history_dict'\n",
    "    # WARNING: Do not add the file extension.\n",
    "    # Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no \n",
    "    # dictionary will be manipulated.\n",
    "    \n",
    "    # DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "    # or from which the model will be retrieved. If no value is provided,\n",
    "    # the DIRECTORY_PATH will be the root: \"/\"\n",
    "    # Notice that the model and the dictionary must be stored in the same path.\n",
    "    # If a model and a dictionary will be exported, they will be stored in the same\n",
    "    # DIRECTORY_PATH.\n",
    "    \n",
    "    # model_type: This parameter has effect only when a model will be manipulated.\n",
    "    # model_type: 'keras' for deep learning keras/ tensorflow models with extension .h5\n",
    "    # model_type = 'sklearn' for models from scikit-learn (non-deep learning)\n",
    "    # model_type = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "    # model_type = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "    # model_type = 'arima' for ARIMA model (Statsmodels)\n",
    "    \n",
    "    # dict_to_export and model_to_export: \n",
    "    # These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "    # must be declared. If ACTION == 'export', keep:\n",
    "    # dict_to_export = None, \n",
    "    # model_to_export = None\n",
    "    # If one of these objects will be exported, substitute None by the name of the object\n",
    "    # e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "    # model_to_export = keras_model. Notice that it must be declared without quotes, since\n",
    "    # it is not a string, but an object.\n",
    "    # For exporting a dictionary named as 'dict':\n",
    "    # dict_to_export = dict\n",
    "    \n",
    "    # use_colab_memory: this parameter has only effect when using Google Colab (or it will\n",
    "    # raise an error). Set as use_colab_memory = True if you want to use the instant memory\n",
    "    # from Google Colaboratory: you will update or download the file and it will be available\n",
    "    # only during the time when the kernel is running. It will be excluded when the kernel\n",
    "    # dies, for instance, when you close the notebook.\n",
    "    \n",
    "    # If action == 'export' and use_colab_memory == True, then the file will be downloaded\n",
    "    # to your computer (running the cell will start the download).\n",
    "    \n",
    "    # Check the directory path\n",
    "    if (directory_path is None):\n",
    "        # set as the root:\n",
    "        directory_path = \"/\"\n",
    "        \n",
    "        \n",
    "    bool_check1 = (objects_manipulated != 'model_only')\n",
    "    # bool_check1 == True if a dictionary will be manipulated\n",
    "    \n",
    "    bool_check2 = (objects_manipulated != 'dict_only')\n",
    "    # bool_check1 == True if a dictionary will be manipulated\n",
    "    \n",
    "    if (bool_check1 == True):\n",
    "        #manipulate a dictionary\n",
    "        \n",
    "        if (dictionary_file_name is None):\n",
    "            print(\"Please, enter a name for the dictionary.\")\n",
    "            return \"error1\"\n",
    "        \n",
    "        else:\n",
    "            # Create the file path for the dictionary:\n",
    "            dict_path = os.path.join(directory_path, dictionary_file_name)\n",
    "            # Extract the file extension\n",
    "            dict_extension = 'pkl'\n",
    "            #concatenate:\n",
    "            dict_path = dict_path + \".\" + dict_extension\n",
    "            \n",
    "    \n",
    "    if (bool_check2 == True):\n",
    "        #manipulate a model\n",
    "        \n",
    "        if (model_file_name is None):\n",
    "            print(\"Please, enter a name for the model.\")\n",
    "            return \"error1\"\n",
    "        \n",
    "        else:\n",
    "            # Create the file path for the dictionary:\n",
    "            model_path = os.path.join(directory_path, model_file_name)\n",
    "            # Extract the file extension\n",
    "            \n",
    "            #check model_type:\n",
    "            if (model_type == 'keras'):\n",
    "                model_extension = 'h5'\n",
    "            \n",
    "            elif (model_type == 'sklearn'):\n",
    "                model_extension = 'dill'\n",
    "                #it could be 'pkl', though\n",
    "            \n",
    "            elif (model_type == 'xgb_regressor'):\n",
    "                model_extension = 'json'\n",
    "                #it could be 'ubj', though\n",
    "            \n",
    "            elif (model_type == 'xgb_classifier'):\n",
    "                model_extension = 'json'\n",
    "                #it could be 'ubj', though\n",
    "            \n",
    "            elif (model_tyoe == 'arima'):\n",
    "                model_extension = 'pkl'\n",
    "            \n",
    "            else:\n",
    "                print(\"Enter a valid model_type: keras, sklearn_xgb, or arima.\")\n",
    "                return \"error2\"\n",
    "            \n",
    "            #concatenate:\n",
    "            model_path = model_path +  \".\" + model_extension\n",
    "            \n",
    "    # Now we have the full paths for the dictionary and for the model.\n",
    "    \n",
    "    if (action == 'import'):\n",
    "        \n",
    "        if (use_colab_memory == True):\n",
    "             \n",
    "            from google.colab import files\n",
    "            # google.colab library must be imported only in case \n",
    "            # it is going to be used, for avoiding \n",
    "            # AWS compatibility issues.\n",
    "            \n",
    "            print(\"Click on the button for file selection and select the files from your machine that will be uploaded in the Colab environment.\")\n",
    "            print(\"Warning: the files will be removed from Colab memory after the Kernel dies or after the notebook is closed.\")\n",
    "            # this functionality requires the previous declaration:\n",
    "            ## from google.colab import files\n",
    "            colab_files_dict = files.upload()\n",
    "            # The files are stored into a dictionary called colab_files_dict where the keys\n",
    "            # are the names of the files and the values are the files themselves.\n",
    "            ## e.g. if you upload a single file named \"dictionary.pkl\", the dictionary will be\n",
    "            ## colab_files_dict = {'dictionary.pkl': file}, where file is actually a big string\n",
    "            ## representing the contents of the file. The length of this value is the size of the\n",
    "            ## uploaded file, in bytes.\n",
    "            ## To access the file is like accessing a value from a dictionary: \n",
    "            ## d = {'key1': 'val1'}, d['key1'] == 'val1'\n",
    "            ## we simply declare the key inside brackets and quotes, the same way we would do for\n",
    "            ## accessing the column of a dataframe.\n",
    "            ## In this example, colab_files_dict['dictionary.pkl'] access the content of the \n",
    "            ## .pkl file, and len(colab_files_dict['dictionary.pkl']) is the size of the .pkl\n",
    "            ## file in bytes.\n",
    "            ## To check the dictionary keys, apply the method .keys() to the dictionary (with empty\n",
    "            ## parentheses): colab_files_dict.keys()\n",
    "            \n",
    "            for key in colab_files_dict.keys():\n",
    "                #loop through each element of the list of keys of the dictionary\n",
    "                # (list colab_files_dict.keys()). Each element is named 'key'\n",
    "                print(f\"User uploaded file {key} with length {len(colab_files_dict[key])} bytes.\")\n",
    "                # The key is the name of the file, and the length of the value\n",
    "                ## correspondent to the key is the file's size in bytes.\n",
    "                ## Notice that the content of the uploaded object must be passed \n",
    "                ## as argument for a proper function to be interpreted. \n",
    "                ## For instance, the content of a xlsx file should be passed as\n",
    "                ## argument for Pandas .read_excel function; the pkl file must be passed as\n",
    "                ## argument for pickle.\n",
    "                ## e.g., if you uploaded 'table.xlsx' and stored it into colab_files_dict you should\n",
    "                ## declare df = pd.read_excel(colab_files_dict['table.xlsx']) to obtain a dataframe\n",
    "                ## df from the uploaded table. Notice that is the value, not the key, that is the\n",
    "                ## argument.\n",
    "        \n",
    "        if (bool_check1 == True):\n",
    "            #manipulate a dictionary\n",
    "            if (use_colab_memory == True):\n",
    "                key = dictionary_file_name + \".\" + dict_extension\n",
    "                #Use the key to access the file content, and pass the file content\n",
    "                # to pickle:\n",
    "                with open(colab_files_dict[key], 'rb') as opened_file:\n",
    "            \n",
    "                    imported_dict = pkl.load(opened_file)\n",
    "                    # The structure imported_dict = pkl.load(open(colab_files_dict[key], 'rb')) relies \n",
    "                    # on the GC to close the file. That's not a good idea: If someone doesn't use \n",
    "                    # CPython the garbage collector might not be using refcounting (which collects \n",
    "                    # unreferenced objects immediately) but e.g. collect garbage only after some time.\n",
    "                    # Since file handles are closed when the associated object is garbage collected or \n",
    "                    # closed explicitly (.close() or .__exit__() from a context manager) the file \n",
    "                    # will remain open until the GC kicks in.\n",
    "                    # Using 'with' ensures the file is closed as soon as the block is left - even if \n",
    "                    # an exception happens inside that block, so it should always be preferred for any \n",
    "                    # real application.\n",
    "                    # source: https://stackoverflow.com/questions/39447362/equivalent-ways-to-json-load-a-file-in-python\n",
    "\n",
    "                print(f\"Dictionary {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "            else:\n",
    "                #standard method\n",
    "                with open(dict_path, 'rb') as opened_file:\n",
    "            \n",
    "                    imported_dict = pkl.load(opened_file)\n",
    "                \n",
    "                # 'rb' stands for read binary (read mode). For writing mode, 'wb', 'write binary'\n",
    "                print(f\"Dictionary successfully imported from {dict_path}.\")\n",
    "                \n",
    "        if (bool_chek2 == True):\n",
    "            #manipulate a model\n",
    "            # select the proper model\n",
    "        \n",
    "            if (model_type == 'keras'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = tf.keras.models.load_model(colab_files_dict[key])\n",
    "                    print(f\"Keras/TensorFlow model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # We previously declared:\n",
    "                    # from keras.models import load_model\n",
    "                    model = tf.keras.models.load_model(model_path)\n",
    "                    print(f\"Keras/TensorFlow model successfully imported from {model_path}.\")\n",
    "\n",
    "            elif (model_type == 'sklearn'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    \n",
    "                    with open(colab_files_dict[key], 'rb') as opened_file:\n",
    "            \n",
    "                        model = dill.load(opened_file)\n",
    "                    \n",
    "                    print(f\"Scikit-learn model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    with open(model_path, 'rb') as opened_file:\n",
    "            \n",
    "                        model = dill.load(opened_file)\n",
    "                \n",
    "                    print(f\"Scikit-learn model successfully imported from {model_path}.\")\n",
    "                    # For loading a pickle model:\n",
    "                    ## model = pkl.load(open(model_path, 'rb'))\n",
    "                    # 'rb' stands for read binary (read mode). For writing mode, 'wb', 'write binary'\n",
    "\n",
    "            elif (model_type == 'xgb_regressor'):\n",
    "                \n",
    "                # Create an instance (object) from the class XGBRegressor:\n",
    "                \n",
    "                model = XGBRegressor()\n",
    "                # Now we can apply the load_model method from this class:\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = model.load_model(colab_files_dict[key])\n",
    "                    print(f\"XGBoost regression model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model = model.load_model(model_path)\n",
    "                    print(f\"XGBoost regression model successfully imported from {model_path}.\")\n",
    "                    # model.load_model(\"model.json\") or model.load_model(\"model.ubj\")\n",
    "                    # .load_model is a method from xgboost object\n",
    "            \n",
    "             elif (model_type == 'xgb_classifier'):\n",
    "                \n",
    "                # Create an instance (object) from the class XGBClassifier:\n",
    "                \n",
    "                model = XGBClassifier()\n",
    "                # Now we can apply the load_model method from this class:\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = model.load_model(colab_files_dict[key])\n",
    "                    print(f\"XGBoost classification model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model = model.load_model(model_path)\n",
    "                    print(f\"XGBoost classification model successfully imported from {model_path}.\")\n",
    "                    # model.load_model(\"model.json\") or model.load_model(\"model.ubj\")\n",
    "                    # .load_model is a method from xgboost object\n",
    "\n",
    "            elif (model_type == 'arima'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = ARIMAResults.load(colab_files_dict[key])\n",
    "                    print(f\"ARIMA model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # We previously declared:\n",
    "                    # from statsmodels.tsa.arima.model import ARIMAResults\n",
    "                    model = ARIMAResults.load(model_path)\n",
    "                    print(f\"ARIMA model successfully imported from {model_path}.\")\n",
    "            \n",
    "            if (objects_manipulated == 'model_only'):\n",
    "                # only the model should be returned\n",
    "                return model\n",
    "            \n",
    "            elif (objects_manipulated == 'dict_only'):\n",
    "                # only the dictionary should be returned:\n",
    "                return imported_dict\n",
    "            \n",
    "            else:\n",
    "                # Both objects are returned:\n",
    "                return model, imported_dict\n",
    "\n",
    "    \n",
    "    elif (action == 'export'):\n",
    "        \n",
    "        #Let's export the models or dictionary:\n",
    "        if (use_colab_memory == True):\n",
    "            \n",
    "            from google.colab import files\n",
    "            # google.colab library must be imported only in case \n",
    "            # it is going to be used, for avoiding \n",
    "            # AWS compatibility issues.\n",
    "            \n",
    "            print(\"The files will be downloaded to your computer.\")\n",
    "        \n",
    "        if (bool_check1 == True):\n",
    "            #manipulate a dictionary\n",
    "            if (use_colab_memory == True):\n",
    "                ## Download the dictionary\n",
    "                key = dictionary_file_name + \".\" + dict_extension\n",
    "                \n",
    "                with open(key, 'wb') as opened_file:\n",
    "            \n",
    "                    pkl.dump(dict_to_export, opened_file)\n",
    "                \n",
    "                # this functionality requires the previous declaration:\n",
    "                ## from google.colab import files\n",
    "                files.download(key)\n",
    "                \n",
    "                print(f\"Dictionary {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "            else:\n",
    "                #standard method \n",
    "                with open(dict_path, 'wb') as opened_file:\n",
    "            \n",
    "                    pkl.dump(dict_to_export, opened_file)\n",
    "                \n",
    "                #to save the file, the mode must be set as 'wb' (write binary)\n",
    "                print(f\"Dictionary successfully exported as {dict_path}.\")\n",
    "                \n",
    "        if (bool_chek2 == True):\n",
    "            #manipulate a model\n",
    "            # select the proper model\n",
    "        \n",
    "            if (model_type == 'keras'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"Keras/TensorFlow model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save(model_path)\n",
    "                    print(f\"Keras/TensorFlow model successfully exported as {model_path}.\")\n",
    "\n",
    "            elif (model_type == 'sklearn'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    \n",
    "                    with open(key, 'wb') as opened_file:\n",
    "\n",
    "                        dill.dump(model_to_export, opened_file)\n",
    "                    \n",
    "                    #to save the file, the mode must be set as 'wb' (write binary)\n",
    "                    files.download(key)\n",
    "                    print(f\"Scikit-learn model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    with open(model_path, 'wb') as opened_file:\n",
    "\n",
    "                        dill.dump(model_to_export, opened_file)\n",
    "                    \n",
    "                    print(f\"Scikit-learn model successfully exported as {model_path}.\")\n",
    "                    # For exporting a pickle model:\n",
    "                    ## pkl.dump(model_to_export, open(model_path, 'wb'))\n",
    "            \n",
    "            elif ((model_type == 'xgb_regressor')|(model_type == 'xgb_classifier')):\n",
    "                # In both cases, the XGBoost object is already loaded in global\n",
    "                # context memory. So there is already the object for using the\n",
    "                # save_model method, available for both classes (XGBRegressor and\n",
    "                # XGBClassifier).\n",
    "                # We can simply check if it is one type OR the other, since the\n",
    "                # method is the same:\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save_model(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"XGBoost model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save_model(model_path)\n",
    "                    print(f\"XGBoost model successfully exported as {model_path}.\")\n",
    "                    # For exporting a pickle model:\n",
    "                    ## pkl.dump(model_to_export, open(model_path, 'wb'))\n",
    "            \n",
    "            elif (model_type == 'arima'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"ARIMA model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save(model_path)\n",
    "                    print(f\"ARIMA model successfully exported as {model_path}.\")\n",
    "        \n",
    "        print(\"Export of files completed.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Enter a valid action, import or export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "58273fe5-d381-4692-841e-2bec3ca3fd5e",
    "id": "gf8iYwZh05-1"
   },
   "source": [
    "# **Function for converting the datasets into NumPy arrays with correct format for CNN and RNN Architectures**\n",
    "- These architectures require the conversion of the dataset to NumPy arrays with specific shapes. \n",
    "    - Use this function for converting the dataset (or list with prediction parameters) to the correct formats before feeding the deep learning models.\n",
    "    - This function must be called before the train-test splitting: pass the arrays obtained from this function to the train-test splitting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "azdata_cell_guid": "94af2682-4b0d-46c7-b75d-9278359e927e",
    "id": "V6GCBlmF05-1"
   },
   "outputs": [],
   "source": [
    "def convert_dataset_into_numpy_arrays (input_data, arrays_for = 'training', architecture_to_be_fed_with_returned_arrays = 'cnn'):\n",
    "    \n",
    "    #WARNING: PASS ONLY THE DESIRED COLUMNS AND KEEP THE RESPONSE VARIABLE AS THE LAST COLUMN\n",
    "    #OF DATAFRAME df\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # input_data is a dataframe or a list passed as input.\n",
    "    \n",
    "    # When making a single-entry prediction:\n",
    "    # It is equivalent to pass a list or to pass a dataframe with \n",
    "    # a single row. In case of passing a list, each element of the list \n",
    "    # should correspond to one variable, in the same order of columns\n",
    "    # of the dataframe used for training. For instance:\n",
    "    # if the dataframe has 3 columns (predictive variables):\n",
    "    # 'col1', 'col2', 'col3', the list should have 3 elements: \n",
    "    # input_data = [val1, val2, val3]. The first\n",
    "    # element val1 is the value for the variable 'col1', val2 corresponds\n",
    "    # to 'col2', and val3 corresponds to 'col3'. Then, the X_array output\n",
    "    # from this function can be input on the model for predicting the\n",
    "    # output for the combination val1, val2, val3.\n",
    "    # Example: input_data = [1.0, 2.3, 7] (point is the decimal separator)\n",
    "    \n",
    "    # arrays_for = 'training' should be used when the generated arrays\n",
    "    # will be used for training. In this case, the last column of the\n",
    "    # input dataset must be the response variable (the labels); and only\n",
    "    # the features selected as predictors should be on the other columns.\n",
    "    \n",
    "    # arrays_for = 'prediction': use this parameter if you are going to\n",
    "    # pass the arrays to the model to obtain a prediction for them.\n",
    "    # In this case, there is no response, so all values will be interpreted\n",
    "    # as belonging to a predictive feature.\n",
    "    \n",
    "    # ARCHITECTURE_TO_BE_FED_WITH_RETURNED_ARRAYS (string) = 'cnn', \n",
    "    # 'lstm', 'encoder_decoder', or 'cnn_ltsm', depending on the model \n",
    "    # that will be fed with the arrays.\n",
    "    # Notice that LSTM and CNN architectures are fed with the same format of\n",
    "    # arrays, so ARCHITECTURE_TO_BE_FED_WITH_RETURNED_ARRAYS = 'lstm'\n",
    "    # and ARCHITECTURE_TO_BE_FED_WITH_RETURNED_ARRAYS = 'cnn' return the same\n",
    "    # results.\n",
    "    \n",
    "    \n",
    "    # CONVERSION OF THE DATASET INTO NUMPY ARRAYS FOR DEEP LEARNING MODELS:\n",
    "    # - Step needed for adapting techniques of image classification (convolutions) and\n",
    "    # text classification (RNNs like LSTMs or GRUs) for structured (tabular) data.\n",
    "    \n",
    "    # X = Subset containing only the predictive variables (columns);\n",
    "    # X contains N rows (N entries) and M columns.\n",
    "    # y = series containing the response variable. y is a single column with N values\n",
    "    # (N rows or entries).\n",
    "    \n",
    "    # 1. We must convert the dataset into NumPy arrays.\n",
    "    # - The dataframe X is converted into a big array X_array.\n",
    "    # - Each row of the original dataset X becomes an element of the array X_array.\n",
    "    #   - X_array is an array of arrays: each element from X_array is itself an array.\n",
    "    #   - Since each row becomes one array, X_array will be an array with N elements, one per row.\n",
    "    #   For a given element (a given array nested in X_array):\n",
    "    #     - Each element on the nested array correspond to one column of the original dataset.\n",
    "    #     - In other words, we pick a row from the dataset X and save it as a separate array.\n",
    "    #     - Then, we append this separate array as a new element of X_array.\n",
    "    #     - Since the nested array is simply one row from X, it contains M values, one per column.\n",
    "    #     - Also, the values are in the same order as the columns from X, since it is simply a copy.\n",
    "    #    Each nested array is a sequence that will be read by the deep learning algorithm.\n",
    "    \n",
    "    # 2. We must convert y to an array of arrays. But this array is simpler: since there is a single\n",
    "    # response, each array contains a single value, i.e., the response variable for a given row.\n",
    "    # There will be N arrays with single value, since there are N rows.\n",
    "    \n",
    "    # Finally, for the LSTM and CNN, the arrays are reshaped as:\n",
    "    # (X_array.shape[0], X_array.shape[1], 1)\n",
    "    # (y_array.shape[0], 1)\n",
    "    \n",
    "    # For the Encoder-Decoder LSTM architecture, though, the shape final shape of the y_arrays\n",
    "    # must be the same as the shape for X_array (i.e., it must have 3 dimensions). So, in this\n",
    "    # case, X_array is still reshaped as:\n",
    "    # (X_array.shape[0], X_array.shape[1], 1)\n",
    "    # but y_array suffers a second reshaped to have 3 dimensions too:\n",
    "    # Firstly (y_array.shape[0], 1), and finally:\n",
    "    # (y_array.shape[0], y_array.shape[1], 1)\n",
    "    \n",
    "    # For the CNN-LSTM architecture, the X_array is a bit different, because it involves another\n",
    "    # modification:\n",
    "    # Basically, when using a hybrid CNN-LSTM model, we will further divide each \n",
    "    # sample into further subsequences. The CNN model will interpret each sub-sequence \n",
    "    # and the LSTM will piece together the interpretations from the subsequences. \n",
    "    # As such, we will split each sample into 2 subsequences of 2 times per subsequence.\n",
    "    # - We further divide each sample X_array into further subsequences, as previously mentioned.\n",
    "    # - We split each sample into 2 subsequences of 2 times per subsequence.\n",
    "    # - So, for a total of M entries = X_train.shape[0] (entries of the original dataset), \n",
    "    # the data must now be converted into arrays of the following format before feeding the model: \n",
    "    #   [X.shape[0], 2, 2, 1]\n",
    "    # Here, y_arrays are the same used in the LSTM and CNN architectures, without a second reshape.\n",
    "    # Then, y_array is simply reshaped as:\n",
    "    # (y_array.shape[0], 1)\n",
    "\n",
    "    \n",
    "    # Check if a valid architecture was selected: \n",
    "    if (architecture_to_be_fed_with_returned_arrays == 'lstm')\n",
    "        print(\"Preparing arrays for the Simplified Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN) Architecture.\\n\")\n",
    "    \n",
    "    elif (architecture_to_be_fed_with_returned_arrays == 'cnn')\n",
    "        print(\"Preparing arrays for the Convolutional Neural Network (CNN) Architecture.\\n\")\n",
    "    \n",
    "    elif (architecture_to_be_fed_with_returned_arrays == 'cnn_ltsm')\n",
    "        print(\"Preparing arrays for the CNN-LSTM Hybrid Architecture.\\n\")\n",
    "    \n",
    "    elif (architecture_to_be_fed_with_returned_arrays == 'encoder_decoder')\n",
    "        print(\"Preparing arrays for the Encoder-Decoder Recurrent Neural Network (RNN) Architecture.\\n\")\n",
    "   \n",
    "    else:\n",
    "        print(\"Please, input a valid architecture: \\'lstm\\', \\'cnn\\', \\'cnn_ltsm\\', or \\'encoder_decoder\\'.\")\n",
    "        return \"error\"\n",
    "    \n",
    "    \n",
    "    boolean_check = (arrays_for == 'training')\n",
    "    # The steps regarding the manipulation of the y-array will only\n",
    "    # take place when boolean_check is True\n",
    "    \n",
    "    # Check input data type:\n",
    "    # Notice that the data type is an object or a type (special word).\n",
    "    # Then, it should not be declared in quotes\n",
    "    if (type(input_data) == list):\n",
    "        input_data_type = 'list'\n",
    "        \n",
    "    else:\n",
    "        # It is a dataframe\n",
    "        input_data_type = 'dataframe'\n",
    "    \n",
    "        print(\"Converting the dataframe to the array format required by CNNs and RNNs.\\n\")\n",
    "        print(\"WARNING: This function should be used for modelling a single response variable.\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        if (boolean_check): # arrays for training\n",
    "            \n",
    "            print(\"Before calling this function, make sure that the response variable is the last column of the dataframe passed as input.\")\n",
    "            print(\"The other columns should contain only the features selected as predictors.\")\n",
    "            print(\"\\n\")\n",
    "            print(\"arrays_for = \\'training\\' - two arrays will be returned: the array with the predictors and the array with the responses.\")\n",
    "            print(\"Notice, though, that the last column will always be interpreted as the response, whereas the others will be interpreted as predictors\")\n",
    "            print(\"\\n\")\n",
    "    \n",
    "    if (arrays_for == 'prediction'):\n",
    "        \n",
    "        print(\"arrays_for = \\'prediction\\' - a single array will be returned: this array must be used as input of the model, to obtain the response.\")\n",
    "        print(\"In this case, all the columns or values input into this function should be representative of predictive features.\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    print(\"WARNING: You must call this function before the train-test splitting: pass the output arrays to the function destined to splitting data into train and test sets.\")\n",
    "    print(\"It is equivalent to pass a list declaring input_data_type = \\'list\\' or to pass a single-row dataframe declaring input_data_type = \\'dataframe\\'.\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    if (input_data_type == 'list'):\n",
    "        \n",
    "        print(\"Input data interpreted as Python list.\")\n",
    "        # Let's convert a single row for making the prediction\n",
    "        \n",
    "        # 1. Let's create a list of columns:\n",
    "        cols_list = []\n",
    "        \n",
    "        for i in range(len(input_data)):\n",
    "            # goes from i = 0 to i = len(input_data) - 1, index of\n",
    "            # the last element of the list:\n",
    "            cols_list.append((\"col_\" + str(i)))\n",
    "            # The addition of i guarantees that all columns have different\n",
    "            # names, so we can create the dataframe\n",
    "        \n",
    "        # Create a dictionary where the cols are the keys and the list\n",
    "        # values are the values. Since cols_list was created by\n",
    "        # looping through the first list, they have same size.\n",
    "        pred_dict = {cols_list: input_data}\n",
    "        \n",
    "        # Now, convert it to a dataframe:\n",
    "        # This dataframe contains a single row. That is why it is\n",
    "        # equivalent to input a single-row dataframe\n",
    "        df = pd.DataFrame(data = pred_dict)\n",
    "        print(\"The list was converted to a single-row Pandas dataframe before processing.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Input data interpreted as a Pandas dataframe.\")\n",
    "        # simply copy input_data into df:\n",
    "        df = input_data\n",
    "    \n",
    "    #Check number of rows and columns of dataframe df\n",
    "    #df.shape[0] is the number of rows, whereas df.shape[1] \n",
    "    #is the number of columns of dataframe df\n",
    "    num_rows = df.shape[0]\n",
    "    num_columns = df.shape[1]\n",
    "    \n",
    "    # Save the following values for comparison after obtaining the\n",
    "    # NumPy arrays too\n",
    "    \n",
    "    # Slice a dataframe: df[i:j]\n",
    "    # Slice the dataframe, getting only row i to row (j-1)\n",
    "    # Indexing naturally starts from 0\n",
    "    # Notice that the slicer defined as df[i:j] takes all columns from\n",
    "    # the dataframe: it copies the dataframe structure (columns), but\n",
    "    # selects only the specified rows.\n",
    "    original_first_row = df[0:1]\n",
    "    # This is equivalent to df[:1] - if there is no start for the\n",
    "    # slicer, the start from 0 is implicit\n",
    "    # slice: get rows from row 0 to row (1-1) = 0\n",
    "    # Therefore, we will obtain a copy of the dataframe, but containing\n",
    "    # only the first row (row 0)\n",
    "    original_last_row = df[(num_rows - 1):(num_rows)] \n",
    "    # slice the dataframe from row (num_rows - 1), the index of the\n",
    "    # last row, to row (num_rows) - 1 = (num_rows - 1)\n",
    "    # Therefore, this slicer is a copy of the dataframe but containing\n",
    "    # only its last row.\n",
    "    \n",
    "    # Slices are (fractions of) pandas dataframes, so elements must be\n",
    "    # accessed through .iloc or .loc method\n",
    "    \n",
    "    # Let's get the first and last elements from the first column:\n",
    "    # They have the index 0 (first index)\n",
    "    first_element_first_col = original_first_row.iloc[0,0]\n",
    "    last_element_first_col = original_last_row.iloc[0,0]\n",
    "    \n",
    "    if (boolean_check):\n",
    "        \n",
    "        #store the first and last response (last column). \n",
    "        # The index of the last column is (num_columns - 1) \n",
    "        # since it starts from zero:\n",
    "        first_element_last_col = original_first_row.iloc[0,(num_columns - 1)]\n",
    "        last_element_last_col = original_last_row.iloc[0,(num_columns - 1)]\n",
    "    \n",
    "\n",
    "    # Now, let's start the data conversion:\n",
    "    # Notice that the steps regarding y, the response variables, only\n",
    "    # take place when the value of the boolean boolean_check is True:\n",
    "    \n",
    "    #Initialize the arrays as empty NumPy arrays:\n",
    "    X = np.array([])\n",
    "    \n",
    "    if (boolean_check): # implicity that only when == True\n",
    "        y = np.array([])\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        \n",
    "        #for loop may be declared as for i in range(N, M).\n",
    "        #In this case, i goes from i = N to i = (M-1).\n",
    "        #Also, for loop may be declared as for i in range (M)\n",
    "        #In this case, i goes from i = 0 to i = (M-1).\n",
    "        #Since the first element was not declared, i goes from zero (first index) to (num_rows-1),\n",
    "        #which is the last index possible for the rows.\n",
    "        #At the end of each loop, i = i + 1 (automatically)\n",
    "        \n",
    "        #Start the lists that will store the attributes/variables' values for that row (x_list);\n",
    "        #and the response variable for that row (y_list).\n",
    "        #Start as empty lists\n",
    "        \n",
    "        x_list = []\n",
    "        \n",
    "        if (boolean_check):\n",
    "            y_list = []\n",
    "        \n",
    "        #loop through each column, appending each value of the variables as a new element of the list\n",
    "        \n",
    "        for j in range((num_columns)-1):\n",
    "            \n",
    "            #j goes from column j = 0 (first column) to column j = (num_columns-2), index of last column\n",
    "            #prior to the response variable\n",
    "            \n",
    "            #NOTICE: RESPONSE VARIABLE MUST BE THE LAST COLUMN\n",
    "            x_list.append(df.iloc[i,j])\n",
    "            #append element of row i and column j\n",
    "        \n",
    "        if (boolean_check):\n",
    "            \n",
    "            y_list.append(df.iloc[i,((num_columns)-1)])\n",
    "            #Append only the value of the response variable\n",
    "            #If you put this command inside the second for loop, one y will be added for each j, so the list will get\n",
    "            #a bunch of equal responses (one response for each column j, instead of a single value)   \n",
    "\n",
    "            #Concatenate the y_list as elements of the NumPy arrays:\n",
    "            #y_array must be in the form array([y1,y2,...,yn])\n",
    "            #i.e., a single array, with all elements in sequence\n",
    "            #To do so, we concatenate the array with the list.\n",
    "            #CONCATENATE method appends a list or a Numpy array in the right to the end of a numpy array in the left,\n",
    "            #forming a single array of elements.\n",
    "            y = np.concatenate((y, (y_list)), axis = 0)\n",
    "\n",
    "            #Theoretically, we could use the method numpy.stack to stack all of the arrays X.\n",
    "            #We want X_array to be an array of arrays/lists, i.e., each element of the array is a list itself,\n",
    "            #obtaining the format [[x1,..., xn],...,[x1,...xn]] - 1 array for each row, and each individual array\n",
    "            #containing a number of elements equals to the number of columns.\n",
    "\n",
    "        #The problem is that the NumPy.stack demmands all stacked arrays to have the same dimension. So, we \n",
    "        #would have to firstly store all the lists and then stack then in a single time.\n",
    "        #The best method is to concatenate and use NUMPY.split to split the biggest array into several smaller\n",
    "        #arrays.\n",
    "        X = np.concatenate((X, (x_list)), axis = 0)\n",
    "        \n",
    "    \n",
    "    #Now X array is a single array containing num_rows x num_columns elements\n",
    "    \n",
    "    X = np.split(X, num_rows)\n",
    "    #np.split(array, N) splits array into an array containing N sub-arrays;\n",
    "    #in this case, N = number of rows = total elements of the array y_array: we want one array for each row.\n",
    "    \n",
    "    #Now, each row corresponds to one element of the array y_array (the response), and one sub-array from\n",
    "    #X_array. Each element of this sub-array contains the value of one column on the original dataset.\n",
    "    \n",
    "    X_array = np.array(X)\n",
    "    \n",
    "    if (boolean_check):\n",
    "        \n",
    "        y_array = np.array(y)\n",
    "    \n",
    "    #Number of elements in each list (each sub-array from X_array):\n",
    "    num_elements_in_each_X_array = len(x_list)\n",
    "    #must be equals to the number of features\n",
    "    print(\"\\n\")\n",
    "    print(\"Successfully converted the data to the array format needed for the CNNs and RNNs.\")\n",
    "    print(\"\\n\")\n",
    "    print(\"Now the data is in the same format as the datasets used for language processing: in these datasets, each row represents a sentence.\")\n",
    "    print(\"In turns, the sentences are split into tokens, which may be the words or punctuation. So, each entry (row) of the dataset corresponds to a sentence, and the value for each column is a token.\")\n",
    "    print(\"Keras demands sequences with equal sizes, independently of the problem: text processing, image processing, etc.\")\n",
    "    print(\"As consequence, the text sequences must be padded, i.e., broke or completed, so that all sequences have same number of tokens. In case of images, the axes lengths must be constant, so that some images may get cropped, and others may have to be filled.\")\n",
    "    print(\"\\n\")\n",
    "    print(\"Here, the sequences are also the rows, and the number of elements of the sequence is the number of columns itself: instead of tokens, we have the values for each variable as the columns, took here as the sequences elements. Naturally, all entries must have the same sequence size, in this case, the same number of columns (attributes).\")\n",
    "    print(\"The total of loops performed for an RNN is the number of elements from each sequence: in word processing: it is the number of tokens; in time series analyzes, it is the number of variables (columns).\")\n",
    "    print(\"e.g. if all sequences have 100 tokens, then the RNN will loop 100 times. If your time series is described by 10 features, the RNN will loop 10 times.\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Now, let\\'s compare the properties and the first and last elements of the arrays and of the original dataframe to check if the array generation did not resulted in an error.\")\n",
    "    print(f\"Original dataframe shape = {df.shape}\")\n",
    "    print(\"\\n\")\n",
    "             \n",
    "    print(\"The generated X-arrays should have one element for each feature on the dataset.\")\n",
    "    \n",
    "    if (boolean_check):\n",
    "        \n",
    "        # Remove 1 (the response column) to get the total of predictive\n",
    "        # variables columns:\n",
    "        print(f\"Original number of column features = {num_columns - 1}\")\n",
    "        print(f\"Number of X-arrays = {num_elements_in_each_X_array}\")\n",
    "        \n",
    "        if (num_elements_in_each_X_array == (num_columns - 1)):\n",
    "            print(\"The X-arrays indeed have one element per predictive variable.\")\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            print(\"WARNING: review the input data passed: the function generated X-arrays with number of elements different from the original number of predictive variables columns.\")\n",
    "            print(\"\\n\")      \n",
    "    \n",
    "    else:\n",
    "        # Do not remove one column. All columns from the original dataset\n",
    "        # are from predictive features\n",
    "        print(f\"Original number of column features (all of the columns) = {num_columns}\")\n",
    "        print(f\"Number of X-arrays = {num_elements_in_each_X_array}\")\n",
    "        \n",
    "        if (num_elements_in_each_X_array == (num_columns)):\n",
    "            print(\"The X-arrays indeed have one element per predictive variable.\")\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            print(\"WARNING: review the input data passed: the function generated X-arrays with number of elements different from the original number of predictive variables columns.\")\n",
    "            print(\"\\n\") \n",
    "    \n",
    "    if (boolean_check):\n",
    "        # Test only when there is an y-array\n",
    "        print(\"The total of X-arrays must be equal to the total of y-arrays:\")\n",
    "        print(f\"Total of X-arrays = {X_array.shape[0]}\")\n",
    "        # X_array.shape is a tuple (N, M), where\n",
    "        # M = total of arrays; N = total of elements in each array = \n",
    "        # num_elements_in_each_X_array (length of the list used to\n",
    "        # generate the array). Then, here we want the first element\n",
    "        # from the tuple:\n",
    "        print(f\"Total of y-arrays = {len(y_array)}\")\n",
    "        # y_array.shape is a tuple (M,), with second position empty\n",
    "        # so we can simply pick the length of the array\n",
    "        \n",
    "        if (X_array.shape[0] == len(y_array)):\n",
    "            print(\"The number of X-arrays and y-arrays are indeed equal.\")\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            print(\"WARNING: review the input data passed: the function generated a total of X-arrays different from the total of y-arrays.\")\n",
    "            print(\"\\n\")\n",
    "    \n",
    "    # Test for both cases:\n",
    "    print(\"The total of X-arrays must be equal to the number of rows of the original dataset: each row (entry) must have been converted into an array.\")\n",
    "    print(f\"Total of X-arrays = {X_array.shape[0]}\")\n",
    "    print(f\"Original number of rows = {num_rows}\")\n",
    "    \n",
    "    if (X_array.shape[0] == num_rows):\n",
    "        print(\"There is indeed one array per row of the original dataset.\")\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(\"WARNING: review the input data passed: the function generated a total of X-arrays different from the total of rows of the original dataset.\")\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    if (boolean_check):\n",
    "        # Test only when there is an y-array\n",
    "        print(\"The first y-array must store the first element from the response column; whereas the last y-array must store the last element of the response column.\")\n",
    "        print(f\"1st element from the response column of the dataset = {first_element_last_col}\")\n",
    "        print(f\"1st y-array = {y_array[0]}\")\n",
    "        \n",
    "        if (y_array[0] == first_element_last_col):\n",
    "            print(\"1st element from the response column was correctly stored in the first y-array.\")\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            print(\"WARNING: review the input data passed: the function generated a 1st y-array different from the 1st element from the response variable column of the dataset.\")\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        print(f\"Last element from the response column of the dataset = {last_element_last_col}\")\n",
    "        print(f\"Last y-array = {y_array[(len(y_array) - 1)]}\")\n",
    "        # there are len(y_array) arrays in total, so the last index is \n",
    "        # len(y_array) - 1\n",
    "       \n",
    "        if (y_array[(len(y_array) - 1)] == last_element_last_col):\n",
    "            print(\"Last element from the response column was correctly stored in the last y-array.\")\n",
    "            print(\"\\n\")\n",
    "        else:\n",
    "            print(\"WARNING: review the input data passed: the function generated a last y-array different from the last element from the response variable column of the dataset.\")\n",
    "            print(\"\\n\")\n",
    "            \n",
    "    # X_array is an array of arrays:\n",
    "    # Each row from the original dataset was converted into a sequence, \n",
    "    # i.e, into a separated array;\n",
    "    # Each of these sequences (arrays) was stored as an element of the\n",
    "    # bigger array named X_array.\n",
    "    \n",
    "    # So, if we have N predictive features, \n",
    "    # and M rows in the dataset, we have now an array like:\n",
    "    \n",
    "    # X_array = array([[val_0_0, val_1_0, ..., val_(N-1)_0],\n",
    "                    # [val_0_1, val_1_1, ..., val_(N-1)_1],\n",
    "                    # ...\n",
    "                    # [val_0_(M-1), val_1_(M-1), ..., val_(N-1)_(M-1)]\n",
    "                    # ])\n",
    "                    \n",
    "    # where val_0_0 is the value of the first column on the first\n",
    "    # entry (row 0),..., val_(N-1)_0 is the value of the last column\n",
    "    # (column N-1) for row 0, ..., val_0_i is the value of the 1st\n",
    "    # element for row i (the row stored as the i-th array), \n",
    "    # val_j_i is the value for column j, row i,..., and \n",
    "    # val_(N-1)_(M-1) is the value for the last column (N-1) and last\n",
    "    # row (M-1)\n",
    "    \n",
    "    # If we get the first array from X_array, we would have:\n",
    "    # X_array[0] = array([[val_0_0, val_1_0, ..., val_(N-1)_0]])\n",
    "    # That is off course the 1st row from the original dataset.\n",
    "    \n",
    "    # So, the row i from the original dataset would be accessed as:\n",
    "    # X_array[i] = array([[val_0_i, val_1_i, ..., val_(N-1)_i]])\n",
    "    \n",
    "    # On the other hand, each element from the array is also indexed.\n",
    "    # For instance: X_array[i][0] = val_0_i\n",
    "    # and the element correspondent to column j, row i is accessed as:\n",
    "    # X_array[i][j] = val_j_i\n",
    "    \n",
    "    # If val_j_i was also an array, it would be also indexed, and the\n",
    "    # element on the index k would be accessed as X_array[i][j][k]\n",
    "    \n",
    "    # Therefore, simply put more brackets to index successive dimensions\n",
    "    # Here we have two dimensions (one array into another), so we must\n",
    "    # index two indices.\n",
    "    \n",
    "    \n",
    "    # Check if the first element from the first X-array corresponds\n",
    "    # to the 1st element of the 1st column of the dataset:\n",
    "    print(\"The 1st element stored in the first X-array must correspond to the element on the 1st column and 1st row of the dataset.\")\n",
    "    print(f\"Element on the 1st row and 1st column of the dataset = {first_element_first_col}\")\n",
    "    print(f\"1st element of the 1st array = {X_array[0][0]}\")\n",
    "    \n",
    "    if (X_array[0][0] == first_element_first_col):\n",
    "        \n",
    "        print(\"1st element from the 1st column was correctly stored as the 1st element of the 1st array.\")\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(\"WARNING: review the input data passed: the function generated a 1st element of the first X-array different from the 1st element of the 1st column of the dataset.\")\n",
    "        print(\"Compare the 1st X-array with the 1st row from the dataset. 1st X array:\")\n",
    "        print(X_array[0])\n",
    "        print(\"1st row of the dataset:\")\n",
    "        print(original_first_row)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Finally, the 1st element stored in the last X-array must correspond to the element on the 1st column and last row of the dataset.\")\n",
    "    print(f\"Element on the last row and 1st column of the dataset = {last_element_first_col}\")\n",
    "    print(f\"1st element of the last array = {X_array[(X_array.shape[0] - 1)][0]}\")\n",
    "    # The total of arrays is X_array.shape[0], so the index of the last\n",
    "    # array is (X_array.shape[0] - 1)\n",
    "    # The index [0] from this last array is its 1st element\n",
    "    \n",
    "    if (X_array[(X_array.shape[0]-1)][0] == last_element_first_col):\n",
    "        \n",
    "        print(\"Last element from the 1st column was correctly stored as the 1st element of the last array.\")\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(\"WARNING: review the input data passed: the function generated a 1st element of the last X-array different from the last element of the 1st column of the dataset.\")\n",
    "        print(\"Compare the last X-array with the last row from the dataset. last X array:\")\n",
    "        print(X_array[(X_array.shape[0] - 1)])\n",
    "        print(\"Last row of the dataset:\")\n",
    "        print(original_last_row)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    # Now that the conversion was checked, perform the last reshapes \n",
    "    # for getting the data ready for the model.\n",
    "    # The final reshapes will result into data much more difficult to\n",
    "    # compare with the original dataset, and so we firstly check the\n",
    "    # conversion. If the array conversion was correct, the final step\n",
    "    # should not result in (extra) shape problems\n",
    "    \n",
    "    print(\"Now that we checked the conversion of the dataset to NumPy arrays, we can perform the last reshapes for getting the data ready for the models.\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # If preparing data for the CNN-RNN Architecture, perform a special \n",
    "    # reshape:\n",
    "    if (architecture_to_be_fed_with_returned_arrays = 'cnn_ltsm'):\n",
    "        \n",
    "        # reshape from [samples, timesteps] into \n",
    "        # [samples, subsequences, timesteps, features]\n",
    "        # As such, we will split each sample into 2 subsequences of 2 times \n",
    "        # per subsequence.\n",
    "        X_array = X_array.reshape((X_array.shape[0], 2, 2, 1))\n",
    "    \n",
    "    else:\n",
    "        # ordinary reshape o X_array, the same for the other 3 architectures:\n",
    "        \n",
    "        # reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "        # We must add a third dimension to X_array\n",
    "        X_array = X_array.reshape(X_array.shape[0], X_array.shape[1], 1)\n",
    "    \n",
    "    print(f\"Final shape of the X-arrays = {X_array.shape}\")\n",
    "    \n",
    "    if (boolean_check):\n",
    "        # If there are y-arrays, reshape them:\n",
    "        \n",
    "        # ordinary reshape o y_array, the same for all:\n",
    "        y_array = y_array.reshape(y_array.shape[0], 1)\n",
    "        \n",
    "        # If preparing data for training the Encoder-Decoder RNN architecture,\n",
    "        # perform the special (second) reshape to obtain the 3rd dimension:\n",
    "        if (architecture_to_be_fed_with_returned_arrays = 'encoder_decoder'):\n",
    "            # When using the encoder-decoder architecture, y_arrays must have\n",
    "            # the same shape as X_arrays (i.e., must have 3 dimensions):\n",
    "            y_array = y_array.reshape(y_array.shape[0], y_array.shape[1], 1)\n",
    "        \n",
    "        # We are working with only a single response y.\n",
    "        # If we had two responses, y would be a numpy array in the \n",
    "        # format [y1, y2], i.e., each row would be an array with two \n",
    "        # elements.\n",
    "        # In this case, the command would be again \n",
    "        # y_array.reshape(y_array.shape[0], y_array.shape[1], 1)\n",
    "        # The command y_array.reshape(y_array.shape[0], 1) is used \n",
    "        # only when we have a single response, the present situation\n",
    "        \n",
    "        # Here, our y_arrays are arrays containing a single element:\n",
    "        # y_array = ([[y_0],\n",
    "                    # [y_1],\n",
    "                    # ...,\n",
    "                    # [y_(M-1)]\n",
    "                    # ])\n",
    "        # This configuration is equivalent to a single array with (M-1)\n",
    "        # elements, so the reshape is simpler.\n",
    "        \n",
    "        print(f\"Final shape of the y-arrays = {y_array.shape}\")\n",
    "        print(\"\\n\")\n",
    "        print(\"Returning X and y arrays in the correct format for the deep learning models.\") \n",
    "        print(\"Now, you can pass X_array and y_array as inputs of the function \\'split_data_into_train_and_test\\' to split them into training and test sets, as usual.\")\n",
    "        \n",
    "        return X_array, y_array\n",
    "    \n",
    "    else:\n",
    "        # There is no y_array to return\n",
    "        print(\"Returning X arrays in the correct format for the deep learning models.\") \n",
    "        print(\"Now, you can pass X_array as input of the trained model to get its predictions.\")\n",
    "        \n",
    "        return X_array\n",
    "\n",
    "    # Final note:\n",
    "    # In this function, we directly applied the .reshape method instead\n",
    "    # of using the function np.reshape. That is because X_array and\n",
    "    # y_array were originally created as NumPy arrays, so they have\n",
    "    # the .reshape method available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for making predictions with the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_predictions (model_object, X, dataframe_for_concatenating_predictions = None, col_with_predictions_suffix = None):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.neural_network import MLPRegressor\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBRegressor\n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    # predict_for = 'subset' or predict_for = 'single_entry'\n",
    "    # The function will automatically detect if it is dealing with lists, NumPy arrays\n",
    "    # or Pandas dataframes. If X is a list or a single-dimension array, predict_for\n",
    "    # will be set as 'single_entry'. If X is a multi-dimension NumPy array (as the\n",
    "    # outputs for preparing data - even single_entry - for deep learning models), or if\n",
    "    # it is a Pandas dataframe, the function will set predict_for = 'subset'\n",
    "    \n",
    "    # X = subset of predictive variables (dataframe, NumPy array, or list).\n",
    "    # If PREDICT_FOR = 'single_entry', X should be a list of parameters values.\n",
    "    # e.g. X = [1.2, 3, 4] (dot is the decimal case separator, comma separate values). \n",
    "    # Notice that the list should contain only the numeric values, in the same order of the\n",
    "    # correspondent columns.\n",
    "    # If PREDICT_FOR = 'subset' (prediction for multiple entries), X should be a dataframe \n",
    "    # (subset) or a multi-dimensional NumPy array of the parameters values, as usual.\n",
    "    \n",
    "    # model_object: object containing the model that will be analyzed. e.g.\n",
    "    # model_object = elastic_net_linear_reg_model\n",
    "    \n",
    "    # dataframe_for_concatenating_predictions: if you want to concatenate the predictions\n",
    "    # to a dataframe, pass it here:\n",
    "    # e.g. dataframe_for_concatenating_predictions = df\n",
    "    # If the dataframe must be the same one passed as X, repeat the dataframe object here:\n",
    "    # X = dataset, dataframe_for_concatenating_predictions = dataset.\n",
    "    # Alternatively, if dataframe_for_concatenating_predictions = None, \n",
    "    # the prediction will be returned as a series or NumPy array, depending on the input format.\n",
    "    # Notice that the concatenated predictions will be added as a new column.\n",
    "    \n",
    "    # col_with_predictions_suffix = None. If the predictions are added as a new column\n",
    "    # of the dataframe dataframe_for_concatenating_predictions, you can declare this\n",
    "    # parameter as string with a suffix for identifying the model. If no suffix is added, the new\n",
    "    # column will be named 'y_pred'.\n",
    "    # e.g. col_with_predictions_suffix = '_keras' will create a column named \"y_pred_keras\". This\n",
    "    # parameter is useful when working with multiple models. Always start the suffix with underscore\n",
    "    # \"_\" so that no blank spaces are added; the suffix will not be merged to the column; and there\n",
    "    # will be no confusion with the dot (.) notation for methods, JSON attributes, etc.\n",
    "    \n",
    "    \n",
    "    # Check the type of input: if we are predicting the output for a subset (NumPy array reshaped\n",
    "    # for deep learning models or Pandas dataframe); or predicting for a single entry (single-\n",
    "    # dimension NumPy array or Python list).\n",
    "    \n",
    "    # 1. Check if a list was input. Lists do not have the attribute shape, present in dataframes\n",
    "    # and NumPy arrays. Accessing the attribute shape from a list will raise the Exception error\n",
    "    # named AttributeError\n",
    "    # Try to access the attribute shape. If the error AttributeError is raised, it is a list, so\n",
    "    # set predict_for = 'single_entry':\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Try accessing the shape attribute\n",
    "        X_shape = X.shape\n",
    "        \n",
    "        # Now, check the type of the object X: if it is a dataframe or a numpy array:\n",
    "        X_type = type(X)\n",
    "        \n",
    "        # type(X) == numpy.ndarray (or np.ndarray if NumPy was imported as np) if it is\n",
    "        # an array\n",
    "        # type(X) == pandas.core.frame.DataFrame (or pd.core.frame.DataFrame if Pandas\n",
    "        # was imported as pd) if it is a pandas dataframe.\n",
    "        # Notice that the object type is not a string, so it should not be declared in quotes.\n",
    "        \n",
    "        if (X_type == np.ndarray):\n",
    "            \n",
    "            # It is a NumPy array\n",
    "            # If this array was previously manipulated for the deep learning models, it has 3\n",
    "            # dimensions, so: X_shape = (N, M, 1), N = number of arrays (the number of rows\n",
    "            # of the original dataset), and M = number of elements on each array (the number\n",
    "            # of columns of the original dataset)\n",
    "            \n",
    "            # If the array has the 3rd dimension, we should consider the prediction for 'subset',\n",
    "            # even if it is for a single entry. That is because the array is already reshaped\n",
    "            # and the single_entry code would reshape again.\n",
    "            \n",
    "            # Let's try to access the 3rd dimension as X_shape[2]. \n",
    "            # If there is no 3rd dimension, the exception error IndexError will be raised, since\n",
    "            # there is no index 2:\n",
    "            try:\n",
    "                \n",
    "                # Try accessing the 3rd dimension:\n",
    "                third_dim = X_shape[2]\n",
    "                \n",
    "                # Since it was accessed, the array is already in the correct shape, so set\n",
    "                # prediction for subset:\n",
    "                predict_for = 'subset'\n",
    "            \n",
    "            except IndexError:\n",
    "                \n",
    "                # The index error was raised because there is no 3rd dimension. Then, we are\n",
    "                # dealing with a numpy array equivalent to a list. Set prediction for single_entry.\n",
    "                # It is true even if there are two dimensions like (N, 1) - (2nd dimension added\n",
    "                # by the function for correcting the array format for deep learning).\n",
    "                predict_for = 'single_entry'\n",
    "        \n",
    "        else:\n",
    "            # It is a Pandas dataframe\n",
    "            # Set prediction for a subset:\n",
    "            predict_for = 'subset'\n",
    "        \n",
    "        \n",
    "    except AttributeError:\n",
    "        \n",
    "        # The AttributeError is raised when there is no attribute. \n",
    "        # Since Python lists do not have the shape attribute, \n",
    "        # the input of a list raises this error when trying to access the object's shape.\n",
    "        # Since it is a list, set predict_for = 'single_entry':\n",
    "        predict_for = 'single_entry'\n",
    "        \n",
    "    \n",
    "    if (predict_for == 'single_entry'):\n",
    "        \n",
    "        print(\"Making prediction for a single entry X.\")\n",
    "        print(\"X must be a list with values in the order of the correspondent columns of the dataset.\")\n",
    "        print(\"In other words: declare X as a Python list of values correspondent to each variable, using the same order of variables (columns) used in the dataset.\")\n",
    "        \n",
    "        # Get reshaped list for making the prediction:\n",
    "        X_reshaped = np.reshape(np.array(X), (-1, 1))\n",
    "        \n",
    "        y_pred = model_object.predict(X_reshaped)\n",
    "            \n",
    "        print(f\"Output value predicted for the entry parameters = {y_pred}\\n\")\n",
    "        print(\"Attention: for classification with Keras/TensorFlow and other deep learning frameworks, this output will not be a class, but an array of probabilities correspondent to the probability that the entry belongs to each class. In this case, it is better to use the function calculate_class_probability below, setting model_type == \\'deep_learning\\'. This function will result into dataframes containing the classes as columns and the probabilities in the respective row.\")\n",
    "        print(\"The output class from the deep learning model is the class with higher probability indicated by the predict method. Again, the order of classes is the order they appear in the training dataset. For instance, when using the ImageDataGenerator, the 1st class is the name of the 1st read directory, the 2nd class is the 2nd directory, and so on.\")\n",
    "            \n",
    "        print(\"Returning only the predicted value.\")\n",
    "            \n",
    "        return y_pred\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # prediction for a subset\n",
    "        y_pred = model_object.predict(X)\n",
    "        print(\"Attention: for classification with Keras/TensorFlow and other deep learning frameworks, this output will not be a class, but an array of probabilities correspondent to the probability that the entry belongs to each class. In this case, it is better to use the function calculate_class_probability below, setting model_type == \\'deep_learning\\'. This function will result into dataframes containing the classes as columns and the probabilities in the respective row.\")\n",
    "        print(\"The output class from the deep learning model is the class with higher probability indicated by the predict method. Again, the order of classes is the order they appear in the training dataset. For instance, when using the ImageDataGenerator, the 1st class is the name of the 1st read directory, the 2nd class is the 2nd directory, and so on.\")\n",
    "        \n",
    "        # If y_pred came from a RNN with the parameter return_sequences = True and/or\n",
    "        # return_states = True, then the hidden and/or cell states from the LSTMs\n",
    "        # were returned. So, the returned array has at least one extra dimensions (two\n",
    "        # if both parameters are True). On the other hand, we want only the first dimension,\n",
    "        # correspondent to the actual output.\n",
    "        \n",
    "        # Remember that, due to the reshapes for preparing data for deep learning models,\n",
    "        # y_pred must have at least 2 dimensions: (N, 1), where N is the number of rows of\n",
    "        # the original dataset. But y_pred returned from a model with return_sequences = True\n",
    "        # or return_states = True will be of dimension (N, N, 1). If both parameters are True,\n",
    "        # the dimension is (N, N, N, 1), since there are extra arrays for both the hidden and\n",
    "        # cell states.\n",
    "        \n",
    "        # The conclusion is that there is a third dimension only for models where return_sequences\n",
    "        # = True or return_states = True\n",
    "        \n",
    "        # Check if y_pred is a numpy array, instead of a Pandas dataframe:\n",
    "        \n",
    "        if (type(y_pred) == np.ndarray):\n",
    "            \n",
    "                # Try accessing the array's 3rd dimension. If there is no 3rd dimension,\n",
    "                # the exception error IndexError will be raised.\n",
    "                # Notice: if 4 or more dimensions are present, we can still access\n",
    "                # the 3rd dimension (naturally).\n",
    "                try:\n",
    "                    \n",
    "                    third_dim = y_pred.shape[2]\n",
    "                \n",
    "                    # If we could access the third_dimension, than return_states and\n",
    "                    # or return_sequences = True\n",
    "                    \n",
    "                    # We want only the values stored as the 1st dimension\n",
    "                    # y_pred is an array where each element is an array with two elements. \n",
    "                    # To get only the first elements:\n",
    "                    # (slice the arrays: get all values only for dimension 0, the 1st dim):\n",
    "                    y_pred = y_pred[:,0]\n",
    "                    # if we used y_pred[:,1] we would get the second element, \n",
    "                    # which is the hidden state h (input of the next LSTM unit).\n",
    "                    # It happens because of the parameter return_sequences = True. \n",
    "                    # If return_states = True, there would be a third element, corresponding \n",
    "                    # to the cell state c.\n",
    "                    # Notice that we want only the 1st dimension (0), no matter the case.\n",
    "                \n",
    "                except IndexError:\n",
    "                \n",
    "                    # The index error was raised because there is no 3rd dimension. Then,\n",
    "                    # we do not have to worry with the returned states\n",
    "                    # simply set y_pred as itself:\n",
    "                    y_pred = y_pred\n",
    "                    # Even though the slicing y_pred = y_pred[:,0] would not generate an\n",
    "                    # error, it would unecessarily modify the shape of the array (extra\n",
    "                    # critical step).\n",
    "                    \n",
    "                    # Also, the array obtained as y_pred[:,0] when there are 3 or more \n",
    "                    # dimensions has same shape as y_pred when there are only 1 or 2 \n",
    "                    # dimensions. So, the extra modification of the shape would eliminate\n",
    "                    # this correspondence.\n",
    "                \n",
    "                # If we wanted only the first array, we could set y_pred = y_pred[0]\n",
    "        \n",
    "        # Check if there is a dataframe to concatenate the predictions\n",
    "        if not (dataframe_for_concatenating_predictions is None):\n",
    "            \n",
    "            # there is a dataframe for concatenating the predictions\n",
    "            \n",
    "            # concatenate the predicted values with dataframe_for_concatenating_predictions.\n",
    "            # Add the predicted values as a column:\n",
    "            \n",
    "            # check if there is a suffix:\n",
    "            if not (col_with_predictions_suffix is None):\n",
    "                # There is a suffix declared\n",
    "                # Since there is a suffix, concatenate it to 'y_pred':\n",
    "                col_name = \"y_pred\" + col_with_predictions_suffix\n",
    "            \n",
    "            else:\n",
    "                # Create the column name as the standard.\n",
    "                # The name of the new column is simply 'y_pred'\n",
    "                col_name = \"y_pred\"\n",
    "            \n",
    "            # Set a local copy of the dataframe to manipulate:\n",
    "            X_copy = dataframe_for_concatenating_predictions\n",
    "            \n",
    "            # Add the predictions as the new column named col_name:\n",
    "            X_copy[col_name] = y_pred\n",
    "            \n",
    "            print(f\"The prediction was added as the new column {col_name} of the dataframe, and this dataframe was returned. Check its 10 first rows:\\n\")\n",
    "            print(X_copy.head(10))\n",
    "            \n",
    "            return X_copy\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            print(\"Returning only the predicted values. Check the 10 first values of the series:\\n\")\n",
    "            print(y_pred[:10]) # slice until 10th element from the series or list\n",
    "            # dataset[:,10]: all rows for column 10 of dataset\n",
    "            # dataset[1,:] - slice of all rows for row 1 of dataset.\n",
    "            \n",
    "            return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for calculating probabilities associated to each class**\n",
    "- Set the list_of_classes returned from function `retrieve_classes_used_to_train` as the input of this function.\n",
    "- The predictions (outputs) from deep learning models (e.g. Keras/TensorFlow models) are themselves the probabilities associated to each possible class.\n",
    "    - For Scikit-learn and XGBoost, we must use a specific method for retrieving the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_probability (model_object, X, list_of_classes, type_of_model = 'other', dataframe_for_concatenating_predictions = None):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    # predict_for = 'subset' or predict_for = 'single_entry'\n",
    "    # The function will automatically detect if it is dealing with lists, NumPy arrays\n",
    "    # or Pandas dataframes. If X is a list or a single-dimension array, predict_for\n",
    "    # will be set as 'single_entry'. If X is a multi-dimension NumPy array (as the\n",
    "    # outputs for preparing data - even single_entry - for deep learning models), or if\n",
    "    # it is a Pandas dataframe, the function will set predict_for = 'subset'\n",
    "    \n",
    "    # X = subset of predictive variables (dataframe, NumPy array, or list).\n",
    "    # If PREDICT_FOR = 'single_entry', X should be a list of parameters values.\n",
    "    # e.g. X = [1.2, 3, 4] (dot is the decimal case separator, comma separate values). \n",
    "    # Notice that the list should contain only the numeric values, in the same order of the\n",
    "    # correspondent columns.\n",
    "    # If PREDICT_FOR = 'subset' (prediction for multiple entries), X should be a dataframe \n",
    "    # (subset) or a multi-dimensional NumPy array of the parameters values, as usual.\n",
    "    \n",
    "    # model_object: object containing the model that will be analyzed. e.g.\n",
    "    # model_object = elastic_net_linear_reg_model\n",
    "    \n",
    "    # list_of_classes is the list of classes effectively used for training\n",
    "    # the model. Set this parameter as the object returned from function\n",
    "    # retrieve_classes_used_to_train\n",
    "    \n",
    "    # type_of_model = 'other' or type_of_model = 'deep_learning'\n",
    "    \n",
    "    # Notice that the output will be an array of probabilities, where each\n",
    "    # element corresponds to a possible class, in the order classes appear.\n",
    "    \n",
    "    # dataframe_for_concatenating_predictions: if you want to concatenate the predictions\n",
    "    # to a dataframe, pass it here:\n",
    "    # e.g. dataframe_for_concatenating_predictions = df\n",
    "    # If the dataframe must be the same one passed as X, repeat the dataframe object here:\n",
    "    # X = dataset, dataframe_for_concatenating_predictions = dataset.\n",
    "    # Alternatively, if dataframe_for_concatenating_predictions = None, \n",
    "    # the prediction will be returned as a series or NumPy array, depending on the input format.\n",
    "    # Notice that the concatenated predictions will be added as a new column.\n",
    "    \n",
    "    # All of the new columns (appended or not) will have the prefix \"prob_class_\" followed\n",
    "    # by the correspondent class name to identify them.\n",
    "    \n",
    "       \n",
    "    # 1. Check if a list was input. Lists do not have the attribute shape, present in dataframes\n",
    "    # and NumPy arrays. Accessing the attribute shape from a list will raise the Exception error\n",
    "    # named AttributeError\n",
    "    # Try to access the attribute shape. If the error AttributeError is raised, it is a list, so\n",
    "    # set predict_for = 'single_entry':\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        # Try accessing the shape attribute\n",
    "        X_shape = X.shape\n",
    "        \n",
    "        # Now, check the type of the object X: if it is a dataframe or a numpy array:\n",
    "        X_type = type(X)\n",
    "        \n",
    "        # type(X) == numpy.ndarray (or np.ndarray if NumPy was imported as np) if it is\n",
    "        # an array\n",
    "        # type(X) == pandas.core.frame.DataFrame (or pd.core.frame.DataFrame if Pandas\n",
    "        # was imported as pd) if it is a pandas dataframe.\n",
    "        # Notice that the object type is not a string, so it should not be declared in quotes.\n",
    "        \n",
    "        if (X_type == np.ndarray):\n",
    "            \n",
    "            # It is a NumPy array\n",
    "            # If this array was previously manipulated for the deep learning models, it has 3\n",
    "            # dimensions, so: X_shape = (N, M, 1), N = number of arrays (the number of rows\n",
    "            # of the original dataset), and M = number of elements on each array (the number\n",
    "            # of columns of the original dataset)\n",
    "            \n",
    "            # If the array has the 3rd dimension, we should consider the prediction for 'subset',\n",
    "            # even if it is for a single entry. That is because the array is already reshaped\n",
    "            # and the single_entry code would reshape again.\n",
    "            \n",
    "            # Let's try to access the 3rd dimension as X_shape[2]. \n",
    "            # If there is no 3rd dimension, the exception error IndexError will be raised, since\n",
    "            # there is no index 2:\n",
    "            try:\n",
    "                \n",
    "                # Try accessing the 3rd dimension:\n",
    "                third_dim = X_shape[2]\n",
    "                \n",
    "                # Since it was accessed, the array is already in the correct shape, so set\n",
    "                # prediction for subset:\n",
    "                predict_for = 'subset'\n",
    "            \n",
    "            except IndexError:\n",
    "                \n",
    "                # The index error was raised because there is no 3rd dimension. Then, we are\n",
    "                # dealing with a numpy array equivalent to a list. Set prediction for single_entry.\n",
    "                # It is true even if there are two dimensions like (N, 1) - (2nd dimension added\n",
    "                # by the function for correcting the array format for deep learning).\n",
    "                predict_for = 'single_entry'\n",
    "        \n",
    "        else:\n",
    "            # It is a Pandas dataframe\n",
    "            # Set prediction for a subset:\n",
    "            predict_for = 'subset'\n",
    "        \n",
    "        \n",
    "    except AttributeError:\n",
    "        \n",
    "        # The AttributeError is raised when there is no attribute. \n",
    "        # Since Python lists do not have the shape attribute, \n",
    "        # the input of a list raises this error when trying to access the object's shape.\n",
    "        # Since it is a list, set predict_for = 'single_entry':\n",
    "        predict_for = 'single_entry'\n",
    "        \n",
    "        \n",
    "    # Check if it is a keras or other deep learning framework; or if it is a sklearn or xgb model:\n",
    "    boolean_check = (type_of_model == 'deep_learning')\n",
    "    \n",
    "    if (boolean_check): # run if it is True\n",
    "        print(\"The predictions (outputs) from deep learning models are themselves the probabilities associated to each possible class.\")\n",
    "        print(\"\\n\") #line break\n",
    "        print(\"The output will be an array of float values: each float represents the probability of one class, in the order the classes appear. For a binary classifier, the first element will correspond to class 0; and the second element will be the probability of class 1.\")\n",
    "    \n",
    "    \n",
    "    if (predict_for == 'single_entry'):\n",
    "        \n",
    "        print(\"Calculating probabilities for a single entry X.\")\n",
    "        print(\"X must be a list with values in the order of the correspondent columns of the dataset.\")\n",
    "        print(\"In other words: declare X as a Python list of values correspondent to each variable, using the same order of variables (columns) used in the dataset.\")\n",
    "        \n",
    "        # Get reshaped list for making the prediction:\n",
    "        X_reshaped = np.reshape(np.array(X), (-1, 1))\n",
    "        \n",
    "        if (boolean_check): \n",
    "            # Use the predict method itself for deep learning models.\n",
    "            # These models do not have the predict_proba method.\n",
    "            # Their output is itself the probability for each class.\n",
    "            y_pred_probabilities = model_object.predict(X_reshaped)\n",
    "        \n",
    "        else:\n",
    "            # use the predict_proba method from sklearn and xgboost:\n",
    "            y_pred_probabilities = model_object.predict_proba(X_reshaped)\n",
    "        \n",
    "        print(\"Probabilities calculated using the entry parameters.\") \n",
    "        print(f\"Probabilities calculated for each one of the classes {list_of_classes} (in the order of classes) = {y_pred_probabilities}\\n\")\n",
    "        \n",
    "        # create a dictionary with the possible classes and the correspondent probabilities:\n",
    "        # Use the list attribute to guarantee that the probabilities are\n",
    "        # retrieved as a list:\n",
    "        probability_dict = {'class': list_of_classes,\n",
    "                            'probability': list(y_pred_probabilities)}\n",
    "            \n",
    "        # Convert it to a Pandas dataframe:\n",
    "        probabilities_df = pd.DataFrame(data = probability_dict)\n",
    "            \n",
    "        print(\"Returning a dataframe containing the classes and the probabilities calculated for the entry to belong to each class. Check it below:\")\n",
    "        print(probabilities_df)\n",
    "            \n",
    "        return probabilities_df\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # prediction for a subset\n",
    "        \n",
    "        if (boolean_check): \n",
    "            # Use the predict method itself for deep learning models.\n",
    "            # These models do not have the predict_proba method.\n",
    "            # Their output is itself the probability for each class.\n",
    "            y_pred_probabilities = model_object.predict(X)\n",
    "            \n",
    "            # If y_pred_probabilities came from a RNN with the parameter return_sequences = True \n",
    "            # and/or return_states = True, then the hidden and/or cell states from the LSTMs\n",
    "            # were returned. So, the returned array has at least one extra dimensions (two\n",
    "            # if both parameters are True). On the other hand, we want only the first dimension,\n",
    "            # correspondent to the actual output.\n",
    "\n",
    "            # Remember that, due to the reshapes for preparing data for deep learning models,\n",
    "            # y_pred_probabilities must have at least 2 dimensions: (N, 1), where N is the number \n",
    "            # of rows of the original dataset. But y_pred_probabilities returned from a model \n",
    "            # with return_sequences = True or return_states = True will be of dimension (N, N, 1). \n",
    "            # If both parameters are True, the dimension is (N, N, N, 1), since there are extra \n",
    "            # arrays for both the hidden and cell states.\n",
    "\n",
    "            # The conclusion is that there is a third dimension only for models where \n",
    "            # return_sequences = True or return_states = True\n",
    "\n",
    "            # Check if y_pred_probabilities is a numpy array, instead of a Pandas dataframe:\n",
    "\n",
    "            if (type(y_pred_probabilities) == np.ndarray):\n",
    "\n",
    "                    # Try accessing the array's 3rd dimension. If there is no 3rd dimension,\n",
    "                    # the exception error IndexError will be raised.\n",
    "                    # Notice: if 4 or more dimensions are present, we can still access\n",
    "                    # the 3rd dimension (naturally).\n",
    "                    try:\n",
    "\n",
    "                        third_dim = y_pred_probabilities.shape[2]\n",
    "\n",
    "                        # If we could access the third_dimension, than return_states and\n",
    "                        # or return_sequences = True\n",
    "\n",
    "                        # We want only the values stored as the 1st dimension\n",
    "                        # y_pred_probabilities is an array where each element is an array with \n",
    "                        # two elements. To get only the first elements:\n",
    "                        # (slice the arrays: get all values only for dimension 0, the 1st dim):\n",
    "                        y_pred_probabilities = y_pred_probabilities[:,0]\n",
    "                        # if we used y_pred_probabilities[:,1] we would get the second element, \n",
    "                        # which is the hidden state h (input of the next LSTM unit).\n",
    "                        # It happens because of the parameter return_sequences = True. \n",
    "                        # If return_states = True, there would be a third element, corresponding \n",
    "                        # to the cell state c.\n",
    "                        # Notice that we want only the 1st dimension (0), no matter the case.\n",
    "\n",
    "                    except IndexError:\n",
    "\n",
    "                        # The index error was raised because there is no 3rd dimension. Then,\n",
    "                        # we do not have to worry with the returned states\n",
    "                        # simply set y_pred_probabilities as itself:\n",
    "                        y_pred_probabilities = y_pred_probabilities\n",
    "                        # Even though the slicing y_pred = y_pred[:,0] would not generate an\n",
    "                        # error, it would unecessarily modify the shape of the array (extra\n",
    "                        # critical step).\n",
    "\n",
    "                        # Also, the array obtained as y_pred[:,0] when there are 3 or more \n",
    "                        # dimensions has same shape as y_pred when there are only 1 or 2 \n",
    "                        # dimensions. So, the extra modification of the shape would eliminate\n",
    "                        # this correspondence.\n",
    "\n",
    "                    # If we wanted only the first array, we could set \n",
    "                    # y_pred_probabilities = y_pred_probabilities[0]\n",
    "        \n",
    "        else:\n",
    "            # use the predict_proba method from sklearn and xgboost:\n",
    "            y_pred_probabilities = model_object.predict_proba(X)\n",
    "        \n",
    "        # y_pred_probabilities is a column containing arrays of probabilities\n",
    "        # Let's create a dataframe separating each element of the array into\n",
    "        # a separate column\n",
    "        \n",
    "        # Get the size of each array. It is the total of elements from\n",
    "        # list_of_classes (total of possible classes):\n",
    "        total_of_classes = len(list_of_classes)\n",
    "        \n",
    "        # Get the total of rows. It is the length of X:\n",
    "        \n",
    "        # If X is a NumPy array, get its first dimension:\n",
    "        if (X_type == np.ndarray):\n",
    "            \n",
    "            # Get the first dimension of the array (dimension 0)\n",
    "            # This dimension is the total of arrays, i.e., the total\n",
    "            # of rows on the original dataset:\n",
    "            # X.shape = (N, M, 1), N = total of arrays (rows of the original\n",
    "            # dataset); M = total of elements in each array (columns of the\n",
    "            # original dataset). Analogously, y.shape = (N, 1)\n",
    "            total_rows = X.shape[0]\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # X is a dataframe, so the number of rows is its length\n",
    "            total_rows = len(X)\n",
    "        \n",
    "        # Starts a dictionary. This dictionary will have the class as the\n",
    "        # key and a list of the probabilities that the element belong to that\n",
    "        # class as the value (in the dataframe, the class will be column,\n",
    "        # with its calculated probability in each row):\n",
    "        probability_dict = {}\n",
    "        \n",
    "        # Loop through each possible class:\n",
    "        for i in range (total_of_classes):\n",
    "            # loops from i = 0 (first index) \n",
    "            # to i = (total_of_classes - 1), index of the last element of list\n",
    "            # 'list_of_classes'\n",
    "            \n",
    "            # Retrieve the name of the class in the list 'list_of_classes'.\n",
    "            # It is the i-th element from list list_of_classes:\n",
    "            class_name = list_of_classes[i]\n",
    "            # Let's concatenate the prefix \"prob_class_\" to this strings.\n",
    "            # This string will be used as column name, so it will be clear \n",
    "            # in the output dataframe that the column is referrent to the \n",
    "            # probability calculated for the class. Since the elements may \n",
    "            # have been saved as numbers use the str attribute to guarantee \n",
    "            # that the element was read as a string, and concatenate the\n",
    "            # prefix to its left:\n",
    "            class_name = \"prob_class_\" + str(class_name)\n",
    "            \n",
    "            # Start a list of probabilities:\n",
    "            prob_list = []\n",
    "            \n",
    "            # Now loop through each row j from the dataframe\n",
    "            # to retrieve the array in the column y_pred_probabilities:\n",
    "            \n",
    "            for j in len(total_rows):\n",
    "                # goes from j = 0 (first row of the dataframe) to\n",
    "                # j = total_rows - 1, index of the last row\n",
    "                # Get the array of probabilities for that row:\n",
    "                prob_array = y_pred_probabilities[j]\n",
    "                \n",
    "                # Append the i-th element of that array in prob_list\n",
    "                # The i-th position of the array is the probability\n",
    "                # of the class being analyzed in the i-th iteration of\n",
    "                # the main loop\n",
    "                prob_list.append(prob_array[i])\n",
    "            \n",
    "            # Now that the probabilities for the class correspondent to\n",
    "            # each row were retrieved as the list prob_list, update the\n",
    "            # dictionary. Use the class name saved as class_name as the\n",
    "            # key, and put the prob_list as the correspondent value:\n",
    "            probability_dict.update({class_name: prob_list})\n",
    "        \n",
    "        # Now that we finished the loop, the probability dictionary contains\n",
    "        # each one of the classes as its keys, and the list of probabilities\n",
    "        # for each row as the correspondent values. \n",
    "        # Also, the keys are identified with the prefix 'prob_class' to\n",
    "        # indicate that they are referrent to the probability of belonging to\n",
    "        # one class. Let's convert this dictionary to a Pandas dataframe:\n",
    "        \n",
    "        probabilities_df = pd.DataFrame(data = probability_dict)\n",
    "        \n",
    "        # Check if there is a dataframe to concatenate the predictions\n",
    "        if not (dataframe_for_concatenating_predictions is None):\n",
    "            \n",
    "            # there is a dataframe for concatenating the predictions.\n",
    "            \n",
    "            # Set a local copy of the dataframe to manipulate:\n",
    "            X_copy = X\n",
    "            \n",
    "            # Append the columns from probabilities_df with Pandas concat\n",
    "            # method, setting axis = 1 (axis = 0  appends rows)\n",
    "            # Use the pandas 'inner' join, which removes entries without\n",
    "            # correspondence. It is the same strategy used for concatenating\n",
    "            # the dataframe obtained from One-Hot Encoding transformation in the\n",
    "            # ETL Workflow (3_Dataset_Transformation)\n",
    "            X_copy = pd.concat([X_copy, probabilities_df], axis = 1, join = \"inner\")\n",
    "      \n",
    "            print(f\"The dataframe X was concatenated to the probabilities calculated for each class and returned. Check its first 10 entries:\\n\")\n",
    "            print(X_copy.head(10))\n",
    "            \n",
    "            return X_copy\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            print(\"Returning only the dataframe with the probabilities calculated for each class. Check its first 10 entries:\\n\")\n",
    "            print(probabilities_df.head(10))\n",
    "            \n",
    "            return probabilities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for merging (joining) dataframes on given keys; and sorting the merged table**\n",
    "- Merge (join) types:\n",
    "    - 'inner': resultant dataframe contains only the rows on the left dataframe with correspondent values on the right dataframe. Can be used for filtering a set of labelled rows. Results in no missing values;\n",
    "    - 'left': resultant dataframe contains all the rows from the left table (even those without correspondence on the right); and the rows from the right table that have correspondence on the left one. Since rows from the left table may not have correspondence, it may result in missing values.\n",
    "    - 'right': resultant dataframe contains all the rows from the right table (even those without correspondence on the right); and the rows from the left table that have correspondence on the right one. Since rows from the right table may not have correspondence, it may result in missing values.\n",
    "    - 'outer': in SQL, the Pandas 'outer' merge usually corresponds to the FULL OUTER JOIN: the resultant dataframe contains all rows from both tables, not taking in account if there is correspondence. So, it may result in missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MERGE_AND_SORT_DATAFRAMES (df_left, df_right, left_key, right_key, how_to_join = \"inner\", merged_suffixes = ('_left', '_right'), sort_merged_df = False, column_to_sort = None, ascending_sorting = True):\n",
    "    \n",
    "    #WARNING: Only two dataframes can be merged on each call of the function.\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # df_left: dataframe to be joined as the left one.\n",
    "    \n",
    "    # df_right: dataframe to be joined as the right one\n",
    "    \n",
    "    # left_key: (String) name of column of the left dataframe to be used as key for joining.\n",
    "    \n",
    "    # right_key: (String) name of column of the right dataframe to be used as key for joining.\n",
    "    \n",
    "    # how_to_join: joining method: \"inner\", \"outer\", \"left\", \"right\". The default is \"inner\".\n",
    "    \n",
    "    # merge_method: which pandas merging method will be applied:\n",
    "    # merge_method = 'ordered' for using the .merge_ordered method.\n",
    "    # merge_method = \"asof\" for using the .merge_asof method.\n",
    "    # WARNING: .merge_asof uses fuzzy matching, so the how_to_join parameter is not applicable.\n",
    "    \n",
    "    # merged_suffixes = ('_left', '_right') - tuple of the suffixes to be added to columns\n",
    "    # with equal names. Simply modify the strings inside quotes to modify the standard\n",
    "    # values. If no tuple is provided, the standard denomination will be used.\n",
    "    \n",
    "    # sort_merged_df = False not to sort the merged dataframe. If you want to sort it,\n",
    "    # set as True. If sort_merged_df = True and column_to_sort = None, the dataframe will\n",
    "    # be sorted by its first column.\n",
    "    \n",
    "    # column_to_sort = None. Keep it None if the dataframe should not be sorted.\n",
    "    # Alternatively, pass a string with a column name to sort, such as:\n",
    "    # column_to_sort = 'col1'; or a list of columns to use for sorting: column_to_sort = \n",
    "    # ['col1', 'col2']\n",
    "    \n",
    "    # ascending_sorting = True. If you want to sort the column(s) passed on column_to_sort in\n",
    "    # ascending order, set as True. Set as False if you want to sort in descending order. If\n",
    "    # you want to sort each column passed as list column_to_sort in a specific order, pass a \n",
    "    # list of booleans like ascending_sorting = [False, True] - the first column of the list\n",
    "    # will be sorted in descending order, whereas the 2nd will be in ascending. Notice that\n",
    "    # the correspondence is element-wise: the boolean in list ascending_sorting will correspond \n",
    "    # to the sorting order of the column with the same position in list column_to_sort.\n",
    "    # If None, the dataframe will be sorted in ascending order.\n",
    "    \n",
    "    # check if the keys are the same:\n",
    "    boolean_check = (left_key == right_key)\n",
    "    # if boolean_check is True, we will merge using the on parameter, instead of left_on and right_on:\n",
    "    \n",
    "    if (boolean_check): # runs if it is True:\n",
    "        \n",
    "        merged_df = df_left.merge(df_right, on = left_key, how = how_to_join, suffixes = merged_suffixes)\n",
    "    \n",
    "    else:\n",
    "        # use left_on and right_on\n",
    "        merged_df = df_left.merge(df_right, left_on = left_key, right_on = right_key, how = how_to_join, suffixes = merged_suffixes)\n",
    "    \n",
    "    # Check if the dataframe should be sorted:\n",
    "    if (sort_merged_df == True):\n",
    "        \n",
    "        # check if column_to_sort = None. If it is, set it as the first column (index 0):\n",
    "        if (column_to_sort is None):\n",
    "            \n",
    "            column_to_sort = merged_df.columns[0]\n",
    "            print(f\"Sorting merged dataframe by its first column = {column_to_sort}\")\n",
    "        \n",
    "        # check if ascending_sorting is None. If it is, set it as True:\n",
    "        if (ascending_sorting is None):\n",
    "            \n",
    "            ascending_sorting = True\n",
    "            print(\"Sorting merged dataframe in ascending order.\")\n",
    "        \n",
    "        # Now, sort the dataframe according to the parameters:\n",
    "        merged_df = merged_df.sort_values(by = column_to_sort, ascending = ascending_sorting)\n",
    "        #sort by the first column, with index 0.\n",
    "    \n",
    "        # Now, reset index positions:\n",
    "        merged_df = merged_df.reset_index(drop = True)\n",
    "        print(\"Merged dataframe successfully sorted.\")\n",
    "    \n",
    "    # Pandas .head(Y) method results in a dataframe containing the first Y rows of the \n",
    "    # original dataframe. The default .head() is Y = 5. Print first 10 rows of the \n",
    "    # new dataframe:\n",
    "    print(\"Dataframe successfully merged. Check its 10 first rows:\\n\")\n",
    "    print(merged_df.head(10))\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for concatenating (SQL UNION) multiple dataframes**\n",
    "- Vertical concatenation of the dataframes.\n",
    "- Equivalent to SQL Union: vertical stack/append of the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNION_DATAFRAMES (list_of_dataframes, what_to_append = 'rows', ignore_index_on_union = True, sort_values_on_union = True, union_join_type = None):\n",
    "    \n",
    "    import pandas as pd\n",
    "    #JOIN can be 'inner' to perform an inner join, eliminating the missing values\n",
    "    #The default (None) is 'outer': the dataframes will be stacked on the columns with\n",
    "    #same names but, in case there is no correspondence, the row will present a missing\n",
    "    #value for the columns which are not present in one of the dataframes.\n",
    "    #When using the 'inner' method, only the common columns will remain\n",
    "    \n",
    "    #list_of_dataframes must be a list containing the dataframe objects\n",
    "    # example: list_of_dataframes = [df1, df2, df3, df4]\n",
    "    #Notice that the dataframes are objects, not strings. Therefore, they should not\n",
    "    # be declared inside quotes.\n",
    "    # There is no limit of dataframes. In this example, we will concatenate 4 dataframes.\n",
    "    # If list_of_dataframes = [df1, df2, df3] we would concatenate 3, and if\n",
    "    # list_of_dataframes = [df1, df2, df3, df4, df5] we would concatenate 5 dataframes.\n",
    "    \n",
    "    # what_to_append = 'rows' for appending the rows from one dataframe\n",
    "    # into the other; what_to_append = 'columns' for appending the columns\n",
    "    # from one dataframe into the other (horizontal or lateral append).\n",
    "    \n",
    "    # When what_to_append = 'rows', Pandas .concat method is defined as\n",
    "    # axis = 0, i.e., the operation occurs in the row level, so the rows\n",
    "    # of the second dataframe are added to the bottom of the first one.\n",
    "    # It is the SQL union, and creates a dataframe with more rows, and\n",
    "    # total of columns equals to the total of columns of the first dataframe\n",
    "    # plus the columns of the second one that were not in the first dataframe.\n",
    "    # When what_to_append = 'columns', Pandas .concat method is defined as\n",
    "    # axis = 1, i.e., the operation occurs in the column level: the two\n",
    "    # dataframes are laterally merged using the index as the key, \n",
    "    # preserving all columns from both dataframes. Therefore, the number of\n",
    "    # rows will be the total of rows of the dataframe with more entries,\n",
    "    # and the total of columns will be the sum of the total of columns of\n",
    "    # the first dataframe with the total of columns of the second dataframe.\n",
    "    \n",
    "    #The other parameters are the same from Pandas .concat method.\n",
    "    # ignore_index_on_union = ignore_index;\n",
    "    # sort_values_on_union = sort\n",
    "    # union_join_type = join\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\n",
    "    \n",
    "    #Check Datacamp course Joining Data with pandas, Chap.3, \n",
    "    # Advanced Merging and Concatenating\n",
    "    \n",
    "    # Check axis:\n",
    "    if (what_to_append == 'rows'):\n",
    "        \n",
    "        AXIS = 0\n",
    "    \n",
    "    elif (what_to_append == 'columns'):\n",
    "        \n",
    "        AXIS = 1\n",
    "    \n",
    "    else:\n",
    "        print(\"No valid string was input to what_to_append, so appending rows (vertical append, equivalent to SQL UNION).\")\n",
    "        AXIS = 0\n",
    "    \n",
    "    if (union_join_type == 'inner'):\n",
    "        \n",
    "        print(\"Warning: concatenating dataframes using the \\'inner\\' join method, that removes missing values.\")\n",
    "        concat_df = pd.concat(list_of_dataframes, axis = AXIS, ignore_index = ignore_index_on_union, sort = sort_values_on_union, join = union_join_type)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #In case None or an invalid value is provided, use the default 'outer', by simply\n",
    "        # not declaring the 'join':\n",
    "        concat_df = pd.concat(list_of_dataframes, axis = AXIS, ignore_index = ignore_index_on_union, sort = sort_values_on_union)\n",
    "    \n",
    "    # Pandas .head(Y) method results in a dataframe containing the first Y rows of the \n",
    "    # original dataframe. The default .head() is Y = 5. Print first 10 rows of the \n",
    "    # new dataframe:\n",
    "    print(\"Dataframes successfully concatenated. Check the 10 first rows of new dataframe:\\n\")\n",
    "    print(concat_df.head(10))\n",
    "    \n",
    "    #Now return the concatenated dataframe:\n",
    "    \n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for plotting the bar chart**\n",
    "- Bars may be vertically or horizontally oriented.\n",
    "- Bar charts are plotted after selecting an aggregation function, and the cumulative percent curve may be obtained and plotted with the bars (in secondary axis).\n",
    "- To obtain a **Pareto chart**, keep `aggregate_function = 'sum'`, `plot_cumulative_percent = True`, and `orientation = 'vertical'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "CELL_7"
    ]
   },
   "outputs": [],
   "source": [
    "def bar_chart (df, categorical_var_name, response_var_name, aggregate_function = 'sum', add_suffix_to_aggregated_col = True, suffix = None, calculate_and_plot_cumulative_percent = True, orientation = 'vertical', limit_of_plotted_categories = None, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # df: dataframe being analyzed\n",
    "    \n",
    "    # categorical_var_name: string (inside quotes) containing the name \n",
    "    # of the column to be analyzed. e.g. \n",
    "    # categorical_var_name = \"column1\"\n",
    "    \n",
    "    # response_var_name: string (inside quotes) containing the name \n",
    "    # of the column that stores the response correspondent to the\n",
    "    # categories. e.g. response_var_name = \"response_feature\" \n",
    "    \n",
    "    # aggregate_function = 'sum': String defining the aggregation \n",
    "    # method that will be applied. Possible values:\n",
    "    # 'median', 'mean', 'mode', 'sum', 'min', 'max', 'variance',\n",
    "    # 'standard_deviation','10_percent_quantile', '20_percent_quantile',\n",
    "    # '25_percent_quantile', '30_percent_quantile', '40_percent_quantile',\n",
    "    # '50_percent_quantile', '60_percent_quantile', '70_percent_quantile',\n",
    "    # '75_percent_quantile', '80_percent_quantile', '90_percent_quantile',\n",
    "    # and '95_percent_quantile'.\n",
    "    # To use another aggregate function, the method must be added to the\n",
    "    # dictionary of methods agg_methods_dict, defined in the function.\n",
    "    # If None or an invalid function is input, 'sum' will be used.\n",
    "    \n",
    "    # add_suffix_to_aggregated_col = True will add a suffix to the\n",
    "    # aggregated column. e.g. 'responseVar_mean'. If add_suffix_to_aggregated_col \n",
    "    # = False, the aggregated column will have the original column name.\n",
    "    \n",
    "    # suffix = None. Keep it None if no suffix should be added, or if\n",
    "    # the name of the aggregate function should be used as suffix, after\n",
    "    # \"_\". Alternatively, set it as a string. As recommendation, put the\n",
    "    # \"_\" sign in the beginning of this string to separate the suffix from\n",
    "    # the original column name. e.g. if the response variable is 'Y' and\n",
    "    # suffix = '_agg', the new aggregated column will be named as 'Y_agg'\n",
    "    \n",
    "    # calculate_and_plot_cumulative_percent = True to calculate and plot\n",
    "    # the line of cumulative percent, or \n",
    "    # calculate_and_plot_cumulative_percent = False to omit it.\n",
    "    # This feature is only shown when aggregate_function = 'sum', 'median',\n",
    "    # 'mean', or 'mode'. So, it will be automatically set as False if \n",
    "    # another aggregate is selected.\n",
    "    \n",
    "    # orientation = 'vertical' is the standard, and plots vertical bars\n",
    "    # (perpendicular to the X axis). In this case, the categories are shown\n",
    "    # in the X axis, and the correspondent responses are in Y axis.\n",
    "    # Alternatively, orientation = 'horizontal' results in horizontal bars.\n",
    "    # In this case, categories are in Y axis, and responses in X axis.\n",
    "    # If None or invalid values are provided, orientation is set as 'vertical'.\n",
    "    \n",
    "    # Note: to obtain a Pareto chart, keep aggregate_function = 'sum',\n",
    "    # plot_cumulative_percent = True, and orientation = 'vertical'.\n",
    "    \n",
    "    # limit_of_plotted_categories: integer value that represents\n",
    "    # the maximum of categories that will be plot. Keep it None to plot\n",
    "    # all categories. Alternatively, set an integer value. e.g.: if\n",
    "    # limit_of_plotted_categories = 4, but there are more categories,\n",
    "    # the dataset will be sorted in descending order and: 1) The remaining\n",
    "    # categories will be sum in a new category named 'others' if the\n",
    "    # aggregate function is 'sum'; 2) Or the other categories will be simply\n",
    "    # omitted from the plot, for other aggregate functions. Notice that\n",
    "    # it limits only the variables in the plot: all of them will be\n",
    "    # returned in the dataframe.\n",
    "    # Use this parameter to obtain a cleaner plot. Notice that the remaining\n",
    "    # columns will be aggregated as 'others' even if there is a single column\n",
    "    # beyond the limit.\n",
    "    \n",
    "    \n",
    "    # Create a local copy of the dataframe to manipulate:\n",
    "    \n",
    "    DATASET = df\n",
    "    \n",
    "    # Create the dictionary of possible aggregates, to define the\n",
    "    # aggregation method, according to the set by the user:\n",
    "    agg_methods_dict = {\n",
    "        \n",
    "        'median': DATASET.groupby(categorical_var_name)[response_var_name].median(),\n",
    "        'mean': DATASET.groupby(categorical_var_name)[response_var_name].mean(),\n",
    "        'mode': DATASET.groupby(categorical_var_name)[response_var_name].mode(),\n",
    "        'sum': DATASET.groupby(categorical_var_name)[response_var_name].sum(),\n",
    "        'min': DATASET.groupby(categorical_var_name)[response_var_name].min(),\n",
    "        'max': DATASET.groupby(categorical_var_name)[response_var_name].max(),\n",
    "        'variance': DATASET.groupby(categorical_var_name)[response_var_name].var(),\n",
    "        'standard_deviation': DATASET.groupby(categorical_var_name)[response_var_name].std(),\n",
    "        '10_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.10),\n",
    "        '20_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.20),\n",
    "        '25_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.25),\n",
    "        '30_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.30),\n",
    "        '40_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.40),\n",
    "        '50_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.50),\n",
    "        '60_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.60),\n",
    "        '70_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.70),\n",
    "        '75_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.75),\n",
    "        '80_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.80),\n",
    "        '90_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.90),\n",
    "        '95_percent_quantile': DATASET.groupby(categorical_var_name)[response_var_name].quantile(0.95)\n",
    "    }\n",
    "    \n",
    "    # check if the function was not set in the dictionary. If not,\n",
    "    # use 'sum'\n",
    "    if (aggregate_function not in (agg_methods_dict.keys())):\n",
    "        \n",
    "        aggregate_function = 'sum'\n",
    "        print(\"Invalid or no aggregation function input, so using the default \\'sum\\'.\")\n",
    "    \n",
    "    # Select the method in the dictionary and apply it. To access a value\n",
    "    # 'val' correspondent to the key 'key' from a dictionary dict, we\n",
    "    # declare: dict['key'], just as accessing a column from a dataframe.\n",
    "    \n",
    "    # The value will be the application of the method itself, i.e., the\n",
    "    # dataset will be aggregated:\n",
    "    DATASET = agg_methods_dict[aggregate_function]\n",
    "    \n",
    "    # If an aggregate function different from 'sum', 'mean', 'median' or 'mode' \n",
    "    # is used with plot_cumulative_percent = True, \n",
    "    # set plot_cumulative_percent = False:\n",
    "    # (check if aggregate function is not in the list of allowed values):\n",
    "    if ((aggregate_function not in ['sum', 'mean', 'median', 'mode']) & (calculate_and_plot_cumulative_percent == True)):\n",
    "        \n",
    "        calculate_and_plot_cumulative_percent = False\n",
    "        print(\"The cumulative percent is only calculated when aggregate_function = \\'sum\\', \\'mean\\', \\'median\\', or \\'mode\\'. So, plot_cumulative_percent was set as False.\")\n",
    "    \n",
    "    # Guarantee that the columns from the aggregated dataset have the correct\n",
    "    \n",
    "    # Let's create a list of the new column names\n",
    "    # The first element is categorical_var_name, which is not modified:\n",
    "    list_of_cols = [categorical_var_name]\n",
    "    \n",
    "    # Check if add_suffix_to_aggregated_col is False. If it is, simply\n",
    "    # repeat the original response_var_name:\n",
    "    if (add_suffix_to_aggregated_col == False):\n",
    "        \n",
    "        list_of_cols.append(response_var_name)\n",
    "    \n",
    "    else:\n",
    "        # Let's add a suffix. Check if suffix is None. If it is,\n",
    "        # set \"_\" + aggregate_function as suffix:\n",
    "        \n",
    "        if (suffix is None):\n",
    "            suffix = \"_\" + aggregate_function\n",
    "        \n",
    "        # Now, append response_var_name + suffix to the list to\n",
    "        # create the name of the new aggregated column:\n",
    "        response_var_name = response_var_name + suffix\n",
    "        list_of_cols.append(response_var_name)\n",
    "    \n",
    "    # Now, rename the columns of the aggregated dataset as the list\n",
    "    # list_of_cols:\n",
    "    DATASET.columns = list_of_cols\n",
    "    \n",
    "    # Let's sort the dataframe.\n",
    "    \n",
    "    # Order the dataframe in descending order by the response.\n",
    "    # If there are equal responses, order them by category, in\n",
    "    # ascending order; put the missing values in the first position\n",
    "    # To pass multiple columns and multiple types of ordering, we use\n",
    "    # lists. If there was a single column to order by, we would declare\n",
    "    # it as a string. If only one order of ascending was used, we would\n",
    "    # declare it as a simple boolean\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html\n",
    "    \n",
    "    DATASET = DATASET.sort_values(by = [response_var_name, categorical_var_name], ascending = [False, True], na_position = 'first')\n",
    "    \n",
    "    # Now, reset index positions:\n",
    "    DATASET = DATASET.reset_index(drop = True)\n",
    "    \n",
    "    # plot_cumulative_percent = True, create a column to store the\n",
    "    # cumulative percent:\n",
    "    if (calculate_and_plot_cumulative_percent): \n",
    "        # Run the following code if the boolean value is True (implicity)\n",
    "        \n",
    "        # Calculate the total sum of the array correspondent to\n",
    "        # the column (series) response_var_name\n",
    "        total_sum = np.sum(np.array(DATASET[response_var_name]))\n",
    "        \n",
    "        # Create a column series for the cumulative sum:\n",
    "        cumsum_col = response_var_name + \"_cumsum\"\n",
    "        DATASET[cumsum_col] = DATASET[response_var_name].cumsum()\n",
    "        \n",
    "        # Now, create a column for the accumulated percent\n",
    "        # by dividing cumsum_col by total_sum and multiplying it by\n",
    "        # 100 (%):\n",
    "        cum_pct_col = response_var_name + \"_cum_pct\"\n",
    "        DATASET[cum_pct_col] = (DATASET[cumsum_col])/(total_sum)*100\n",
    "        print(f\"Successfully calculated cumulative sum and cumulative percent correspondent to the response variable {response_var_name}.\")\n",
    "    \n",
    "    print(\"Successfully aggregated and ordered the dataset to plot. Check the 10 first rows of this returned dataset:\\n\")\n",
    "    print(DATASET.head(10))\n",
    "    \n",
    "    # Check if the total of plotted categories is limited:\n",
    "    if not (limit_of_plotted_categories is None):\n",
    "        \n",
    "        # Since the value is not None, we have to limit it\n",
    "        # Check if the limit is lower than or equal to the length of the dataframe.\n",
    "        # If it is, we simply copy the columns to the series (there is no need of\n",
    "        # a memory-consuming loop or of applying the head method to a local copy\n",
    "        # of the dataframe):\n",
    "        df_length = len(DATASET)\n",
    "            \n",
    "        if (limit_of_plotted_categories <= df_length):\n",
    "            # Simply copy the columns to the graphic series:\n",
    "            categories = DATASET[categorical_var_name]\n",
    "            responses = DATASET[response_var_name]\n",
    "            # If there is a cum_pct column, copy it to a series too:\n",
    "            if (calculate_and_plot_cumulative_percent):\n",
    "                cum_pct = plotted_df[cum_pct_col]\n",
    "        \n",
    "        else:\n",
    "            # The limit is lower than the total of categories,\n",
    "            # so we actually have to limit the size of plotted df:\n",
    "        \n",
    "            # If aggregate_function is not 'sum', we simply apply\n",
    "            # the head method to obtain the first rows (number of\n",
    "            # rows input as parameter; if no parameter is input, the\n",
    "            # number of 5 rows is used):\n",
    "            if (aggregate_function != 'sum'):\n",
    "                # Limit to the number limit_of_plotted_categories:\n",
    "                # create another local copy of the dataframe not to\n",
    "                # modify the returned dataframe object:\n",
    "                plotted_df = DATASET.head(limit_of_plotted_categories)\n",
    "\n",
    "                # Create the series of elements to plot:\n",
    "                categories = plotted_df[categorical_var_name]\n",
    "                responses = plotted_df[response_var_name]\n",
    "                # If the cumulative percent was obtained, create the series for it:\n",
    "                if (calculate_and_plot_cumulative_percent):\n",
    "                    cum_pct = plotted_df[cum_pct_col]\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Firstly, copy the elements that will be kept to x, y and (possibly) cum_pct\n",
    "                # lists.\n",
    "                # Start the lists:\n",
    "                categories = []\n",
    "                responses = []\n",
    "                if (calculate_and_plot_cumulative_percent):\n",
    "                    cum_pct = [] # start this list only if its needed to save memory\n",
    "\n",
    "                for i in range (0, limit_of_plotted_categories):\n",
    "                    # i goes from 0 (first index) to limit_of_plotted_categories - 1\n",
    "                    # (index of the last category to be kept):\n",
    "                    # copy the elements from the DATASET to the list\n",
    "                    # category is the 1st column (column 0); response is the 2nd (col 1);\n",
    "                    # and cumulative percent is the 4th (col 3):\n",
    "                    categories.append(DATASET.iloc[i, 0])\n",
    "                    responses.append(DATASET.iloc[i, 1])\n",
    "                    \n",
    "                    if (calculate_and_plot_cumulative_percent):\n",
    "                        cum_pct.append(DATASET.iloc[i, 3]) # only if there is something to iloc\n",
    "                    \n",
    "                # Now, i = limit_of_plotted_categories - 1\n",
    "                # Create a variable to store the sum of other responses\n",
    "                other_responses = 0\n",
    "                # loop from i = limit_of_plotted_categories to i = df_length-1, index\n",
    "                # of the last element. Notice that this loop may have a single call, if there\n",
    "                # is only one element above the limit:\n",
    "                for i in range (limit_of_plotted_categories, (df_length - 1)):\n",
    "                    \n",
    "                    other_responses = other_responses + (DATASET.iloc[i, 1])\n",
    "                \n",
    "                # Now, add the last elements to the series:\n",
    "                # The last category is named 'others':\n",
    "                categories.append('others')\n",
    "                # The correspondent aggregated response is the value \n",
    "                # stored in other_responses:\n",
    "                responses.append(other_responses)\n",
    "                # The cumulative percent is 100%, since this must be the sum of all\n",
    "                # elements (the previous ones plus the ones aggregated as 'others'\n",
    "                # must totalize 100%).\n",
    "                # On the other hand, the cumulative percent is stored only if needed:\n",
    "                cum_pct.append(100)\n",
    "    \n",
    "    else:\n",
    "        # This is the situation where there is no limit of plotted categories. So, we\n",
    "        # simply copy the columns to the plotted series (it is equivalent to the \n",
    "        # situation where there is a limit, but the limit is equal or inferior to the\n",
    "        # size of the dataframe):\n",
    "        categories = DATASET[categorical_var_name]\n",
    "        responses = DATASET[response_var_name]\n",
    "        # If there is a cum_pct column, copy it to a series too:\n",
    "        if (calculate_and_plot_cumulative_percent):\n",
    "            cum_pct = plotted_df[cum_pct_col]\n",
    "    \n",
    "    \n",
    "    # Now the data is prepared and we only have to plot \n",
    "    # categories, responses, and cum_pct:\n",
    "    \n",
    "    # Set labels and titles for the case they are None\n",
    "    if (plot_title is None):\n",
    "        plot_title = f\"Bar_chart_for_{response_var_name}_by_{categorical_var_name}\"\n",
    "    \n",
    "    if (horizontal_axis_title is None):\n",
    "\n",
    "        horizontal_axis_title = categorical_var_name\n",
    "\n",
    "    if (vertical_axis_title is None):\n",
    "        # Notice that response_var_name already has the suffix indicating the\n",
    "        # aggregation function\n",
    "        vertical_axis_title = response_var_name\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 70 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    plt.title(plot_title)\n",
    "    ax1.set_xlabel(horizontal_axis_title)\n",
    "    ax1.set_ylabel(vertical_axis_title, color = 'blue')\n",
    "    \n",
    "    if (orientation == 'horizontal'):\n",
    "        \n",
    "        # Horizontal bars used - barh method (bar horizontal):\n",
    "        # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.barh.html\n",
    "        # Now, the categorical variables stored in series categories must be\n",
    "        # positioned as the vertical axis Y, whereas the correspondent responses\n",
    "        # must be in the horizontal axis X.\n",
    "        ax1.barh(categories, responses, color = 'blue', label = categorical_var_name)\n",
    "        #.barh(y, x, ...)\n",
    "        \n",
    "        if (calculate_and_plot_cumulative_percent):\n",
    "            # Let's plot the line for the cumulative percent\n",
    "            # Set the grid for the bar chart as False. If it is True, there will\n",
    "            # be to grids, one for the bars and other for the percents, making \n",
    "            # the image difficult to interpretate:\n",
    "            ax1.grid(False)\n",
    "            \n",
    "            # Create the twin plot for the cumulative percent:\n",
    "            ax2 = ax1.twinx()\n",
    "            # Here, the x axis must be the cum_pct value, and the Y\n",
    "            # axis must be categories (it must be correspondent to the\n",
    "            # bar chart)\n",
    "            ax2.plot(cum_pct, categories, '-ro', color = 'red', label = \"cumulative\\npercent\")\n",
    "            #.plot(x, y, ...)\n",
    "            ax2.tick_params('x', color = 'red')\n",
    "            ax2.set_ylabel(\"Cumulative Percent (\\%)\", color = 'red')\n",
    "            ax2.legend()\n",
    "            ax2.grid(grid) # shown if user set grid = True\n",
    "            # If user wants to see the grid, it is shown only for the cumulative line.\n",
    "        \n",
    "        else:\n",
    "            # There is no cumulative line, so the parameter grid must control \n",
    "            # the bar chart's grid\n",
    "            ax1.legend()\n",
    "            ax1.grid(grid)\n",
    "        \n",
    "    else: \n",
    "        # If None or an invalid orientation was used, set it as vertical\n",
    "        # Use Matplotlib standard bar method (vertical bar):\n",
    "        # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html#matplotlib.pyplot.bar\n",
    "        \n",
    "        # In this standard case, the categorical variables (categories) are positioned\n",
    "        # as X, and the responses as Y:\n",
    "        ax1.bar(categories, responses, color = 'blue', label = categorical_var_name)\n",
    "        #.bar(x, y, ...)\n",
    "        \n",
    "        if (calculate_and_plot_cumulative_percent):\n",
    "            # Let's plot the line for the cumulative percent\n",
    "            # Set the grid for the bar chart as False. If it is True, there will\n",
    "            # be to grids, one for the bars and other for the percents, making \n",
    "            # the image difficult to interpretate:\n",
    "            ax1.grid(False)\n",
    "            \n",
    "            # Create the twin plot for the cumulative percent:\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.plot(categories, cum_pct, '-ro', color = 'red', label = \"cumulative\\npercent\")\n",
    "            #.plot(x, y, ...)\n",
    "            ax2.tick_params('y', color = 'red')\n",
    "            ax2.set_ylabel(\"Cumulative Percent (\\%)\", color = 'red')\n",
    "            ax2.legend()\n",
    "            ax2.grid(grid) # shown if user set grid = True\n",
    "            # If user wants to see the grid, it is shown only for the cumulative line.\n",
    "        \n",
    "        else:\n",
    "            # There is no cumulative line, so the parameter grid must control \n",
    "            # the bar chart's grid\n",
    "            ax1.legend()\n",
    "            ax1.grid(grid)\n",
    "    \n",
    "    # Notice that the .plot method is used for generating the plot for both orientations.\n",
    "    # It is different from .bar and .barh, which specify the orientation of a bar; or\n",
    "    # .hline (creation of an horizontal constant line); or .vline (creation of a vertical\n",
    "    # constant line).\n",
    "    \n",
    "    # Now the parameters specific to the configurations are finished, so we can go back\n",
    "    # to the general code:\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "        \n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"/\"\n",
    "        \n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"bar_chart\"\n",
    "        \n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 110 dpi\n",
    "            png_resolution_dpi = 110\n",
    "        \n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "        \n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "    \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for time series visualization**\n",
    "        x1, y1, lab1: blue\n",
    "        x2, y2, lab2: red\n",
    "        x3, y3, lab3: green\n",
    "        x4, y4, lab4: black\n",
    "        x5, y5, lab5: magenta\n",
    "        x6, y6, lab6: yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "azdata_cell_guid": "ef571494-3eb2-4dcc-9ebb-00c1fd6e9aad",
    "tags": [
     "CELL_7"
    ]
   },
   "outputs": [],
   "source": [
    "def time_series_vis (x1 = None, y1 = None, x2 = None, y2 = None, x3 = None, y3 = None, x4 = None, y4 = None, x5 = None, y5 = None, x6 = None, y6 = None, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, add_splines_lines = True, add_scatter_dots = False, lab1 = None, lab2 = None, lab3 = None, lab4 = None, lab5 = None, lab6 = None, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 110):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    if (add_splines_lines == True):\n",
    "        line_value = '-'\n",
    "    else:\n",
    "        line_value = ''\n",
    "    \n",
    "    if (add_scatter_dots == True):\n",
    "        marker_value = 'o'\n",
    "    else:\n",
    "        marker_value = ''\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    \n",
    "    if not (lab1 is None):\n",
    "        \n",
    "        label_1 = lab1\n",
    "    \n",
    "    else:\n",
    "        label_1 = \"Y1\"\n",
    "\n",
    "    if not (x1 is None):\n",
    "        ax.plot(x1, y1, linestyle = line_value, marker = marker_value, color='blue', label=label_1)\n",
    "    \n",
    "    if not (x2 is None):\n",
    "        #runs only when both are present\n",
    "        if not (lab2 is None):\n",
    "            label_2 = lab2\n",
    "        else:\n",
    "            label_2 = \"Y2\"\n",
    "        \n",
    "        ax.plot(x2, y2, linestyle = line_value, marker = marker_value, color='red', label=label_2)\n",
    "    \n",
    "    if not (x3 is None):\n",
    "                \n",
    "        if not (lab3 is None):\n",
    "            label_3 = lab3\n",
    "        else:\n",
    "            label_3 = \"Y3\"\n",
    "        \n",
    "        ax.plot(x3, y3, linestyle = line_value, marker = marker_value, color='green', label=label_3)\n",
    "    \n",
    "    if not (x4 is None):\n",
    "                \n",
    "        if not (lab4 is None):\n",
    "            label_4 = lab4\n",
    "        else:\n",
    "            label_4 = \"Y4\"\n",
    "        \n",
    "        ax.plot(x4, y4, linestyle = line_value, marker = marker_value, color='black', label=label_4)\n",
    "    \n",
    "    if not (x5 is None):\n",
    "               \n",
    "        if not (lab5 is None):\n",
    "            label_5 = lab5\n",
    "        else:\n",
    "            label_5 = \"Y5\"\n",
    "        \n",
    "        ax.plot(x5, y5, linestyle = line_value, marker = marker_value, color='magenta', label=label_5)\n",
    "   \n",
    "    if not (x6 is None):\n",
    "               \n",
    "        if not (lab6 is None):\n",
    "            label_6 = lab6\n",
    "        else:\n",
    "            label_6 = \"Y6\"\n",
    "        \n",
    "        ax.plot(x6, y6, linestyle = line_value, marker = marker_value, color='yellow', label=label_6)\n",
    "   \n",
    "    if not (plot_title is None):\n",
    "        #graphic's title\n",
    "        ax.set_title(plot_title) \n",
    "    \n",
    "    if not (horizontal_axis_title is None):\n",
    "        #X-axis title\n",
    "        ax.set_xlabel(horizontal_axis_title)\n",
    "    \n",
    "    if not (vertical_axis_title is None):\n",
    "        #Y-axis title\n",
    "        ax.set_ylabel(vertical_axis_title)\n",
    "    \n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 70 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    ax.grid(grid)\n",
    "    ax.legend()\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "        \n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"/\"\n",
    "        \n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"time_series_vis\"\n",
    "        \n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 110 dpi\n",
    "            png_resolution_dpi = 110\n",
    "        \n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "        \n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "    \n",
    "    #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for column filtering (selecting) or column renaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_filter_rename (df, cols_list, mode = 'filter'):\n",
    "    \n",
    "    import pandas as pd\n",
    "    \n",
    "    #mode = 'filter' for filtering only the list of columns passed as cols_list;\n",
    "    #mode = 'rename' for renaming the columns with the names passed as cols_list.\n",
    "    \n",
    "    #cols_list = list of strings containing the names (headers) of the columns to select\n",
    "    # (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "    # For instance: cols_list = ['col1', 'col2', 'col3'] will \n",
    "    # select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "    # Declare the names inside quotes.\n",
    "    \n",
    "    print(f\"Original columns in the dataframe:\\n{df.columns}\")\n",
    "    \n",
    "    if (mode == 'filter'):\n",
    "        \n",
    "        #filter the dataframe so that it will contain only the cols_list.\n",
    "        df = df[cols_list]\n",
    "        print(\"Dataframe filtered according to the list provided.\")\n",
    "        \n",
    "    elif (mode == 'rename'):\n",
    "        \n",
    "        #Check if the number of columns of the dataset is equal to the number of elements\n",
    "        # of the new list. It will avoid raising an exception error.\n",
    "        boolean_filter = (len(cols_list) == len(df.columns))\n",
    "        \n",
    "        if (boolean_filter == False):\n",
    "            #Impossible to rename, number of elements are different.\n",
    "            print(\"The number of columns of the dataframe is different from the number of elements of the list. Please, provide a list with number of elements equals to the number of columns.\")\n",
    "        \n",
    "        else:\n",
    "            #Same number of elements, so that we can update the columns' names.\n",
    "            df.columns = cols_list\n",
    "            print(\"Dataframe columns renamed according to the list provided.\")\n",
    "            print(\"Warning: the substitution is element-wise: the first element of the list is now the name of the first column, and so on, ..., so that the last element is the name of the last column.\")\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"Enter a valid mode: \\'filter\\' or \\'rename\\'.\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for reversing the log-transform - applying the exponential transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_log_transform(df, subset = None, create_new_columns = True, new_columns_suffix = \"_originalScale\"):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    #### WARNING: This function will eliminate rows where the selected variables present \n",
    "    #### values lower or equal to zero (condition for the logarithm to be applied).\n",
    "    \n",
    "    # subset = None\n",
    "    # Set subset = None to transform the whole dataset. Alternatively, pass a list with \n",
    "    # columns names for the transformation to be applied. For instance:\n",
    "    # subset = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "    # as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "    # Declaring the full list of columns is equivalent to setting subset = None.\n",
    "    \n",
    "    # create_new_columns = True\n",
    "    # Alternatively, set create_new_columns = True to store the transformed data into new\n",
    "    # columns. Or set create_new_columns = False to overwrite the existing columns\n",
    "    \n",
    "    #new_columns_suffix = \"_log\"\n",
    "    # This value has effect only if create_new_column = True.\n",
    "    # The new column name will be set as column + new_columns_suffix. Then, if the original\n",
    "    # column was \"column1\" and the suffix is \"_originalScale\", the new column will be named \n",
    "    # as \"collumn1_originalScale\".\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    # Check if a subset was defined. If so, make columns_list = subset \n",
    "    if not (subset is None):\n",
    "        \n",
    "        columns_list = subset\n",
    "    \n",
    "    else:\n",
    "        #There is no declared subset. Then, make columns_list equals to the list of\n",
    "        # columns of the dataframe.\n",
    "        columns_list = subset.columns\n",
    "    \n",
    "    #Loop through each column:\n",
    "    for column in columns_list:\n",
    "        #access each element in the list column_list. The element is named 'column'.\n",
    "        \n",
    "        # The exponential transformation can be applied to zero and negative values,\n",
    "        # so we remove the boolean filter.\n",
    "        \n",
    "        #Check if a new column will be created, or if the original column should be\n",
    "        # substituted.\n",
    "        if (create_new_columns == True):\n",
    "            # Create a new column.\n",
    "            \n",
    "            # The new column name will be set as column + new_columns_suffix\n",
    "            new_column_name = column + new_columns_suffix\n",
    "        \n",
    "        else:\n",
    "            # Overwrite the existing column. Simply set new_column_name as the value 'column'\n",
    "            new_column_name = column\n",
    "        \n",
    "        # Calculate the column value as the log transform of the original series (column)\n",
    "        df[new_column_name] = np.exp(df[column])\n",
    "    \n",
    "    print(\"The log_transform was successfully reversed through the exponential transformation. Check the 10 first rows of the new dataset:\\n\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for reversing Box-Cox transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "CELL_13"
    ]
   },
   "outputs": [],
   "source": [
    "def reverse_box_cox (df, column_to_transform, lambda_boxcox, suffix = '_ReversedBoxCox'):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # This function will process a single column column_to_transform \n",
    "    # of the dataframe df per call.\n",
    "    \n",
    "    # Check https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html\n",
    "    ## Box-Cox transform is given by:\n",
    "    ## y = (x**lmbda - 1) / lmbda,  for lmbda != 0\n",
    "    ## log(x),                  for lmbda = 0\n",
    "    \n",
    "    # column_to_transform must be a string with the name of the column.\n",
    "    # e.g. column_to_transform = 'column1' to transform a column named as 'column1'\n",
    "    \n",
    "    # lambda_boxcox must be a float value. e.g. lamda_boxcox = 1.7\n",
    "    # If you calculated lambda from the function box_cox_transform and saved the\n",
    "    # transformation data summary dictionary as data_sum_dict, simply set:\n",
    "    # lambda_boxcox = data_sum_dict['lambda_boxcox']\n",
    "    # This will access the value on the key 'lambda_boxcox' of the dictionary, which\n",
    "    # contains the lambda. \n",
    "    \n",
    "    # Analogously, spec_lim_dict['Inf_spec_lim_transf'] access\n",
    "    # the inferior specification limit transformed; and spec_lim_dict['Sup_spec_lim_transf'] \n",
    "    # access the superior specification limit transformed.\n",
    "    \n",
    "    #suffix: string (inside quotes).\n",
    "    # How the transformed column will be identified in the returned data_transformed_df.\n",
    "    # If y_label = 'Y' and suffix = '_ReversedBoxCox', the transformed column will be\n",
    "    # identified as '_ReversedBoxCox'.\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name\n",
    "    \n",
    "    y = df[column_to_transform]\n",
    "    \n",
    "    if (lambda_boxcox == 0):\n",
    "        #ytransf = np.log(y), according to Box-Cox definition. Then\n",
    "        #y_retransform = np.exp(y)\n",
    "        #In the case of this function, ytransf is passed as the argument y.\n",
    "        y_transform = np.exp(y)\n",
    "    \n",
    "    else:\n",
    "        #apply Box-Cox function:\n",
    "        #y_transf = (y**lmbda - 1) / lmbda. Then,\n",
    "        #y_retransf ** (lmbda) = (y_transf * lmbda) + 1\n",
    "        #y_retransf = ((y_transf * lmbda) + 1) ** (1/lmbda), where ** is the potentiation\n",
    "        #In the case of this function, ytransf is passed as the argument y.\n",
    "        y_transform = ((y * lambda_boxcox) + 1) ** (1/lambda_boxcox)\n",
    "    \n",
    "    if not (suffix is None):\n",
    "        #only if a suffix was declared\n",
    "        #concatenate the column name to the suffix\n",
    "        new_col = column_to_transform + suffix\n",
    "    \n",
    "    else:\n",
    "        #concatenate the column name to the standard '_ReversedBoxCox' suffix\n",
    "        new_col = column_to_transform + '_ReversedBoxCox'\n",
    "    \n",
    "    data_retransformed_df = df\n",
    "    data_retransformed_df[new_col] = y_transform\n",
    "    #dataframe contendo os dados transformados\n",
    "    \n",
    "    print(\"Data successfully retransformed. Check the 10 first retransformed rows:\\n\")\n",
    "    print(data_retransformed_df.head(10))\n",
    "    print(\"\\n\") #line break\n",
    " \n",
    "    return data_retransformed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for One-Hot Encoding categorical features**\n",
    "- Transform categorical values without notion of order into numerical (binary) features.\n",
    "- Process a single categorical column per function call.\n",
    "- For each category, the One-Hot Encoder creates a new column in the dataset. This new column is represented by a binary variable which is equals to zero if the row is not classified in that category; and is equals to 1 when the row represents an element in that category.\n",
    "- The new columns will be named as the original possible categories.\n",
    "- Each column is a binary variable of the type \"is classified in this category or not\".\n",
    "\n",
    "Therefore, for a category \"A\", a column named \"A\" is created.\n",
    "- If the row is an element from category \"A\", the value for the column \"A\" is 1.\n",
    "- If not, the value for column \"A\" is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "dp2axJ_OsfZV"
   },
   "outputs": [],
   "source": [
    "def OneHotEncode_df (df, subset_of_features_to_be_encoded):\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    \n",
    "    #df: the whole dataframe to be processed.\n",
    "    \n",
    "    #subset_of_features_to_be_encoded: list of strings (inside quotes), \n",
    "    # containing the names of the columns with the categorical variables that will be \n",
    "    # encoded. If a single column will be encoded, declare this parameter as list with\n",
    "    # only one element e.g.subset_of_features_to_be_encoded = [\"column1\"] \n",
    "    # will analyze the column named as 'column1'; \n",
    "    # subset_of_features_to_be_encoded = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "    # with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "    \n",
    "    #Start an encoding dictionary empty:\n",
    "    encoding_dict = {}\n",
    "    \n",
    "    #Start a copy of the original dataframe. This copy will be updated to create the new\n",
    "    # transformed dataframe. Then, we avoid manipulating the original object.\n",
    "    new_df = df\n",
    "    \n",
    "    #loop through each column of the subset:\n",
    "    for column in subset_of_features_to_be_encoded:\n",
    "        \n",
    "        # Loop through each element (named 'column') of the list of columns to analyze,\n",
    "        # subset_of_features_to_be_encoded\n",
    "        \n",
    "        #We could process the whole subset at once, but it could make us lose information\n",
    "        # about the generated columns\n",
    "        \n",
    "        # set a subset of the dataframe X containing 'column' as the only column:\n",
    "        # it will be equivalent to using .reshape(-1,1) to set a 1D-series\n",
    "        # or array in the shape for scikit-learn:\n",
    "        # For doing so, pass a list of columns for column filtering, containing\n",
    "        # the object column as its single element:\n",
    "        X  = df[[column]]\n",
    "        \n",
    "        #Start the OneHotEncoder object:\n",
    "        encoded_X = OneHotEncoder()\n",
    "        \n",
    "        #Fit the object to that column:\n",
    "        encoded_X = encoded_X.fit_transform(X) \n",
    "        \n",
    "        #It will create a scipy sparse matrix full of null values.\n",
    "        #Show encoded categories and store this array. \n",
    "        #It will give the proper columns' names:\n",
    "        encoded_columns = encoded_X.categories_\n",
    "\n",
    "        #encoded_columns is a list containing a single element.\n",
    "        # This element is an array like:\n",
    "        # array(['cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8'], dtype=object)\n",
    "        # Then, this array is the element of index 0 from the list encoded_columns.\n",
    "        # It is represented as encoded_columns[0]\n",
    "\n",
    "        #Therefore, we actually want the array which is named as encoded_columns[0]\n",
    "        # Each element of this array is the name of one of the encoded columns. In the\n",
    "        # example above, the element 'cat2' would be accessed as encoded_columns[0][1],\n",
    "        # since it is the element of index [1] (second element) from the array \n",
    "        # encoded_columns[0].\n",
    "        \n",
    "        #Update the dictionary to store the original column name as key, and the categories\n",
    "        # array as the value:\n",
    "        encoding_dict.update({column: encoded_columns[0]})\n",
    "\n",
    "        #Create the dense array:\n",
    "        encoded_X = encoded_X.toarray()\n",
    "        #print(\"One-Hot Encoding Matrix:\")\n",
    "        #print(encoded_X)\n",
    "\n",
    "        #Convert it into a dataframe:\n",
    "        encoded_X_df = pd.DataFrame(encoded_X)\n",
    "\n",
    "        #modify the names of the columns for the ones stored in the array encoded_columns[0]\n",
    "        # Simply access the values stored in the dictionary. To access a value, simply pass\n",
    "        # the name of the key (in quotes) inside brackets after the name of the dictionary,\n",
    "        # just as accessing a column from a dataframe:\n",
    "        encoded_X_df.columns = encoding_dict[column]\n",
    "        \n",
    "        #Inner join the new dataset with the encoded dataset.\n",
    "        # Use the index as the key, since indices are necessarily correspondent.\n",
    "        # To use join on index, we apply pandas .concat method.\n",
    "        # To join on a specific key, we could use pandas .merge method with the arguments\n",
    "        # left_on = 'left_key', right_on = 'right_key'; or, if the keys have same name,\n",
    "        # on = 'key':\n",
    "        # Check Pandas merge and concat documentation:\n",
    "        # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge.html\n",
    "        \n",
    "        new_df = pd.concat([new_df, encoded_X_df], axis = 1, join = \"inner\")\n",
    "        # When axis = 0, the .concat operation occurs in the row level, so the rows\n",
    "        # of the second dataframe are added to the bottom of the first one.\n",
    "        # It is the SQL union, and creates a dataframe with more rows, and\n",
    "        # total of columns equals to the total of columns of the first dataframe\n",
    "        # plus the columns of the second one that were not in the first dataframe.\n",
    "        # When axis = 1, the operation occurs in the column level: the two\n",
    "        # dataframes are laterally merged using the index as the key, \n",
    "        # preserving all columns from both dataframes. Therefore, the number of\n",
    "        # rows will be the total of rows of the dataframe with more entries,\n",
    "        # and the total of columns will be the sum of the total of columns of\n",
    "        # the first dataframe with the total of columns of the second dataframe.\n",
    "        \n",
    "        print(f\"Successfully encoded column \\'{column}\\' and merged the encoded columns to the dataframe.\")\n",
    "        print(\"Check first 5 rows of the encoded table that was merged:\\n\")\n",
    "        print(encoded_X_df.head())\n",
    "        # The default of the head method, when no parameter is printed, is to show 5 rows; if an\n",
    "        # integer number Y is passed as argument .head(Y), Pandas shows the first Y-rows.\n",
    "    \n",
    "    print(\"Finished One-Hot Encoding. Returning the new transformed dataframe; and an encoding dictionary with the original columns as keys, and arrays containing the categories on those columns as the correspondent values.\")\n",
    "    print(f\"For each category in the columns \\'{subset_of_features_to_be_encoded}\\', a new column has value 1, if it is the actual category of that row; or is 0 if not.\")\n",
    "    print(\"Check the first 10 rows of the new dataframe:\\n\")\n",
    "    print(new_df.head(10))\n",
    "\n",
    "    #return the transformed dataframe and the encoding dictionary:\n",
    "    return new_df, encoding_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for reversing the scaling of the features**\n",
    "- `mode = 'standard'`.\n",
    "- `mode = 'min_max'`.\n",
    "- `mode = 'factor'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_feature_scaling (df, subset_of_features_to_scale, scaling_params, mode = 'standard', suffix = '_reverseScaling'):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    # Scikit-learn Preprocessing data guide:\n",
    "    # https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler\n",
    "    # Standard scaler documentation:\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "    # Min-Max scaler documentation:\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.set_params\n",
    "    \n",
    "    ## Machine Learning algorithms are extremely sensitive to scale. \n",
    "    \n",
    "    ## This function provides 3 methods (modes) of scaling:\n",
    "    ## mode = 'standard': applies the standard scaling, \n",
    "    ##  which creates a new variable with mean = 0; and standard deviation = 1.\n",
    "    ##  Each value Y is transformed as Ytransf = (Y - u)/s, where u is the mean \n",
    "    ##  of the training samples, and s is the standard deviation of the training samples.\n",
    "    \n",
    "    ## mode = 'min_max': applies min-max normalization, with a resultant feature \n",
    "    ## ranging from 0 to 1. each value Y is transformed as \n",
    "    ## Ytransf = (Y - Ymin)/(Ymax - Ymin), where Ymin and Ymax are the minimum and \n",
    "    ## maximum values of Y, respectively.\n",
    "    \n",
    "    ## mode = 'factor': divides the whole series by a numeric value provided as argument. \n",
    "    ## For a factor F, the new Y values will be Ytransf = Y/F.\n",
    "    \n",
    "    #df: the whole dataframe to be processed.\n",
    "    \n",
    "    #subset_of_features_to_be_scaled: list of strings (inside quotes), \n",
    "    # containing the names of the columns with the categorical variables that will be \n",
    "    # encoded. If a single column will be encoded, declare this parameter as list with\n",
    "    # only one element e.g.subset_of_features_to_be_scaled = [\"column1\"] \n",
    "    # will analyze the column named as 'column1'; \n",
    "    # subset_of_features_to_be_scaled = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "    # with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "    \n",
    "    ## WARNING: The mode 'factor' demmands the input of the list of factors that will be \n",
    "    # used for normalizing each column.\n",
    "    \n",
    "    ## For the mode 'factor', declare scaling_params as a dictionary containing the \n",
    "    # column name as the key and the correspondent factor as the value.\n",
    "    # e.g. subset_of_features_to_scale = ['col1', 'col2'], 'col1' will be divided by 2.0, \n",
    "    # and 'col2' will be divided by 3.2,  then:\n",
    "    # scaling_params = {'col1': 2.0, 'col2': 3.2}\n",
    "    \n",
    "    ## WARNING: For scaling_params (when scale_with_new_params = False and \n",
    "    # mode = 'standard' or mode = 'min_max'), the dictionary must be declared with the\n",
    "    # column name as the key, and the whole dictionary of parameters as the correspondent\n",
    "    # value. Then, it will be a dictionary of dictionaries, where there is a dictionary \n",
    "    # correspondent to each key. Each dictionary should be declared in the same way as the \n",
    "    # scaling_dictionary printed as output when the scaler is trained.\n",
    "    \n",
    "    #suffix: string (inside quotes).\n",
    "    # How the transformed column will be identified in the returned data_transformed_df.\n",
    "    # If y_label = 'Y' and suffix = '_reverseScaling', the transformed column will be\n",
    "    # identified as '_reverseScaling'.\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name\n",
    "      \n",
    "    if (suffix is None):\n",
    "        #set as the default\n",
    "        suffix = '_reverseScaling'\n",
    "    \n",
    "    #Start a copy of the original dataframe. This copy will be updated to create the new\n",
    "    # transformed dataframe. Then, we avoid manipulating the original object.\n",
    "    new_df = df\n",
    "    \n",
    "    # Use a previously obtained scaling_dict:\n",
    "        \n",
    "    scaling_dict = scaling_params\n",
    "        \n",
    "    if (mode == 'factor'):\n",
    "            \n",
    "        for column in subset_of_features_to_scale:\n",
    "            # Loop through each element (named 'column') of the list of columns \n",
    "            # to analyze:\n",
    "                \n",
    "            # Create the new_column name:\n",
    "            new_column = column + suffix\n",
    "            # Create the new_column.\n",
    "            # Once the scaling was performed through division, the reverse of it consists\n",
    "            # on a multiplication:\n",
    "            \n",
    "            new_df[new_column] = (new_df[column])*(scaling_dict[column])\n",
    "                \n",
    "            print(f\"Successfully re-scaled column {column}.\")\n",
    "\n",
    "            print(\"Successfully re-scaled the dataframe.\")\n",
    "            print(\"Check 10 first rows of the new dataframe:\\n\")\n",
    "            print(new_df.head(10))\n",
    "\n",
    "            return new_df\n",
    "        \n",
    "    elif (mode == 'standard'):\n",
    "            \n",
    "        for column in subset_of_features_to_scale:\n",
    "            # Loop through each element (named 'column') of the list of columns \n",
    "            # to analyze:\n",
    "                \n",
    "            #Create a dataframe X by subsetting only the analyzed column\n",
    "            # it will be equivalent to using .reshape(-1,1) to set a 1D-series\n",
    "            # or array in the shape for scikit-learn:\n",
    "            # For doing so, pass a list of columns for column filtering, containing\n",
    "            # the object column as its single element:\n",
    "            X = new_df[[column]]\n",
    "                    \n",
    "            #start the scaler:\n",
    "            scaler = StandardScaler()\n",
    "                    \n",
    "            #Get the dictionary of scaling parameters for the feature 'column':\n",
    "            # For that, access the key: 'column' in the scaling_dict dictionary\n",
    "            # to retrieve its value, i.e., the dictionary for that feature:\n",
    "            scaling_params = scaling_dict[column]\n",
    "                    \n",
    "            # Now, set the scaler parameters to be equal to the values retrieved\n",
    "            # as the dictionary scaling_params:\n",
    "            scaler = scaler.set_params(scaling_params)\n",
    "            # Notice that the .set_params method substitute the step where we applied\n",
    "            # the .fit method.\n",
    "                    \n",
    "            #Invert the scaling of the feature, and store it as new array:\n",
    "            scaled_feature = scaler.inverse_transform(X)\n",
    "            # Notice that this step substitutes the application of the method\n",
    "            # scaler.transform(X), used for scaling the variable.\n",
    "\n",
    "            # Create the new_column name:\n",
    "            new_column = column + suffix\n",
    "            # Create the new_column by dividing the previous column by the scaling factor:\n",
    "                    \n",
    "            # Set the new column as scaled_feature\n",
    "            new_df[new_column] = scaled_feature\n",
    "                    \n",
    "            print(f\"Successfully re-scaled column {column}.\")\n",
    "                \n",
    "        print(\"Successfully re-scaled the dataframe.\")\n",
    "        print(\"Check 10 first rows of the new dataframe:\\n\")\n",
    "        print(new_df.head(10))\n",
    "                \n",
    "        return new_df\n",
    "        \n",
    "    elif (mode == 'min_max'):\n",
    "            \n",
    "        for column in subset_of_features_to_scale:\n",
    "            # Loop through each element (named 'column') of the list of columns \n",
    "            # to analyze:\n",
    "                \n",
    "            #Create a dataframe X by subsetting only the analyzed column\n",
    "            # it will be equivalent to using .reshape(-1,1) to set a 1D-series\n",
    "            # or array in the shape for scikit-learn:\n",
    "            # For doing so, pass a list of columns for column filtering, containing\n",
    "            # the object column as its single element:\n",
    "            X = new_df[[column]]\n",
    "                    \n",
    "            #start the scaler:\n",
    "            scaler = MinMaxScaler()\n",
    "                    \n",
    "            #Get the dictionary of scaling parameters for the feature 'column':\n",
    "            # For that, access the key: 'column' in the scaling_dict dictionary\n",
    "            # to retrieve its value, i.e., the dictionary for that feature:\n",
    "            scaling_params = scaling_dict[column]\n",
    "                    \n",
    "            # Now, set the scaler parameters to be equal to the values retrieved\n",
    "            # as the dictionary scaling_params:\n",
    "            scaler = scaler.set_params(scaling_params)\n",
    "            # Notice that the .set_params method substitute the step where we applied\n",
    "            # the .fit method.\n",
    "                    \n",
    "            #Invert the scaling of the feature, and store it as new array:\n",
    "            scaled_feature = scaler.inverse_transform(X)\n",
    "            # Notice that this step substitutes the application of the method\n",
    "            # scaler.transform(X), used for scaling the variable.\n",
    "                \n",
    "            # Create the new_column name:\n",
    "            new_column = column + suffix\n",
    "            # Create the new_column by dividing the previous column by the scaling factor:\n",
    "                    \n",
    "            # Set the new column as scaled_feature\n",
    "            new_df[new_column] = scaled_feature\n",
    "                    \n",
    "            print(f\"Successfully re-scaled column {column}.\")\n",
    "                \n",
    "        print(\"Successfully re-scaled the dataframe.\")\n",
    "        print(\"Check 10 first rows of the new dataframe:\\n\")\n",
    "        print(new_df.head(10))\n",
    "                \n",
    "        return new_df\n",
    "        \n",
    "    else:\n",
    "\n",
    "        print(\"Select a valid mode: standard, min_max, or factor.\")\n",
    "        return \"error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "## **Call the functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = 'aws'\n",
    "# SOURCE = 'google' for mounting the google drive;\n",
    "# SOURCE = 'aws' for accessing an AWS S3 bucket\n",
    "\n",
    "## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN SOURCE == 'aws':\n",
    "\n",
    "PATH_TO_STORE_IMPORTED_S3_BUCKET = '/'\n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET: path of the Python environment to which the\n",
    "# S3 bucket contents will be imported. If it is None, or if \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = '/', bucket will be imported to the root path. \n",
    "# Alternatively, input the path as a string (in quotes). e.g. \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = '/copied_s3_bucket'\n",
    "\n",
    "S3_BUCKET_NAME = 'name_of_aws_s3_bucket_to_be_accessed'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_KEY_PREFIX_FOLDER = None\n",
    "# S3_OBJECT_KEY_PREFIX_FOLDER = None. Keep it None or as an empty string \n",
    "# (S3_OBJECT_KEY_PREFIX_FOLDER = '') to import the whole bucket content, instead of a \n",
    "# single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, key_prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "mount_storage_system (source = SOURCE, path_to_store_imported_s3_bucket = PATH_TO_STORE_IMPORTED_S3_BUCKET, s3_bucket_name = S3_BUCKET_NAME, s3_obj_key_prefix = S3_OBJECT_KEY_PREFIX_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Downloading a file from Google Colab or AWS S3 to the local machine or uploading a file from the machine to S3 or to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = 'aws'\n",
    "# SOURCE = 'google' for downloading from (or uploading to) Google Colab's instant memory;\n",
    "# SOURCE = 'aws' for downloading from (or uploading to) an AWS S3 bucket.\n",
    "\n",
    "ACTION = 'download'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to AWS S3 or to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "OBJECT_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# OBJECT_TO_DOWNLOAD_FROM_COLAB = None. This option has effect only when\n",
    "# SOURCE == 'google'. In this case, this parameter is obbligatory. \n",
    "# Declare as OBJECT_TO_DOWNLOAD_FROM_COLAB the object that you want to download.\n",
    "# Since it is an object and not a string, it should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, OBJECT_TO_DOWNLOAD_FROM_COLAB = dict.\n",
    "# To download a dataframe named df, declare OBJECT_TO_DOWNLOAD_FROM_COLAB = df.\n",
    "# To export a model named keras_model, declare OBJECT_TO_DOWNLOAD_FROM_COLAB = keras_model\n",
    "    \n",
    "## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN source == 'aws'\n",
    "\n",
    "S3_BUCKET_NAME = None\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. S3_BUCKET_NAME = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "LOCAL_PATH_OF_STORAGE = '/'\n",
    "# LOCAL_PATH_OF_STORAGE: path of the local computer environment \n",
    "# to which the S3 bucket contents will be downloaded (ACTION == 'download'); or\n",
    "# path of the folder containing the file that will be uploaded in S3 (ACTION = 'upload'). \n",
    "# If it is None, or if LOCAL_PATH_OF_STORAGE = '/', files \n",
    "# will be imported to the root path. Alternatively, input the path as a string (in quotes). \n",
    "# Examples: LOCAL_PATH_OF_STORAGE = '/copied_s3_bucket'; \n",
    "# LOCAL_PATH_OF_STORAGE = \"/My_folder\"; LOCAL_PATH_OF_STORAGE = \"/Users/Me/Documents/\"\n",
    "# Notice that only the directories should be declared: do not include the file name and\n",
    "# its extension.\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = None\n",
    "# FILE_NAME_WITH_EXTENSION: string, in quotes, containing the file name which will be\n",
    "# downloaded from S3; or uploaded from S3, followed by its extension. \n",
    "## This parameter is obbligatory when SOURCE == 'aws'\n",
    "# Examples:\n",
    "# FILE_NAME_WITH_EXTENSION = 'Screen_Shot.png'; FILE_NAME_WITH_EXTENSION = 'dataset.csv',\n",
    "# FILE_NAME_WITH_EXTENSION = \"dictionary.pkl\", FILE_NAME_WITH_EXTENSION = \"model.h5\",\n",
    "# FILE_NAME_WITH_EXTENSION = 'doc.pdf', FILE_NAME_WITH_EXTENSION = 'model.dill'\n",
    "\n",
    "download_or_upload_file (source = SOURCE, action = ACTION, object_to_download_from_colab = OBJECT_TO_DOWNLOAD_FROM_COLAB, s3_bucket_name = S3_BUCKET_NAME, local_path_of_storage = LOCAL_PATH_OF_STORAGE, file_name_with_extension = FILE_NAME_WITH_EXTENSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "azdata_cell_guid": "0d4292a3-e6c6-41b2-bbda-0743ee04195e",
    "id": "g3zcdInJyHHm"
   },
   "outputs": [],
   "source": [
    "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, etc), \n",
    "## JSON, txt, or CSV (comma separated values) files.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"/\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"/\" \n",
    "# or FILE_DIRECTORY_PATH = \"/folder\"\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
    "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "# FILE_NAME_WITH_EXTENSION = \"file.csv\"\n",
    "\n",
    "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
    "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True \n",
    "# if you want to read a file with txt extension containing a text formatted as JSON \n",
    "# (but not saved as JSON).\n",
    "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the \n",
    "# function (below) must be set. If not, an error message will be raised.\n",
    "    \n",
    "HAS_HEADER = True\n",
    "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
    "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
    "\n",
    "## Parameters for loading txt or CSV files:\n",
    "\n",
    "TXT_CSV_COL_SEP = \"comma\"\n",
    "# TXT_CSV_COL_SEP = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "# or 'csv'. It informs how the different columns are separated.\n",
    "# Alternatively, TXT_CSV_COL_SEP = \"comma\" for columns separated by comma (\",\")\n",
    "# TXT_CSV_COL_SEP = \"whitespace\" for columns separated by simple spaces (\" \").\n",
    "\n",
    "SHEET_TO_LOAD = None\n",
    "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
    "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
    "# will be loaded.\n",
    "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
    "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
    "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "# name to load the sheet with that name.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "#The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = load_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, has_header = HAS_HEADER, txt_csv_col_sep = TXT_CSV_COL_SEP, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Converting JSON object to dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "azdata_cell_guid": "0d4292a3-e6c6-41b2-bbda-0743ee04195e",
    "id": "g3zcdInJyHHm"
   },
   "outputs": [],
   "source": [
    "JSON_OBJ_TO_CONVERT = json_object #Alternatively: object containing the JSON to be converted\n",
    "\n",
    "# JSON_OBJ_TO_CONVERT: object containing JSON, or string with JSON content to parse.\n",
    "# Objects may be: string with JSON formatted text;\n",
    "# list with nested dictionaries (JSON formatted);\n",
    "# dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "#The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = json_obj_to_dataframe (json_obj_to_convert = JSON_OBJ_TO_CONVERT, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing or exporting models and dictionaries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: import only a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_FILE_NAME = None\n",
    "# DICTIONARY_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no dictionary will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = '/'\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"/\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "#Model object saved as model.\n",
    "# Simply modify this object on the left of equality:\n",
    "model = import_export_model_or_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_file_name = DICTIONARY_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_to_export = DICT_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2: import only a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'dict_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_FILE_NAME = None\n",
    "# DICTIONARY_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no dictionary will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = '/'\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"/\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Dictionary saved as imported_dict.\n",
    "# Simply modify this object on the left of equality:\n",
    "imported_dict = import_export_model_or_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_file_name = DICTIONARY_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_to_export = DICT_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 3: import a model and a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_and_dict'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_FILE_NAME = None\n",
    "# DICTIONARY_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_FILE_NAME = None if no dictionary will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = '/'\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"/\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'arima'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model. Dictionary saved as imported_dict.\n",
    "# Simply modify these objects on the left of equality:\n",
    "model, imported_dict = import_export_model_or_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_file_name = DICTIONARY_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_to_export = DICT_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Making predictions with the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OBJECT = lstm_model # Alternatively: object storing another model\n",
    "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
    "# MODEL_OBJECT = elastic_net_linear_reg_model\n",
    "\n",
    "X_df = X\n",
    "# predict_for = 'subset' or predict_for = 'single_entry'\n",
    "# The function will automatically detect if it is dealing with lists, NumPy arrays\n",
    "# or Pandas dataframes. If X_df is a list or a single-dimension array, predict_for\n",
    "# will be set as 'single_entry'. If X is a multi-dimension NumPy array (as the\n",
    "# outputs for preparing data - even single_entry - for deep learning models), or if\n",
    "# it is a Pandas dataframe, the function will set predict_for = 'subset'\n",
    "    \n",
    "# X_df = subset of predictive variables (dataframe, NumPy array, or list).\n",
    "# If PREDICT_FOR = 'single_entry', X_df should be a list of parameters values.\n",
    "# e.g. X_df = [1.2, 3, 4] (dot is the decimal case separator, comma separate values). \n",
    "# Notice that the list should contain only the numeric values, in the same order of the\n",
    "# correspondent columns.\n",
    "# If PREDICT_FOR = 'subset' (prediction for multiple entries), X_df should be a dataframe \n",
    "# (subset) or a multi-dimensional NumPy array of the parameters values, as usual.\n",
    "\n",
    "DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset  \n",
    "# DATAFRAME_FOR_CONCATENATING_PREDICTIONS: if you want to concatenate the predictions\n",
    "# to a dataframe, pass it here:\n",
    "# e.g. DATAFRAME_FOR_CONCATENATING_PREDICTIONS = df\n",
    "# If the dataframe must be the same one passed as X, repeat the dataframe object here:\n",
    "# X_df = dataset, DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset.\n",
    "# Alternatively, if DATAFRAME_FOR_CONCATENATING_PREDICTIONS = None, \n",
    "# the prediction will be returned as a series or NumPy array, depending on the input format.\n",
    "# Notice that the concatenated predictions will be added as a new column.\n",
    "\n",
    "COLUMN_WITH_PREDICTIONS_SUFFIX = None\n",
    "# COLUMN_WITH_PREDICTIONS_SUFFIX = None. If the predictions are added as a new column\n",
    "# of the dataframe DATAFRAME_FOR_CONCATENATING_PREDICTIONS, you can declare this\n",
    "# parameter as string with a suffix for identifying the model. If no suffix is added, the new\n",
    "# column will be named 'y_pred'.\n",
    "# e.g. COLUMN_WITH_PREDICTIONS_SUFFIX = '_keras' will create a column named \"y_pred_keras\". This\n",
    "# parameter is useful when working with multiple models. Always start the suffix with underscore\n",
    "# \"_\" so that no blank spaces are added; the suffix will not be merged to the column; and there\n",
    "# will be no confusion with the dot (.) notation for methods, JSON attributes, etc.\n",
    "\n",
    "# Predictions returned as prediction_output\n",
    "# Simply modify this object (or variable) on the left of equality:\n",
    "prediction_output = make_model_predictions (model_object = MODEL_OBJECT, X = X_df, dataframe_for_concatenating_predictions = DATAFRAME_FOR_CONCATENATING_PREDICTIONS, col_with_predictions_suffix = COLUMN_WITH_PREDICTIONS_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculating probabilities associated to each class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OBJECT = lstm_model # Alternatively: object storing another model\n",
    "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
    "# MODEL_OBJECT = mlp_model\n",
    "\n",
    "X_df = X\n",
    "# predict_for = 'subset' or predict_for = 'single_entry'\n",
    "# The function will automatically detect if it is dealing with lists, NumPy arrays\n",
    "# or Pandas dataframes. If X_df is a list or a single-dimension array, predict_for\n",
    "# will be set as 'single_entry'. If X is a multi-dimension NumPy array (as the\n",
    "# outputs for preparing data - even single_entry - for deep learning models), or if\n",
    "# it is a Pandas dataframe, the function will set predict_for = 'subset'\n",
    "    \n",
    "# X_df = subset of predictive variables (dataframe, NumPy array, or list).\n",
    "# If PREDICT_FOR = 'single_entry', X_df should be a list of parameters values.\n",
    "# e.g. X_df = [1.2, 3, 4] (dot is the decimal case separator, comma separate values). \n",
    "# Notice that the list should contain only the numeric values, in the same order of the\n",
    "# correspondent columns.\n",
    "# If PREDICT_FOR = 'subset' (prediction for multiple entries), X_df should be a dataframe \n",
    "# (subset) or a multi-dimensional NumPy array of the parameters values, as usual.\n",
    "\n",
    "LIST_OF_CLASSES = list_of_classes\n",
    "# LIST_OF_CLASSES is the list of classes effectively used for training\n",
    "# the model. Set this parameter as the object returned from function\n",
    "# retrieve_classes_used_to_train\n",
    "\n",
    "TYPE_OF_MODEL = 'deep_learning'\n",
    "# TYPE_OF_MODEL = 'deep_learning' if Keras/TensorFlow or other deep learning\n",
    "# framework was used to obtain the model;\n",
    "# TYPE_OF_MODEL = 'other' for Scikit-learn or XGBoost models.\n",
    "\n",
    "DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset  \n",
    "# DATAFRAME_FOR_CONCATENATING_PREDICTIONS: if you want to concatenate the predictions\n",
    "# to a dataframe, pass it here:\n",
    "# e.g. DATAFRAME_FOR_CONCATENATING_PREDICTIONS = df\n",
    "# If the dataframe must be the same one passed as X, repeat the dataframe object here:\n",
    "# X_df = dataset, DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset.\n",
    "# Alternatively, if DATAFRAME_FOR_CONCATENATING_PREDICTIONS = None, \n",
    "# the prediction will be returned as a series or NumPy array, depending on the input format.\n",
    "# Notice that the concatenated predictions will be added as a new column.    \n",
    "# All of the new columns (appended or not) will have the prefix \"prob_class_\" followed\n",
    "# by the correspondent class name to identify them.\n",
    "\n",
    "\n",
    "# Probabilities returned as calculated_probability\n",
    "# Simply modify this object (or variable) on the left of equality:\n",
    "calculated_probability = calculate_class_probability (model_object = MODEL_OBJECT, X = X_df, list_of_classes = LIST_OF_CLASSES, type_of_model = TYPE_OF_MODEL, dataframe_for_concatenating_predictions = DATAFRAME_FOR_CONCATENATING_PREDICTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Merging (joining) dataframes on given keys; and sorting the merged table**\n",
    "- Merge (join) types:\n",
    "    - 'inner': resultant dataframe contains only the rows on the left dataframe with correspondent values on the right dataframe. Can be used for filtering a set of labelled rows. Results in no missing values;\n",
    "    - 'left': resultant dataframe contains all the rows from the left table (even those without correspondence on the right); and the rows from the right table that have correspondence on the left one. Since rows from the left table may not have correspondence, it may result in missing values.\n",
    "    - 'right': resultant dataframe contains all the rows from the right table (even those without correspondence on the right); and the rows from the left table that have correspondence on the right one. Since rows from the right table may not have correspondence, it may result in missing values.\n",
    "    - 'outer': in SQL, the Pandas 'outer' merge usually corresponds to the FULL OUTER JOIN: the resultant dataframe contains all rows from both tables, not taking in account if there is correspondence. So, it may result in missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_LEFT = dataset1 #Alternatively: object containing the dataset to be joined on the left\n",
    "DF_RIGHT = dataset2 #Alternatively: object containing the dataset to be joined on the right\n",
    "\n",
    "LEFT_KEY = \"left_key_column\" \n",
    "#Alternatively: (string) name of the column of the left dataframe to be used as key for \n",
    "# joining. Keep inside quotes.\n",
    "RIGHT_KEY = \"right_key_column\"\n",
    "#Alternatively: (string) name of the column of the right dataframe to be used as key for \n",
    "# joining. Keep inside quotes.\n",
    "\n",
    "HOW_TO_JOIN = \"inner\"\n",
    "#Alternatively: \"inner\", \"outer\", \"left\", \"right\".\n",
    "\n",
    "MERGED_SUFFIXES = ('_left', '_right')\n",
    "# SUFFIXES = ('_left', '_right') - tuple of the suffixes to be added to columns.\n",
    "# Example: suppose both datasets have the column 'Value'. The column from the left dataset\n",
    "# will be renamed as \"Value_left\", and the column from the right dataset will be renamed as\n",
    "# \"Value_right\".\n",
    "# Alternatively: modify the strings inside quotes to modify the standard values. \n",
    "# Do not eliminate the parenthesis that indicate the tuple object.\n",
    "# Any unmutable list is a tuple. A tuple can be also declared as an unmutable list of two\n",
    "# objects inside parenthesis instead of the brackets used for lists: []\n",
    "\n",
    "SORT_MERGED_DF = False\n",
    "# SORT_MERGED_DF = False not to sort the merged dataframe. If you want to sort it,\n",
    "# set as True. If SORT_MERGED_DF = True and COLUMN_TO_SORT = None, the dataframe will\n",
    "# be sorted by its first column.\n",
    "\n",
    "COLUMN_TO_SORT = None\n",
    "# COLUMN_TO_SORT = None. Keep it None if the dataframe should not be sorted.\n",
    "# Alternatively, pass a string with a column name to sort, such as:\n",
    "# COLUMN_TO_SORT = 'col1'; or a list of columns to use for sorting: COLUMN_TO_SORT = \n",
    "# ['col1', 'col2']\n",
    "\n",
    "ASCENDING_SORTING = True\n",
    "# ascending_sorting = True. If you want to sort the column(s) passed on column_to_sort in\n",
    "# ascending order, set as True. Set as False if you want to sort in descending order. If\n",
    "# you want to sort each column passed as list column_to_sort in a specific order, pass a \n",
    "# list of booleans like ASCENDING_SORTING = [False, True] - the first column of the list\n",
    "# will be sorted in descending order, whereas the 2nd will be in ascending. Notice that\n",
    "# the correspondence is element-wise: the boolean in list ASCENDING_SORTING will correspond \n",
    "# to the sorting order of the column with the same position in list COLUMN_TO_SORT.\n",
    "# If None, the dataframe will be sorted in ascending order.\n",
    "    \n",
    "\n",
    "#New dataframe saved as merged_df. Simply modify this object on the left of equality:\n",
    "merged_df = MERGE_AND_SORT_DATAFRAMES (df_left = DF_LEFT, df_right = DF_RIGHT, left_key = LEFT_KEY, right_key = RIGHT_KEY, how_to_join = HOW_TO_JOIN, merged_suffixes = MERGED_SUFFIXES, sort_merged_df = SORT_MERGED_DF, column_to_sort = COLUMN_TO_SORT, ascending_sorting = ASCENDING_SORTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Concatenating (SQL UNION) multiple dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_DATAFRAMES = [dataset1, dataset2]\n",
    "# LIST_OF_DATAFRAMES must be a list containing the dataframe objects\n",
    "# example: list_of_dataframes = [df1, df2, df3, df4]\n",
    "# Notice that the dataframes are objects, not strings. Therefore, they should not\n",
    "# be declared inside quotes.\n",
    "# There is no limit of dataframes. In this example, we will concatenate 4 dataframes.\n",
    "# If LIST_OF_DATAFRAMES = [df1, df2, df3] we would concatenate 3, and if\n",
    "# LIST_OF_DATAFRAMES = [df1, df2, df3, df4, df5] we would concatenate 5 dataframes.\n",
    "\n",
    "WHAT_TO_APPEND = 'rows'\n",
    "# WHAT_TO_APPEND = 'rows' for appending the rows from one dataframe\n",
    "# into the other; WHAT_TO_APPEND = 'columns' for appending the columns\n",
    "# from one dataframe into the other (horizontal or lateral append).\n",
    "\n",
    "IGNORE_INDEX_ON_UNION = True # Alternatively: True or False\n",
    "\n",
    "SORT_VALUES_ON_UNION = True # Alternatively: True or False\n",
    "\n",
    "UNION_JOIN_TYPE = None\n",
    "# JOIN can be 'inner' to perform an inner join, eliminating the missing values\n",
    "# The default (None) is 'outer': the dataframes will be stacked on the columns with\n",
    "# same names but, in case there is no correspondence, the row will present a missing\n",
    "# value for the columns which are not present in one of the dataframes.\n",
    "# When using the 'inner' method, only the common columns will remain.\n",
    "# Alternatively, keep UNION_JOIN_TYPE = None for the standard outer join; or set\n",
    "# UNION_JOIN_TYPE = \"inner\" (inside quotes) for using the inner join.\n",
    "    \n",
    "#These 3 last parameters are the same from Pandas .concat method:\n",
    "# IGNORE_INDEX_ON_UNION = ignore_index;\n",
    "# SORT_VALUES_ON_UNION = sort\n",
    "# UNION_JOIN_TYPE = join\n",
    "# Check Datacamp course Joining Data with pandas, Chap.3, \n",
    "# Advanced Merging and Concatenating\n",
    "    \n",
    "\n",
    "#New dataframe saved as concat_df. Simply modify this object on the left of equality:\n",
    "concat_df = UNION_DATAFRAMES (list_of_dataframes = LIST_OF_DATAFRAMES, what_to_append = WHAT_TO_APPEND, ignore_index_on_union = IGNORE_INDEX_ON_UNION, sort_values_on_union = SORT_VALUES_ON_UNION, union_join_type = UNION_JOIN_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Filtering (selecting) or renaming columns of the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "MODE = 'filter'\n",
    "# MODE = 'filter' for filtering only the list of columns passed as cols_list;\n",
    "# MODE = 'rename' for renaming the columns with the names passed as cols_list.\n",
    "\n",
    "COLS_LIST = ['column1', 'column2', 'column3']\n",
    "# COLS_LIST = list of strings containing the names (headers) of the columns to select\n",
    "# (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "# For instance: COLS_LIST = ['col1', 'col2', 'col3'] will \n",
    "# select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "# Declare the names inside quotes.\n",
    "# Simply substitute the list by the list of columns that you want to select; or the\n",
    "# list of the new names you want to give to the dataset columns.\n",
    "\n",
    "#New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = col_filter_rename (df = DATASET, cols_list = COLS_LIST, mode = MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Reversing the log-transform - Exponentially transforming variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SUBSET = None\n",
    "# Set SUBSET = None to transform the whole dataset. Alternatively, pass a list with \n",
    "# columns names for the transformation to be applied. For instance:\n",
    "# SUBSET = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "# as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "# Declaring the full list of columns is equivalent to setting SUBSET = None.\n",
    "\n",
    "CREATE_NEW_COLUMNS = True\n",
    "# Alternatively, set CREATE_NEW_COLUMNS = True to store the transformed data into new\n",
    "# columns. Or set CREATE_NEW_COLUMNS = False to overwrite the existing columns\n",
    "    \n",
    "NEW_COLUMNS_SUFFIX = \"_originalScale\"\n",
    "# This value has effect only if CREATE_NEW_COLUMNS = True.\n",
    "# The new column name will be set as column + NEW_COLUMNS_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_originalScale\", the new column will be named as\n",
    "# \"collumn1_originalScale\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "#New dataframe saved as rescaled_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "rescaled_df = reverse_log_transform(df = DATASET, subset = SUBSET, create_new_columns = CREATE_NEW_COLUMNS, new_columns_suffix = NEW_COLUMNS_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Reversing Box-Cox transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will process a single column column_to_transform of the dataframe df \n",
    "# per call.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "COLUMN_TO_TRANSFORM = 'column_to_transform'\n",
    "# COLUMN_TO_TRANSFORM must be a string with the name of the column.\n",
    "# e.g. COLUMN_TO_TRANSFORM = 'column1' to transform a column named as 'column1'\n",
    "\n",
    "LAMBDA_BOXCOX = None\n",
    "# LAMBDA_BOXCOX must be a float value. e.g. lamda_boxcox = 1.7\n",
    "# If you calculated lambda from the function box_cox_transform and saved the\n",
    "# transformation data summary dictionary as data_sum_dict, simply set:\n",
    "## LAMBDA_BOXCOX = data_sum_dict['lambda_boxcox']\n",
    "# This will access the value on the key 'lambda_boxcox' of the dictionary, which\n",
    "# contains the lambda. \n",
    "# If lambda_boxcox is None, the mode will be automatically set as 'calculate_and_apply'.\n",
    "\n",
    "SUFFIX = '_ReversedBoxCox'\n",
    "#suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_ReversedBoxCox', the transformed column will be\n",
    "# identified as 'Y_ReversedBoxCox'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "#New dataframe saved as retransformed_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "retransformed_df = reverse_box_cox (df = DATASET, column_to_transform = COLUMN_TO_TRANSFORM, lambda_boxcox = LAMBDA_BOXCOX, suffix = SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **One-Hot Encoding the categorical variables**\n",
    "- For each category, the One-Hot Encoder creates a new column in the dataset. This new column is represented by a binary variable which is equals to zero if the row is not classified in that category; and is equals to 1 when the row represents an element in that category.For a category \"A\", a column named \"A\" is created.\n",
    "    - If the row is an element from category \"A\", the value for the column \"A\" is 1.\n",
    "    - If not, the value for column \"A\" is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "SUBSET_OF_FEATURES_TO_BE_ENCODED = ['COLUMN1', 'COLUMN2', 'COLUMN3']\n",
    "#subset_of_features_to_be_encoded: list of strings (inside quotes), \n",
    "# containing the names of the columns with the categorical variables that will be \n",
    "# encoded. If a single column will be encoded, declare this parameter as list with\n",
    "# only one element e.g.subset_of_features_to_be_encoded = [\"column1\"] \n",
    "# will analyze the column named as 'column1'; \n",
    "# subset_of_features_to_be_encoded = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "# with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "\n",
    "#New dataframe saved as one_hot_encoded_df; dictionary saved as encoding_dict.\n",
    "# Simply modify this object on the left of equality:\n",
    "one_hot_encoded_df, encoding_dict = OneHotEncode_df (df = DATASET, subset_of_features_to_be_encoded = SUBSET_OF_FEATURES_TO_BE_ENCODED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reversing scaling of the features - Standard scaler, Min-Max scaler, division by factor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "SUBSET_OF_FEATURES_TO_SCALE = ['COLUMN1', 'COLUMN2', 'COLUMN3']\n",
    "#subset_of_features_to_be_encoded: list of strings (inside quotes), \n",
    "# containing the names of the columns with the categorical variables that will be \n",
    "# encoded. If a single column will be encoded, declare this parameter as list with\n",
    "# only one element e.g.subset_of_features_to_be_encoded = [\"column1\"] \n",
    "# will analyze the column named as 'column1'; \n",
    "# subset_of_features_to_be_encoded = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "# with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "\n",
    "MODE = 'standard'\n",
    "## Alternatively: MODE = 'standard', MODE = 'min_max', MODE = 'factor'\n",
    "## This function provides 3 methods (modes) of scaling:\n",
    "## MODE = 'standard': applies the standard scaling, \n",
    "##  which creates a new variable with mean = 0; and standard deviation = 1.\n",
    "##  Each value Y is transformed as Ytransf = (Y - u)/s, where u is the mean \n",
    "##  of the training samples, and s is the standard deviation of the training samples.\n",
    "    \n",
    "## MODE = 'min_max': applies min-max normalization, with a resultant feature \n",
    "## ranging from 0 to 1. each value Y is transformed as \n",
    "## Ytransf = (Y - Ymin)/(Ymax - Ymin), where Ymin and Ymax are the minimum and \n",
    "## maximum values of Y, respectively.\n",
    "    \n",
    "## MODE = 'factor': divides the whole series by a numeric value provided as argument. \n",
    "## For a factor F, the new Y values will be Ytransf = Y/F.\n",
    "\n",
    "SCALING_PARAMS = {None}\n",
    "# This variable has effect only when SCALE_WITH_NEW_PARAMS = False\n",
    "## For the MODE 'factor', declare SCALING_PARAMS as a dictionary containing the \n",
    "# column name as the key and the correspondent factor as the value.\n",
    "# e.g. SUBSET_OF_FEATURES_TO_SCALE = ['col1', 'col2'], 'col1' will be divided by 2.0, \n",
    "# and 'col2' will be divided by 3.2,  then:\n",
    "# SCALING_PARAMS = {'col1': 2.0, 'col2': 3.2}\n",
    "    \n",
    "## WARNING: For SCALING_PARAMS when SCALE_WITH_NEW_PARAMS = True and \n",
    "# MODE = 'standard' or MODE = 'min_max', the dictionary must be declared with the\n",
    "# column name as the key, and the whole dictionary of parameters as the correspondent\n",
    "# value. Then, it will be a dictionary of dictionaries, where there is a dictionary \n",
    "# correspondent to each key. Each dictionary should be declared in the same way as the \n",
    "# scaling_dictionary printed as output when the scaler is trained.\n",
    "\n",
    "SUFFIX = '_reverseScaling'\n",
    "#suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_reverseScaling', the transformed column will be\n",
    "# identified as 'Y_reverseScaling'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "#New dataframe saved as new_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "new_df = reverse_feature_scaling (df = DATASET, subset_of_features_to_scale = SUBSET_OF_FEATURES_TO_SCALE, scaling_params = SCALING_PARAMS, mode = MODE, suffix = SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plotting a bar chart**\n",
    "- Bars may be vertically or horizontally oriented.\n",
    "- Bar charts are plotted after selecting an aggregation function, and the cumulative percent curve may be obtained and plotted with the bars (in secondary axis).\n",
    "- To obtain a Pareto chart, keep aggregate_function = 'sum', plot_cumulative_percent = True, and orientation = 'vertical'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "CATEGORICAL_VAR_NAME = 'categorical_column_name'\n",
    "# CATEGORICAL_VAR_NAME: string (inside quotes) containing the name \n",
    "# of the column to be analyzed. e.g. \n",
    "# CATEGORICAL_VAR_NAME = \"column1\"\n",
    "\n",
    "RESPONSE_VAR_NAME = \"response_column_name\"\n",
    "# RESPONSE_VAR_NAME: string (inside quotes) containing the name \n",
    "# of the column that stores the response correspondent to the\n",
    "# categories. e.g. RESPONSE_VAR_NAME = \"response_feature\"\n",
    "\n",
    "AGGREGATE_FUNCTION = 'sum'\n",
    "# AGGREGATE_FUNCTION = 'sum': String defining the aggregation \n",
    "# method that will be applied. Possible values:\n",
    "# 'median', 'mean', 'mode', 'sum', 'min', 'max', 'variance',\n",
    "# 'standard_deviation','10_percent_quantile', '20_percent_quantile',\n",
    "# '25_percent_quantile', '30_percent_quantile', '40_percent_quantile',\n",
    "# '50_percent_quantile', '60_percent_quantile', '70_percent_quantile',\n",
    "# '75_percent_quantile', '80_percent_quantile', '90_percent_quantile',\n",
    "# and '95_percent_quantile'.\n",
    "# To use another aggregate function, the method must be added to the\n",
    "# dictionary of methods agg_methods_dict, defined in the function.\n",
    "# If None or an invalid function is input, 'sum' will be used.\n",
    "\n",
    "ADD_SUFFIX_TO_AGGREGATED_COL = True\n",
    "# ADD_SUFFIX_TO_AGGREGATED_COL = True will add a suffix to the\n",
    "# aggregated column. e.g. 'responseVar_mean'. If ADD_SUFFIX_TO_AGGREGATED_COL\n",
    "# = False, the aggregated column will have the original column name.\n",
    "SUFFIX = None\n",
    "# suffix = None. Keep it None if no suffix should be added, or if\n",
    "# the name of the aggregate function should be used as suffix, after\n",
    "# \"_\". Alternatively, set it as a string. As recommendation, put the\n",
    "# \"_\" sign in the beginning of this string to separate the suffix from\n",
    "# the original column name. e.g. if the response variable is 'Y' and\n",
    "# suffix = '_agg', the new aggregated column will be named as 'Y_agg'\n",
    "CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = True\n",
    "# CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = True to calculate and plot\n",
    "# the line of cumulative percent, or \n",
    "# CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = False to omit it.\n",
    "# This feature is only shown when AGGREGATE_FUNCTION = 'sum', 'median',\n",
    "# 'mean', or 'mode'. So, it will be automatically set as False if \n",
    "# another aggregate is selected.\n",
    "ORIENTATION = 'vertical'\n",
    "# ORIENTATION = 'vertical' is the standard, and plots vertical bars\n",
    "# (perpendicular to the X axis). In this case, the categories are shown\n",
    "# in the X axis, and the correspondent responses are in Y axis.\n",
    "# Alternatively, ORIENTATION = 'horizontal' results in horizontal bars.\n",
    "# In this case, categories are in Y axis, and responses in X axis.\n",
    "# If None or invalid values are provided, orientation is set as 'vertical'.\n",
    "LIMIT_OF_PLOTTED_CATEGORIES = None\n",
    "# LIMIT_OF_PLOTTED_CATEGORIES: integer value that represents\n",
    "# the maximum of categories that will be plot. Keep it None to plot\n",
    "# all categories. Alternatively, set an integer value. e.g.: if\n",
    "# LIMIT_OF_PLOTTED_CATEGORIES = 4, but there are more categories,\n",
    "# the dataset will be sorted in descending order and: 1) The remaining\n",
    "# categories will be sum in a new category named 'others' if the\n",
    "# aggregate function is 'sum'; 2) Or the other categories will be simply\n",
    "# omitted from the plot, for other aggregate functions. Notice that\n",
    "# it limits only the variables in the plot: all of them will be\n",
    "# returned in the dataframe.\n",
    "# Use this parameter to obtain a cleaner plot. Notice that the remaining\n",
    "# columns will be aggregated as 'others' even if there is a single column\n",
    "# beyond the limit.\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'scatter_plot_lin_reg.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "# New dataframe saved as aggregated_sorted_df. \n",
    "# Simply modify this object on the left of equality:\n",
    "aggregated_sorted_df = bar_chart (df = DATASET, categorical_var_name = CATEGORICAL_VAR_NAME, response_var_name = RESPONSE_VAR_NAME, aggregate_function = AGGREGATE_FUNCTION, add_suffix_to_aggregated_col = ADD_SUFFIX_TO_AGGREGATED_COL, suffix = SUFFIX, calculate_and_plot_cumulative_percent = CALCULATE_AND_PLOT_CUMULATIVE_PERCENT, orientation = ORIENTATION, limit_of_plotted_categories = LIMIT_OF_PLOTTED_CATEGORIES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualizing time series**\n",
    "        x1, y1, lab1: blue\n",
    "        x2, y2, lab2: red\n",
    "        x3, y3, lab3: green\n",
    "        x4, y4, lab4: black\n",
    "        x5, y5, lab5: magenta\n",
    "        x6, y6, lab6: yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "080be5e2-0ebf-4e84-97ef-97e177d0e989",
    "tags": [
     "CELL_8"
    ]
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "#X1 = dataset.index to use the index as the axis itself\n",
    "X1 = (DATASET['DATE']).astype('datetime64[D]') \n",
    "#Alternatively: None; or other column in quotes, substituting 'DATE'\n",
    "# WARNING: Modify only the object in the first parenthesis: DATASET['DATE']\n",
    "# Do not modify the method .astype('datetime64[D]')\n",
    "#Remove .astype('datetime64[D]') if it is not a datetime.\n",
    "# e.g. X1 = DATASET['Time'] for a X variable named 'Time', if 'Time' is a float, not a\n",
    "# a datetime64. If 'Time' should be interpreted as a timestamp, then, we would declare as:\n",
    "\n",
    "# X1 = (DATASET['Time']).astype('datetime64[D]')\n",
    "\n",
    "# In summary: apply the method .astype('datetime64[D]') if you want the value to be\n",
    "# interpreted (correctly) as a timestamp.\n",
    "\n",
    "#Notice that there is a data transforming step to guarantee that the 'DATE' was interpreted as a timestamp, not as object or string.\n",
    "#The astype method defines the type of variable as 'datetime64[D]'. If we wanted the timestamps to be resolved in seconds, we should use\n",
    "# 'datetime64[ns]'.\n",
    "Y1 = DATASET['Y1'] \n",
    "#Alternatively: None; or other column in quotes, substituting 'Y1'\n",
    "# e.g. Y1 = DATASET['Speed'] for a Y variable named 'Speed'\n",
    "\n",
    "X2 = None #Alternatively: series for X2 (analogous to X1)\n",
    "Y2 = None #Alternatively: series for Y2 (analogous to Y1)\n",
    "X3 = None #Alternatively: series for X3 (analogous to X1)\n",
    "Y3 = None #Alternatively: series for Y3 (analogous to Y1)\n",
    "X4 = None #Alternatively: series for X4 (analogous to X1)\n",
    "Y4 = None #Alternatively: series for Y4 (analogous to Y1)\n",
    "X5 = None #Alternatively: series for X5 (analogous to X1)\n",
    "Y5 = None #Alternatively: series for Y5 (analogous to Y1)\n",
    "X6 = None #Alternatively: series for X6 (analogous to X1)\n",
    "Y6 = None #Alternatively: series for Y6 (analogous to Y1)\n",
    "# Warning: if X2, X3, X4, X5, and X6 were timestamps, do not forget to use the method\n",
    "# .astype('datetime64[D]'). e.g.: X2 = (DATASET['DATE']).astype('datetime64[D]')\n",
    "# If all X axis are the same, you can also declare: X2 = X1, X3 = X1, X4 = X1, X5 = X1\n",
    "# and X6 = X1.\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "ADD_SPLINE_LINES = True #Alternatively: True or False\n",
    "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
    "ADD_SCATTER_DOTS = False #Alternatively: True or False\n",
    "# If ADD_SCATTER_DOTS = False, the dots (scatter plot) are omitted, so only the lines\n",
    "# correspondent to the series are shown.\n",
    "\n",
    "# Notice that adding the dots and omitting the spline lines is equivalent to obtain a\n",
    "# scatter plot. If you want to do so, consider using the scatter_plot_lin_reg function, \n",
    "# capable of calculating the linear regressions.\n",
    "\n",
    "LAB1 = None #Alternatively: string inside quotes containing the label for series 1\n",
    "LAB2 = None #Alternatively: string inside quotes containing the label for series 2\n",
    "LAB3 = None #Alternatively: string inside quotes containing the label for series 3\n",
    "LAB4 = None #Alternatively: string inside quotes containing the label for series 4\n",
    "LAB5 = None #Alternatively: string inside quotes containing the label for series 5\n",
    "LAB6 = None #Alternatively: string inside quotes containing the label for series 6\n",
    "#e.g. LAB1 = \"Y1_values\"\n",
    "\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"/\" \n",
    "# or DIRECTORY_TO_SAVE = \"/folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"/\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'time_series_vis.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 110\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 110.\n",
    "\n",
    "time_series_vis (x1 = X1, y1 = Y1, x2 = X2, y2 = Y2, x3 = X3, y3 = Y3, x4 = X4, y4 = Y4, x5 = X5, y5 = Y5, x6 = X6, y6 = Y6, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, add_scatter_dots = ADD_SCATTER_DOTS, lab1 = LAB1, lab2 = LAB2, lab3 = LAB3, lab4 = LAB4, lab5 = LAB5, lab6 = LAB6, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1"
   },
   "source": [
    "****"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
