{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "4ac8d15b-1dc5-4c92-876f-fd5a3b78f416"
   },
   "source": [
    "# **Process Simulation**\n",
    "- Input datasets for sensitivity analysis into the models; and reverse transformations to simulate and visualize different conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "62763090-2f62-4fcf-a285-99ce09c3b436"
   },
   "source": [
    "## _Machine Learning Modelling Workflow Notebook 6_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "9f61e619-58c8-41b9-b838-a43e9d2f1956"
   },
   "source": [
    "## Content:\n",
    "1. Loading the dataframes;\n",
    "2. Loading the models;\n",
    "3. Converting the datasets into NumPy arrays with correct format for CNN and RNN Architectures;\n",
    "4. Using the models to predict outputs;\n",
    "5. Using the classification models to predict probabilities;\n",
    "6. Merging (joining) dataframes on given keys; and sorting the merged table;\n",
    "7. Concatenating (SQL Union/Stacking/Appending) dataframes;\n",
    "8. Column filtering (selecting) or column renaming;\n",
    "9. Reversing transforms: log-transform (exponentially transforming variables); \n",
    "10. Box-Cox transform; \n",
    "11. One-Hot Encoding;\n",
    "12. Feature scaling;\n",
    "13. Bar chart visualization;\n",
    "14. Time series visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267"
   },
   "source": [
    "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
    "- marcosoares.feq@gmail.com\n",
    "- marco.soares@bayer.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "71f66063-9177-409a-9f95-e49d477437a0",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# To install a library (e.g. tensorflow), unmark and run:\n",
    "# ! pip install tensorflow\n",
    "# to update a library (e.g. tensorflow), unmark and run:\n",
    "# ! pip install tensorflow --upgrade\n",
    "# to update pip, unmark and run:\n",
    "# ! pip install pip --upgrade\n",
    "# to show if a library is installed and visualize its information, unmark and run\n",
    "# (e.g. tensorflow):\n",
    "# ! pip show tensorflow\n",
    "# To run a Python file (e.g idsw_etl.py) saved in the notebook's workspace directory,\n",
    "# unmark and run:\n",
    "# import idsw_etl\n",
    "# or:\n",
    "# import idsw_etl as etl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea"
   },
   "source": [
    "## **Load Python Libraries in Global Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c2dbd266-d9b7-4dda-af39-bdcc7b625ba1",
    "id": "bzZgOvXCyHHl",
    "language": "python",
    "tags": [
     "CELL_4"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "9b050e9a-61a0-4c9c-a201-a0ee15e0d648"
   },
   "source": [
    "# **Function for mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e182bc93-6418-44ac-b0ae-15c8762d8599",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def mount_storage_system (source = 'aws', path_to_store_imported_s3_bucket = '', s3_bucket_name = None, s3_obj_prefix = None):\n",
    "    \n",
    "    # source = 'google' for mounting the google drive;\n",
    "    # source = 'aws' for mounting an AWS S3 bucket.\n",
    "    \n",
    "    # THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN source == 'aws'\n",
    "    \n",
    "    # path_to_store_imported_s3_bucket: path of the Python environment to which the\n",
    "    # S3 bucket contents will be imported. If it is None, or if \n",
    "    # path_to_store_imported_s3_bucket = '/', bucket will be imported to the root path. \n",
    "    # Alternatively, input the path as a string (in quotes). e.g. \n",
    "    # path_to_store_imported_s3_bucket = 'copied_s3_bucket'\n",
    "    \n",
    "    # s3_bucket_name = None.\n",
    "    ## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "    # with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "    # \"aws-bucket-1\"\n",
    "    \n",
    "    # s3_obj_prefix = None. Keep it None or as an empty string (s3_obj_key_prefix = '')\n",
    "    # to import the whole bucket content, instead of a single object from it.\n",
    "    # Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "    # Suppose that your bucket (admin-created) has four objects with the following object \n",
    "    # keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "    # s3-dg.pdf. The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "    # at the root level of the bucket. If you open the Development/ folder, you see \n",
    "    # the Projects.xlsx object in it.\n",
    "    # Check Amazon documentation:\n",
    "    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html\n",
    "    \n",
    "    # In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "    # where 'bucket' is the bucket's name, key_prefix = 'my_path/.../', without the\n",
    "    # 'file.csv' (file name with extension) last part.\n",
    "    \n",
    "    # So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "    # a given folder (directory) of the bucket.\n",
    "    # DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "    # DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "    # Alternatively, provide the full path of a given file if you want to import only it:\n",
    "    # S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "    # where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "    # Attention: after running this function for fetching AWS Simple Storage System (S3), \n",
    "    # your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "    # The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "    # other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "    # and the prefix. All of these are sensitive information from the organization.\n",
    "    # Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "    # and of removing such information from the strings.\n",
    "    # Remember that these data may contain privilege for accessing the information, so it should not\n",
    "    # be used for non-authorized people.\n",
    "\n",
    "    # Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "    # The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "    # workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "    # the notebook's workspace.\n",
    "    \n",
    "    \n",
    "    if (source == 'google'):\n",
    "        \n",
    "        from google.colab import drive\n",
    "        # Google Colab library must be imported only in case it is\n",
    "        # going to be used, for avoiding AWS compatibility issues.\n",
    "        \n",
    "        print(\"Associate the Python environment to your Google Drive account, and authorize the access in the opened window.\")\n",
    "        \n",
    "        drive.mount('/content/drive')\n",
    "        \n",
    "        print(\"Now your Python environment is connected to your Google Drive: the root directory of your environment is now the root of your Google Drive.\")\n",
    "        print(\"In Google Colab, navigate to the folder icon (\\'Files\\') of the left navigation menu to find a specific folder or file in your Google Drive.\")\n",
    "        print(\"Click on the folder or file name and select the elipsis (...) icon on the right of the name to reveal the option \\'Copy path\\', which will give you the path to use as input for loading objects and files on your Python environment.\")\n",
    "        print(\"Caution: save your files into different directories of the Google Drive. If files are all saved in a same folder or directory, like the root path, they may not be accessible from your Python environment.\")\n",
    "        print(\"If you still cannot see the file after moving it to a different folder, reload the environment.\")\n",
    "    \n",
    "    elif (source == 'aws'):\n",
    "        \n",
    "        import os\n",
    "        import boto3\n",
    "        # boto3 is AWS S3 Python SDK\n",
    "        # sagemaker and boto3 libraries must be imported only in case \n",
    "        # they are going to be used, for avoiding \n",
    "        # Google Colab compatibility issues.\n",
    "        from getpass import getpass\n",
    "\n",
    "        # Check if path_to_store_imported_s3_bucket is None. If it is, make it the root directory:\n",
    "        if ((path_to_store_imported_s3_bucket is None)|(str(path_to_store_imported_s3_bucket) == \"/\")):\n",
    "            \n",
    "            # For the S3 buckets, the path should not start with slash. Assign the empty\n",
    "            # string instead:\n",
    "            path_to_store_imported_s3_bucket = \"\"\n",
    "            print(\"Bucket\\'s content will be copied to the notebook\\'s root directory.\")\n",
    "        \n",
    "        elif (str(path_to_store_imported_s3_bucket) == \"\"):\n",
    "            # Guarantee that the path is the empty string.\n",
    "            # Avoid accessing the else condition, what would raise an error\n",
    "            # since the empty string has no character of index 0\n",
    "            path_to_store_imported_s3_bucket = str(path_to_store_imported_s3_bucket)\n",
    "            print(\"Bucket\\'s content will be copied to the notebook\\'s root directory.\")\n",
    "        \n",
    "        else:\n",
    "            # Use the str attribute to guarantee that the path was read as a string:\n",
    "            path_to_store_imported_s3_bucket = str(path_to_store_imported_s3_bucket)\n",
    "            \n",
    "            if(path_to_store_imported_s3_bucket[0] == \"/\"):\n",
    "                # the first character is the slash. Let's remove it\n",
    "\n",
    "                # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "                # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "                # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "                # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "                # The slash is character 0. Then, we want all characters from character 1 (the\n",
    "                # second) to character len(str(path_to_store_imported_s3_bucket)) - 1, the index\n",
    "                # of the last character. So, we can slice the string from position 1 to position\n",
    "                # the slicing syntax is: string[1:] - all string characters from character 1\n",
    "                # string[:10] - all string characters from character 10-1 = 9 (including 9); or\n",
    "                # string[1:10] - characters from 1 to 9\n",
    "                # So, slice the whole string, starting from character 1:\n",
    "                path_to_store_imported_s3_bucket = path_to_store_imported_s3_bucket[1:]\n",
    "                # attention: even though strings may be seem as list of characters, that can be\n",
    "                # sliced, we cannot neither simply assign a character to a given position nor delete\n",
    "                # a character from a position.\n",
    "\n",
    "        # Ask the user to provide the credentials:\n",
    "        ACCESS_KEY = input(\"Enter your AWS Access Key ID here (in the right). It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "        print(\"\\n\") # line break\n",
    "        SECRET_KEY = getpass(\"Enter your password (Secret key) here (in the right). It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "        \n",
    "        # The use of 'getpass' instead of 'input' hide the password behind dots.\n",
    "        # So, the password is not visible by other users and cannot be copied.\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"WARNING: The bucket\\'s name, the prefix, the AWS access key ID, and the AWS Secret access key are all sensitive information, which may grant access to protected information from the organization.\\n\")\n",
    "        print(\"After copying data from S3 to your workspace, remember of removing these information from the notebook, specially if it is going to be shared. Also, remember of removing the files from the workspace.\\n\")\n",
    "        print(\"The cost for storing files in Simple Storage Service is quite inferior than the one for storing directly in SageMaker workspace. Also, files stored in S3 may be accessed for other users than those with access the notebook\\'s workspace.\\n\")\n",
    "\n",
    "        # Check if the user actually provided the mandatory inputs, instead\n",
    "        # of putting None or empty string:\n",
    "        if ((ACCESS_KEY is None) | (ACCESS_KEY == '')):\n",
    "            print(\"AWS Access Key ID is missing. It is the value stored in the field \\'Access key ID\\' from your AWS user credentials CSV file.\")\n",
    "            return \"error\"\n",
    "        elif ((SECRET_KEY is None) | (SECRET_KEY == '')):\n",
    "            print(\"AWS Secret Access Key is missing. It is the value stored in the field \\'Secret access key\\' from your AWS user credentials CSV file.\")\n",
    "            return \"error\"\n",
    "        elif ((s3_bucket_name is None) | (s3_bucket_name == '')):\n",
    "            print (\"Please, enter a valid S3 Bucket\\'s name. Do not add sub-directories or folders (prefixes), only the name of the bucket itself.\")\n",
    "            return \"error\"\n",
    "        \n",
    "        else:\n",
    "            # Use the str attribute to guarantee that all AWS parameters were properly read as strings, and not as\n",
    "            # other variables (like integers or floats):\n",
    "            ACCESS_KEY = str(ACCESS_KEY)\n",
    "            SECRET_KEY = str(SECRET_KEY)\n",
    "            s3_bucket_name = str(s3_bucket_name)\n",
    "        \n",
    "        if(s3_bucket_name[0] == \"/\"):\n",
    "                # the first character is the slash. Let's remove it\n",
    "\n",
    "                # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "                # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "                # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "                # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "                # So, slice the whole string, starting from character 1 (as did for \n",
    "                # path_to_store_imported_s3_bucket):\n",
    "                s3_bucket_name = s3_bucket_name[1:]\n",
    "\n",
    "        # Remove any possible trailing (white and tab spaces) spaces\n",
    "        # That may be present in the string. Use the Python string\n",
    "        # rstrip method, which is the equivalent to the Trim function:\n",
    "        # When no arguments are provided, the whitespaces and tabulations\n",
    "        # are the removed characters\n",
    "        # https://www.w3schools.com/python/ref_string_rstrip.asp?msclkid=ee2d05c3c56811ecb1d2189d9f803f65\n",
    "        s3_bucket_name = s3_bucket_name.rstrip()\n",
    "        ACCESS_KEY = ACCESS_KEY.rstrip()\n",
    "        SECRET_KEY = SECRET_KEY.rstrip()\n",
    "        # Since the user manually inputs the parameters ACCESS and SECRET_KEY,\n",
    "        # it is easy to input whitespaces without noticing that.\n",
    "\n",
    "        # Now process the non-obbligatory parameter.\n",
    "        # Check if a prefix was passed as input parameter. If so, we must select only the names that start with\n",
    "        # The prefix.\n",
    "        # Example: in the bucket 'my_bucket' we have a directory 'dir1'.\n",
    "        # In the main (root) directory, we have a file 'file1.json' like: '/file1.json'\n",
    "        # If we pass the prefix 'dir1', we want only the files that start as '/dir1/'\n",
    "        # such as: 'dir1/file2.json', excluding the file in the main (root) directory and excluding the files in other\n",
    "        # directories. Also, we want to eliminate the file names with no extensions, like 'dir1/' or 'dir1/dir2',\n",
    "        # since these object names represent folders or directories, not files.\t\n",
    "\n",
    "        if (s3_obj_prefix is None):\n",
    "            print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "            print (f\"Then, retrieving all content from the bucket \\'{s3_bucket_name}\\'.\\n\")\n",
    "        elif ((s3_obj_prefix == \"/\") | (s3_obj_prefix == '')):\n",
    "            # The root directory in the bucket must not be specified starting with the slash\n",
    "            # If the root \"/\" or the empty string '' is provided, make\n",
    "            # it equivalent to None (no directory)\n",
    "            s3_obj_prefix = None\n",
    "            print (\"No prefix, specific object, or subdirectory provided.\") \n",
    "            print (f\"Then, retrieving all content from the bucket \\'{s3_bucket_name}\\'.\\n\")\n",
    "    \n",
    "        else:\n",
    "            # Since there is a prefix, use the str attribute to guarantee that the path was read as a string:\n",
    "            s3_obj_prefix = str(s3_obj_prefix)\n",
    "            \n",
    "            if(s3_obj_prefix[0] == \"/\"):\n",
    "                # the first character is the slash. Let's remove it\n",
    "\n",
    "                # In AWS, neither the prefix nor the path to which the file will be imported\n",
    "                # (file from S3 to workspace) or from which the file will be exported to S3\n",
    "                # (the path in the notebook's workspace) may start with slash, or the operation\n",
    "                # will not be concluded. Then, we have to remove this character if it is present.\n",
    "\n",
    "                # So, slice the whole string, starting from character 1 (as did for \n",
    "                # path_to_store_imported_s3_bucket):\n",
    "                s3_obj_prefix = s3_obj_prefix[1:]\n",
    "\n",
    "            # Remove any possible trailing (white and tab spaces) spaces\n",
    "            # That may be present in the string. Use the Python string\n",
    "            # rstrip method, which is the equivalent to the Trim function:\n",
    "            s3_obj_prefix = s3_obj_prefix.rstrip()\n",
    "            \n",
    "            # Store the total characters in the prefix string after removing the initial slash\n",
    "            # and trailing spaces:\n",
    "            prefix_len = len(s3_obj_prefix)\n",
    "            \n",
    "            print(\"AWS Access Credentials, and bucket\\'s prefix, object or subdirectory provided.\\n\")\t\n",
    "\n",
    "            \n",
    "        print (\"Starting connection with the S3 bucket.\\n\")\n",
    "        \n",
    "        try:\n",
    "            # Start S3 client as the object 's3_client'\n",
    "            s3_client = boto3.resource('s3', aws_access_key_id = ACCESS_KEY, aws_secret_access_key = SECRET_KEY)\n",
    "        \n",
    "            print(f\"Credentials accepted by AWS. S3 client successfully started.\\n\")\n",
    "            # An object 'data_table.xlsx' in the main (root) directory of the s3_bucket is stored in Python environment as:\n",
    "            # s3.ObjectSummary(bucket_name='bucket_name', key='data_table.xlsx')\n",
    "            # The name of each object is stored as the attribute 'key' of the object.\n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect to AWS Simple Storage Service (S3). Review if your credentials are correct.\")\n",
    "            print(\"The variable \\'access_key\\' must be set as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"The variable \\'secret_key\\' must be set as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "        \n",
    "        try:\n",
    "            # Connect to the bucket specified as 'bucket_name'.\n",
    "            # The bucket is started as the object 's3_bucket':\n",
    "            s3_bucket = s3_client.Bucket(s3_bucket_name)\n",
    "            print(f\"Connection with bucket \\'{s3_bucket_name}\\' stablished.\\n\")\n",
    "            \n",
    "        except:\n",
    "            \n",
    "            print(\"Failed to connect with the bucket, which usually happens when declaring a wrong bucket\\'s name.\") \n",
    "            print(\"Check the spelling of your bucket_name string and remember that it must be all in lower-case.\\n\")\n",
    "                \n",
    "\n",
    "        # Then, let's obtain a list of all objects in the bucket (list bucket_objects):\n",
    "        \n",
    "        bucket_objects_list = []\n",
    "\n",
    "        # Loop through all objects of the bucket:\n",
    "        for stored_obj in s3_bucket.objects.all():\n",
    "            \n",
    "            # Loop through all elements 'stored_obj' from s3_bucket.objects.all()\n",
    "            # Which stores the ObjectSummary for all objects in the bucket s3_bucket:\n",
    "            # Let's store only the key attribute and use the str function\n",
    "            # to guarantee that all values were stored as strings.\n",
    "            bucket_objects_list.append(str(stored_obj.key))\n",
    "        \n",
    "        # Now start a support list to store only the elements from\n",
    "        # bucket_objects_list that are not folders or directories\n",
    "        # (objects with extensions).\n",
    "        # If a prefix was provided, only files with that prefix should\n",
    "        # be added:\n",
    "        support_list = []\n",
    "        \n",
    "        for stored_obj in bucket_objects_list:\n",
    "            \n",
    "            # Loop through all elements 'stored_obj' from the list\n",
    "            # bucket_objects_list\n",
    "\n",
    "            # Check the file extension.\n",
    "            file_extension = os.path.splitext(stored_obj)[1][1:]\n",
    "            \n",
    "            # The os.path.splitext method splits the string into its FIRST dot (\".\") to\n",
    "            # separate the file extension from the full path. Example:\n",
    "            # \"C:/dir1/dir2/data_table.csv\" is split into:\n",
    "            # \"C:/dir1/dir2/data_table\" (root part) and '.csv' (extension part)\n",
    "            # https://www.geeksforgeeks.org/python-os-path-splitext-method/?msclkid=2d56198fc5d311ec820530cfa4c6d574\n",
    "\n",
    "            # os.path.splitext(stored_obj) is a tuple of strings: the first is the complete file\n",
    "            # root with no extension; the second is the extension starting with a point: '.txt'\n",
    "            # When we set os.path.splitext(stored_obj)[1], we are selecting the second element of\n",
    "            # the tuple. By selecting os.path.splitext(stored_obj)[1][1:], we are taking this string\n",
    "            # from the second character (index 1), eliminating the dot: 'txt'\n",
    "\n",
    "\n",
    "            # Check if the file extension is not an empty string '' (i.e., that it is different from != the empty\n",
    "            # string:\n",
    "            if (file_extension != ''):\n",
    "                    \n",
    "                    # The extension is different from the empty string, so it is not neither a folder nor a directory\n",
    "                    # The object is actually a file and may be copied if it satisfies the prefix condition. If there\n",
    "                    # is no prefix to check, we may simply copy the object to the list.\n",
    "\n",
    "                    # If there is a prefix, the first characters of the stored_obj must be the prefix:\n",
    "                    if not (s3_obj_prefix is None):\n",
    "                        \n",
    "                        # Check the characters from the position 0 (1st character) to the position\n",
    "                        # prefix_len - 1. Since a prefix was declared, we want only the objects that this first portion\n",
    "                        # corresponds to the prefix. string[i:j] slices the string from index i to index j-1\n",
    "                        # Then, the 1st portion of the string to check is: string[0:(prefix_len)]\n",
    "\n",
    "                        # Slice the string stored_obj from position 0 (1st character) to position prefix_len - 1,\n",
    "                        # The position that the prefix should end.\n",
    "                        obj_name_first_part = (stored_obj)[0:(prefix_len)]\n",
    "                        \n",
    "                        # If this first part is the prefix, then append the object to \n",
    "                        # support list:\n",
    "                        if (obj_name_first_part == (s3_obj_prefix)):\n",
    "\n",
    "                                support_list.append(stored_obj)\n",
    "\n",
    "                    else:\n",
    "                        # There is no prefix, so we can simply append the object to the list:\n",
    "                        support_list.append(stored_obj)\n",
    "\n",
    "            \n",
    "        # Make the objects list the support list itself:\n",
    "        bucket_objects_list = support_list\n",
    "            \n",
    "        # Now, bucket_objects_list contains the names of all objects from the bucket that must be copied.\n",
    "\n",
    "        print(\"Finished mapping objects to fetch. Now, all these objects from S3 bucket will be copied to the notebook\\'s workspace, in the specified directory.\\n\")\n",
    "        print(f\"A total of {len(bucket_objects_list)} files were found in the specified bucket\\'s prefix (\\'{s3_obj_prefix}\\').\")\n",
    "        print(f\"The first file found is \\'{bucket_objects_list[0]}\\'; whereas the last file found is \\'{bucket_objects_list[len(bucket_objects_list) - 1]}\\'.\")\n",
    "            \n",
    "        # Now, let's try copying the files:\n",
    "            \n",
    "        try:\n",
    "            \n",
    "            # Loop through all objects in the list bucket_objects and copy them to the workspace:\n",
    "            for copied_object in bucket_objects_list:\n",
    "\n",
    "                # Select the object in the bucket previously started as 's3_bucket':\n",
    "                selected_object = s3_bucket.Object(copied_object)\n",
    "            \n",
    "                # Now, copy this object to the workspace:\n",
    "                # Set the new file_path. Notice that by now, copied_object may be a string like:\n",
    "                # 'dir1/.../dirN/file_name.ext', where dirN is the n-th directory and ext is the file extension.\n",
    "                # We want only the file_name to joing with the path to store the imported bucket. So, we can use the\n",
    "                # str.split method specifying the separator sep = '/' to break the string into a list of substrings.\n",
    "                # The last element from this list will be 'file_name.ext'\n",
    "                # https://www.w3schools.com/python/ref_string_split.asp?msclkid=135399b6c63111ecada75d7d91add056\n",
    "\n",
    "                # 1. Break the copied_object full path into the list object_path_list, using the .split method:\n",
    "                object_path_list = copied_object.split(sep = \"/\")\n",
    "\n",
    "                # 2. Get the last element from this list. Since it has length len(object_path_list) and indexing starts from\n",
    "                # zero, the index of the last element is (len(object_path_list) - 1):\n",
    "                fetched_object = object_path_list[(len(object_path_list) - 1)]\n",
    "\n",
    "                # 3. Finally, join the string fetched_object with the new path (path on the notebook's workspace) to finish\n",
    "                # The new object's file_path:\n",
    "\n",
    "                file_path = os.path.join(path_to_store_imported_s3_bucket, fetched_object)\n",
    "\n",
    "                # Download the selected object to the workspace in the specified file_path\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" copies a xlsx file named 'my_table' to the notebook's main (root)\n",
    "                # directory\n",
    "                selected_object.download_file(Filename = file_path)\n",
    "\n",
    "                print(f\"The file \\'{fetched_object}\\' was successfully copied to notebook\\'s workspace.\\n\")\n",
    "\n",
    "                \n",
    "            print(\"Finished copying the files from the bucket to the notebook\\'s workspace. It may take a couple of minutes untill they be shown in SageMaker environment.\\n\") \n",
    "            print(\"Do not forget to delete these copies after finishing the analysis. They will remain stored in the bucket.\\n\")\n",
    "\n",
    "\n",
    "        except:\n",
    "\n",
    "            # Run this code for any other exception that may happen (no exception error\n",
    "            # specified, so any exception runs the following code).\n",
    "            # Check: https://pythonbasics.org/try-except/?msclkid=4f6b4540c5d011ecb1fe8a4566f632a6\n",
    "            # for seeing how to handle successive exceptions\n",
    "\n",
    "            print(\"Attention! The function raised an exception error, which is probably due to the AWS Simple Storage Service (S3) permissions.\")\n",
    "            print(\"Before running again this function, check this quick guide for configuring the permission roles in AWS.\\n\")\n",
    "            print(\"It is necessary to create an user with full access permissions to interact with S3 from SageMaker. To configure the User, go to the upper ribbon of AWS, click on Services, and select IAM – Identity and Access Management.\")\n",
    "            print(\"1. In IAM\\'s lateral panel, search for \\'Users\\' in the group of Access Management.\")\n",
    "            print(\"2. Click on the \\'Add users\\' button.\")\n",
    "            print(\"3. Set an user name in the text box \\'User name\\'.\")\n",
    "            print(\"Attention: users and S3 buckets cannot be written in upper case. Also, selecting a name already used by an Amazon user or bucket will raise an error message.\\n\")\n",
    "            print(\"4. In the field \\'Select type of Access to AWS\\'-\\'Select type of AWS credentials\\' select the option \\'Access key - Programmatic access\\'. After that, click on the button \\'Next: Permissions\\'.\")\n",
    "            print(\"5. In the field \\'Set Permissions\\', keep the \\'Add user to a group\\' button marked.\")\n",
    "            print(\"6. In the field \\'Add user to a group\\', click on \\'Create group\\' (alternatively, you can be added to a group already configured or copy the permissions of another user.\")\n",
    "            print(\"7. In the text box \\'Group\\'s name\\', set a name for the new group of permissions.\")\n",
    "            print(\"8. In the search bar below (\\'Filter politics\\'), search for a politics that fill your needs, and check the option button on the left of this politic. The politics \\'AmazonS3FullAccess\\' grants full access to the S3 content.\")\n",
    "            print(\"9. Finally, click on \\'Create a group\\'.\")\n",
    "            print(\"10. After the group is created, it will appear with a check box marked, over the previous groups. Keep it marked and click on the button \\'Next: Tags\\'.\")\n",
    "            print(\"11. Create and note down the Access key ID and Secret access key. You can also download a comma separated values (CSV) file containing the credentials for future use.\")\n",
    "            print(\"ATTENTION: These parameters are required for accessing the bucket\\'s content from any application, including AWS SageMaker.\")\n",
    "            print(\"12. Click on \\'Next: Review\\' and review the user credentials information and permissions.\")\n",
    "            print(\"13. Click on \\'Create user\\' and click on the download button to download the CSV file containing the user credentials information.\")\n",
    "            print(\"The headers of the CSV file (the stored fields) is: \\'User name, Password, Access key ID, Secret access key, Console login link\\'.\")\n",
    "            print(\"You need both the values indicated as \\'Access key ID\\' and as \\'Secret access key\\' to fetch the S3 bucket.\")\n",
    "            print(\"\\n\") # line break\n",
    "            print(\"After acquiring the necessary user privileges, use the boto3 library to fetch the bucket from the Python code. boto3 is AWS S3 Python SDK.\")\n",
    "            print(\"For fetching a specific bucket\\'s file use the following code:\\n\")\n",
    "            print(\"1. Set a variable \\'access_key\\' as the value (string) stored as \\'Access key ID\\' in your user security credentials CSV file.\")\n",
    "            print(\"2. Set a variable \\'secret_key\\' as the value (string) stored as \\'Secret access key\\' in your user security credentials CSV file.\")\n",
    "            print(\"3. Set a variable \\'bucket_name\\' as a string containing only the name of the bucket. Do not add subdirectories, folders (prefixes), or file names.\")\n",
    "            print(\"Example: if your bucket is named \\'my_bucket\\' and its main directory contains folders like \\'folder1\\', \\'folder2\\', etc, do not declare bucket_name = \\'my_bucket/folder1\\', even if you only want files from folder1.\")\n",
    "            print(\"ALWAYS declare only the bucket\\'s name: bucket_name = \\'my_bucket\\'.\")\n",
    "            print(\"4. Set a variable \\'file_path\\' containing the path from the bucket\\'s subdirectories to the file you want to fetch. Include the file name and its extension.\")\n",
    "            print(\"If the file is stored in the bucket\\'s root (main) directory: file_path = \\\"my_file.ext\\\".\")\n",
    "            print(\"If the path of the file in the bucket is: \\'dir1/…/dirN/my_file.ext\\', where dirN is the N-th subdirectory, and dir1 is a folder or directory of the main (root) bucket\\'s directory: file_path = \\\"dir1/…/dirN/my_file.ext\\\".\")\n",
    "            print(\"Also, we say that \\'dir1/…/dirN/\\' is the file\\'s prefix. Notice that the name of the bucket is never declared here as the path for fetching its content from the Python code.\")\n",
    "            print(\"5. Set a variable named \\'new_path\\' to store the path of the file copied to the notebook’s workspace. This path must contain the file name and its extension.\")\n",
    "            print(\"Example: if you want to copy \\'my_file.ext\\' to the root directory of the notebook’s workspace, set: new_path = \\\"/my_file.ext\\\".\")\n",
    "            print(\"6. Finally, declare the following code, which refers to the defined variables:\\n\")\n",
    "\n",
    "            # Let's use triple quotes to declare a formated string\n",
    "            example_code = \"\"\"\n",
    "                import boto3\n",
    "                # Start S3 client as the object 's3_client'\n",
    "                s3_client = boto3.resource('s3', aws_access_key_id = access_key, aws_secret_access_key = secret_key)\n",
    "                # Connect to the bucket specified as 'bucket_name'.\n",
    "                # The bucket is started as the object 's3_bucket':\n",
    "                s3_bucket = s3_client.Bucket(bucket_name)\n",
    "                # Select the object in the bucket previously started as 's3_bucket':\n",
    "                selected_object = s3_bucket.Object(file_path)\n",
    "                # Download the selected object to the workspace in the specified file_path\n",
    "                # The parameter Filename must be input with the path of the copied file, including its name and\n",
    "                # extension. Example Filename = \"/my_table.xlsx\" copies a xlsx file named 'my_table' to the notebook's main (root)\n",
    "                # directory\n",
    "                selected_object.download_file(Filename = new_path)\n",
    "                \"\"\"\n",
    "\n",
    "            print(example_code)\n",
    "\n",
    "            print(\"An object \\'my_file.ext\\' in the main (root) directory of the s3_bucket is stored in Python environment as:\")\n",
    "            print(\"\"\"s3.ObjectSummary(bucket_name='bucket_name', key='my_file.ext'\"\"\") \n",
    "            # triple quotes to keep the internal quotes without using too much backslashes \"\\\" (the ignore next character)\n",
    "            print(\"Then, the name of each object is stored as the attribute \\'key\\' of the object. To view all objects, we can loop through their \\'key\\' attributes:\\n\")\n",
    "            example_code = \"\"\"\n",
    "                # Loop through all objects of the bucket:\n",
    "                for stored_obj in s3_bucket.objects.all():\t\t\n",
    "                    # Loop through all elements 'stored_obj' from s3_bucket.objects.all()\n",
    "                    # Which stores the ObjectSummary for all objects in the bucket s3_bucket:\n",
    "                    # Print the object’s names:\n",
    "                    print(stored_obj.key)\n",
    "                    \"\"\"\n",
    "\n",
    "            print(example_code)\n",
    "\n",
    "                \n",
    "    else:\n",
    "        \n",
    "        print(\"Select a valid source: \\'google\\' for mounting Google Drive; or \\'aws\\' for accessing AWS S3 Bucket.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c4c7199e-616c-4b16-9ac7-2814d0c5cc83"
   },
   "source": [
    "# **Function for downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "azdata_cell_guid": "b89f4e11-ee13-4dbe-a2bb-e472a71c46ea",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def upload_to_or_download_file_from_colab (action = 'download', file_to_download_from_colab = None):\n",
    "    \n",
    "    # action = 'download' to download the file to the local machine\n",
    "    # action = 'upload' to upload a file from local machine to\n",
    "    # Google Colab's instant memory\n",
    "    \n",
    "    # file_to_download_from_colab = None. This parameter is obbligatory when\n",
    "    # action = 'download'. \n",
    "    # Declare as file_to_download_from_colab the file that you want to download, with\n",
    "    # the correspondent extension.\n",
    "    # It should not be declared in quotes.\n",
    "    # e.g. to download a dictionary named dict, object_to_download_from_colab = 'dict.pkl'\n",
    "    # To download a dataframe named df, declare object_to_download_from_colab = 'df.csv'\n",
    "    # To export a model named keras_model, declare object_to_download_from_colab = 'keras_model.h5'\n",
    " \n",
    "    from google.colab import files\n",
    "    # google.colab library must be imported only in case \n",
    "    # it is going to be used, for avoiding \n",
    "    # AWS compatibility issues.\n",
    "        \n",
    "    if (action == 'upload'):\n",
    "            \n",
    "        print(\"Click on the button for file selection and select the files from your machine that will be uploaded in the Colab environment.\")\n",
    "        print(\"Warning: the files will be removed from Colab memory after the Kernel dies or after the notebook is closed.\")\n",
    "        # this functionality requires the previous declaration:\n",
    "        ## from google.colab import files\n",
    "            \n",
    "        colab_files_dict = files.upload()\n",
    "            \n",
    "        # The files are stored into a dictionary called colab_files_dict where the keys\n",
    "        # are the names of the files and the values are the files themselves.\n",
    "        ## e.g. if you upload a single file named \"dictionary.pkl\", the dictionary will be\n",
    "        ## colab_files_dict = {'dictionary.pkl': file}, where file is actually a big string\n",
    "        ## representing the contents of the file. The length of this value is the size of the\n",
    "        ## uploaded file, in bytes.\n",
    "        ## To access the file is like accessing a value from a dictionary: \n",
    "        ## d = {'key1': 'val1'}, d['key1'] == 'val1'\n",
    "        ## we simply declare the key inside brackets and quotes, the same way we would do for\n",
    "        ## accessing the column of a dataframe.\n",
    "        ## In this example, colab_files_dict['dictionary.pkl'] access the content of the \n",
    "        ## .pkl file, and len(colab_files_dict['dictionary.pkl']) is the size of the .pkl\n",
    "        ## file in bytes.\n",
    "        ## To check the dictionary keys, apply the method .keys() to the dictionary (with empty\n",
    "        ## parentheses): colab_files_dict.keys()\n",
    "            \n",
    "        for key in colab_files_dict.keys():\n",
    "            #loop through each element of the list of keys of the dictionary\n",
    "            # (list colab_files_dict.keys()). Each element is named 'key'\n",
    "            print(f\"User uploaded file {key} with length {len(colab_files_dict[key])} bytes.\")\n",
    "            # The key is the name of the file, and the length of the value\n",
    "            ## correspondent to the key is the file's size in bytes.\n",
    "            ## Notice that the content of the uploaded object must be passed \n",
    "            ## as argument for a proper function to be interpreted. \n",
    "            ## For instance, the content of a xlsx file should be passed as\n",
    "            ## argument for Pandas .read_excel function; the pkl file must be passed as\n",
    "            ## argument for pickle.\n",
    "            ## e.g., if you uploaded 'table.xlsx' and stored it into colab_files_dict you should\n",
    "            ## declare df = pd.read_excel(colab_files_dict['table.xlsx']) to obtain a dataframe\n",
    "            ## df from the uploaded table. Notice that is the value, not the key, that is the\n",
    "            ## argument.\n",
    "                \n",
    "            print(\"The uploaded files are stored into a dictionary object named as colab_files_dict.\")\n",
    "            print(\"Each key from this dictionary is the name of an uploaded file. The value correspondent to that key is the file itself.\")\n",
    "            print(\"The structure of a general Python dictionary is dict = {\\'key1\\': value1}. To access value1, declare file = dict[\\'key1\\'], as if you were accessing a column from a dataframe.\")\n",
    "            print(\"Then, if you uploaded a file named \\'table.xlsx\\', you can access this file as:\")\n",
    "            print(\"uploaded_file = colab_files_dict[\\'table.xlsx\\']\")\n",
    "            print(\"Notice, though, that the object uploaded_file is the whole file content, not a Python object already converted. To convert to a Python object, pass this element as argument for a proper function or method.\")\n",
    "            print(\"In this example, to convert the object uploaded_file to a dataframe, Pandas pd.read_excel function could be used. In the following line, a df dataframe object is obtained from the uploaded file:\")\n",
    "            print(\"df = pd.read_excel(uploaded_file)\")\n",
    "            print(\"Also, the uploaded file itself will be available in the Colaboratory Notebook\\'s workspace.\")\n",
    "            \n",
    "            return colab_files_dict\n",
    "        \n",
    "    elif (action == 'download'):\n",
    "            \n",
    "        if (file_to_download_from_colab is None):\n",
    "                \n",
    "            #No object was declared\n",
    "            print(\"Please, inform a file to download from the notebook\\'s workspace. It should be declared in quotes and with the extension: e.g. \\'table.csv\\'.\")\n",
    "            \n",
    "        else:\n",
    "                \n",
    "            print(\"The file will be downloaded to your computer.\")\n",
    "\n",
    "            files.download(file_to_download_from_colab)\n",
    "\n",
    "            print(f\"File {file_to_download_from_colab} successfully downloaded from Colab environment.\")\n",
    "\n",
    "    else:\n",
    "            \n",
    "            print(\"Please, select a valid action, \\'download\\' or \\'upload\\'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "ac645bd5-6b74-40c0-92f5-bee2692ee52b"
   },
   "source": [
    "# **Function for loading the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "azdata_cell_guid": "8263bcb2-5f5d-4dfe-a19e-ab69cbc58609",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def load_pandas_dataframe (file_directory_path, file_name_with_extension, load_txt_file_with_json_format = False, how_missing_values_are_registered = None, has_header = True, decimal_separator = '.', txt_csv_col_sep = \"comma\", load_all_sheets_at_once = False, sheet_to_load = None, json_record_path = None, json_field_separator = \"_\", json_metadata_prefix_list = None):\n",
    "    \n",
    "    # Pandas documentation:\n",
    "    # pd.read_csv: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "    # pd.read_excel: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html\n",
    "    # pd.json_normalize: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html\n",
    "    # Python JSON documentation:\n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    \n",
    "    import os\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    \n",
    "    ## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "    ## JSON, txt, or CSV (comma separated values) files.\n",
    "    \n",
    "    # file_directory_path - (string, in quotes): input the path of the directory (e.g. folder path) \n",
    "    # where the file is stored. e.g. file_directory_path = \"/\" or file_directory_path = \"/folder\"\n",
    "    \n",
    "    # FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "    # extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "    # FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "    # Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
    "    \n",
    "    # load_txt_file_with_json_format = False. Set load_txt_file_with_json_format = True \n",
    "    # if you want to read a file with txt extension containing a text formatted as JSON \n",
    "    # (but not saved as JSON).\n",
    "    # WARNING: if load_txt_file_with_json_format = True, all the JSON file parameters of the \n",
    "    # function (below) must be set. If not, an error message will be raised.\n",
    "    \n",
    "    # HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "    # empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "    # This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "    # By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "    #‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "    # ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "    # If a different denomination is used, indicate it as a string. e.g.\n",
    "    # HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "    # HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "    # If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "    # only in column 'numeric_col', you can specify the following dictionary:\n",
    "    # how_missing_values_are_registered = {'numeric-col': 0}\n",
    "    \n",
    "    \n",
    "    # has_header = True if the the imported table has headers (row with columns names).\n",
    "    # Alternatively, has_header = False if the dataframe does not have header.\n",
    "    \n",
    "    # DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "    # the decimal separator. Alternatively, specify here the separator.\n",
    "    # e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "    # It manipulates the argument 'decimal' from Pandas functions.\n",
    "    \n",
    "    # txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "    # or 'csv'. It informs how the different columns are separated.\n",
    "    # Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "    # for columns separated by comma;\n",
    "    # txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "    # for columns separated by simple spaces.\n",
    "    # You can also set a specific separator as string. For example:\n",
    "    # txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "    # is used as separator for the columns - '\\t' represents the tab character).\n",
    "    \n",
    "    \n",
    "    ## Parameters for loading Excel files:\n",
    "    \n",
    "    # load_all_sheets_at_once = False - This parameter has effect only when for Excel files.\n",
    "    # If load_all_sheets_at_once = True, the function will return a list of dictionaries, each\n",
    "    # dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
    "    # value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
    "    # and its value will be the pandas dataframe object obtained from that sheet.\n",
    "    # This argument has preference over sheet_to_load. If it is True, all sheets will be loaded.\n",
    "    \n",
    "    # sheet_to_load - This parameter has effect only when for Excel files.\n",
    "    # keep sheet_to_load = None not to specify a sheet of the file, so that the first sheet\n",
    "    # will be loaded.\n",
    "    # sheet_to_load may be an integer or an string (inside quotes). sheet_to_load = 0\n",
    "    # loads the first sheet (sheet with index 0); sheet_to_load = 1 loads the second sheet\n",
    "    # of the file (index 1); sheet_to_load = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "    # Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "    # name to load the sheet with that name.\n",
    "    \n",
    "    \n",
    "    ## Parameters for loading JSON files:\n",
    "    \n",
    "    # json_record_path (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "    # Path in each object to list of records. If not passed, data will be assumed to \n",
    "    # be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "    # dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "    # 'books' stores a nested JSON, declare, json_record_path = 'books'\n",
    "    \n",
    "    # json_field_separator = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "    # Nested records will generate names separated by sep. \n",
    "    # e.g., for json_field_separator = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "    # Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "    # the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "    # separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "    \n",
    "    # json_metadata_prefix_list: list of strings (in quotes). Manipulates the parameter \n",
    "    # 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "    # table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "    # will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "    \n",
    "    # e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "    # 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "    # Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "    # are 'name' and 'last'.\n",
    "    # Then, json_record_path = 'books'\n",
    "    # json_metadata_prefix_list = ['name', 'last']\n",
    "    \n",
    "    \n",
    "    # Create the complete file path:\n",
    "    file_path = os.path.join(file_directory_path, file_name_with_extension)\n",
    "    # Extract the file extension\n",
    "    file_extension = os.path.splitext(file_path)[1][1:]\n",
    "    # os.path.splitext(file_path) is a tuple of strings: the first is the complete file\n",
    "    # root with no extension; the second is the extension starting with a point: '.txt'\n",
    "    # When we set os.path.splitext(file_path)[1], we are selecting the second element of\n",
    "    # the tuple. By selecting os.path.splitext(file_path)[1][1:], we are taking this string\n",
    "    # from the second character (index 1), eliminating the dot: 'txt'\n",
    "    \n",
    "    # Check if the decimal separator is None. If it is, set it as '.' (period):\n",
    "    if (decimal_separator is None):\n",
    "        decimal_separator = '.'\n",
    "    \n",
    "    if ((file_extension == 'txt') | (file_extension == 'csv')): \n",
    "        # The operator & is equivalent to 'And' (intersection).\n",
    "        # The operator | is equivalent to 'Or' (union).\n",
    "        # pandas.read_csv method must be used.\n",
    "        if (load_txt_file_with_json_format == True):\n",
    "            \n",
    "            print(\"Reading a txt file containing JSON parsed data. A reading error will be raised if you did not set the JSON parameters.\\n\")\n",
    "            \n",
    "            with open(file_path, 'r') as opened_file:\n",
    "                # 'r' stands for read mode; 'w' stands for write mode\n",
    "                # read the whole file as a string named 'file_full_text'\n",
    "                file_full_text = opened_file.read()\n",
    "                # if we used the readlines() method, we would be reading the\n",
    "                # file by line, not the whole text at once.\n",
    "                # https://stackoverflow.com/questions/8369219/how-to-read-a-text-file-into-a-string-variable-and-strip-newlines?msclkid=a772c37bbfe811ec9a314e3629df4e1e\n",
    "                # https://www.tutorialkart.com/python/python-read-file-as-string/#:~:text=example.py%20%E2%80%93%20Python%20Program.%20%23open%20text%20file%20in,and%20prints%20it%20to%20the%20standard%20output.%20Output.?msclkid=a7723a1abfe811ecb68bba01a2b85bd8\n",
    "                \n",
    "            #Now, file_full_text is a string containing the full content of the txt file.\n",
    "            json_file = json.loads(file_full_text)\n",
    "            # json.load() : This method is used to parse JSON from URL or file.\n",
    "            # json.loads(): This method is used to parse string with JSON content.\n",
    "            # e.g. .json.loads() must be used to read a string with JSON and convert it to a flat file\n",
    "            # like a dataframe.\n",
    "            # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.\n",
    "            dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "        \n",
    "        else:\n",
    "            # Not a JSON txt\n",
    "        \n",
    "            if (has_header == True):\n",
    "\n",
    "                if ((txt_csv_col_sep == \"comma\") | (txt_csv_col_sep == \",\")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                    #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                    # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                    # parsing speed by 5-10x.\n",
    "\n",
    "                elif ((txt_csv_col_sep == \"whitespace\") | (txt_csv_col_sep == \" \")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, delim_whitespace = True, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    try:\n",
    "                        \n",
    "                        # Try using the character specified as the argument txt_csv_col_sep:\n",
    "                        dataset = pd.read_csv(file_path, sep = txt_csv_col_sep, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    \n",
    "                    except:\n",
    "                        # An error was raised, the separator is not valid\n",
    "                        print(f\"Enter a valid column separator for the {file_extension} file, like: \\'comma\\' or \\'whitespace\\'.\")\n",
    "\n",
    "\n",
    "            else:\n",
    "                # has_header == False\n",
    "\n",
    "                if ((txt_csv_col_sep == \"comma\") | (txt_csv_col_sep == \",\")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "\n",
    "                    \n",
    "                elif ((txt_csv_col_sep == \"whitespace\") | (txt_csv_col_sep == \" \")):\n",
    "\n",
    "                    dataset = pd.read_csv(file_path, delim_whitespace = True, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    try:\n",
    "                        \n",
    "                        # Try using the character specified as the argument txt_csv_col_sep:\n",
    "                        dataset = pd.read_csv(file_path, sep = txt_csv_col_sep, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True, infer_datetime_format = True, decimal = decimal_separator)\n",
    "                    \n",
    "                    except:\n",
    "                        # An error was raised, the separator is not valid\n",
    "                        print(f\"Enter a valid column separator for the {file_extension} file, like: \\'comma\\' or \\'whitespace\\'.\")\n",
    "\n",
    "    elif (file_extension == 'json'):\n",
    "        \n",
    "        with open(file_path, 'r') as opened_file:\n",
    "            \n",
    "            json_file = json.load(opened_file)\n",
    "            # The structure json_file = json.load(open(file_path)) relies on the GC to close the file. That's not a \n",
    "            # good idea: If someone doesn't use CPython the garbage collector might not be using refcounting (which \n",
    "            # collects unreferenced objects immediately) but e.g. collect garbage only after some time.\n",
    "            # Since file handles are closed when the associated object is garbage collected or closed \n",
    "            # explicitly (.close() or .__exit__() from a context manager) the file will remain open until \n",
    "            # the GC kicks in.\n",
    "            # Using 'with' ensures the file is closed as soon as the block is left - even if an exception \n",
    "            # happens inside that block, so it should always be preferred for any real application.\n",
    "            # source: https://stackoverflow.com/questions/39447362/equivalent-ways-to-json-load-a-file-in-python\n",
    "            \n",
    "        # json.load() : This method is used to parse JSON from URL or file.\n",
    "        # json.loads(): This method is used to parse string with JSON content.\n",
    "        # Then, json.load for a .json file\n",
    "        # and json.loads for text file containing json\n",
    "        # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.   \n",
    "        dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "    \n",
    "    else:\n",
    "        # If it is not neither a csv nor a txt file, let's assume it is one of different\n",
    "        # possible Excel files.\n",
    "        print(\"Excel file inferred. If an error message is shown, check if a valid file extension was used: \\'xlsx\\', \\'xls\\', etc.\\n\")\n",
    "        # For Excel type files, Pandas automatically detects the decimal separator and requires only the parameter parse_dates.\n",
    "        # Firstly, the argument infer_datetime_format was present on read_excel function, but was removed.\n",
    "        # From version 1.4 (beta, in 10 May 2022), it will be possible to pass the parameter 'decimal' to\n",
    "        # read_excel function for detecting decimal cases in strings. For numeric variables, it is not needed, though\n",
    "        \n",
    "        if (load_all_sheets_at_once == True):\n",
    "            \n",
    "            # Corresponds to setting sheet_name = None\n",
    "            \n",
    "            if (has_header == True):\n",
    "                \n",
    "                xlsx_doc = pd.read_excel(file_path, sheet_name = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                # parsing speed by 5-10x.\n",
    "                \n",
    "            else:\n",
    "                #No header\n",
    "                xlsx_doc = pd.read_excel(file_path, sheet_name = None, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "            \n",
    "            # xlsx_doc is a dictionary containing the sheet names as keys, and dataframes as items.\n",
    "            # Let's convert it to the desired format.\n",
    "            # Dictionary dict, dict.keys() is the array of keys; dict.values() is an array of the values;\n",
    "            # and dict.items() is an array of tuples with format ('key', value)\n",
    "            \n",
    "            # Create a list of returned datasets:\n",
    "            list_of_datasets = []\n",
    "            \n",
    "            # Let's iterate through the array of tuples. The first element returned is the key, and the\n",
    "            # second is the value\n",
    "            for sheet_name, dataframe in (xlsx_doc.items()):\n",
    "                # sheet_name = key; dataframe = value\n",
    "                # Define the dictionary with the standard format:\n",
    "                df_dict = {'sheet': sheet_name,\n",
    "                            'df': dataframe}\n",
    "                \n",
    "                # Add the dictionary to the list:\n",
    "                list_of_datasets.append(df_dict)\n",
    "            \n",
    "            print(\"\\n\")\n",
    "            print(f\"A total of {len(list_of_datasets)} dataframes were retrieved from the Excel file.\\n\")\n",
    "            print(f\"The dataframes correspond to the following Excel sheets: {list(xlsx_doc.keys())}\\n\")\n",
    "            print(\"Returning a list of dictionaries. Each dictionary contains the key \\'sheet\\', with the original sheet name; and the key \\'df\\', with the Pandas dataframe object obtained.\\n\")\n",
    "            print(f\"Check the 10 first rows of the dataframe obtained from the first sheet, named {list_of_datasets[0]['sheet']}:\\n\")\n",
    "            \n",
    "            try:\n",
    "                # only works in Jupyter Notebook:\n",
    "                from IPython.display import display\n",
    "                display((list_of_datasets[0]['df']).head(10))\n",
    "            \n",
    "            except: # regular mode\n",
    "                print((list_of_datasets[0]['df']).head(10))\n",
    "            \n",
    "            return list_of_datasets\n",
    "            \n",
    "        elif (sheet_to_load is not None):        \n",
    "        #Case where the user specifies which sheet of the Excel file should be loaded.\n",
    "            \n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                # verbose = True for showing number of NA values placed in non-numeric columns.\n",
    "                #  parse_dates = True: try parsing the index; infer_datetime_format = True : If True and parse_dates is enabled, pandas will attempt to infer the format of the datetime strings in \n",
    "                # the columns, and if it can be inferred, switch to a faster method of parsing them. In some cases this can increase the \n",
    "                # parsing speed by 5-10x.\n",
    "                \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, sheet_name = sheet_to_load, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                \n",
    "        \n",
    "        else:\n",
    "            #No sheet specified\n",
    "            if (has_header == True):\n",
    "                \n",
    "                dataset = pd.read_excel(file_path, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                \n",
    "            else:\n",
    "                #No header\n",
    "                dataset = pd.read_excel(file_path, header = None, na_values = how_missing_values_are_registered, verbose = True, parse_dates = True)\n",
    "                \n",
    "    print(f\"Dataset extracted from {file_path}. Check the 10 first rows of this dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(dataset.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(dataset.head(10))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b38b4726-9284-4b25-83ee-16522631145f"
   },
   "source": [
    "# **Function for converting JSON object to dataframe**\n",
    "- Objects may be:\n",
    "    - String with JSON formatted text;\n",
    "    - List with nested dictionaries (JSON formatted);\n",
    "    - Each dictionary may contain nested dictionaries, or nested lists of dictionaries (nested JSON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "azdata_cell_guid": "32a43e7d-b3d8-44da-8499-66abe88d354f",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def json_obj_to_pandas_dataframe (json_obj_to_convert, json_obj_type = 'list', json_record_path = None, json_field_separator = \"_\", json_metadata_prefix_list = None):\n",
    "    \n",
    "    import json\n",
    "    import pandas as pd\n",
    "    from pandas import json_normalize\n",
    "    \n",
    "    # JSON object in terms of Python structure: list of dictionaries, where each value of a\n",
    "    # dictionary may be a dictionary or a list of dictionaries (nested structures).\n",
    "    # example of highly nested structure saved as a list 'json_formatted_list'. Note that the same\n",
    "    # structure could be declared and stored into a string variable. For instance, if you have a txt\n",
    "    # file containing JSON, you could read the txt and save its content as a string.\n",
    "    # json_formatted_list = [{'field1': val1, 'field2': {'dict_val': dict_val}, 'field3': [{\n",
    "    # 'nest1': nest_val1}, {'nest2': nestval2}]}, {'field1': val1, 'field2': {'dict_val': dict_val}, \n",
    "    # 'field3': [{'nest1': nest_val1}, {'nest2': nestval2}]}]    \n",
    "\n",
    "    # json_obj_type = 'list', in case the object was saved as a list of dictionaries (JSON format)\n",
    "    # json_obj_type = 'string', in case it was saved as a string (text) containing JSON.\n",
    "\n",
    "    # json_obj_to_convert: object containing JSON, or string with JSON content to parse.\n",
    "    # Objects may be: string with JSON formatted text;\n",
    "    # list with nested dictionaries (JSON formatted);\n",
    "    # dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "    \n",
    "    # https://docs.python.org/3/library/json.html\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html#pandas.json_normalize\n",
    "    \n",
    "    # json_record_path (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "    # Path in each object to list of records. If not passed, data will be assumed to \n",
    "    # be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "    # dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "    # 'books' stores a nested JSON, declare, json_record_path = 'books'\n",
    "    \n",
    "    # json_field_separator = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "    # Nested records will generate names separated by sep. \n",
    "    # e.g., for json_field_separator = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "    # Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "    # the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "    # separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "    \n",
    "    # json_metadata_prefix_list: list of strings (in quotes). Manipulates the parameter \n",
    "    # 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "    # table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "    # will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "    \n",
    "    # e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "    # 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "    # Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "    # are 'name' and 'last'.\n",
    "    # Then, json_record_path = 'books'\n",
    "    # json_metadata_prefix_list = ['name', 'last']\n",
    "\n",
    "    \n",
    "    if (json_obj_type == 'string'):\n",
    "        # Use the json.loads method to convert the string to json\n",
    "        json_file = json.loads(json_obj_to_convert)\n",
    "        # json.load() : This method is used to parse JSON from URL or file.\n",
    "        # json.loads(): This method is used to parse string with JSON content.\n",
    "        # e.g. .json.loads() must be used to read a string with JSON and convert it to a flat file\n",
    "        # like a dataframe.\n",
    "        # check: https://www.pythonpip.com/python-tutorials/how-to-load-json-file-using-python/#:~:text=The%20json.load%20%28%29%20is%20used%20to%20read%20the,and%20alter%20data%20in%20our%20application%20or%20system.\n",
    "    \n",
    "    elif (json_obj_type == 'list'):\n",
    "        \n",
    "        # make the json_file the object itself:\n",
    "        json_file = json_obj_to_convert\n",
    "    \n",
    "    else:\n",
    "        print (\"Enter a valid JSON object type: \\'list\\', in case the JSON object is a list of dictionaries in JSON format; or \\'string\\', if the JSON is stored as a text (string variable).\")\n",
    "        return \"error\"\n",
    "    \n",
    "    dataset = json_normalize(json_file, record_path = json_record_path, sep = json_field_separator, meta = json_metadata_prefix_list)\n",
    "    \n",
    "    print(f\"JSON object converted to a flat dataframe object. Check the 10 first rows of this dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(dataset.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(dataset.head(10))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "2420e86d-77a7-462b-b5f5-e6805be5f5cc"
   },
   "source": [
    "# **Function for importing or exporting models, lists, or dictionaries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_export_model_list_dict (action = 'import', objects_manipulated = 'model_only', model_file_name = None, dictionary_or_list_file_name = None, directory_path = '', model_type = 'keras', dict_or_list_to_export = None, model_to_export = None, use_colab_memory = False):\n",
    "    \n",
    "    import os\n",
    "    import pickle\n",
    "    import dill\n",
    "    import tarfile\n",
    "    import tensorflow as tf\n",
    "    from zipfile import ZipFile\n",
    "    # https://docs.python.org/3/library/tarfile.html#tar-examples\n",
    "    # https://docs.python.org/3/library/zipfile.html#zipfile-objects\n",
    "    # pickle and dill save the file in binary (bits) serialized mode. So, we must use\n",
    "    # open 'rb' or 'wb' when calling the context manager. The 'b' stands for 'binary',\n",
    "    # informing the context manager (with statement) that a bit-file will be processed\n",
    "    from statsmodels.tsa.arima.model import ARIMA, ARIMAResults\n",
    "    from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler, MinMaxScaler\n",
    "    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression\n",
    "    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "    from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "    from xgboost import XGBRegressor, XGBClassifier\n",
    "    \n",
    "    # action = 'import' for importing a model and/or a dictionary;\n",
    "    # action = 'export' for exporting a model and/or a dictionary.\n",
    "    \n",
    "    # objects_manipulated = 'model_only' if only a model will be manipulated.\n",
    "    # objects_manipulated = 'dict_or_list_only' if only a dictionary or list will be manipulated.\n",
    "    # objects_manipulated = 'model_and_dict' if both a model and a dictionary will be\n",
    "    # manipulated.\n",
    "    \n",
    "    # model_file_name: string with the name of the file containing the model (for 'import');\n",
    "    # or of the name that the exported file will have (for 'export')\n",
    "    # e.g. model_file_name = 'model'\n",
    "    # WARNING: Do not add the file extension.\n",
    "    # Keep it in quotes. Keep model_file_name = None if no model will be manipulated.\n",
    "    \n",
    "    # dictionary_or_list_file_name: string with the name of the file containing the dictionary \n",
    "    # (for 'import');\n",
    "    # or of the name that the exported file will have (for 'export')\n",
    "    # e.g. dictionary_or_list_file_name = 'history_dict'\n",
    "    # WARNING: Do not add the file extension.\n",
    "    # Keep it in quotes. Keep dictionary_or_list_file_name = None if no \n",
    "    # dictionary or list will be manipulated.\n",
    "    \n",
    "    # DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "    # or from which the model will be retrieved. If no value is provided,\n",
    "    # the DIRECTORY_PATH will be the root: \"/\"\n",
    "    # Notice that the model and the dictionary must be stored in the same path.\n",
    "    # If a model and a dictionary will be exported, they will be stored in the same\n",
    "    # DIRECTORY_PATH.\n",
    "    \n",
    "    # model_type: This parameter has effect only when a model will be manipulated.\n",
    "    # model_type = 'keras' for deep learning keras/ tensorflow models with extension .h5\n",
    "    # model_type = 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "    # custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "    # model_type = 'sklearn' for models from scikit-learn (non-deep learning)\n",
    "    # model_type = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "    # model_type = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "    # model_type = 'arima' for ARIMA model (Statsmodels)\n",
    "    \n",
    "    # dict_or_list_to_export and model_to_export: \n",
    "    # These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "    # must be declared. If ACTION == 'export', keep:\n",
    "    # dict_or_list_to_export = None, \n",
    "    # model_to_export = None\n",
    "    # If one of these objects will be exported, substitute None by the name of the object\n",
    "    # e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "    # model_to_export = keras_model. Notice that it must be declared without quotes, since\n",
    "    # it is not a string, but an object.\n",
    "    # For exporting a dictionary named as 'dict':\n",
    "    # dict_or_list_to_export = dict\n",
    "    \n",
    "    # use_colab_memory: this parameter has only effect when using Google Colab (or it will\n",
    "    # raise an error). Set as use_colab_memory = True if you want to use the instant memory\n",
    "    # from Google Colaboratory: you will update or download the file and it will be available\n",
    "    # only during the time when the kernel is running. It will be excluded when the kernel\n",
    "    # dies, for instance, when you close the notebook.\n",
    "    \n",
    "    # If action == 'export' and use_colab_memory == True, then the file will be downloaded\n",
    "    # to your computer (running the cell will start the download).\n",
    "    \n",
    "    # Check the directory path\n",
    "    if (directory_path is None):\n",
    "        # set as the root (empty string):\n",
    "        directory_path = \"\"\n",
    "        \n",
    "        \n",
    "    bool_check1 = (objects_manipulated != 'model_only')\n",
    "    # bool_check1 == True if a dictionary will be manipulated\n",
    "    \n",
    "    bool_check2 = (objects_manipulated != 'dict_or_list_only')\n",
    "    # bool_check1 == True if a dictionary will be manipulated\n",
    "    \n",
    "    if (bool_check1 == True):\n",
    "        #manipulate a dictionary\n",
    "        \n",
    "        if (dictionary_or_list_file_name is None):\n",
    "            print(\"Please, enter a name for the dictionary or list.\")\n",
    "            return \"error1\"\n",
    "        \n",
    "        else:\n",
    "            # Create the file path for the dictionary:\n",
    "            dict_path = os.path.join(directory_path, dictionary_or_list_file_name)\n",
    "            # Extract the file extension\n",
    "            dict_extension = 'pkl'\n",
    "            #concatenate:\n",
    "            dict_path = dict_path + \".\" + dict_extension\n",
    "            \n",
    "    \n",
    "    if (bool_check2 == True):\n",
    "        #manipulate a model\n",
    "        \n",
    "        if (model_file_name is None):\n",
    "            print(\"Please, enter a name for the model.\")\n",
    "            return \"error1\"\n",
    "        \n",
    "        else:\n",
    "            # Create the file path for the dictionary:\n",
    "            model_path = os.path.join(directory_path, model_file_name)\n",
    "            # Extract the file extension\n",
    "            \n",
    "            #check model_type:\n",
    "            if (model_type == 'keras'):\n",
    "                model_extension = 'h5'\n",
    "            \n",
    "            elif (model_type == 'sklearn'):\n",
    "                model_extension = 'dill'\n",
    "                #it could be 'pkl', though\n",
    "            \n",
    "            elif (model_type == 'xgb_regressor'):\n",
    "                model_extension = 'json'\n",
    "                #it could be 'ubj', though\n",
    "            \n",
    "            elif (model_type == 'xgb_classifier'):\n",
    "                model_extension = 'json'\n",
    "                #it could be 'ubj', though\n",
    "            \n",
    "            elif (model_type == 'arima'):\n",
    "                model_extension = 'pkl'\n",
    "            \n",
    "            else:\n",
    "                print(\"Enter a valid model_type: keras, sklearn_xgb, or arima.\")\n",
    "                return \"error2\"\n",
    "            \n",
    "            #concatenate:\n",
    "            model_path = model_path +  \".\" + model_extension\n",
    "            \n",
    "    # Now we have the full paths for the dictionary and for the model.\n",
    "    \n",
    "    if (action == 'import'):\n",
    "        \n",
    "        if (use_colab_memory == True):\n",
    "             \n",
    "            from google.colab import files\n",
    "            # google.colab library must be imported only in case \n",
    "            # it is going to be used, for avoiding \n",
    "            # AWS compatibility issues.\n",
    "            \n",
    "            print(\"Click on the button for file selection and select the files from your machine that will be uploaded in the Colab environment.\")\n",
    "            print(\"Warning: the files will be removed from Colab memory after the Kernel dies or after the notebook is closed.\")\n",
    "            # this functionality requires the previous declaration:\n",
    "            ## from google.colab import files\n",
    "            colab_files_dict = files.upload()\n",
    "            # The files are stored into a dictionary called colab_files_dict where the keys\n",
    "            # are the names of the files and the values are the files themselves.\n",
    "            ## e.g. if you upload a single file named \"dictionary.pkl\", the dictionary will be\n",
    "            ## colab_files_dict = {'dictionary.pkl': file}, where file is actually a big string\n",
    "            ## representing the contents of the file. The length of this value is the size of the\n",
    "            ## uploaded file, in bytes.\n",
    "            ## To access the file is like accessing a value from a dictionary: \n",
    "            ## d = {'key1': 'val1'}, d['key1'] == 'val1'\n",
    "            ## we simply declare the key inside brackets and quotes, the same way we would do for\n",
    "            ## accessing the column of a dataframe.\n",
    "            ## In this example, colab_files_dict['dictionary.pkl'] access the content of the \n",
    "            ## .pkl file, and len(colab_files_dict['dictionary.pkl']) is the size of the .pkl\n",
    "            ## file in bytes.\n",
    "            ## To check the dictionary keys, apply the method .keys() to the dictionary (with empty\n",
    "            ## parentheses): colab_files_dict.keys()\n",
    "            \n",
    "            for key in colab_files_dict.keys():\n",
    "                #loop through each element of the list of keys of the dictionary\n",
    "                # (list colab_files_dict.keys()). Each element is named 'key'\n",
    "                print(f\"User uploaded file {key} with length {len(colab_files_dict[key])} bytes.\")\n",
    "                # The key is the name of the file, and the length of the value\n",
    "                ## correspondent to the key is the file's size in bytes.\n",
    "                ## Notice that the content of the uploaded object must be passed \n",
    "                ## as argument for a proper function to be interpreted. \n",
    "                ## For instance, the content of a xlsx file should be passed as\n",
    "                ## argument for Pandas .read_excel function; the pkl file must be passed as\n",
    "                ## argument for pickle.\n",
    "                ## e.g., if you uploaded 'table.xlsx' and stored it into colab_files_dict you should\n",
    "                ## declare df = pd.read_excel(colab_files_dict['table.xlsx']) to obtain a dataframe\n",
    "                ## df from the uploaded table. Notice that is the value, not the key, that is the\n",
    "                ## argument.\n",
    "        \n",
    "        if (bool_check1 == True):\n",
    "            #manipulate a dictionary\n",
    "            if (use_colab_memory == True):\n",
    "                key = dictionary_file_name + \".\" + dict_extension\n",
    "                #Use the key to access the file content, and pass the file content\n",
    "                # to pickle:\n",
    "                with open(colab_files_dict[key], 'rb') as opened_file:\n",
    "            \n",
    "                    imported_dict = pickle.load(opened_file)\n",
    "                    # The structure imported_dict = pkl.load(open(colab_files_dict[key], 'rb')) relies \n",
    "                    # on the GC to close the file. That's not a good idea: If someone doesn't use \n",
    "                    # CPython the garbage collector might not be using refcounting (which collects \n",
    "                    # unreferenced objects immediately) but e.g. collect garbage only after some time.\n",
    "                    # Since file handles are closed when the associated object is garbage collected or \n",
    "                    # closed explicitly (.close() or .__exit__() from a context manager) the file \n",
    "                    # will remain open until the GC kicks in.\n",
    "                    # Using 'with' ensures the file is closed as soon as the block is left - even if \n",
    "                    # an exception happens inside that block, so it should always be preferred for any \n",
    "                    # real application.\n",
    "                    # source: https://stackoverflow.com/questions/39447362/equivalent-ways-to-json-load-a-file-in-python\n",
    "\n",
    "                print(f\"Dictionary or list {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "            else:\n",
    "                #standard method\n",
    "                with open(dict_path, 'rb') as opened_file:\n",
    "            \n",
    "                    imported_dict = pickle.load(opened_file)\n",
    "                \n",
    "                # 'rb' stands for read binary (read mode). For writing mode, 'wb', 'write binary'\n",
    "                print(f\"Dictionary or list successfully imported from {dict_path}.\")\n",
    "                \n",
    "        if (bool_check2 == True):\n",
    "            #manipulate a model\n",
    "            # select the proper model\n",
    "        \n",
    "            if (model_type == 'keras'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = tf.keras.models.load_model(colab_files_dict[key])\n",
    "                    print(f\"Keras/TensorFlow model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # We previously declared:\n",
    "                    # from keras.models import load_model\n",
    "                    model = tf.keras.models.load_model(model_path)\n",
    "                    print(f\"Keras/TensorFlow model successfully imported from {model_path}.\")\n",
    "            \n",
    "            elif (model_type == 'tensorflow_general'):\n",
    "                \n",
    "                print(\"Warning, save the model in a directory called 'saved_model' (before compressing.)\\n\")\n",
    "                # Create a temporary folder in case it does not exist:\n",
    "                # https://www.geeksforgeeks.org/python-os-makedirs-method/\n",
    "                # Set exist_ok = True\n",
    "                os.makedirs(\"tmp/\", exist_ok = True)\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    \n",
    "                    key = model_file_name\n",
    "                    \n",
    "                    try:\n",
    "                        model_extension = \".tar\"\n",
    "                        key = key + model_extension\n",
    "                        model_path = colab_files_dict[key]\n",
    "                        # Open the context manager\n",
    "                        with tarfile.open (model_path, 'r:') as compressed_model:\n",
    "                            #extract all to the tmp directory:\n",
    "                            compressed_model.extractall(\"tmp/\")\n",
    "                        \n",
    "                        # if you were not using the context manager, it would be necessary to apply\n",
    "                        # close method: tar = tarfile.open(fname, \"r:gz\"); tar.extractall(); tar.close()\n",
    "                    \n",
    "                    except:\n",
    "                        \n",
    "                        try:\n",
    "                            # try tar.gz extension\n",
    "                            model_extension = \".tar.gz\"\n",
    "                            key = key + model_extension\n",
    "                            model_path = colab_files_dict[key]\n",
    "                            \n",
    "                            # Open the context manager\n",
    "                            with tarfile.open (model_path, 'r:gz') as compressed_model:\n",
    "                                #extract all to the tmp directory:\n",
    "                                compressed_model.extractall(\"tmp/\")\n",
    "                        \n",
    "                        except:\n",
    "                            # try .zip extension\n",
    "                            try:\n",
    "                                model_extension = \".zip\"\n",
    "                                key = key + model_extension\n",
    "                                model_path = colab_files_dict[key]\n",
    "                                \n",
    "                                # Open the context manager\n",
    "                                with ZipFile (model_path, 'r') as compressed_model:\n",
    "                                    #extract all to the tmp directory:\n",
    "                                    compressed_model.extractall(\"tmp/\")\n",
    "                            \n",
    "                            except:\n",
    "                                print(\"Failed to load the model. Compress it as zip, tar or tar.gz file.\\n\")\n",
    "                    \n",
    "                    \n",
    "                    # Compress the directory using tar\n",
    "                    # https://www.gnu.org/software/tar/manual/tar.html\n",
    "                    #    ! tar --extract --file=model_path --verbose --verbose tmp/\n",
    "                    \n",
    "                    try:\n",
    "                        model = tf.keras.models.load_model(\"tmp/saved_model\")\n",
    "                        print(f\"TensorFlow model: {model_path} successfully imported to Colab environment.\")\n",
    "                    \n",
    "                    except:\n",
    "                        print(\"Failed to load the model. Save it in a directory named 'saved_model' before compressing.\\n\")\n",
    "                    \n",
    "                else:\n",
    "                    #standard method\n",
    "                    \n",
    "                    # Try simply accessing the directory:\n",
    "                    try:\n",
    "                        model = tf.keras.models.load_model(\"tmp/saved_model\")\n",
    "                    \n",
    "                    except:\n",
    "                        \n",
    "                        try:\n",
    "                            model = tf.keras.models.load_model(model_file_name)\n",
    "                        \n",
    "                        except:\n",
    "                            \n",
    "                            # It is compressed\n",
    "                            try:\n",
    "                                model_extension = \".tar\"\n",
    "                                model_path = model_file_name\n",
    "\n",
    "                                # Open the context manager\n",
    "                                with tarfile.open (model_path, 'r:') as compressed_model:\n",
    "                                    #extract all to the tmp directory:\n",
    "                                    compressed_model.extractall(\"tmp/\")\n",
    "\n",
    "                                # if you were not using the context manager, it would be necessary to apply\n",
    "                                # close method: tar = tarfile.open(fname, \"r:gz\"); tar.extractall(); tar.close()\n",
    "\n",
    "                            except:\n",
    "\n",
    "                                try:\n",
    "                                    # try tar.gz extension\n",
    "                                    model_extension = \".tar.gz\"\n",
    "                                    model_path = model_file_name\n",
    "\n",
    "                                    # Open the context manager\n",
    "                                    with tarfile.open (model_path, 'r:gz') as compressed_model:\n",
    "                                        #extract all to the tmp directory:\n",
    "                                        compressed_model.extractall(\"tmp/\")\n",
    "\n",
    "                                except:\n",
    "                                    # try .zip extension\n",
    "                                    try:\n",
    "                                        model_extension = \".zip\"\n",
    "                                        model_path = model_file_name\n",
    "\n",
    "                                        # Open the context manager\n",
    "                                        with ZipFile (model_path, 'r') as compressed_model:\n",
    "                                            #extract all to the tmp directory:\n",
    "                                            compressed_model.extractall(\"tmp/\")\n",
    "\n",
    "                                    except:\n",
    "                                        print(\"Failed to load the model. Compress it as zip, tar or tar.gz file.\\n\")\n",
    "\n",
    "                    \n",
    "                    try:\n",
    "                        model = tf.keras.models.load_model(\"tmp/saved_model\")\n",
    "                        print(f\"TensorFlow model: {model_path} successfully imported to Colab environment.\")\n",
    "                    \n",
    "                    except:\n",
    "                        print(\"Failed to load the model. Save it in a directory named 'saved_model' before compressing.\\n\")\n",
    "                    \n",
    "                    \n",
    "            elif (model_type == 'sklearn'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    \n",
    "                    with open(colab_files_dict[key], 'rb') as opened_file:\n",
    "            \n",
    "                        model = dill.load(opened_file)\n",
    "                    \n",
    "                    print(f\"Scikit-learn model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    with open(model_path, 'rb') as opened_file:\n",
    "            \n",
    "                        model = dill.load(opened_file)\n",
    "                \n",
    "                    print(f\"Scikit-learn model successfully imported from {model_path}.\")\n",
    "                    # For loading a pickle model:\n",
    "                    ## model = pkl.load(open(model_path, 'rb'))\n",
    "                    # 'rb' stands for read binary (read mode). For writing mode, 'wb', 'write binary'\n",
    "\n",
    "            elif (model_type == 'xgb_regressor'):\n",
    "                \n",
    "                # Create an instance (object) from the class XGBRegressor:\n",
    "                \n",
    "                model = XGBRegressor()\n",
    "                # Now we can apply the load_model method from this class:\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = model.load_model(colab_files_dict[key])\n",
    "                    print(f\"XGBoost regression model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model = model.load_model(model_path)\n",
    "                    print(f\"XGBoost regression model successfully imported from {model_path}.\")\n",
    "                    # model.load_model(\"model.json\") or model.load_model(\"model.ubj\")\n",
    "                    # .load_model is a method from xgboost object\n",
    "            \n",
    "            elif (model_type == 'xgb_classifier'):\n",
    "\n",
    "                # Create an instance (object) from the class XGBClassifier:\n",
    "\n",
    "                model = XGBClassifier()\n",
    "                # Now we can apply the load_model method from this class:\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = model.load_model(colab_files_dict[key])\n",
    "                    print(f\"XGBoost classification model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model = model.load_model(model_path)\n",
    "                    print(f\"XGBoost classification model successfully imported from {model_path}.\")\n",
    "                    # model.load_model(\"model.json\") or model.load_model(\"model.ubj\")\n",
    "                    # .load_model is a method from xgboost object\n",
    "\n",
    "            elif (model_type == 'arima'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model = ARIMAResults.load(colab_files_dict[key])\n",
    "                    print(f\"ARIMA model: {key} successfully imported to Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    # We previously declared:\n",
    "                    # from statsmodels.tsa.arima.model import ARIMAResults\n",
    "                    model = ARIMAResults.load(model_path)\n",
    "                    print(f\"ARIMA model successfully imported from {model_path}.\")\n",
    "            \n",
    "            if (objects_manipulated == 'model_only'):\n",
    "                # only the model should be returned\n",
    "                return model\n",
    "            \n",
    "            elif (objects_manipulated == 'dict_only'):\n",
    "                # only the dictionary should be returned:\n",
    "                return imported_dict\n",
    "            \n",
    "            else:\n",
    "                # Both objects are returned:\n",
    "                return model, imported_dict\n",
    "\n",
    "    \n",
    "    elif (action == 'export'):\n",
    "        \n",
    "        #Let's export the models or dictionary:\n",
    "        if (use_colab_memory == True):\n",
    "            \n",
    "            from google.colab import files\n",
    "            # google.colab library must be imported only in case \n",
    "            # it is going to be used, for avoiding \n",
    "            # AWS compatibility issues.\n",
    "            \n",
    "            print(\"The files will be downloaded to your computer.\")\n",
    "        \n",
    "        if (bool_check1 == True):\n",
    "            #manipulate a dictionary\n",
    "            if (use_colab_memory == True):\n",
    "                ## Download the dictionary\n",
    "                key = dictionary_or_list_file_name + \".\" + dict_extension\n",
    "                \n",
    "                with open(key, 'wb') as opened_file:\n",
    "            \n",
    "                    pickle.dump(dict_or_list_to_export, opened_file)\n",
    "                \n",
    "                # this functionality requires the previous declaration:\n",
    "                ## from google.colab import files\n",
    "                files.download(key)\n",
    "                \n",
    "                print(f\"Dictionary or list {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "            else:\n",
    "                #standard method \n",
    "                with open(dict_path, 'wb') as opened_file:\n",
    "            \n",
    "                    pickle.dump(dict_or_list_to_export, opened_file)\n",
    "                \n",
    "                #to save the file, the mode must be set as 'wb' (write binary)\n",
    "                print(f\"Dictionary or list successfully exported as {dict_path}.\")\n",
    "                \n",
    "        if (bool_check2 == True):\n",
    "            #manipulate a model\n",
    "            # select the proper model\n",
    "        \n",
    "            if (model_type == 'keras'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"Keras/TensorFlow model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save(model_path)\n",
    "                    print(f\"Keras/TensorFlow model successfully exported as {model_path}.\")\n",
    "            \n",
    "            elif (model_type == 'tensorflow_general'):\n",
    "                \n",
    "                # Save your model in the SavedModel format\n",
    "                # Save as a directory named 'saved_model'\n",
    "                model_to_export.save('saved_model')\n",
    "                model_path = 'saved_model'\n",
    "            \n",
    "                try:\n",
    "                    model_path = model_path + \".tar.gz\"\n",
    "                    \n",
    "                    # Open the context manager\n",
    "                    with tarfile.open (model_path, 'w:gz') as compressed_model:\n",
    "                        #Add the folder:\n",
    "                        compressed_model.add('saved_model/')    \n",
    "                        # if you were not using the context manager, it would be necessary to apply\n",
    "                        # close method: tar = tarfile.open(fname, \"r:gz\"); tar.extractall(); tar.close()\n",
    "                \n",
    "                except:\n",
    "                    # try compressing as tar:\n",
    "                    try:\n",
    "                        model_path = model_path + \".tar\"\n",
    "                        # Open the context manager\n",
    "                        with tarfile.open (model_path, 'w:') as compressed_model:\n",
    "                            #Add the folder:\n",
    "                            compressed_model.add('saved_model/') \n",
    "                    \n",
    "                    except:\n",
    "                        # compress as zip:\n",
    "                        model_path = model_path + \".zip\"\n",
    "                        with ZipFile (model_path, 'w') as compressed_model:\n",
    "                            compressed_model.write('saved_model/')\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    \n",
    "                    key = model_path\n",
    "                    files.download(key)\n",
    "                    print(f\"TensorFlow model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    print(f\"TensorFlow model successfully exported as {model_path}.\")\n",
    "\n",
    "            elif (model_type == 'sklearn'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    \n",
    "                    with open(key, 'wb') as opened_file:\n",
    "\n",
    "                        dill.dump(model_to_export, opened_file)\n",
    "                    \n",
    "                    #to save the file, the mode must be set as 'wb' (write binary)\n",
    "                    files.download(key)\n",
    "                    print(f\"Scikit-learn model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    with open(model_path, 'wb') as opened_file:\n",
    "\n",
    "                        dill.dump(model_to_export, opened_file)\n",
    "                    \n",
    "                    print(f\"Scikit-learn model successfully exported as {model_path}.\")\n",
    "                    # For exporting a pickle model:\n",
    "                    ## pkl.dump(model_to_export, open(model_path, 'wb'))\n",
    "            \n",
    "            elif ((model_type == 'xgb_regressor')|(model_type == 'xgb_classifier')):\n",
    "                # In both cases, the XGBoost object is already loaded in global\n",
    "                # context memory. So there is already the object for using the\n",
    "                # save_model method, available for both classes (XGBRegressor and\n",
    "                # XGBClassifier).\n",
    "                # We can simply check if it is one type OR the other, since the\n",
    "                # method is the same:\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save_model(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"XGBoost model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save_model(model_path)\n",
    "                    print(f\"XGBoost model successfully exported as {model_path}.\")\n",
    "                    # For exporting a pickle model:\n",
    "                    ## pkl.dump(model_to_export, open(model_path, 'wb'))\n",
    "            \n",
    "            elif (model_type == 'arima'):\n",
    "                \n",
    "                if (use_colab_memory == True):\n",
    "                    ## Download the model\n",
    "                    key = model_file_name + \".\" + model_extension\n",
    "                    model_to_export.save(key)\n",
    "                    files.download(key)\n",
    "                    print(f\"ARIMA model: {key} successfully downloaded from Colab environment.\")\n",
    "            \n",
    "                else:\n",
    "                    #standard method\n",
    "                    model_to_export.save(model_path)\n",
    "                    print(f\"ARIMA model successfully exported as {model_path}.\")\n",
    "        \n",
    "        print(\"Export of files completed.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"Enter a valid action, import or export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d2388930-d286-4ea7-a9fe-d17bc6209f81"
   },
   "source": [
    "# **Classes for Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_checking:\n",
    "            \n",
    "    # Initialize instance attributes.\n",
    "    # define the Class constructor, i.e., how are its objects:\n",
    "    def __init__(self, model_object = None, model_type = 'regression', model_package = 'tensorflow', column_map_dict = None, training_history_object = None, X = None, y_train = None, y_preds_for_train = None, y_test = None, y_preds_for_test = None, y_valid = None, y_preds_for_validation = None):\n",
    "        \n",
    "        import numpy as np\n",
    "        import tensorflow as tf\n",
    "\n",
    "        # Add the model:        \n",
    "        self.model = model_object\n",
    "        # It can be None: user can firstly call the object to retrieve the total classes, and\n",
    "        # then call it again with the model adjusted for that amount of classes.\n",
    "        \n",
    "        # model_type = 'regression' or 'classification'\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        if (model_type == 'regression'):\n",
    "            self.metrics_name = 'mse'\n",
    "        \n",
    "        else:\n",
    "            self.metrics_name = 'crossentropy'\n",
    "        \n",
    "        # Add model package: 'tensorflow' (and keras), 'sklearn', or 'xgboost':\n",
    "        self.package = model_package\n",
    "\n",
    "        # Add the columns names:\n",
    "        self.column_map_dict = column_map_dict\n",
    "        # Add the training history to the class:\n",
    "        self.history = training_history_object\n",
    "\n",
    "        # Add the y series for computing general metrics:\n",
    "        # Guarantee that they are tensorflow tensors\n",
    "        if (y_train is not None):\n",
    "            self.y_train = tf.constant(y_train)\n",
    "        else:\n",
    "            self.y_train = None\n",
    "        if (y_preds_for_train is not None):\n",
    "            self.y_preds_for_train = tf.constant(y_preds_for_train)\n",
    "        else:\n",
    "            self.y_train = None\n",
    "        if (y_test is not None):\n",
    "            self.y_test = tf.constant(y_test)\n",
    "        else:\n",
    "            self.y_test = None\n",
    "        if (y_preds_for_test is not None):\n",
    "            self.y_preds_for_test = tf.constant(y_preds_for_test)\n",
    "        else:\n",
    "            self.y_preds_for_test = None\n",
    "        if (y_valid is not None):\n",
    "            self.y_valid = tf.constant(y_valid)\n",
    "        else:\n",
    "            self.y_valid = None\n",
    "        if (y_preds_for_validation is not None):\n",
    "            self.y_preds_for_validation = tf.constant(y_preds_for_validation)\n",
    "        else:\n",
    "            self.y_preds_for_validation = None\n",
    "\n",
    "        # X can be X_train, X_test, or X_valid. \n",
    "        # We only want to obtain the total number of predictors. X.shape is like:\n",
    "        # TensorShape([253, 11]). Second index [1] is the number of predictors:\n",
    "        if (X is not None):\n",
    "            # make sure it is a tensor:\n",
    "            X = tf.constant(X)\n",
    "            total_predictors = X.shape[1]\n",
    "            self.total_predictors = total_predictors\n",
    "\n",
    "        # to check the class attributes, use the __dict__ method or the vars function. Examples:\n",
    "        ## object.__dict__ will show all attributes from object\n",
    "        ## vars(object) shows the same.\n",
    "                \n",
    "    # Define the class methods.\n",
    "    # All methods must take an object from the class (self) as one of the parameters\n",
    "    \n",
    "    def model_metrics (self, show_confusion_matrix_values = True, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "        \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        import tensorflow as tf\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/keras/metrics?authuser=1\n",
    "        from sklearn.metrics import classification_report, confusion_matrix, r2_score\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "        # Retrieve type of problem:\n",
    "        model_type = self.model_type\n",
    "\n",
    "        # Retrieve the tensors.\n",
    "        tensors_dict = {}\n",
    "        tensors_dict['training'] = {'actual': self.y_train, 'predictions': self.y_preds_for_train}\n",
    "        tensors_dict['testing'] = {'actual': self.y_test, 'predictions': self.y_preds_for_test}\n",
    "        tensors_dict['validation'] = {'actual': self.y_valid, 'predictions': self.y_preds_for_validation}\n",
    "\n",
    "        metrics_dict = {}\n",
    "        \n",
    "        # Loop through the keys:\n",
    "        for key in tensors_dict.keys():\n",
    "          \n",
    "            # Retrieve the nested dictionary:\n",
    "            nested_dict = tensors_dict[key]\n",
    "            # Retrieve actual and predicted values:\n",
    "            y_true =  nested_dict['actual']\n",
    "            y_pred = nested_dict['predictions']\n",
    "            # Check if there is no None value stored:\n",
    "            if ((y_true is not None) & (y_pred is not None)):\n",
    "\n",
    "                calculated_metrics = {}\n",
    "\n",
    "                # Regression metrics:\n",
    "                if (model_type == 'regression'):\n",
    "\n",
    "                    print(f\"Metrics for {key}:\\n\")\n",
    "                    mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)\n",
    "                    #https://www.tensorflow.org/api_docs/python/tf/keras/metrics/mean_squared_error?authuser=1\n",
    "                    # The function returns a NumPy array containing a single element. Extract it as\n",
    "                    # variable:\n",
    "                    # Then, some situations may return numpy arrays instead of scalars. We can convert\n",
    "                    # to a scalar by selecting only the first and single element from the array.\n",
    "                    \n",
    "                    try:\n",
    "                        mse = mse[0]\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    # Print in scientific notation:\n",
    "                    try:\n",
    "                        print(f\"Mean squared error (MSE) = {mse:e}\")\n",
    "                    except:\n",
    "                        print(f\"Mean squared error (MSE) = {mse}\")\n",
    "                    # Add to calculated metrics:\n",
    "                    calculated_metrics['mse'] = mse\n",
    "\n",
    "                    # rmse is not available as function, only class. Use numpy method to convert to value\n",
    "                    # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RootMeanSquaredError?authuser=1\n",
    "                    # Create the object:\n",
    "                    rmse = tf.keras.metrics.RootMeanSquaredError()\n",
    "                    # Update its state:\n",
    "                    rmse = rmse.update_state(y_true, y_pred)\n",
    "                    # Use the numpy method to retrieve only the value:\n",
    "                    rmse = rmse.numpy()\n",
    "                    # Here, numpy method already returns a scalar\n",
    "                    # Print in scientific notation:\n",
    "                    \n",
    "                    try:\n",
    "                        rmse = rmse[0]\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    try:\n",
    "                        print(f\"Root mean squared error (RMSE) = {rmse:e}\")\n",
    "                    except:\n",
    "                        print(f\"Root mean squared error (RMSE) = {rmse}\")\n",
    "                    # Add to calculated metrics:\n",
    "                    calculated_metrics['rmse'] = rmse\n",
    "\n",
    "                    # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/mean_absolute_error?authuser=1\n",
    "                    mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
    "                    # The function returns a NumPy array containing a single element. Extract it as\n",
    "                    # variable:\n",
    "                    \n",
    "                    try:\n",
    "                        mae = mae[0]\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    # Print in scientific notation:\n",
    "                    try:\n",
    "                        print(f\"Mean absolute error (MAE) = {mae:e}\")\n",
    "                    except:\n",
    "                        print(f\"Mean absolute error (MAE) = {mae}\")\n",
    "                    # Add to calculated metrics:\n",
    "                    calculated_metrics['mae'] = mae\n",
    "\n",
    "                    # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/mean_absolute_percentage_error?authuser=1\n",
    "                    mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "                    # The function returns a NumPy array containing a single element. Extract it as\n",
    "                    # variable:\n",
    "                    \n",
    "                    try:\n",
    "                        mape = mape[0]\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    # Print in scientific notation:\n",
    "                    try:\n",
    "                        print(f\"Mean absolute percentage error (MAPE) = {mape:e}\")\n",
    "                    except:\n",
    "                        print(f\"Mean absolute percentage error (MAPE) = {mape}\")\n",
    "                    # Add to calculated metrics:\n",
    "                    calculated_metrics['mape'] = mape\n",
    "                    \n",
    "                    try:\n",
    "                        import tensorflow_addons as tfa\n",
    "                        # https://www.tensorflow.org/addons\n",
    "                        # R2 and R2-adj are available only as tfa object:\n",
    "                        # https://www.tensorflow.org/addons/api_docs/python/tfa/metrics/RSquare\n",
    "                        # Create the object:\n",
    "                        r2 = tfa.metrics.RSquare()\n",
    "                        # Update its state:\n",
    "                        # tfa method returns None, so we must only call the method:\n",
    "                        r2.update_state(y_true, y_pred)\n",
    "                        # Use the numpy method to retrieve only the value:\n",
    "                        r2 = r2.result().numpy() # already a scalar\n",
    "                        # for this tfa metrics, the methods result and numpy must be chained\n",
    "                        # otherwise, an error will be raised.\n",
    "                        \n",
    "                        try:\n",
    "                            r2 = r2[0]\n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "                        try:\n",
    "                            print(f\"Coefficient of linear correlation R² = {r2:e}\")\n",
    "                        except:\n",
    "                            print(f\"Coefficient of linear correlation R² = {r2}\")\n",
    "                        # Add to calculated metrics:\n",
    "                        calculated_metrics['r_squared'] = r2\n",
    "                        \n",
    "                    except:\n",
    "                        r2 = r2_score(y_true.numpy(), y_pred.numpy())\n",
    "                        # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html\n",
    "                        try:\n",
    "                            r2 = r2[0]\n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "                        \n",
    "                        try:\n",
    "                            print(f\"Coefficient of linear correlation R² = {r2:e}\")\n",
    "                        except:\n",
    "                            print(f\"Coefficient of linear correlation R² = {r2}\")\n",
    "                        \n",
    "                        # Add to calculated metrics:\n",
    "                        calculated_metrics['r_squared'] = r2\n",
    "                    \n",
    "                    try:\n",
    "                        # Try to calculate the adjusted R² by accessing the number of predictors:\n",
    "                        # This number may not be present.\n",
    "                        total_predictors = self.total_predictors\n",
    "                        # Create the object:\n",
    "                        r2_adj = tfa.metrics.RSquare(num_regressors = total_predictors)\n",
    "                        # Update its state. Again, method returns None:\n",
    "                        r2_adj.update_state(y_true, y_pred)\n",
    "                        # Use the numpy method to retrieve only the value:\n",
    "                        r2_adj = r2_adj.result().numpy() # scalar\n",
    "                        # Again, the methods result and numpy must be chained\n",
    "                        \n",
    "                        try:\n",
    "                            r2_adj = r2_adj[0]\n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "                        try:\n",
    "                            print(f\"Adjusted coefficient of correlation R²-adj = {r2_adj:e}\")\n",
    "                        except:\n",
    "                            print(f\"Adjusted coefficient of correlation R²-adj = {r2_adj}\")\n",
    "                        # Add to calculated metrics:\n",
    "                        calculated_metrics['r_squared_adj'] = r2_adj\n",
    "\n",
    "                    except:\n",
    "                        # Manually correct R²:\n",
    "                        # n_size_train = number of sample size\n",
    "                        # k_model = number of independent variables of the defined model\n",
    "                        k_model = self.total_predictors\n",
    "                        #numer of rows\n",
    "                        n_size = len(y_true)\n",
    "                        r2_adj = 1 - (1 - r2)*(n_size - 1)/(n_size - k_model - 1)\n",
    "                        \n",
    "                        try:\n",
    "                            r2_adj = r2_adj[0]\n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "                        \n",
    "                        try:\n",
    "                            print(f\"Adjusted coefficient of correlation R²-adj = {r2_adj:e}\")\n",
    "                        except:\n",
    "                            print(f\"Adjusted coefficient of correlation R²-adj = {r2_adj}\")\n",
    "                        \n",
    "                        # Add to calculated metrics:\n",
    "                        calculated_metrics['r_squared_adj'] = r2_adj\n",
    "                    \n",
    "                    print(\"\\n\")\n",
    "                    # Now, add the metrics to the metrics_dict:\n",
    "                    metrics_dict[key] = calculated_metrics\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    print(f\"Metrics for {key}:\\n\")\n",
    "                    # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC\n",
    "                    # Create the object:\n",
    "                    auc = tf.keras.metrics.AUC()\n",
    "                    # Update its state:\n",
    "                    auc.update_state(y_true, y_pred)\n",
    "                    # Use the numpy method to retrieve only the value:\n",
    "                    auc = auc.result().numpy() # scalar\n",
    "                    try:\n",
    "                        auc = auc[0]\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    try:\n",
    "                        print(f\"AUC = {auc:e}\")\n",
    "                    except:\n",
    "                        print(f\"AUC = {auc}\")\n",
    "                    # Add to calculated metrics:\n",
    "                    calculated_metrics['auc'] = auc\n",
    "\n",
    "                    # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy\n",
    "                    # Create the object:\n",
    "                    acc = tf.keras.metrics.Accuracy()\n",
    "                    # Update its state:\n",
    "                    acc.update_state(y_true, y_pred)\n",
    "                    # Use the numpy method to retrieve only the value:\n",
    "                    acc = acc.result().numpy() # scalar\n",
    "                    try:\n",
    "                        acc = acc[0]\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    try:\n",
    "                        print(f\"Accuracy = {acc:e}\")\n",
    "                    except:\n",
    "                        print(f\"Accuracy = {acc}\")\n",
    "                    # Add to calculated metrics:\n",
    "                    calculated_metrics['accuracy'] = acc\n",
    "\n",
    "                    # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision\n",
    "                    # Create the object:\n",
    "                    precision = tf.keras.metrics.Precision()\n",
    "                    # Update its state:\n",
    "                    precision.update_state(y_true, y_pred)\n",
    "                    # Use the numpy method to retrieve only the value:\n",
    "                    precision = precision.result().numpy() # scalar\n",
    "                    try:\n",
    "                        precision = precision[0]\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    try:\n",
    "                        print(f\"Precision = {precision:e}\")\n",
    "                    except:\n",
    "                        print(f\"Precision = {precision}\")\n",
    "                    # Add to calculated metrics:\n",
    "                    calculated_metrics['precision'] = precision\n",
    "\n",
    "                    # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall\n",
    "                    # Create the object:\n",
    "                    recall = tf.keras.metrics.Recall()\n",
    "                    # Update its state:\n",
    "                    recall.update_state(y_true, y_pred)\n",
    "                    # Use the numpy method to retrieve only the value:\n",
    "                    recall = recall.result().numpy() # scalar\n",
    "                    try:\n",
    "                        recall = recall[0]\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    try:\n",
    "                        print(f\"Recall = {recall:e}\")\n",
    "                    except:\n",
    "                        print(f\"Recall = {recall}\")\n",
    "                    # Add to calculated metrics:\n",
    "                    calculated_metrics['recall'] = recall\n",
    "                    \n",
    "                    # The method update_state returns None, so it must be called without and equality\n",
    "\n",
    "                    # Get the classification report:\n",
    "                    print(\"\\n\")\n",
    "                    print(\"Classification Report:\\n\")\n",
    "                    # Convert tensors to NumPy arrays\n",
    "                    report = classification_report (y_true.numpy(), y_pred.numpy())\n",
    "                    print(report)\n",
    "                    # Add to calculated metrics:\n",
    "                    calculated_metrics['classification_report'] = report\n",
    "                    print(\"\\n\")\n",
    "\n",
    "                    # Get the confusion matrix:\n",
    "                    # Convert tensors to NumPy arrays\n",
    "                    matrix = confusion_matrix (y_true.numpy(), y_pred.numpy())\n",
    "                    # Add to calculated metrics:\n",
    "                    calculated_metrics['confusion_matrix'] = report\n",
    "                    print(\"Confusion matrix:\\n\")\n",
    "\n",
    "                    fig, ax = plt.subplots(figsize = (12, 8))\n",
    "                    # possible color schemes (cmap) for the heat map: None, 'Blues_r',\n",
    "                    # \"YlGnBu\",\n",
    "                    # https://seaborn.pydata.org/generated/seaborn.heatmap.html?msclkid=73d24a00c1b211ec8aa1e7ab656e3ff4\n",
    "                    # http://seaborn.pydata.org/tutorial/color_palettes.html?msclkid=daa091f1c1b211ec8c74553348177b45\n",
    "                    ax = sns.heatmap(matrix, annot = show_confusion_matrix_values, fmt = \".0f\", linewidths = .5, square = True, cmap = 'Blues_r');\n",
    "                    #annot = True: shows the number corresponding to each square\n",
    "                    #annot = False: do not show the number\n",
    "                    plot_title = f\"Accuracy Score for {key} = {acc:.2f}\"\n",
    "                    ax.set_title(plot_title)\n",
    "                    ax.set_ylabel('Actual class')\n",
    "                    ax.set_xlabel('Predicted class')\n",
    "                    \n",
    "                    if (export_png == True):\n",
    "                        # Image will be exported\n",
    "                        import os\n",
    "                        \n",
    "                        #check if the user defined a directory path. If not, set as the default root path:\n",
    "                        if (directory_to_save is None):\n",
    "                            #set as the default\n",
    "                            directory_to_save = \"\"\n",
    "                        \n",
    "                        #check if the user defined a file name. If not, set as the default name for this\n",
    "                        # function.\n",
    "                        if (file_name is None):\n",
    "                            #set as the default\n",
    "                            file_name = \"confusion_matrix_\" + key\n",
    "                        \n",
    "                        else:\n",
    "                            # add the train suffix, to differentiate from the test matrix:\n",
    "                            file_name = file_name + \"_\" + key\n",
    "                        \n",
    "                        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "                        # resolution.\n",
    "                        if (png_resolution_dpi is None):\n",
    "                            #set as 330 dpi\n",
    "                            png_resolution_dpi = 330\n",
    "                        \n",
    "                        #Get the new_file_path\n",
    "                        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "                        \n",
    "                        #Export the file to this new path:\n",
    "                        # The extension will be automatically added by the savefig method:\n",
    "                        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "                        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "                        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "                        #transparent = True or False\n",
    "                        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "                        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "                    \n",
    "                    #fig.tight_layout()\n",
    "                    \n",
    "                    ## Show an image read from an image file:\n",
    "                    ## import matplotlib.image as pltimg\n",
    "                    ## img=pltimg.imread('mydecisiontree.png')\n",
    "                    ## imgplot = plt.imshow(img)\n",
    "                    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "                    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "                    ##  '03_05_END.ipynb'\n",
    "                    plt.show()\n",
    "\n",
    "                    print(\"\\n\")\n",
    "                    # Now, add the metrics to the metrics_dict:\n",
    "                    metrics_dict[key] = calculated_metrics\n",
    "          \n",
    "        # Now that we finished calculating metrics for all tensors, save the\n",
    "        # dictionary as a class variable (attribute) and return the object:\n",
    "        self.metrics_dict = metrics_dict\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def feature_importance_ranking (self, model_class = 'linear', orientation = 'vertical', horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        # model_class = 'linear' or model_class = 'tree'\n",
    "        # Retrieve the model:\n",
    "        model = self.model\n",
    "        # Return the mapping dictionary:\n",
    "        column_map_dict = self.column_map_dict\n",
    "        model_type = self.model_type\n",
    "\n",
    "        if (model_class == 'linear'):\n",
    "\n",
    "            # Get the list of coefficients\n",
    "            reg_coefficients = model.coef_\n",
    "              \n",
    "            try: \n",
    "                trial_access = reg_coefficients[1]\n",
    "                # If the trial succeeded, reg_coefficients is in the correct format [coef1, coef2, ...]\n",
    "                \n",
    "                # reg_coefficients[0] is a scalar, not an array.\n",
    "                # Convert the numpy array:\n",
    "                reg_coefficients = np.array(reg_coefficients)\n",
    "                abs_reg_coefficients = abs(reg_coefficients)\n",
    "                \n",
    "            except: \n",
    "                # The trial fails when reg_coefficients is an array containing a single array like \n",
    "                # [[coef1, coef2, ...]]\n",
    "                # So, the index 0 stores the array of interest. \n",
    "                # Since coefficients may be negative, pick the absolute values from the array in index 0\n",
    "                # (NumPy arrays accept vectorial operations, lists do not):\n",
    "                reg_coefficients = np.array(reg_coefficients[0])\n",
    "                abs_reg_coefficients = abs(reg_coefficients)\n",
    "                # Already numpy arrays\n",
    "            \n",
    "            if (column_map_dict is not None):\n",
    "                # Retrieve the values (columns' names):\n",
    "                # Set as list\n",
    "                columns_list = list(column_map_dict['features'].values())\n",
    "            \n",
    "            else:\n",
    "                # Retrieve the values (columns' names):\n",
    "                columns_list = [i for i in range(0, len(reg_coefficients))]\n",
    "            \n",
    "            # Get the intercept coefficient:\n",
    "            print(f\"Calculated model intercept = {model.intercept_}\\n\")\n",
    "            \n",
    "            try:\n",
    "                # Create the regression dictionary:\n",
    "                reg_dict = {'predictive_features': columns_list,\n",
    "                          'regression_coefficients': reg_coefficients,\n",
    "                           'abs_reg_coefficients': abs_reg_coefficients}\n",
    "\n",
    "                # Convert it to a Pandas dataframe:\n",
    "                feature_importance_df = pd.DataFrame(data = reg_dict)\n",
    "\n",
    "                # Now sort the dataframe in descending order of coefficient, and ascending order of\n",
    "                # feature (when sorting by multiple columns, we pass a list of columns to by and a \n",
    "                # list of booleans to ascending, instead of passing a simple string to by and a boolean\n",
    "                # to ascending. The element on a given index from the list by corresponds to the boolean\n",
    "                # with the same index in ascending):\n",
    "                feature_importance_df = feature_importance_df.sort_values(by = ['abs_reg_coefficients', 'regression_coefficients', 'predictive_features'], ascending = [False, False, True])\n",
    "\n",
    "                # Now that the dataframe is sorted in descending order, it represents the feature\n",
    "                # importance ranking.\n",
    "\n",
    "                # Restart the indices:\n",
    "                feature_importance_df = feature_importance_df.reset_index(drop = True)\n",
    "            \n",
    "            except:\n",
    "                print(\"Model has number of coefficients different from number of predictors.\")\n",
    "                print(f\"Model's coefficients = {reg_coefficients}\\n\")\n",
    "\n",
    "\n",
    "        elif (model_class == 'tree'):\n",
    "\n",
    "            # Set the list of the predictors:\n",
    "            # Use the list attribute to guarantee that it is a list:\n",
    "            \n",
    "            # Get the list of feature importances. Apply the list method to convert the\n",
    "            # array from .feature_importances_ to a list:\n",
    "            feature_importances = model.feature_importances_\n",
    "                 \n",
    "            try: \n",
    "                trial_access = feature_importances[1]\n",
    "                # If the trial succeeded, feature_importances is in the correct format \n",
    "                # [coef1, coef2, ...]\n",
    "                # feature_importances[0] is a scalar, not an array.\n",
    "                feature_importances = np.array(feature_importances)\n",
    "                abs_feature_importances = abs(feature_importances)\n",
    "                             \n",
    "            except: \n",
    "                # The trial fails when reg_coefficients is an array containing a single array like \n",
    "                # [[coef1, coef2, ...]]\n",
    "                # So, the index 0 stores the array of interest. \n",
    "                # Since coefficients may be negative, pick the absolute values from the array in index 0\n",
    "                # (NumPy arrays accept vectorial operations, lists do not):\n",
    "                feature_importances = np.array(feature_importances[0])\n",
    "                abs_feature_importances = abs(feature_importances)\n",
    "                # feature_importances and abs_feature_importances are already numpy arrays\n",
    "            \n",
    "            if (column_map_dict is not None):\n",
    "                # Retrieve the values (columns' names):\n",
    "                columns_list = list(column_map_dict['features'].values())\n",
    "                \n",
    "            else:\n",
    "                # Retrieve the values (columns' names):\n",
    "                columns_list = [i for i in range(0, len(feature_importances))]\n",
    "            \n",
    "            try:\n",
    "                # Create the model dictionary:\n",
    "                model_dict = {'predictive_features': columns_list,\n",
    "                            'feature_importances': feature_importances,\n",
    "                            'abs_feature_importances': abs_feature_importances}\n",
    "\n",
    "                # Convert it to a Pandas dataframe:\n",
    "                feature_importance_df = pd.DataFrame(data = model_dict)\n",
    "            \n",
    "                # Now sort the dataframe in descending order of importance, and ascending order of\n",
    "                # feature (when sorting by multiple columns, we pass a list of columns to by and a \n",
    "                # list of booleans to ascending, instead of passing a simple string to by and a boolean\n",
    "                # to ascending. The element on a given index from the list by corresponds to the boolean\n",
    "                # with the same index in ascending):\n",
    "                feature_importance_df = feature_importance_df.sort_values(by = ['abs_feature_importances', 'feature_importances', 'predictive_features'], ascending = [False, False, True])\n",
    "\n",
    "                # Now that the dataframe is sorted in descending order, it represents the feature\n",
    "                # importance ranking.\n",
    "\n",
    "                # Restart the indices:\n",
    "                feature_importance_df = feature_importance_df.reset_index(drop = True)\n",
    "            \n",
    "            except:\n",
    "                print(\"Model feature importance ranking generated a total of values different from number of predictors.\")\n",
    "                print(f\"Model's feature_importances = {feature_importances}\\n\")\n",
    "\n",
    "        try:  \n",
    "\n",
    "            try:\n",
    "                print(\"Feature importance ranking - until 20 most important features:\\n\")\n",
    "                # only works in Jupyter Notebook:\n",
    "                from IPython.display import display\n",
    "                display(feature_importance_df.head(20))\n",
    "\n",
    "            except: # regular mode\n",
    "                print(\"Feature importance ranking - until 20 most important features:\\n\")\n",
    "                print(feature_importance_df.head(20))\n",
    "\n",
    "            # Save the feature importance ranking as a class variable (attribute):\n",
    "            self.feature_importance_df = feature_importance_df\n",
    "\n",
    "            features = feature_importance_df['predictive_features']\n",
    "\n",
    "            if (model_class == 'linear'):\n",
    "                importances = feature_importance_df['abs_reg_coefficients']\n",
    "\n",
    "            elif (model_class == 'tree'):\n",
    "                importances = feature_importance_df['abs_feature_importances']\n",
    "\n",
    "            data_label = \"feature_importance_ranking\"\n",
    "\n",
    "            # Normalize the importances by dividing all of them by the maximum:\n",
    "            max_importance = max(importances)\n",
    "            importances = importances/max_importance\n",
    "\n",
    "            # Now, limit to 10 values to plot:\n",
    "            importances = importances[:10]\n",
    "            features = features[:10]\n",
    "\n",
    "            # Now, plot the bar chart\n",
    "            print(\"\\n\")\n",
    "            print(\"Feature relative importance bar chart:\\n\")\n",
    "            # Now the data is prepared and we only have to plot \n",
    "            # categories, responses, and cum_pct:\n",
    "\n",
    "            # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "            # so that the bars do not completely block other views.\n",
    "            OPACITY = 0.95\n",
    "\n",
    "            # Set labels and titles for the case they are None\n",
    "            if (plot_title is None):\n",
    "\n",
    "                plot_title = \"feature_importance_bar_chart\"\n",
    "\n",
    "            if (horizontal_axis_title is None):\n",
    "\n",
    "                horizontal_axis_title = \"feature\"\n",
    "\n",
    "            if (vertical_axis_title is None):\n",
    "                # Notice that response_var_name already has the suffix indicating the\n",
    "                # aggregation function\n",
    "                vertical_axis_title = \"importance_score\"\n",
    "\n",
    "            fig, ax = plt.subplots(figsize = (12, 8))\n",
    "            # Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "\n",
    "            #ROTATE X AXIS IN XX DEGREES\n",
    "            plt.xticks(rotation = x_axis_rotation)\n",
    "            # XX = 70 DEGREES x_axis (Default)\n",
    "            #ROTATE Y AXIS IN XX DEGREES:\n",
    "            plt.yticks(rotation = y_axis_rotation)\n",
    "            # XX = 0 DEGREES y_axis (Default)\n",
    "\n",
    "            plt.title(plot_title)\n",
    "\n",
    "            if (orientation == 'horizontal'):\n",
    "\n",
    "                # invert the axes in relation to the default (vertical, below)\n",
    "                ax.set_ylabel(horizontal_axis_title)\n",
    "                ax.set_xlabel(vertical_axis_title, color = 'darkblue')\n",
    "\n",
    "                # Horizontal bars used - barh method (bar horizontal):\n",
    "                # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.barh.html\n",
    "                # Now, the categorical variables stored in series categories must be\n",
    "                # positioned as the vertical axis Y, whereas the correspondent responses\n",
    "                # must be in the horizontal axis X.\n",
    "                ax.barh(features, importances, color = 'darkblue', alpha = OPACITY, label = data_label)\n",
    "                #.barh(y, x, ...)\n",
    "\n",
    "            else: \n",
    "\n",
    "                ax.set_xlabel(horizontal_axis_title)\n",
    "                ax.set_ylabel(vertical_axis_title, color = 'darkblue')\n",
    "                # If None or an invalid orientation was used, set it as vertical\n",
    "                # Use Matplotlib standard bar method (vertical bar):\n",
    "                # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html#matplotlib.pyplot.bar\n",
    "\n",
    "                # In this standard case, the categorical variables (categories) are positioned\n",
    "                # as X, and the responses as Y:\n",
    "                ax.bar(features, importances, color = 'darkblue', alpha = OPACITY, label = data_label)\n",
    "                #.bar(x, y, ...)\n",
    "\n",
    "            ax.legend()\n",
    "            ax.grid(grid)\n",
    "\n",
    "            # Notice that the .plot method is used for generating the plot for both orientations.\n",
    "            # It is different from .bar and .barh, which specify the orientation of a bar; or\n",
    "            # .hline (creation of an horizontal constant line); or .vline (creation of a vertical\n",
    "            # constant line).\n",
    "\n",
    "            # Now the parameters specific to the configurations are finished, so we can go back\n",
    "            # to the general code:\n",
    "\n",
    "            if (export_png == True):\n",
    "                # Image will be exported\n",
    "                import os\n",
    "\n",
    "                #check if the user defined a directory path. If not, set as the default root path:\n",
    "                if (directory_to_save is None):\n",
    "                    #set as the default\n",
    "                    directory_to_save = \"\"\n",
    "\n",
    "                #check if the user defined a file name. If not, set as the default name for this\n",
    "                # function.\n",
    "                if (file_name is None):\n",
    "                    #set as the default\n",
    "                    file_name = \"feature_importance_ranking\"\n",
    "\n",
    "                #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "                # resolution.\n",
    "                if (png_resolution_dpi is None):\n",
    "                    #set as 330 dpi\n",
    "                    png_resolution_dpi = 330\n",
    "\n",
    "                #Get the new_file_path\n",
    "                new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "                #Export the file to this new path:\n",
    "                # The extension will be automatically added by the savefig method:\n",
    "                plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "                #quality could be set from 1 to 100, where 100 is the best quality\n",
    "                #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "                #transparent = True or False\n",
    "                # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "                print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "            #fig.tight_layout()\n",
    "\n",
    "            ## Show an image read from an image file:\n",
    "            ## import matplotlib.image as pltimg\n",
    "            ## img=pltimg.imread('mydecisiontree.png')\n",
    "            ## imgplot = plt.imshow(img)\n",
    "            ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "            ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "            ##  '03_05_END.ipynb'\n",
    "            plt.show()\n",
    "        \n",
    "        except:\n",
    "            print(\"Unable to generate plot correlating feature to its importance.\\n\")\n",
    "            self.feature_importance_df = pd.DataFrame() # empty dataframe\n",
    "        \n",
    "        if (model_type == 'classification'):\n",
    "            \n",
    "            # Print meaning of classification metrics\n",
    "            print(\"Metrics definitions:\\n\")\n",
    "            print(\"True Positive (TP): the model correctly predicts a positive class output, i.e., it correctly predicts that the classified element belongs to that class (in binary classification, like in logistic regression, the model predicts the output 1 and the real output is also 1).\")\n",
    "            print(\"\\n\")\n",
    "            print(\"True Negative (TN): the model correctly predicts a negative class output, i.e., it correctly predicts that the classified element do not belong to that class (in binary classification, the model predicts the output 0 and the real output is also 0).\")\n",
    "            print(\"\\n\")\n",
    "            print(\"False Positive (FP, type 1 error): the model incorrectly predicts a positive class for a negative class-element, i.e., it predicts that the element belongs to that class, but it actually does not (in binary classification, the model predicts an output 1, but the correct output is 0).\")\n",
    "            print(\"\\n\")\n",
    "            print(\"False Negative (FN, type 2 error): the model incorrectly predicts a negative class for a positive class-element, i.e., it predicts that the element does not belong to that class, but it actually does (in binary classification, the model predicts an output 0, but the correct output is 1).\")\n",
    "            print(\"\\n\")\n",
    "            print(\"Naturally, the total number (TOTAL) of classifications is the sum of total correct predictions with total incorrect predictions, i.e., TOTAL = TP + TN + FP + FN\")\n",
    "            print(\"\\n\") # line break\n",
    "            print(\"Accuracy: relation between the total number of correct classifications and the total number of classifications performed, i.e., Accuracy = (TP + TN)/(TOTAL)\")\n",
    "            print(\"\\n\")\n",
    "            print(\"Precision: it is referrent to the attempt of answering the question: \\'What is the proportion of positive identifications that were actually correct?\\'.\")\n",
    "            print(\"In other words, Precision is the relation between the number of true positives and the total of positively-labelled classifications (true and false positives), i.e., Precision = (TP)/(TP + FP)\")\n",
    "            print(\"\\n\")\n",
    "            print(\"Recall: it is referrent to the attempt of answering the question: \\'What is the proportion of elements from positive class that were correctly classified?\\'.\")\n",
    "            print(\"In other words, Recall is the relation between the number of true positives and the total of elements from the positive class (true positives and false negatives), i.e., Recall = (TP)/(TP + FN)\")\n",
    "            print(\"\\n\")\n",
    "            print(\"F1: is the ROC-AUC score. In a generic classification problem, this metric is representative of the capability of the model in distinguishing classes.\")     \n",
    "            print(\"F1 =2/((1/Precision)+(1/Recall)) = (2*(Precision)*(Recall))/(Precision + Recall)\")\n",
    "            print(\"\\n\") # line break\n",
    "            # Check:\n",
    "            # https://towardsdatascience.com/how-to-evaluate-your-machine-learning-models-with-python-code-5f8d2d8d945b\n",
    "                  \n",
    "            print(\"Confusion Matrix Interpretation:\\n\")\n",
    "            print(\"The confusion matrix is a table commonly used for describing the performance of a classification model (a classifier). It visually compares the model outputs with the correct data labels.\")\n",
    "            print(\"The matrix is divided into several sectors. For a binary classifier, it is divided into 4 quadrants.\")\n",
    "            print(\"\\n\")\n",
    "            print(\"Each sector represents a given classification: in the vertical (Y) axis, the real observed labels are shown; whereas the predicted classes (model's outputs) are represented in the horizontal (X) axis.\")\n",
    "            print(\"Then, for each possible class, the following situations may happen: 1. The model predicted that the element belong to a given class, but it does not (incorrect prediction); or 2. The model predicted that the element belong to a given class, and it does (correct prediction).\")\n",
    "            print(\"If the output predicted y_pred (X-coordinate in the confusion matrix = y_pred) is the real label, then the Y-coordinate in the confusion matrix is also y_pred. For an element to have X and Y coordinates equal, it must be positioned on the principal diagonal of the matrix.\")\n",
    "            print(\"\\n\") #line break to highlight the next sentence\n",
    "            print(\"So, we conclude that all the correct predictions of the model are positioned on the main or principal diagonal of the confusion matrix.\")\n",
    "            print(\"\\n\") # line break\n",
    "            print(\"We also may conclude that an increase on model general accuracy is observed as an increase on the values shown in the main diagonal of the confusion matrix.\")\n",
    "            print(\"\\n\")\n",
    "            print(\"Notice that this interpretation takes in account a matrix organized starting from the bottom to the top of Y axis (i.e., lower classes on the origin), and from the left to the right of the X-axis, with lower classes closer to the origin. If the order was the opposite, then the secondary diagonal that would contain the correct predictions.\")\n",
    "            print(\"If we have N possible classifications, than we have N values on axis X, and N values in axis Y. So, we have N x N = N2 (N squared) sectors (values) in the confusion matrix.\\n\")\n",
    "            print(\"Confusion matrix for a binary classifier:\\n\")\n",
    "            print(\"For a binary classifier, we have to possible outputs: 0 (the origin of the matrix) and 1. In the vertical axis, 1 is the topper value; in the horizontal axis, 1 is the value on the extreme right (the positions more distant from the origin).\")\n",
    "            print(\"Since N = 2, we have 2 x 2 = 4 quadrants (sectors or values).Starting from the origin, clockwise, we have 4 situations:\")\n",
    "            print(\"\\n\")\n",
    "            print(\"Situation 1: X = 0 and Y = 0 - the model correctly predicted a negative output (it is a true negative prediction, TN).\")\n",
    "            print(\"Situation 2: X = 0 and Y = 1 - the model predicted a negative output for a positive class element (it is a false negative, FN).\")\n",
    "            print(\"Situation 3: X = 1 and Y = 1 - the model correctly predicted a positive class (TP).\")\n",
    "            print(\"Situation 4: X = 1 and Y = 0 - the model predicted a positive output for a negative class element (FP)\\n\")\n",
    "            print(\"Each position of the confusion matrix represents the total of elements in each of the possible situations. Then, the sum of all values must be equal to the total of elements classified, and the relation between the sum of the main diagonal and the total of elements must be the accuracy.\")\n",
    "            print(\"So, use the confusion matrix to analyze the performance of the model in classifying each class, separately, and to observe the false negatives and false positives. Also, the confusion matrix will reveal if the classes are balanced, or ir a given class has much more elements than the other, what could impart the capability on differentiating the classes.\")\n",
    "            print(\"\\n\")\n",
    "            print(\"For some models, the proportion of false positives may be very different from the proportion of false negatives. It is not a problem, though, and depend on the application of the classifier.\")\n",
    "            print(\"It is an important situation that would be masked by the general metrics that take in account all the predictions, without seggregating them through the classes.\")\n",
    "            print(\"\\n\")\n",
    "            print(\"A classical example: suppose the classifier is used for predicting cancer. In this case, the model must have a proportion of false negatives much inferior than the proportion of false positives. That is because the risk associated to a false negative output is much higher.\")\n",
    "            print(\"A person who is incorrectly classified as having cancer will perform several more detailed exams to confirm the diagnosis, so the false positive may get detected without a great problem (in fact, the patient will probably feel good about it and keep taking care of the health). But a person incorrectly classified as not having cancer (when he has cancer) may feel comfortable, not taking care of his health and not making other exams (because he trusts the algorithm). Then, it may be too late when he founds out that was a false negative.\")\n",
    "            print(\"\\n\") # line break\n",
    "\n",
    "            # AUC = Area under the curve\n",
    "            print(\"AUC (Area under the curve) of the ROC (Receiver operating characteristic; default) or PR (Precision Recall) curves are quality measures of binary classifiers.\\n\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def plot_training_history (self, metrics_name = 'mse', x_axis_rotation = 0, y_axis_rotation = 0, grid = True, horizontal_axis_title = None, metrics_vertical_axis_title = None, loss_vertical_axis_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        # metrics_name = 'mse', 'sparse_categorical_crossentropy', etc\n",
    "\n",
    "        history = self.history\n",
    "        # Set the validation metrics name.\n",
    "        # to access the validation metrics, simply put a 'val_' prefix:\n",
    "        val_metrics_name = 'val_' + metrics_name\n",
    "\n",
    "        # Retrieve data from the history dictionary:\n",
    "        # Access values for training sample:\n",
    "        train_metrics = history.history[metrics_name]\n",
    "        validation_metrics = history.history[val_metrics_name]\n",
    "        \n",
    "        # Try accessing data from validation sample (may not be present):\n",
    "        has_validation = False\n",
    "        # Maps if there are validation data: this variable is updated when values are present.\n",
    "        \n",
    "        try:\n",
    "            train_loss = history.history['loss']\n",
    "            validation_loss = history.history['val_loss']\n",
    "            has_validation = True\n",
    "        \n",
    "        except: # simply pass\n",
    "            pass\n",
    "        \n",
    "        # Notice that history is not exactly a dictionary: it is an object with attribute history.\n",
    "        # This attribute is where the dictionary is actually stored.\n",
    "        \n",
    "        # Access the list of epochs, stored as the epoch attribute from the history object\n",
    "        list_of_epochs = history.epoch\n",
    "        # epochs start from zero\n",
    "        \n",
    "        if (horizontal_axis_title is None):\n",
    "            horizontal_axis_title = \"epoch\"\n",
    "        \n",
    "        if (metrics_vertical_axis_title is None):\n",
    "            metrics_vertical_axis_title = \"metrics_value\"\n",
    "        \n",
    "        if (loss_vertical_axis_title is None):\n",
    "            loss_vertical_axis_title = \"loss_value\"\n",
    "\n",
    "        # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "        # so that the bars do not completely block other views.\n",
    "        OPACITY = 0.95\n",
    "            \n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        fig = plt.figure(figsize = (12, 8))\n",
    "        ax1 = fig.add_subplot(211)\n",
    "        #ax1.set_xlabel(\"Lags\")\n",
    "        ax1.set_ylabel(metrics_vertical_axis_title)\n",
    "        \n",
    "        # Scatter plot of time series:\n",
    "        ax1.plot(list_of_epochs, train_metrics, linestyle = \"-\", marker = '', color = 'darkblue', alpha = OPACITY, label = \"train_metrics\")\n",
    "        if (has_validation):\n",
    "            # If present, plot validation data:\n",
    "            ax1.plot(list_of_epochs, validation_metrics, linestyle = \"-\", marker = '', color = 'crimson', alpha = OPACITY, label = \"validation_metrics\")\n",
    "        # Axes.plot documentation:\n",
    "        # https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.plot.html?msclkid=42bc92c1d13511eca8634a2c93ab89b5\n",
    "        \n",
    "        #ROTATE X AXIS IN XX DEGREES\n",
    "        plt.xticks(rotation = x_axis_rotation)\n",
    "        # XX = 0 DEGREES x_axis (Default)\n",
    "        #ROTATE Y AXIS IN XX DEGREES:\n",
    "        plt.yticks(rotation = y_axis_rotation)\n",
    "        # XX = 0 DEGREES y_axis (Default)\n",
    "        \n",
    "        ax1.grid(grid)\n",
    "        ax1.legend(loc = \"upper right\")\n",
    "        \n",
    "        ax2 = fig.add_subplot(212)\n",
    "        ax2.plot(list_of_epochs, train_loss, linestyle = \"-\", marker = '', color = 'darkgreen', alpha = OPACITY, label = \"train_loss\")\n",
    "        \n",
    "        if (has_validation):\n",
    "            # If present, plot validation data:\n",
    "            ax2.plot(list_of_epochs, validation_loss, linestyle = \"-\", marker = '', color = 'fuchsia', alpha = OPACITY, label = \"validation_loss\")\n",
    "        \n",
    "        ax2.set_xlabel(horizontal_axis_title)\n",
    "        ax2.set_ylabel(loss_vertical_axis_title)\n",
    "        \n",
    "        ax2.grid(grid)\n",
    "        ax2.legend(loc = \"upper right\")\n",
    "\n",
    "        #ROTATE X AXIS IN XX DEGREES\n",
    "        plt.xticks(rotation = x_axis_rotation)\n",
    "        # XX = 0 DEGREES x_axis (Default)\n",
    "        #ROTATE Y AXIS IN XX DEGREES:\n",
    "        plt.yticks(rotation = y_axis_rotation)\n",
    "        # XX = 0 DEGREES y_axis (Default)\n",
    "        \n",
    "        if (export_png == True):\n",
    "            # Image will be exported\n",
    "            import os\n",
    "\n",
    "            #check if the user defined a directory path. If not, set as the default root path:\n",
    "            if (directory_to_save is None):\n",
    "                #set as the default\n",
    "                directory_to_save = \"\"\n",
    "\n",
    "            #check if the user defined a file name. If not, set as the default name for this\n",
    "            # function.\n",
    "            if (file_name is None):\n",
    "                #set as the default\n",
    "                file_name = \"history_loss_and_metrics\"\n",
    "\n",
    "            #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "            # resolution.\n",
    "            if (png_resolution_dpi is None):\n",
    "                #set as 330 dpi\n",
    "                png_resolution_dpi = 330\n",
    "\n",
    "            #Get the new_file_path\n",
    "            new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "            #Export the file to this new path:\n",
    "            # The extension will be automatically added by the savefig method:\n",
    "            plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "            #quality could be set from 1 to 100, where 100 is the best quality\n",
    "            #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "            #transparent = True or False\n",
    "            # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "            print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        #plt.figure(figsize = (12, 8))\n",
    "        #fig.tight_layout()\n",
    "\n",
    "        ## Show an image read from an image file:\n",
    "        ## import matplotlib.image as pltimg\n",
    "        ## img=pltimg.imread('mydecisiontree.png')\n",
    "        ## imgplot = plt.imshow(img)\n",
    "        ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "        ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "        ##  '03_05_END.ipynb'\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_history_multiresponses (self, x_axis_rotation = 0, y_axis_rotation = 0, grid = True, horizontal_axis_title = None, metrics_vertical_axis_title = None, loss_vertical_axis_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        # metrics_name = 'mse', 'sparse_categorical_crossentropy', etc\n",
    "\n",
    "        history = self.history\n",
    "        \n",
    "        \"\"\"\n",
    "        history object has a format like (2 responses, 1 epoch, metrics = 'mse'), when we apply the\n",
    "        .__dict__ method:\n",
    "\n",
    "        'history': {'loss': [2.977898597717285],\n",
    "          'response1_loss': [0.052497703582048416],\n",
    "          'response2_loss': [2.457101345062256],\n",
    "          'response1_mse': [0.052497703582048416],\n",
    "          'response2_mse': [2.457101345062256],\n",
    "          'val_loss': [2.007075071334839],\n",
    "          'val_response1_loss': [0.02299179881811142],\n",
    "          'val_response2_loss': [1.8660322427749634],\n",
    "          'val_response1_mse': [0.02299179881811142],\n",
    "          'val_response2_mse': [1.8660322427749634],\n",
    "         'params': {'verbose': 1, 'epochs': 1, 'steps': 1},\n",
    "         'epoch': [0]}\n",
    "\n",
    "         Here, the history attribute stores a dictionary with the training history, whereas the epoch\n",
    "         attribute stores the list of epochs, starting from zero.\n",
    "         - Keys 'loss' and 'val_loss' store the general losses for the whole network.\n",
    "         - Other keys store the metrics for the responses.\n",
    "\n",
    "        \"\"\"\n",
    "        # Access the list of epochs, stored as the epoch attribute from the history object\n",
    "        list_of_epochs = history.epoch\n",
    "        # epochs start from zero\n",
    "        \n",
    "        # access history attribute to retrieve the series of metrics.\n",
    "        history_dict = history.history\n",
    "        \n",
    "        metrics_dict = {}\n",
    "        #Get the global one:\n",
    "        nested_dict = {'loss': history_dict['loss']}\n",
    "        \n",
    "        \n",
    "        # Try accessing validation information\n",
    "        has_validation = False\n",
    "        # Maps if there are validation data: this variable is updated when values are present.\n",
    "        \n",
    "        try:\n",
    "            nested_dict['val_loss'] = history_dict['val_loss']\n",
    "            has_validation = True\n",
    "        \n",
    "        except: # simply pass\n",
    "            pass\n",
    "        \n",
    "        nested_dict['response'] = 'global'\n",
    "        \n",
    "        metrics_dict['global'] = nested_dict\n",
    "        \n",
    "        # Let's find out the metrics name\n",
    "        for key in history_dict.keys():\n",
    "            \n",
    "            if ((key != 'loss') & (key != 'val_loss')):\n",
    "                # These are the globals, which were already saved\n",
    "                \n",
    "                # Split the string in the underscores: 'response2_loss'\n",
    "                # will generate a list of two elements ['response2', 'loss']. We pick the last element\n",
    "                # with index -1.\n",
    "                # Attention: guarantee that the key was read as a string, not as a number\n",
    "                list_of_substrings = str(key).split(\"_\")\n",
    "                first_portion = list_of_substrings[0]\n",
    "                last_portion = list_of_substrings[-1]\n",
    "\n",
    "                # Get the total of characters of the last portion\n",
    "                total_characters = len(last_portion)\n",
    "                # pick the string eliminating its last portion\n",
    "                response = key[:(-1*(total_characters + 1))]\n",
    "                # if we had a string like 'response1_loss', now response = 'response1_' if we did\n",
    "                # not sum another character. By summing 1, we eliminate the last underscore\n",
    "                \n",
    "                if (first_portion == 'val'):\n",
    "                    # In this case, the response variable by now stores val_response1, i.e., the first\n",
    "                    # we should eliminate characters from positions 0 to 3, starting the string from\n",
    "                    # character 4:\n",
    "                    response = response[4:]\n",
    "                \n",
    "                # try accessing the nested dict:\n",
    "                try:\n",
    "                    nested_dict = metrics_dict[response]\n",
    "\n",
    "                except:\n",
    "                    # There is no nested_dict yet, so create one:\n",
    "                    nested_dict = {'response': response}\n",
    "                \n",
    "                if (last_portion != 'loss'):\n",
    "                    \n",
    "                    if (first_portion != 'val'):\n",
    "                        # Insert the metrics name only once:\n",
    "                        nested_dict['metrics'] = last_portion\n",
    "                        nested_dict[last_portion] = history_dict[key]\n",
    "                    \n",
    "                    else:\n",
    "                        nested_dict[(\"val_\" + last_portion)] = history_dict[key]\n",
    "                \n",
    "                else:\n",
    "                    if (first_portion != 'val'):\n",
    "                        # Insert the metrics name only once:\n",
    "                        nested_dict['loss'] = history_dict[key]\n",
    "                    \n",
    "                    else:\n",
    "                        nested_dict[\"val_loss\"] = history_dict[key]\n",
    "                \n",
    "                #Update nested dictionary\n",
    "                metrics_dict[response] = nested_dict\n",
    "        \n",
    "        # metrics_dict keys: responses without the 'val_' and '_loss' and '_' + metrics. Stores\n",
    "        # the nested dictionary.\n",
    "        # nested_dict keys: 'response': name of the response variable;\n",
    "        # 'metrics': name of the metrics; metrics (key with name that varies):\n",
    "        # series of the metrics registered during training; \"val_\" + metrics (key with name that \n",
    "        # varies): series of the metrics registered during training for validation data; 'loss':\n",
    "        # series of losses obtained during training; 'val_loss': losses for validation data.\n",
    "        \n",
    "        # Loop through the responses and nested dictionaries in the metrics_dict:\n",
    "        for response, nested_dict in metrics_dict.items():\n",
    "            \n",
    "            try:\n",
    "                metrics_name = nested_dict['metrics']\n",
    "\n",
    "                # Set the validation metrics name.\n",
    "                # to access the validation metrics, simply put a 'val_' prefix:\n",
    "                val_metrics_name = 'val_' + metrics_name\n",
    "            \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                train_loss = nested_dict['loss']\n",
    "                \n",
    "                if (has_validation):\n",
    "                    validation_loss = nested_dict['val_loss']\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                train_metrics = nested_dict[metrics_name]\n",
    "                \n",
    "                if (has_validation):\n",
    "                    validation_metrics = nested_dict[val_metrics_name]\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        \n",
    "            if (horizontal_axis_title is None):\n",
    "                horizontal_axis_title = \"epoch\"\n",
    "\n",
    "            if (metrics_vertical_axis_title is None):\n",
    "                metrics_vertical_axis_title = \"metrics_value\"\n",
    "\n",
    "            if (loss_vertical_axis_title is None):\n",
    "                loss_vertical_axis_title = \"loss_value\"\n",
    "\n",
    "            # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "            # so that the bars do not completely block other views.\n",
    "            OPACITY = 0.95\n",
    "\n",
    "            #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "            fig = plt.figure(figsize = (12, 8))\n",
    "            try:\n",
    "                ax1 = fig.add_subplot(211)\n",
    "                #ax1.set_xlabel(\"Lags\")\n",
    "                ax1.set_ylabel(metrics_vertical_axis_title)\n",
    "\n",
    "                # Scatter plot of time series:\n",
    "                ax1.plot(list_of_epochs, train_metrics, linestyle = \"-\", marker = '', color = 'darkblue', alpha = OPACITY, label = (\"train_metrics_\" + response[:10]))\n",
    "                if (has_validation):\n",
    "                    # If present, plot validation data:\n",
    "                    ax1.plot(list_of_epochs, validation_metrics, linestyle = \"-\", marker = '', color = 'crimson', alpha = OPACITY, label = (\"validation_metrics_\" + response[:10]))\n",
    "                # Axes.plot documentation:\n",
    "                # https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.plot.html?msclkid=42bc92c1d13511eca8634a2c93ab89b5\n",
    "\n",
    "                #ROTATE X AXIS IN XX DEGREES\n",
    "                plt.xticks(rotation = x_axis_rotation)\n",
    "                # XX = 0 DEGREES x_axis (Default)\n",
    "                #ROTATE Y AXIS IN XX DEGREES:\n",
    "                plt.yticks(rotation = y_axis_rotation)\n",
    "                # XX = 0 DEGREES y_axis (Default)\n",
    "\n",
    "                ax1.grid(grid)\n",
    "                ax1.legend(loc = \"upper right\")\n",
    "            \n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                ax2 = fig.add_subplot(212)\n",
    "                ax2.plot(list_of_epochs, train_loss, linestyle = \"-\", marker = '', color = 'darkgreen', alpha = OPACITY, label = (\"train_loss_\" + response[:10]))\n",
    "\n",
    "                if (has_validation):\n",
    "                    # If present, plot validation data:\n",
    "                    ax2.plot(list_of_epochs, validation_loss, linestyle = \"-\", marker = '', color = 'fuchsia', alpha = OPACITY, label = (\"validation_loss_\" + response[:10]))\n",
    "\n",
    "                ax2.set_xlabel(horizontal_axis_title)\n",
    "                ax2.set_ylabel(loss_vertical_axis_title)\n",
    "\n",
    "                ax2.grid(grid)\n",
    "                ax2.legend(loc = \"upper right\")\n",
    "\n",
    "                #ROTATE X AXIS IN XX DEGREES\n",
    "                plt.xticks(rotation = x_axis_rotation)\n",
    "                # XX = 0 DEGREES x_axis (Default)\n",
    "                #ROTATE Y AXIS IN XX DEGREES:\n",
    "                plt.yticks(rotation = y_axis_rotation)\n",
    "                # XX = 0 DEGREES y_axis (Default)\n",
    "            \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if (export_png == True):\n",
    "                # Image will be exported\n",
    "                import os\n",
    "\n",
    "                #check if the user defined a directory path. If not, set as the default root path:\n",
    "                if (directory_to_save is None):\n",
    "                    #set as the default\n",
    "                    directory_to_save = \"\"\n",
    "\n",
    "                #check if the user defined a file name. If not, set as the default name for this\n",
    "                # function.\n",
    "                if (file_name is None):\n",
    "                    #set as the default\n",
    "                    file_name = (\"history_\" + response[:10])\n",
    "\n",
    "                #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "                # resolution.\n",
    "                if (png_resolution_dpi is None):\n",
    "                    #set as 330 dpi\n",
    "                    png_resolution_dpi = 330\n",
    "\n",
    "                #Get the new_file_path\n",
    "                new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "                #Export the file to this new path:\n",
    "                # The extension will be automatically added by the savefig method:\n",
    "                plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "                #quality could be set from 1 to 100, where 100 is the best quality\n",
    "                #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "                #transparent = True or False\n",
    "                # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "                print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "            #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "            #plt.figure(figsize = (12, 8))\n",
    "            #fig.tight_layout()\n",
    "\n",
    "            ## Show an image read from an image file:\n",
    "            ## import matplotlib.image as pltimg\n",
    "            ## img=pltimg.imread('mydecisiontree.png')\n",
    "            ## imgplot = plt.imshow(img)\n",
    "            ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "            ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "            ##  '03_05_END.ipynb'\n",
    "            plt.show()\n",
    "            print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    def model_metrics_multiresponses (self, output_dictionary, show_confusion_matrix_values = True, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "        \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        import tensorflow as tf\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/keras/metrics?authuser=1\n",
    "        from sklearn.metrics import classification_report, confusion_matrix, r2_score\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "        # Retrieve type of problem:\n",
    "        model_type = self.model_type\n",
    "        \n",
    "        # output_dictionary structure:\n",
    "        # {'response_variable': {\n",
    "        # 'type': 'regression', 'number_of_classes':}}\n",
    "\n",
    "        list_of_responses = list((output_dictionary).keys())\n",
    "        # Total of responses\n",
    "        total_of_responses = len(list_of_responses)\n",
    "\n",
    "        # Retrieve the tensors.\n",
    "        tensors_dict = {}\n",
    "        tensors_dict['training'] = {'actual': self.y_train, 'predictions': self.y_preds_for_train}\n",
    "        tensors_dict['testing'] = {'actual': self.y_test, 'predictions': self.y_preds_for_test}\n",
    "        tensors_dict['validation'] = {'actual': self.y_valid, 'predictions': self.y_preds_for_validation}\n",
    "\n",
    "        metrics_dict = {}\n",
    "\n",
    "        # Loop through the keys:\n",
    "        for key in tensors_dict.keys():\n",
    "          \n",
    "            # Retrieve the nested dictionary:\n",
    "            nested_dict = tensors_dict[key]\n",
    "            # Retrieve actual and predicted values:\n",
    "            y_true_tensor =  nested_dict['actual']\n",
    "            y_pred_tensor = nested_dict['predictions']\n",
    "            \n",
    "            y_true_array = np.array(y_true_tensor)\n",
    "            # Total of entries in the dataset:\n",
    "            # Get the total of values for the first response, by isolating the index 0 of 2nd dimension\n",
    "            total_data = len(y_true_array[:, 0])\n",
    "            \n",
    "            # Reshape y_pred so that it is in the same format as the y_true tensor\n",
    "            # The predictions may come in a different shape, depending on the algorithm that\n",
    "            # generates them.\n",
    "            y_pred_array = np.array(y_pred_tensor)\n",
    "            y_pred_array = y_pred_array.reshape(total_data, total_of_responses)\n",
    "            \n",
    "            # Check if there is no None value stored:\n",
    "            if ((y_true_array is not None) & (y_pred_array is not None)):\n",
    "\n",
    "                calculated_metrics = {}\n",
    "                print(f\"Metrics for {key}:\\n\")\n",
    "                \n",
    "                nested_metrics = {}\n",
    "                \n",
    "                for index, response in enumerate(list_of_responses):\n",
    "                    # enumerate will get tuples like (0, response1), (1, response2), etc\n",
    "                    print(f\"Evaluation of metrics for response variable '{response}':\\n\")\n",
    "\n",
    "                    type_of_problem = output_dictionary[response]['type']\n",
    "                    # select only the arrays in position 'index' of the tensors y_true_tensor\n",
    "                    # and y_pred_tensor:\n",
    "                    y_true = y_true_array[:, index]\n",
    "                    y_pred = y_pred_array[:, index]\n",
    "                    \n",
    "                \n",
    "                    # Regression metrics:\n",
    "                    if (model_type == 'regression'):\n",
    "\n",
    "                        mse = tf.keras.metrics.mean_squared_error(y_true, y_pred)\n",
    "                        #https://www.tensorflow.org/api_docs/python/tf/keras/metrics/mean_squared_error?authuser=1\n",
    "                        # The function returns a NumPy array containing a single element. Extract it as\n",
    "                        # variable:\n",
    "                        # Then, some situations may return numpy arrays instead of scalars. We can convert\n",
    "                        # to a scalar by selecting only the first and single element from the array.\n",
    "\n",
    "                        try:\n",
    "                            mse = mse[0]\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        # Print in scientific notation:\n",
    "                        try:\n",
    "                            print(f\"Mean squared error (MSE) = {mse:e}\")\n",
    "                        except:\n",
    "                            print(f\"Mean squared error (MSE) = {mse}\")\n",
    "                        # Add to calculated metrics:\n",
    "                        calculated_metrics['mse'] = mse\n",
    "\n",
    "                        # rmse is not available as function, only class. Use numpy method to convert to value\n",
    "                        # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RootMeanSquaredError?authuser=1\n",
    "                        # Create the object:\n",
    "                        rmse = tf.keras.metrics.RootMeanSquaredError()\n",
    "                        # Update its state:\n",
    "                        rmse = rmse.update_state(y_true, y_pred)\n",
    "                        # Use the numpy method to retrieve only the value:\n",
    "                        rmse = rmse.numpy()\n",
    "                        # Here, numpy method already returns a scalar\n",
    "                        # Print in scientific notation:\n",
    "\n",
    "                        try:\n",
    "                            rmse = rmse[0]\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        try:\n",
    "                            print(f\"Root mean squared error (RMSE) = {rmse:e}\")\n",
    "                        except:\n",
    "                            print(f\"Root mean squared error (RMSE) = {rmse}\")\n",
    "                        # Add to calculated metrics:\n",
    "                        calculated_metrics['rmse'] = rmse\n",
    "\n",
    "                        # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/mean_absolute_error?authuser=1\n",
    "                        mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
    "                        # The function returns a NumPy array containing a single element. Extract it as\n",
    "                        # variable:\n",
    "\n",
    "                        try:\n",
    "                            mae = mae[0]\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        # Print in scientific notation:\n",
    "                        try:\n",
    "                            print(f\"Mean absolute error (MAE) = {mae:e}\")\n",
    "                        except:\n",
    "                            print(f\"Mean absolute error (MAE) = {mae}\")\n",
    "                        # Add to calculated metrics:\n",
    "                        calculated_metrics['mae'] = mae\n",
    "\n",
    "                        # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/mean_absolute_percentage_error?authuser=1\n",
    "                        mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "                        # The function returns a NumPy array containing a single element. Extract it as\n",
    "                        # variable:\n",
    "\n",
    "                        try:\n",
    "                            mape = mape[0]\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        # Print in scientific notation:\n",
    "                        try:\n",
    "                            print(f\"Mean absolute percentage error (MAPE) = {mape:e}\")\n",
    "                        except:\n",
    "                            print(f\"Mean absolute percentage error (MAPE) = {mape}\")\n",
    "                        # Add to calculated metrics:\n",
    "                        calculated_metrics['mape'] = mape\n",
    "\n",
    "                        try:\n",
    "                            import tensorflow_addons as tfa\n",
    "                            # https://www.tensorflow.org/addons\n",
    "                            # R2 and R2-adj are available only as tfa object:\n",
    "                            # https://www.tensorflow.org/addons/api_docs/python/tfa/metrics/RSquare\n",
    "                            # Create the object:\n",
    "                            r2 = tfa.metrics.RSquare()\n",
    "                            # Update its state:\n",
    "                            # tfa method returns None, so we must only call the method:\n",
    "                            r2.update_state(y_true, y_pred)\n",
    "                            # Use the numpy method to retrieve only the value:\n",
    "                            r2 = r2.result().numpy() # already a scalar\n",
    "                            # for this tfa metrics, the methods result and numpy must be chained\n",
    "                            # otherwise, an error will be raised.\n",
    "\n",
    "                            try:\n",
    "                                r2 = r2[0]\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                            try:\n",
    "                                print(f\"Coefficient of linear correlation R² = {r2:e}\")\n",
    "                            except:\n",
    "                                print(f\"Coefficient of linear correlation R² = {r2}\")\n",
    "                            # Add to calculated metrics:\n",
    "                            calculated_metrics['r_squared'] = r2\n",
    "\n",
    "                        except:\n",
    "                            r2 = r2_score(y_true, y_pred)\n",
    "                            # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html\n",
    "                            try:\n",
    "                                r2 = r2[0]\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "\n",
    "                            try:\n",
    "                                print(f\"Coefficient of linear correlation R² = {r2:e}\")\n",
    "                            except:\n",
    "                                print(f\"Coefficient of linear correlation R² = {r2}\")\n",
    "\n",
    "                            # Add to calculated metrics:\n",
    "                            calculated_metrics['r_squared'] = r2\n",
    "\n",
    "                        try:\n",
    "                            # Try to calculate the adjusted R² by accessing the number of predictors:\n",
    "                            # This number may not be present.\n",
    "                            total_predictors = self.total_predictors\n",
    "                            # Create the object:\n",
    "                            r2_adj = tfa.metrics.RSquare(num_regressors = total_predictors)\n",
    "                            # Update its state. Again, method returns None:\n",
    "                            r2_adj.update_state(y_true, y_pred)\n",
    "                            # Use the numpy method to retrieve only the value:\n",
    "                            r2_adj = r2_adj.result().numpy() # scalar\n",
    "                            # Again, the methods result and numpy must be chained\n",
    "\n",
    "                            try:\n",
    "                                r2_adj = r2_adj[0]\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                            try:\n",
    "                                print(f\"Adjusted coefficient of correlation R²-adj = {r2_adj:e}\")\n",
    "                            except:\n",
    "                                print(f\"Adjusted coefficient of correlation R²-adj = {r2_adj}\")\n",
    "                            # Add to calculated metrics:\n",
    "                            calculated_metrics['r_squared_adj'] = r2_adj\n",
    "\n",
    "                        except:\n",
    "                            # Manually correct R²:\n",
    "                            # n_size_train = number of sample size\n",
    "                            # k_model = number of independent variables of the defined model\n",
    "                            k_model = self.total_predictors\n",
    "                            #numer of rows\n",
    "                            n_size = len(y_true)\n",
    "                            r2_adj = 1 - (1 - r2)*(n_size - 1)/(n_size - k_model - 1)\n",
    "\n",
    "                            try:\n",
    "                                r2_adj = r2_adj[0]\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "\n",
    "                            try:\n",
    "                                print(f\"Adjusted coefficient of correlation R²-adj = {r2_adj:e}\")\n",
    "                            except:\n",
    "                                print(f\"Adjusted coefficient of correlation R²-adj = {r2_adj}\")\n",
    "\n",
    "                            # Add to calculated metrics:\n",
    "                            calculated_metrics['r_squared_adj'] = r2_adj\n",
    "\n",
    "                        print(\"\\n\")\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/AUC\n",
    "                        # Create the object:\n",
    "                        auc = tf.keras.metrics.AUC()\n",
    "                        # Update its state:\n",
    "                        auc.update_state(y_true, y_pred)\n",
    "                        # Use the numpy method to retrieve only the value:\n",
    "                        auc = auc.result().numpy() # scalar\n",
    "                        try:\n",
    "                            auc = auc[0]\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        try:\n",
    "                            print(f\"AUC = {auc:e}\")\n",
    "                        except:\n",
    "                            print(f\"AUC = {auc}\")\n",
    "                        # Add to calculated metrics:\n",
    "                        calculated_metrics['auc'] = auc\n",
    "\n",
    "                        # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy\n",
    "                        # Create the object:\n",
    "                        acc = tf.keras.metrics.Accuracy()\n",
    "                        # Update its state:\n",
    "                        acc.update_state(y_true, y_pred)\n",
    "                        # Use the numpy method to retrieve only the value:\n",
    "                        acc = acc.result().numpy() # scalar\n",
    "                        try:\n",
    "                            acc = acc[0]\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        try:\n",
    "                            print(f\"Accuracy = {acc:e}\")\n",
    "                        except:\n",
    "                            print(f\"Accuracy = {acc}\")\n",
    "                        # Add to calculated metrics:\n",
    "                        calculated_metrics['accuracy'] = acc\n",
    "\n",
    "                        # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision\n",
    "                        # Create the object:\n",
    "                        precision = tf.keras.metrics.Precision()\n",
    "                        # Update its state:\n",
    "                        precision.update_state(y_true, y_pred)\n",
    "                        # Use the numpy method to retrieve only the value:\n",
    "                        precision = precision.result().numpy() # scalar\n",
    "                        try:\n",
    "                            precision = precision[0]\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        try:\n",
    "                            print(f\"Precision = {precision:e}\")\n",
    "                        except:\n",
    "                            print(f\"Precision = {precision}\")\n",
    "                        # Add to calculated metrics:\n",
    "                        calculated_metrics['precision'] = precision\n",
    "\n",
    "                        # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall\n",
    "                        # Create the object:\n",
    "                        recall = tf.keras.metrics.Recall()\n",
    "                        # Update its state:\n",
    "                        recall.update_state(y_true, y_pred)\n",
    "                        # Use the numpy method to retrieve only the value:\n",
    "                        recall = recall.result().numpy() # scalar\n",
    "                        try:\n",
    "                            recall = recall[0]\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        try:\n",
    "                            print(f\"Recall = {recall:e}\")\n",
    "                        except:\n",
    "                            print(f\"Recall = {recall}\")\n",
    "                        # Add to calculated metrics:\n",
    "                        calculated_metrics['recall'] = recall\n",
    "\n",
    "                        # The method update_state returns None, so it must be called without and equality\n",
    "\n",
    "                        # Get the classification report:\n",
    "                        print(\"\\n\")\n",
    "                        print(\"Classification Report:\\n\")\n",
    "                        # Convert tensors to NumPy arrays\n",
    "                        report = classification_report (y_true, y_pred)\n",
    "                        print(report)\n",
    "                        # Add to calculated metrics:\n",
    "                        calculated_metrics['classification_report'] = report\n",
    "                        print(\"\\n\")\n",
    "\n",
    "                        # Get the confusion matrix:\n",
    "                        # Convert tensors to NumPy arrays\n",
    "                        matrix = confusion_matrix (y_true, y_pred)\n",
    "                        # Add to calculated metrics:\n",
    "                        calculated_metrics['confusion_matrix'] = report\n",
    "                        print(\"Confusion matrix:\\n\")\n",
    "\n",
    "                        fig, ax = plt.subplots(figsize = (12, 8))\n",
    "                        # possible color schemes (cmap) for the heat map: None, 'Blues_r',\n",
    "                        # \"YlGnBu\",\n",
    "                        # https://seaborn.pydata.org/generated/seaborn.heatmap.html?msclkid=73d24a00c1b211ec8aa1e7ab656e3ff4\n",
    "                        # http://seaborn.pydata.org/tutorial/color_palettes.html?msclkid=daa091f1c1b211ec8c74553348177b45\n",
    "                        ax = sns.heatmap(matrix, annot = show_confusion_matrix_values, fmt = \".0f\", linewidths = .5, square = True, cmap = 'Blues_r');\n",
    "                        #annot = True: shows the number corresponding to each square\n",
    "                        #annot = False: do not show the number\n",
    "                        plot_title = f\"Accuracy Score for {key} = {acc:.2f}\"\n",
    "                        ax.set_title(plot_title)\n",
    "                        ax.set_ylabel('Actual class')\n",
    "                        ax.set_xlabel('Predicted class')\n",
    "\n",
    "                        if (export_png == True):\n",
    "                            # Image will be exported\n",
    "                            import os\n",
    "\n",
    "                            #check if the user defined a directory path. If not, set as the default root path:\n",
    "                            if (directory_to_save is None):\n",
    "                                #set as the default\n",
    "                                directory_to_save = \"\"\n",
    "\n",
    "                            #check if the user defined a file name. If not, set as the default name for this\n",
    "                            # function.\n",
    "                            if (file_name is None):\n",
    "                                #set as the default\n",
    "                                file_name = \"confusion_matrix_\" + response\n",
    "\n",
    "                            else:\n",
    "                                # add the train suffix, to differentiate from the test matrix:\n",
    "                                file_name = file_name + \"_\" + key\n",
    "\n",
    "                            #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "                            # resolution.\n",
    "                            if (png_resolution_dpi is None):\n",
    "                                #set as 330 dpi\n",
    "                                png_resolution_dpi = 330\n",
    "\n",
    "                            #Get the new_file_path\n",
    "                            new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "                            #Export the file to this new path:\n",
    "                            # The extension will be automatically added by the savefig method:\n",
    "                            plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "                            #quality could be set from 1 to 100, where 100 is the best quality\n",
    "                            #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "                            #transparent = True or False\n",
    "                            # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "                            print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "                        #fig.tight_layout()\n",
    "\n",
    "                        ## Show an image read from an image file:\n",
    "                        ## import matplotlib.image as pltimg\n",
    "                        ## img=pltimg.imread('mydecisiontree.png')\n",
    "                        ## imgplot = plt.imshow(img)\n",
    "                        ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "                        ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "                        ##  '03_05_END.ipynb'\n",
    "                        plt.show()\n",
    "\n",
    "                        print(\"\\n\")\n",
    "                        # Now, add the metrics to the metrics_dict:\n",
    "                        \n",
    "                    nested_metrics[response] = calculated_metrics\n",
    "                        \n",
    "                metrics_dict[key] = nested_metrics\n",
    "          \n",
    "        # Now that we finished calculating metrics for all tensors, save the\n",
    "        # dictionary as a class variable (attribute) and return the object:\n",
    "        self.metrics_dict = metrics_dict\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def retrieve_classes_used_for_training (self):\n",
    "        \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "\n",
    "        # Retrieve attributes:\n",
    "        # Add the model:        \n",
    "        y_train = self.y_train\n",
    "        \n",
    "        # Use numpy.unique to collect the unique classes, in the\n",
    "        # order they appear:\n",
    "        # They are the unique values from series xgb_y_train\n",
    "        # https://numpy.org/doc/stable/reference/generated/numpy.unique.html?msclkid=ce35d85ec24511ec82dc9f13c97be8ce\n",
    "        list_of_classes = np.unique(y_train)\n",
    "        \n",
    "        # Now use the list attribute to convert the array to a list:\n",
    "        list_of_classes = list(list_of_classes)\n",
    "        number_of_classes = len(list_of_classes)\n",
    "        print(\"\\n\") # line break\n",
    "        print(f\"Number of different classes in the training set = {number_of_classes}\\n\")\n",
    "        print(\"List of classes:\\n\")\n",
    "        print(list_of_classes)\n",
    "        print(\"\\n\") # line break\n",
    "        \n",
    "        # Store this information as class attributes:\n",
    "        self.list_of_classes = list_of_classes\n",
    "        self.number_of_classes = number_of_classes\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tf_models:\n",
    "    \n",
    "    # TensorFlow models with single input, single output, or single input and possibly\n",
    "    # 1 regression and 1 classification output\n",
    "    # Use preferentially for these situations\n",
    "    \n",
    "    def __init__(self, X_train, y_train, X_valid = None, y_valid = None, type_of_problem = 'regression', number_of_classes = 2, optimizer = None):\n",
    "        \n",
    "        # type_of_problem = 'regression', 'classification', 'both'\n",
    "        # optimizer: tf.keras.optimizers.Optimizer object:\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer\n",
    "        # use the object to set parameters such as learning rate and selection of the optimizer\n",
    "        \n",
    "        import tensorflow as tf\n",
    "        \n",
    "        # Guarantee it is an array:\n",
    "        self.X_train = np.array(X_train)\n",
    "        self.y_train = np.array(y_train)\n",
    "        \n",
    "        # Input layer with shape given by the number of columns of the tensors. If using an image\n",
    "        # it would be the number of pixels in X and Y axis, with the image depth\n",
    "        # The batch (last) dimension should not be provided to the input layer\n",
    "        self.input_layer = tf.keras.layers.Input(shape = (X_train.shape)[1:], name = \"input_layer\")\n",
    "        \n",
    "        if ((X_valid is not None) & (y_valid is not None)):\n",
    "            \n",
    "            self.X_valid = np.array(X_valid)\n",
    "            self.y_valid = np.array(y_valid)\n",
    "        \n",
    "        else:\n",
    "            self.X_valid = None\n",
    "            self.y_valid = None\n",
    "        \n",
    "        self.type_of_problem = type_of_problem\n",
    "        self.number_of_classes = number_of_classes\n",
    "        \n",
    "        if (type_of_problem == 'regression'):    \n",
    "            self.metrics = 'mse'\n",
    "            self.loss = 'mse'\n",
    "            self.output_layer = tf.keras.layers.Dense(units = 1, name = 'output')\n",
    "            self.metrics_name = 'mse'\n",
    "        \n",
    "        elif (type_of_problem == 'classification'):\n",
    "            \n",
    "            self.metrics = 'acc'\n",
    "            self.metrics_name = 'acc'\n",
    "            \n",
    "            if (number_of_classes == 2):\n",
    "                self.output_layer = tf.keras.layers.Dense(units = 1, activation = 'sigmoid', name = 'output')\n",
    "                # Dense(1) activated through sigmoid is the logistic regression: generayes a\n",
    "                # probability between 0 and 1\n",
    "                self.loss = 'binary_crossentropy'\n",
    "                # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryCrossentropy\n",
    "            else:\n",
    "                self.output_layer = tf.keras.layers.Dense(units = number_of_classes, activation = 'softmax', name = 'output')\n",
    "                self.loss = 'sparse_categorical_crossentropy'\n",
    "                # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalCrossentropy\n",
    "        \n",
    "        else: # both\n",
    "            \n",
    "            self.metrics_regression = 'mse'\n",
    "            self.loss_regression = 'mse'\n",
    "            self.output_regression_layer = tf.keras.layers.Dense(units = 1, name = 'output_regression')\n",
    "            \n",
    "            self.metrics_classification = 'acc'\n",
    "            \n",
    "            if (number_of_classes == 2):\n",
    "                self.output_classification_layer = tf.keras.layers.Dense(units = 1, activation = 'sigmoid', name = 'output_classification')\n",
    "                self.loss_classification = 'binary_crossentropy'\n",
    "                # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryCrossentropy\n",
    "            else:\n",
    "                self.output_classification_layer = tf.keras.layers.Dense(units = number_of_classes, activation = 'softmax', name = 'output_classification')\n",
    "                self.loss_classification = 'sparse_categorical_crossentropy'\n",
    "                # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalCrossentropy\n",
    "\n",
    "        # create a model attribute, and an history attribute:\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "        \n",
    "        # Set the optimizer:\n",
    "        if (optimizer is None):\n",
    "            # use Adam with default arguments\n",
    "            # https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "            optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        # Save the optimizer as an attribute:\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    \n",
    "    def compile_model(self):\n",
    "        \n",
    "        import tensorflow as tf\n",
    "        \n",
    "        optimizer = self.optimizer\n",
    "        input_layer = self.input_layer\n",
    "        model = self.model\n",
    "        type_of_problem = self.type_of_problem\n",
    "        \n",
    "        if ((type_of_problem == 'regression')|((type_of_problem == 'classification'))):\n",
    "            # model = tf.keras.models.Model(inputs = [input_layer], outputs = [output_layer])\n",
    "            # When declaring the metrics as an object, they must be provided to the compile\n",
    "            # method within a list:\n",
    "            # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RootMeanSquaredError\n",
    "            # or as the attribute ._name, which contains the correspondent string\n",
    "            \n",
    "            output_layer = self.output_layer\n",
    "            loss = self.loss\n",
    "            metrics = self.metrics\n",
    "            \n",
    "            print(\"Check model architecture:\\n\")\n",
    "            tf.keras.utils.plot_model(model)\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            # Compile model:\n",
    "            model.compile(optimizer = optimizer,\n",
    "                          loss = loss,\n",
    "                          metrics = metrics)\n",
    "        \n",
    "        else:\n",
    "            # model = tf.keras.models.Model(inputs = [input_layer], outputs = [output_regression_layer, output_classification_layer])\n",
    "            \n",
    "            metrics_regression = self.metrics_regression\n",
    "            loss_regression = self.loss_regression\n",
    "            output_regression_layer = self.output_regression_layer\n",
    "            metrics_classification = self.metrics_classification\n",
    "            output_classification_layer = self.output_classification_layer\n",
    "            loss_classification = self.loss_classification\n",
    "            \n",
    "        \n",
    "            \"\"\"\n",
    "            For multiple output layers:\n",
    "            \n",
    "            'wine_type' and 'wine_quality' are the 'name's of the output layers:\n",
    "\n",
    "            model.compile(optimizer=rms, \n",
    "                           loss = {'wine_type' : 'binary_crossentropy',\n",
    "                                   'wine_quality' : 'mean_squared_error'\n",
    "                                  },\n",
    "                           metrics = {'wine_type' : 'accuracy',\n",
    "                                      'wine_quality': tf.keras.metrics.RootMeanSquaredError()\n",
    "                                    }\n",
    "                          )\n",
    "\n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            print(\"Check model architecture:\\n\")\n",
    "            tf.keras.utils.plot_model(model)\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            # Compile model:\n",
    "            model.compile(optimizer = optimizer,\n",
    "                          loss = {'output_classification': loss_classification,\n",
    "                                  'output_regression': loss_regression},\n",
    "                          metrics = {'output_classification': metrics_classification,\n",
    "                                    'output_regression': metrics_regression})\n",
    "        \n",
    "        \n",
    "        print(\"Check model summary:\\n\")\n",
    "        try:\n",
    "            # only works in Jupyter Notebook:\n",
    "            from IPython.display import display\n",
    "            display(model.summary())\n",
    "                    \n",
    "        except: # regular mode\n",
    "            print(model.summary())\n",
    "        \n",
    "        # Now, save the model in the attribute and return the object:\n",
    "        self.model = model\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_model(self, epochs = 2000, batch_size = 200, verbose = 1):\n",
    "        \n",
    "        import tensorflow as tf\n",
    "        \n",
    "        # If you set verbose = 0, It will show nothing. If you set verbose = 1, It will \n",
    "        # show the output like this: Epoch 1/200 55/55[=====] - 10s 307ms/step - loss: 0.56 - \n",
    "        # accuracy: 0.4949\n",
    "        \n",
    "        model = self.model\n",
    "        \n",
    "        X_train = self.X_train\n",
    "        y_train = self.y_train\n",
    "        \n",
    "        X_valid = self.X_valid\n",
    "        y_valid = self.y_valid\n",
    "        \n",
    "        if ((X_valid is not None) & (y_valid is not None)):   \n",
    "            has_validation = True\n",
    "        \n",
    "        else:\n",
    "            has_validation = False\n",
    "        \n",
    "        # Fit the model:\n",
    "        if (has_validation):\n",
    "            history = model.fit(X_train, \n",
    "                                y_train,\n",
    "                                batch_size = batch_size,\n",
    "                                epochs = epochs,\n",
    "                                verbose = verbose,\n",
    "                                validation_data = (X_valid, y_valid))\n",
    "        \n",
    "        else: # no validation set\n",
    "            history = model.fit(X_train, \n",
    "                                y_train,\n",
    "                                batch_size = batch_size,\n",
    "                                epochs = epochs,\n",
    "                                verbose = verbose)\n",
    "        \n",
    "        # Update the attributes and return the object:\n",
    "        self.history = history\n",
    "        self.model = model\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def tf_simple_dense (self, epochs = 2000, batch_size = 200, verbose = 1):\n",
    "        \n",
    "        import tensorflow as tf\n",
    "\n",
    "        \"\"\"\n",
    "        Wide and deep example:\n",
    "\n",
    "        # define inputs\n",
    "        input_a = Input(shape=[1], name=\"Wide_Input\")\n",
    "        input_b = Input(shape=[1], name=\"Deep_Input\")\n",
    "\n",
    "        # define deep path\n",
    "        hidden_1 = Dense(30, activation=\"relu\")(input_b)\n",
    "        hidden_2 = Dense(30, activation=\"relu\")(hidden_1)\n",
    "\n",
    "        # define merged path\n",
    "        concat = concatenate([input_a, hidden_2])\n",
    "        output = Dense(1, name=\"Output\")(concat)\n",
    "\n",
    "        # define another output for the deep path\n",
    "        aux_output = Dense(1,name=\"aux_Output\")(hidden_2)\n",
    "\n",
    "        # build the model\n",
    "        model = Model(inputs=[input_a, input_b], outputs=[output, aux_output])\n",
    "\n",
    "        \"\"\"\n",
    "        input_layer = self.input_layer\n",
    "        output_layer = self.output_layer\n",
    "        \n",
    "        # define inputs\n",
    "        input_1 = input_layer\n",
    "        # Creating blocks of Simple dense with the following \n",
    "        # (filters, kernel_size, repetitions) configurations:\n",
    "        # tf.keras.layers.Dense(128, activation = 'relu', input_dim = INPUT_DIMENSION)\n",
    "        # tf.keras.layers.Dense(1)\n",
    "\n",
    "        # First hidden layer:\n",
    "        x = tf.keras.layers.Dense(units = 128, activation = 'relu', name = 'dense_1')(input_1)\n",
    "        # 'relu' = ReLU, the Rectified Linear Unit function, returns f(x) = max(0, x)\n",
    "        # (i.e., if x <=0, relu(x) = 0; if (x > 0), relu(x) = 0)  \n",
    "        # The integer passed as parameter for Dense is the parameter 'units' from Dense\n",
    "        # layer. The number (\"units\") used as input for the dense function Dense(units) is a \n",
    "        # Positive integer that represents the dimensionality of the output space.\n",
    "        # Here, 'units' = 100, so this first hidden-layer has 100 neurons\n",
    "        \n",
    "        output_1 = output_layer(x)\n",
    "        \n",
    "        model = tf.keras.models.Model(inputs = input_1, outputs = output_1, name = 'tf_simple_dense')\n",
    "        # Update model attribute:\n",
    "        self.model = model\n",
    "        \n",
    "        # Compile the model:\n",
    "        self = self.compile_model()\n",
    "        # Fit the model:\n",
    "        self = self.fit_model(epochs = epochs, batch_size = batch_size, verbose = verbose)\n",
    "        \n",
    "        # return compiled model:\n",
    "        return self\n",
    "    \n",
    "    def tf_double_dense (self, epochs = 2000, batch_size = 200, verbose = 1):\n",
    " \n",
    "        import tensorflow as tf\n",
    "\n",
    "        input_layer = self.input_layer\n",
    "        output_layer = self.output_layer\n",
    "        \n",
    "        # define inputs\n",
    "        input_1 = input_layer\n",
    "        # Creating blocks of Simple dense with the following \n",
    "        # (filters, kernel_size, repetitions) configurations:\n",
    "        # tf.keras.layers.Dense(128, activation = 'relu', input_dim = INPUT_DIMENSION)\n",
    "        # tf.keras.layers.Dense(1)\n",
    "\n",
    "        # First hidden layer:\n",
    "        x = tf.keras.layers.Dense(units = 128, activation = 'relu', name = 'dense_1')(input_1)\n",
    "        # 'relu' = ReLU, the Rectified Linear Unit function, returns f(x) = max(0, x)\n",
    "        # (i.e., if x <=0, relu(x) = 0; if (x > 0), relu(x) = 0)  \n",
    "        # The integer passed as parameter for Dense is the parameter 'units' from Dense\n",
    "        # layer. The number (\"units\") used as input for the dense function Dense(units) is a \n",
    "        # Positive integer that represents the dimensionality of the output space.\n",
    "        # Here, 'units' = 100, so this first hidden-layer has 100 neurons\n",
    "        x = tf.keras.layers.Dense(units = 128, activation = 'relu', name = 'dense_2')(x)\n",
    "        \n",
    "        output_1 = output_layer(x)\n",
    "        \n",
    "        model = tf.keras.models.Model(inputs = input_1, outputs = output_1, name = 'tf_double_dense')\n",
    "        # Update model attribute:\n",
    "        self.model = model\n",
    "        \n",
    "        # Compile the model:\n",
    "        self = self.compile_model()\n",
    "        # Fit the model:\n",
    "        self = self.fit_model(epochs = epochs, batch_size = batch_size, verbose = verbose)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def tf_cnn_time_series (self, epochs = 2000, batch_size = 200, verbose = 1):\n",
    "        \n",
    "        import tensorflow as tf\n",
    "\n",
    "        input_layer = self.input_layer\n",
    "        output_layer = self.output_layer\n",
    "        \n",
    "        # define inputs\n",
    "        input_1 = input_layer\n",
    "        # Creating blocks of Simple dense with the following \n",
    "        # (filters, kernel_size, repetitions) configurations:\n",
    "        # tf.keras.layers.Dense(128, activation = 'relu', input_dim = INPUT_DIMENSION)\n",
    "        # tf.keras.layers.Dense(1)\n",
    "        \n",
    "        # ATTENTION: if the first layer is a Dense, we must specify\n",
    "        # the parameter 'input_dim'. If it is a CNN or RNN, we\n",
    "        # specify 'input_shape', instead. These parameters are only\n",
    "        # specified for the first layer of the network.\n",
    "        \n",
    "        # First convolution:\n",
    "        x = tf.keras.layers.Conv1D(filters = 64, kernel_size = 2, activation = 'relu', input_shape = (self.X_train.shape[1], 1), name = 'convolution')(input_1)\n",
    "        # First Max Pooling to enhance select the characteristics highlighted by the convolution:\n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size = 2, name = 'pooling')(x)\n",
    "        # Reduces to half the original size.\n",
    "            \n",
    "        # The convolutions and pooling reduce the dimensionality of the data.\n",
    "        # Under the hood, TensorFlow is testing different combinations of filters until it finds\n",
    "        # filters that actually highlight important characteristics.\n",
    "        # On the other hand, apply too much convolutional layers may reduce too much the data, making\n",
    "        # it difficult for the last dense layer to perform a good prediction.\n",
    "            \n",
    "        # Flatten the data for making it adequate for feeding the dense layers:\n",
    "        x = tf.keras.layers.Flatten(name = 'flatten')(x)\n",
    "            \n",
    "        # Feed the first dense layer, with 50 neurons:\n",
    "        x = tf.keras.layers.Dense(units = 50, activation = 'relu', name = 'dense_1')(x)\n",
    "        # 'relu' = ReLU, the Rectified Linear Unit function, returns f(x) = max(0, x)\n",
    "        # (i.e., if x <=0, relu(x) = 0; if (x > 0), relu(x) = 0)\n",
    "        \n",
    "        output_1 = output_layer(x)\n",
    "        \n",
    "        model = tf.keras.models.Model(inputs = input_1, outputs = output_1, name = 'tf_cnn_time_series')\n",
    "        # Update model attribute:\n",
    "        self.model = model\n",
    "        \n",
    "        # Compile the model:\n",
    "        self = self.compile_model()\n",
    "        # Fit the model:\n",
    "        self = self.fit_model(epochs = epochs, batch_size = batch_size, verbose = verbose)\n",
    "        \n",
    "        # return compiled model:\n",
    "        return self\n",
    "    \n",
    "    def tf_lstm_time_series (self, epochs = 2000, batch_size = 200, verbose = 1):\n",
    "        \n",
    "        import tensorflow as tf\n",
    "        \n",
    "        input_layer = self.input_layer\n",
    "        output_layer = self.output_layer\n",
    "        # Number of columns (sequence length):\n",
    "        \n",
    "        # define inputs\n",
    "        input_1 = input_layer\n",
    "        # Creating blocks of Simple dense with the following \n",
    "        # (filters, kernel_size, repetitions) configurations:\n",
    "        # tf.keras.layers.Dense(128, activation = 'relu', input_dim = INPUT_DIMENSION)\n",
    "        # tf.keras.layers.Dense(1)\n",
    "        \n",
    "        # LSTM layer: 1 cycle per sequence element (number of iterations = \n",
    "        # SEQUENCE_LENGTH):\n",
    "        # LSTM with 50 neurons:\n",
    "        x = tf.keras.layers.LSTM(units = 50, activation = 'relu', input_shape = (self.X_train.shape[1], 1), name = 'lstm')(input_1)\n",
    "        # 'relu' = ReLU, the Rectified Linear Unit function, returns f(x) = max(0, x)\n",
    "        # (i.e., if x <=0, relu(x) = 0; if (x > 0), relu(x) = 0)\n",
    "            \n",
    "        # We will use one LSTM layer to process each input sub-sequence \n",
    "        # of N time steps, followed by a Dense layer to interpret the \n",
    "        # summary of the input sequence.\n",
    "        \n",
    "        output_1 = output_layer(x)\n",
    "        \n",
    "        model = tf.keras.models.Model(inputs = input_1, outputs = output_1, name = 'tf_lstm_time_series')\n",
    "        # Update model attribute:\n",
    "        self.model = model\n",
    "        \n",
    "        # Compile the model:\n",
    "        self = self.compile_model()\n",
    "        # Fit the model:\n",
    "        self = self.fit_model(epochs = epochs, batch_size = batch_size, verbose = verbose)\n",
    "        \n",
    "        # return compiled model:\n",
    "        return self\n",
    "    \n",
    "    def tf_encoder_decoder_time_series (self, epochs = 2000, batch_size = 200, verbose = 1):\n",
    "        \n",
    "        import tensorflow as tf\n",
    "        \n",
    "        input_layer = self.input_layer\n",
    "        output_layer = self.output_layer\n",
    "        \n",
    "        \n",
    "        # define inputs\n",
    "        input_1 = input_layer\n",
    "        # Creating blocks of Simple dense with the following \n",
    "        # (filters, kernel_size, repetitions) configurations:\n",
    "        # tf.keras.layers.Dense(128, activation = 'relu', input_dim = INPUT_DIMENSION)\n",
    "        # tf.keras.layers.Dense(1)\n",
    "        \n",
    "        # LSTM layer: 1 cycle per sequence element (number of iterations = SEQUENCE_LENGTH):\n",
    "        # LSTM with 100 neurons:\n",
    "        # Encoder:\n",
    "        x = tf.keras.layers.LSTM(units = 100, activation = 'relu', input_shape = (self.X_train.shape[1], 1), name = 'lstm_encoder')(input_1)\n",
    "            \n",
    "        # The encoded sequence will be repeated 2 times by the model for the two output time steps \n",
    "        # required by the model using a RepeatVector layer. These will be fed to a decoder LSTM layer \n",
    "        # before using a Dense output layer wrapped in a TimeDistributed layer that will produce one \n",
    "        # output for each step in the output sequence.\n",
    "        x = tf.keras.layers.RepeatVector(2, name = 'repeat_vector')(x)\n",
    "        # Decoder: LSTM with 100 neurons\n",
    "        x = tf.keras.layers.LSTM(units = 100, activation = 'relu', return_sequences = True, name = 'lstm_decoder')(x)\n",
    "        # return_sequences = True returns the hidden states h.\n",
    "        # This generates an output with an extra dimension (output consists on an array of two values: \n",
    "        # the prediction and the hidden state).\n",
    "            \n",
    "        # Last dense - output layer ('linear' activation):\n",
    "        # Apply a TimeDistributed layer for compatibility with the Encoder-Decoder Archictecture:\n",
    "        # Wrap the output into this layer:\n",
    "        output_1 = tf.keras.layers.TimeDistributed(output_layer, name = 'time_distributed_output')(x)\n",
    "        \n",
    "        model = tf.keras.models.Model(inputs = input_1, outputs = output_1, name = 'tf_encoder_decoder_time_series')\n",
    "        # Update model attribute:\n",
    "        self.model = model\n",
    "        \n",
    "        # Compile the model:\n",
    "        self = self.compile_model()\n",
    "        # Fit the model:\n",
    "        self = self.fit_model(epochs = epochs, batch_size = batch_size, verbose = verbose)\n",
    "        \n",
    "        # return compiled model:\n",
    "        return self\n",
    "    \n",
    "    def tf_cnn_lstm_time_series (self, epochs = 2000, batch_size = 200, verbose = 1):\n",
    "        \n",
    "        import numpy as np\n",
    "        import tensorflow as tf\n",
    "        \n",
    "        input_layer = self.input_layer\n",
    "        output_layer = self.output_layer\n",
    "                \n",
    "        convolution_layer = tf.keras.layers.Conv1D(filters = 64, kernel_size = 1, activation = 'relu', input_shape = (None, 2, 1))\n",
    "        # Originally: input_shape = (None, 2, 1)\n",
    "        max_pooling_layer = tf.keras.layers.MaxPooling1D(pool_size = 2)\n",
    "        flatten_layer = tf.keras.layers.Flatten()\n",
    "        \n",
    "        # define inputs\n",
    "        input_1 = input_layer\n",
    "        # Creating blocks of Simple dense with the following \n",
    "        # (filters, kernel_size, repetitions) configurations:\n",
    "        # tf.keras.layers.Dense(128, activation = 'relu', input_dim = INPUT_DIMENSION)\n",
    "        # tf.keras.layers.Dense(1)\n",
    "      \n",
    "        # The entire CNN model is wrapped in TimeDistributed wrapper layers so that it can be applied to \n",
    "        # each subsequence in the  sample. The results are then interpreted by the LSTM layer \n",
    "        # before the model outputs a prediction.\n",
    "        \n",
    "        # First time-distributed convolution (for compatibility with the LSTM):\n",
    "        x  = tf.keras.layers.TimeDistributed(convolution_layer, name = 'convolution')(input_1)\n",
    "        # First time distributed Max Pooling (for compatibility with the LSTM):\n",
    "        # Max Pooling: enhance select the characteristics highlighted by the convolution:\n",
    "        x = tf.keras.layers.TimeDistributed(max_pooling_layer, name = 'pooling')(x)\n",
    "        # Reduces to half the original size.\n",
    "        # The convolutions and pooling reduce the dimensionality of the data.\n",
    "        # Under the hood, TensorFlow is testing different combinations of filters until it finds filters \n",
    "        # that actually highlight important characteristics.\n",
    "        # On the other hand, apply too much convolutional layers may reduce too much the data, making\n",
    "        # it difficult for the last dense layer to perform a good prediction.        \n",
    "        # Flatten the data for making it adequate for feeding the dense layers:\n",
    "        # Time distributed Flatten for compatibility with the LSTM:\n",
    "        x = tf.keras.layers.TimeDistributed(flatten_layer, name = 'flatten')(x)\n",
    "            \n",
    "        # Now, feed the LSTM:\n",
    "        # The LSTM performs 1 cycle per element of the sequence. Since each sequence now have 2 elements,\n",
    "        # the LSTM will perform two iterations:\n",
    "        # LSTM with 50 neurons:\n",
    "        x = tf.keras.layers.LSTM(units = 50, activation = 'relu', name = 'lstm')(x)\n",
    "        # 'relu' = ReLU, the Rectified Linear Unit function, returns f(x) = max(0, x)\n",
    "        # (i.e., if x <=0, relu(x) = 0; if (x > 0), relu(x) = 0)\n",
    "        output_1 = output_layer(x)    \n",
    "       \n",
    "        model = tf.keras.models.Model(inputs = input_1, outputs = output_1, name = 'tf_cnn_lstm_time_series')\n",
    "        # Update model attribute:\n",
    "        self.model = model\n",
    "        \n",
    "        # Compile the model:\n",
    "        self = self.compile_model()\n",
    "        # Fit the model:\n",
    "        self = self.fit_model(epochs = epochs, batch_size = batch_size, verbose = verbose)\n",
    "        \n",
    "        # return compiled model:\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class siamese_networks:\n",
    "    \n",
    "    # TensorFlow models for multiple responses. The neural networks are replicated for each one\n",
    "    # of the responses\n",
    "    \n",
    "    def __init__(self, output_dictionary, X_train, y_train, X_valid = None, y_valid = None):\n",
    "        \n",
    "        # type_of_problem = 'regression', 'classification', 'both'\n",
    "        # optimizer: tf.keras.optimizers.Optimizer object:\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "        # https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer\n",
    "        # use the object to set parameters such as learning rate and selection of the optimizer\n",
    "        \n",
    "        # ATTENTION, y_train, y_valid must be tensors or numpy arrays containing the responses in the\n",
    "        # exact same order as the responses declared in the output dictionary\n",
    "\n",
    "        # output_dictionary:\n",
    "        # output_dictionary structure:\n",
    "        # {'response_variable': {\n",
    "        # 'type': 'regression', 'number_of_classes':}}\n",
    "        # 'response_variable': name of the column used as response for one of the outputs. This key\n",
    "        # gives access to the nested dictionary containing the following keys: \n",
    "        # 'type': type of problem. Must contain the string 'regression' or 'classification';\n",
    "        # 'number_of_classes': integer. This key may not be declared for regression problems. Do not\n",
    "        # include the key, set as 1, or set the number of classes used for training.\n",
    "        \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import tensorflow as tf\n",
    "        \n",
    "        self.output_dictionary = output_dictionary\n",
    "        \n",
    "        # Retrieve the list of responses (the keys from this dictionary)\n",
    "        self.list_of_responses = list((self.output_dictionary).keys())\n",
    "\n",
    "        # Define an internal function for reformating the data in the format needed.\n",
    "        # We need a tuple formed by each one of the responses at time:\n",
    "        # (response1, response2, ...)\n",
    "        def format_output(data, list_of_responses):\n",
    "            # Convert to NumPy array, if it is a tensor:\n",
    "            data = np.array(data)\n",
    "            \n",
    "            # number of data columns must be equal to the total of responses\n",
    "            try:\n",
    "                assert data.shape[1] == len(list_of_responses)\n",
    "            \n",
    "            except:\n",
    "                raise AssertionError (f\"Response tensor contains {data.shape[1]} responses, but found only {len(list_of_responses)} responses ({list_of_responses}) in the dictionary. Check your output dictionary before running this function.\\n\")\n",
    "            \n",
    "            # Convert to Pandas dataframe:\n",
    "            data = pd.DataFrame(data = data, columns = list_of_responses)\n",
    "            # Start a list of tensors:\n",
    "            tensors_list = []\n",
    "            # Apply the pop method to return a particular column and drop it from data:\n",
    "            for column in list_of_responses:\n",
    "                response = data.pop(column)\n",
    "                # Convert the response Pandas Series to a numpy array:\n",
    "                response = np.array(response)\n",
    "                # Add it to tensors list:\n",
    "                tensors_list.append(response)\n",
    "            # Convert the list to tuple and return it:\n",
    "            return tuple(tensors_list)\n",
    "\n",
    "        # Guarantee it is an array:\n",
    "        self.X_train = np.array(X_train)\n",
    "        self.y_train = format_output(data = y_train, list_of_responses = self.list_of_responses)\n",
    "    \n",
    "        \n",
    "        if ((X_valid is not None) & (y_valid is not None)):\n",
    "            \n",
    "            self.X_valid = np.array(X_valid)\n",
    "            self.y_valid = format_output(data = y_valid, list_of_responses = self.list_of_responses)\n",
    "        \n",
    "        else:\n",
    "            self.X_valid = None\n",
    "            self.y_valid = None\n",
    "        \n",
    "        # Input layer with shape given by the number of columns of the tensors. If using an image\n",
    "        # it would be the number of pixels in X and Y axis, with the image depth\n",
    "        # The batch (last) dimension should not be provided to the input layer\n",
    "        input_layer = tf.keras.layers.Input(shape = (X_train.shape)[1:], name = \"input_layer\")\n",
    "        # Save it as an attribute\n",
    "        self.inputs = input_layer\n",
    "    \n",
    "    \n",
    "    def fit_model(self, epochs = 2000, batch_size = 200, verbose = 1):\n",
    "        \n",
    "        import tensorflow as tf\n",
    "        \n",
    "        # If you set verbose = 0, It will show nothing. If you set verbose = 1, It will \n",
    "        # show the output like this: Epoch 1/200 55/55[=====] - 10s 307ms/step - loss: 0.56 - \n",
    "        # accuracy: 0.4949\n",
    "        \n",
    "        model = self.model\n",
    "        \n",
    "        # There must be one copy of the input by response. So, before fitting the model, let's create\n",
    "        # tuples with same size as the total of responses, where all elements of the tuple are equal\n",
    "        # to the X tensor. Each element will be sequentially used by the model to fit one of the\n",
    "        # responses: first element for the first branch, 2nd for the 2nd branch, and so on...\n",
    "        \n",
    "        # Retrieve the outputs dictionary\n",
    "        output_dictionary = self.output_dictionary\n",
    "        \n",
    "        X_train = self.X_train\n",
    "        y_train = self.y_train\n",
    "        \n",
    "        X_valid = self.X_valid\n",
    "        y_valid = self.y_valid\n",
    "        \n",
    "        if ((X_valid is not None) & (y_valid is not None)):   \n",
    "            has_validation = True\n",
    "    \n",
    "        else:\n",
    "            has_validation = False\n",
    "        \n",
    "        # Fit the model:\n",
    "        if (has_validation):\n",
    "            history = model.fit(X_train, \n",
    "                                y_train,\n",
    "                                batch_size = batch_size,\n",
    "                                epochs = epochs,\n",
    "                                verbose = verbose,\n",
    "                                validation_data = (X_valid, y_valid))\n",
    "        \n",
    "        else: # no validation set\n",
    "            history = model.fit(X_train, \n",
    "                                y_train,\n",
    "                                batch_size = batch_size,\n",
    "                                epochs = epochs,\n",
    "                                verbose = verbose)\n",
    "        \n",
    "        # Update the attributes and return the object:\n",
    "        self.history = history\n",
    "        self.model = model\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def base_model_simple_dense (self, input_layer, response):\n",
    "        \n",
    "        import tensorflow as tf\n",
    "        \n",
    "        # First hidden layer:\n",
    "        x = tf.keras.layers.Dense(units = 128, activation = 'relu', name = ('dense_1'+ '_' + response))(input_layer)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def base_model_double_dense (self, input_layer, response):\n",
    "        \n",
    "        import tensorflow as tf\n",
    "        \n",
    "        # First hidden layer:\n",
    "        x = tf.keras.layers.Dense(units = 128, activation = 'relu', name = ('dense_1' + '_' + response))(input_layer)\n",
    "        x = tf.keras.layers.Dense(units = 128, activation = 'relu', name = ('dense_2' + '_' + response))(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def base_model_cnn_time_series (self, input_layer, response):\n",
    "        \n",
    "        import tensorflow as tf\n",
    "        \n",
    "        # First convolution:\n",
    "        x = tf.keras.layers.Conv1D(filters = 64, kernel_size = 2, activation = 'relu', name = ('convolution' + '_' + response))(input_layer)\n",
    "        # First Max Pooling to enhance select the characteristics highlighted by the convolution:\n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size = 2, name = ('pooling' + '_' + response))(x)\n",
    "        # Reduces to half the original size.\n",
    "            \n",
    "        # The convolutions and pooling reduce the dimensionality of the data.\n",
    "        # Under the hood, TensorFlow is testing different combinations of filters until it finds\n",
    "        # filters that actually highlight important characteristics.\n",
    "        # On the other hand, apply too much convolutional layers may reduce too much the data, making\n",
    "        # it difficult for the last dense layer to perform a good prediction.\n",
    "            \n",
    "        # Flatten the data for making it adequate for feeding the dense layers:\n",
    "        x = tf.keras.layers.Flatten(name = ('flatten' + '_' + response))(x)\n",
    "            \n",
    "        # Feed the first dense layer, with 50 neurons:\n",
    "        x = tf.keras.layers.Dense(units = 50, activation = 'relu', name = ('dense_1' + '_' + response))(x)\n",
    "        # 'relu' = ReLU, the Rectified Linear Unit function, returns f(x) = max(0, x)\n",
    "        # (i.e., if x <=0, relu(x) = 0; if (x > 0), relu(x) = 0)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def base_model_lstm_time_series (self, input_layer, response):\n",
    "        \n",
    "        import tensorflow as tf\n",
    "        \n",
    "        # LSTM layer: 1 cycle per sequence element (number of iterations = \n",
    "        # SEQUENCE_LENGTH):\n",
    "        # LSTM with 50 neurons:\n",
    "        x = tf.keras.layers.LSTM(units = 50, activation = 'relu', input_shape = (self.X_train.shape[1], 1), name = ('lstm' + '_' + response))(input_layer)\n",
    "        # 'relu' = ReLU, the Rectified Linear Unit function, returns f(x) = max(0, x)\n",
    "        # (i.e., if x <=0, relu(x) = 0; if (x > 0), relu(x) = 0)\n",
    "            \n",
    "        # We will use one LSTM layer to process each input sub-sequence \n",
    "        # of N time steps, followed by a Dense layer to interpret the \n",
    "        # summary of the input sequence.\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def base_model_encoder_decoder_time_series (self, input_layer, response):\n",
    "        \n",
    "        import tensorflow as tf\n",
    "        \n",
    "        # LSTM layer: 1 cycle per sequence element (number of iterations = SEQUENCE_LENGTH):\n",
    "        # LSTM with 100 neurons:\n",
    "        # Encoder:\n",
    "        x = tf.keras.layers.LSTM(units = 100, activation = 'relu', input_shape = (self.X_train.shape[1], 1), name = ('lstm_encoder' + '_' + response))(input_layer)\n",
    "            \n",
    "        # The encoded sequence will be repeated 2 times by the model for the two output time steps \n",
    "        # required by the model using a RepeatVector layer. These will be fed to a decoder LSTM layer \n",
    "        # before using a Dense output layer wrapped in a TimeDistributed layer that will produce one \n",
    "        # output for each step in the output sequence.\n",
    "        x = tf.keras.layers.RepeatVector(2, name = ('repeat_vector' + '_' + response))(x)\n",
    "        # Decoder: LSTM with 100 neurons\n",
    "        x = tf.keras.layers.LSTM(units = 100, activation = 'relu', return_sequences = True, name = ('lstm_decoder' + '_' + response))(x)\n",
    "        # return_sequences = True returns the hidden states h.\n",
    "        # This generates an output with an extra dimension (output consists on an array of two values: \n",
    "        # the prediction and the hidden state).\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def base_model_cnn_lstm_time_series (self, input_layer, response):\n",
    "        \n",
    "        import numpy as np\n",
    "        import tensorflow as tf\n",
    "        \n",
    "        convolution_layer = tf.keras.layers.Conv1D(filters = 64, kernel_size = 1, activation = 'relu', input_shape = (None, 2, 1))\n",
    "        # Originally: input_shape = (None, 2, 1)\n",
    "        max_pooling_layer = tf.keras.layers.MaxPooling1D(pool_size = 2)\n",
    "        flatten_layer = tf.keras.layers.Flatten()\n",
    "      \n",
    "        # The entire CNN model is wrapped in TimeDistributed wrapper layers so that it can be applied to \n",
    "        # each subsequence in the  sample. The results are then interpreted by the LSTM layer \n",
    "        # before the model outputs a prediction.\n",
    "        \n",
    "        # First time-distributed convolution (for compatibility with the LSTM):\n",
    "        x  = tf.keras.layers.TimeDistributed(convolution_layer,  name = ('convolution' + '_' + response))(input_layer)\n",
    "        # First time distributed Max Pooling (for compatibility with the LSTM):\n",
    "        # Max Pooling: enhance select the characteristics highlighted by the convolution:\n",
    "        x = tf.keras.layers.TimeDistributed(max_pooling_layer, name = ('pooling' + '_' + response))(x)\n",
    "        # Reduces to half the original size.\n",
    "        # The convolutions and pooling reduce the dimensionality of the data.\n",
    "        # Under the hood, TensorFlow is testing different combinations of filters until it finds filters \n",
    "        # that actually highlight important characteristics.\n",
    "        # On the other hand, apply too much convolutional layers may reduce too much the data, making\n",
    "        # it difficult for the last dense layer to perform a good prediction.        \n",
    "        # Flatten the data for making it adequate for feeding the dense layers:\n",
    "        # Time distributed Flatten for compatibility with the LSTM:\n",
    "        x = tf.keras.layers.TimeDistributed(flatten_layer, name = ('flatten'+ '_' + response))(x)\n",
    "            \n",
    "        # Now, feed the LSTM:\n",
    "        # The LSTM performs 1 cycle per element of the sequence. Since each sequence now have 2 elements,\n",
    "        # the LSTM will perform two iterations:\n",
    "        # LSTM with 50 neurons:\n",
    "        x = tf.keras.layers.LSTM(units = 50, activation = 'relu', name = ('lstm' + '_' + response))(x)\n",
    "        # 'relu' = ReLU, the Rectified Linear Unit function, returns f(x) = max(0, x)\n",
    "        # (i.e., if x <=0, relu(x) = 0; if (x > 0), relu(x) = 0)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def network_branch (self, response_variable, type_of_problem = 'regression', number_of_classes = 2, architecture = 'simple_dense'):\n",
    "        \n",
    "        # Generate a full branch from the siamese network. We will have one branch per response\n",
    "        \n",
    "        import tensorflow as tf\n",
    "        \n",
    "        # architecture = 'simple_dense': base_model_simple_dense from class siamese_networks;\n",
    "        # architecture = 'double_dense': base_model_double_dense from class siamese_networks;\n",
    "        # architecture = 'cnn': base_model_cnn_time_series from class siamese_networks;\n",
    "        # architecture = 'lstm': base_model_lstm_time_series from class siamese_networks;\n",
    "        # architecture = 'encoder_decoder': base_model_encoder_decoder_time_series from class siamese_networks;\n",
    "        # architecture = 'cnn_lstm': hybrid base_model_cnn_lstm_time_series from class siamese_networks.\n",
    "        \n",
    "        \n",
    "        input_layer = self.inputs\n",
    "        \n",
    "        if (architecture == 'double_dense'):\n",
    "            x = self.base_model_double_dense(input_layer = input_layer, response = response_variable)\n",
    "            \n",
    "        elif (architecture == 'cnn'):\n",
    "            x = self.base_model_cnn_time_series(input_layer = input_layer, response = response_variable)\n",
    "            \n",
    "        elif (architecture == 'lstm'):\n",
    "            x = self.base_model_lstm_time_series(input_layer = input_layer, response = response_variable)\n",
    "            \n",
    "        elif (architecture == 'encoder_decoder'):\n",
    "            x = self.base_model_encoder_decoder_time_series(input_layer = input_layer, response = response_variable)\n",
    "            \n",
    "        elif (architecture == 'cnn_lstm'):\n",
    "            x = self.base_model_cnn_lstm_time_series(input_layer = input_layer, response = response_variable)\n",
    "            \n",
    "        else:\n",
    "            x = self.base_model_simple_dense(input_layer = input_layer, response = response_variable)\n",
    "        \n",
    "        if (type_of_problem == 'regression'):\n",
    "            # Scalar output: 1 neuron with linear activation\n",
    "            output = tf.keras.layers.Dense(units = 1, name = ('output_' + response_variable))\n",
    "            if (architecture == 'encoder_decoder'):\n",
    "                output = tf.keras.layers.TimeDistributed(output, name = ('output_' + response_variable))(x)\n",
    "            else:\n",
    "                    output = output(x)\n",
    "           \n",
    "        else:\n",
    "            # Classification\n",
    "            if (number_of_classes == 2):\n",
    "                # 1 neuron activated through sigmoid, analogous to logistic regression\n",
    "                output = tf.keras.layers.Dense(units = 1, activation = 'sigmoid', name = ('output_' + response_variable))\n",
    "                if (architecture == 'encoder_decoder'):\n",
    "                    output = tf.keras.layers.TimeDistributed(output, name = ('output_' + response_variable))(x)\n",
    "                else:\n",
    "                    output = output(x)\n",
    "                \n",
    "            else:\n",
    "                # 1 neuron per class, activated through softmax\n",
    "                output = tf.keras.layers.Dense(units = number_of_classes, activation = 'softmax', name = ('output_' + response_variable))\n",
    "                if (architecture == 'encoder_decoder'):\n",
    "                    output = tf.keras.layers.TimeDistributed(output, name = ('output_' + response_variable))(x)\n",
    "                else:\n",
    "                    output = output(x)\n",
    "                    \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def compile_model (self, architecture, optimizer = None):\n",
    "        \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import tensorflow as tf\n",
    "        \n",
    "        # output_dictionary structure:\n",
    "        # {'response_variable': {\n",
    "        # 'type': 'regression', 'number_of_classes':}}\n",
    "        \n",
    "        # Retrieve the outputs dictionary\n",
    "        output_dictionary = self.output_dictionary\n",
    "        \n",
    "        # Initialize a loss and a metrics dictionary\n",
    "        loss_dict = {}\n",
    "        metrics_dict = {}\n",
    "        \n",
    "        # Initialize a list of output layers:\n",
    "        outputs_list = []\n",
    "        \n",
    "        # Loop through all the keys from the output_dictionary:\n",
    "        for response in output_dictionary.keys():\n",
    "            \n",
    "            nested_dict = output_dictionary[response]\n",
    "            # Retrieve the type of problem and the number of classes:\n",
    "            \n",
    "            type_of_problem = nested_dict['type']\n",
    "            \n",
    "            try:\n",
    "                number_of_classes = nested_dict['number_of_classes']\n",
    "            \n",
    "            except:\n",
    "                # The number was not passed for a regression problem\n",
    "                number_of_classes = 1\n",
    "            \n",
    "            # Get the output layer for that branch\n",
    "            output_layer = self.network_branch (response_variable = response, type_of_problem = type_of_problem, number_of_classes = number_of_classes, architecture = architecture)\n",
    "            \n",
    "            # Append the output_layer to the list:\n",
    "            outputs_list.append(output_layer)\n",
    "            \n",
    "            # When declaring the metrics as an object, they must be provided to the compile\n",
    "            # method within a list:\n",
    "            # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/RootMeanSquaredError\n",
    "            # or as the attribute ._name, which contains the correspondent string\n",
    "        \n",
    "            if (type_of_problem == 'regression'):    \n",
    "                metrics = 'mse'\n",
    "                loss = 'mse'\n",
    "                # Add it to the losses and metrics dictionaries:\n",
    "                # Concatenate \"output_\" since the names of the output layers start as this\n",
    "                loss_dict[(\"output_\" + response)] = loss\n",
    "                metrics_dict[(\"output_\" + response)] = metrics\n",
    "                \n",
    "            elif (type_of_problem == 'classification'):\n",
    "\n",
    "                metrics = 'acc'\n",
    "                \n",
    "                if (number_of_classes == 2):\n",
    "                    loss = 'binary_crossentropy'\n",
    "                    # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryCrossentropy\n",
    "                else:\n",
    "                    loss = 'sparse_categorical_crossentropy'\n",
    "                    # https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalCrossentropy\n",
    "                \n",
    "                # Add it to the losses and metrics dictionaries:\n",
    "                # Concatenate \"output_\" since the names of the output layers start as this\n",
    "                loss_dict[(\"output_\" + response)] = loss\n",
    "                metrics_dict[(\"output_\" + response)] = metrics\n",
    "        \n",
    "        # Now, compile the model\n",
    "        \n",
    "        # Set the optimizer:\n",
    "        if (optimizer is None):\n",
    "            # use Adam with default arguments\n",
    "            # https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "            optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        # define the model using the input and output layers\n",
    "        \"\"\"\n",
    "        for multiple inputs, inputs should be a list, like in:\n",
    "        model = Model(inputs=[input_a, input_b], outputs=[output, aux_output])\n",
    "        for a single input, it can be simply the input layer used\n",
    "        \"\"\"\n",
    "        \n",
    "        # retrieve the inputs attribute, containing the list of inputs:\n",
    "        # try accessing the attribute inputs:\n",
    "        inputs = self.inputs\n",
    "        \n",
    "        model = tf.keras.models.Model(inputs = inputs, outputs = outputs_list, name = 'siamese_neural_net')\n",
    "        \n",
    "        model.compile(optimizer = optimizer, \n",
    "               loss = loss_dict,\n",
    "               metrics = metrics_dict)\n",
    "        \n",
    "        print(\"Check model architecture:\\n\")\n",
    "        tf.keras.utils.plot_model(model)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        print(\"Check model summary:\\n\")\n",
    "        try:\n",
    "            # only works in Jupyter Notebook:\n",
    "            from IPython.display import display\n",
    "            display(model.summary())\n",
    "                    \n",
    "        except: # regular mode\n",
    "            print(model.summary())\n",
    "        \n",
    "        # Save as class attribute:\n",
    "        self.model = model\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "58e06e68-f19f-4491-804b-a40ac0d7d6ad"
   },
   "source": [
    "# **Function for separating and preparing features and responses tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "0cde4c5f-30ff-4134-b47f-40cd59522658",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def separate_and_prepare_features_and_responses (df, features_columns, response_columns):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "    except:\n",
    "        pass\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/Tensor\n",
    "\n",
    "    # features_columns: list of strings or string containing the names of columns\n",
    "    # with predictive variables in the original dataframe. \n",
    "    # Example: features_columns = ['col1', 'col2']; features_columns = 'predictor';\n",
    "    # features_columns = ['predictor'].\n",
    "    # response_columns: list of strings or string containing the names of columns\n",
    "    # with response variables in the original dataframe. \n",
    "    # Example: response_columns = ['col3', 'col4']; response_columns = 'response';\n",
    "    # response_columns = ['response']\n",
    "\n",
    "    # Set a local copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "\n",
    "    # Check if features_columns and response_columns are lists:\n",
    "    if ((type(features_columns) != list) & (type(features_columns) != tuple)):\n",
    "        #put inside a list:\n",
    "        features_columns = [features_columns]\n",
    "    \n",
    "    elif (type(features_columns) == tuple):\n",
    "        features_columns = list(features_columns)\n",
    "\n",
    "    if ((type(response_columns) != list) & (type(response_columns) != tuple)):\n",
    "        #put inside a list:\n",
    "        response_columns = [response_columns]\n",
    "    \n",
    "    elif (type(response_columns) == tuple):\n",
    "        response_columns = list(response_columns)\n",
    "\n",
    "    # Now, subset the dataframe:\n",
    "    X = DATASET[features_columns].copy(deep = True)\n",
    "    y = DATASET[response_columns].copy(deep = True)\n",
    "    # since response_columns is a list, not a string, y is a DataFrame, not a Series.\n",
    "    # So, the copy method accepts the argument deep = True\n",
    "\n",
    "    # Try the conversion to tensors. Since the values should not be modified, we\n",
    "    # will create the tensors as tf.constant, instead of tf.Variable:\n",
    "    try:\n",
    "\n",
    "        X = tf.constant(X)\n",
    "        y = tf.constant(y)\n",
    "\n",
    "        \"\"\"\n",
    "            Tensor with format as:\n",
    "            <tf.Tensor: shape=(253, 12), dtype=float64, numpy=\n",
    "            array([[ 1.        ,  1.        ,  1.        , ...,  4.18450387,\n",
    "                10.49874623,  2.09639084],\n",
    "               ...,\n",
    "               [12.        ,  4.        ,  6.        , ...,  4.40752786,\n",
    "                10.71241577,  3.30032431]])>\n",
    "        \"\"\"\n",
    "\n",
    "    except:\n",
    "\n",
    "        # Simply convert them to NumPy arrays. The arrays can be processed through\n",
    "        # deep learning and do not add features names to the model information (what\n",
    "        # raises error if we try to use the model to a set without names):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "\n",
    "        \"\"\"\n",
    "            Array with format as:\n",
    "            array([[ 1.        ,  1.        ,  1.        , ...,  4.42690937,\n",
    "                 4.18450387, 10.49874623],\n",
    "               ...,\n",
    "               [12.        ,  4.        ,  6.        , ...,  4.43083069,\n",
    "                 4.40752786, 10.71241577]])\n",
    "        \"\"\"\n",
    "\n",
    "    print(\"Check the 5 first elements from the tensors or arrays obtained:\\n\")\n",
    "    print(\"Features tensor or array:\\n\")\n",
    "    print(X[:5])\n",
    "    print(\"\\n\")\n",
    "    print(f\"Shape of the complete X tensor or array = {X.shape}\\n\")\n",
    "    # shape attribute is common to tf.Tensor, pd.DataFrame, pd.Series, and np.array\n",
    "    print(\"Responses tensor or array:\\n\")\n",
    "    print(y[:5])\n",
    "    print(\"\\n\")\n",
    "    print(f\"Shape of the complete y tensor or array = {y.shape}\\n\")\n",
    "\n",
    "    # Notice that tensors and arrays are sliced in the same way as lists.\n",
    "    # The slicing also modify the shape attribute from Tensors.\n",
    "    # We can convert a tf.Tensor object named tensor to a np.array object by\n",
    "    # simply making array = np.array(tensor) \n",
    "\n",
    "    # Now, since the arrays do not have a column header, let's create a mapping dictionary, correlating\n",
    "    # the array position with the original column name:\n",
    "    features_dict = {}\n",
    "    responses_dict = {}\n",
    "\n",
    "    for column_number, column in enumerate(features_columns):\n",
    "        # The enumerate object created from a list can be decoupled into two values:\n",
    "        # The index (number) - position in the list, and the element itself. example:\n",
    "        # 0, 'first_column':\n",
    "        # Add it to the features dictionary, with the column number as key:\n",
    "        features_dict[column_number] = column\n",
    "\n",
    "    # Repeat the process for the responses:\n",
    "    for column_number, column in enumerate(response_columns):\n",
    "        responses_dict[column_number] = column\n",
    "\n",
    "    # Finally, add both dictionaries to a mapping dict:\n",
    "    column_map_dict = {'features': features_dict, 'responses': responses_dict}\n",
    "    print(\"The mapping of the arrays' positions with the columns original names was returned as 'column_map_dict'.\")\n",
    "\n",
    "    return X, y, column_map_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1474c985-8196-4e15-b4e2-9d39146de7bc"
   },
   "source": [
    "# **Function for converting a whole dataframe or array-like object to tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "9494970f-e73f-4c78-a5fa-a168b84d22e7",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def convert_to_tensor (df_or_array_to_convert, columns_to_convert = None, columns_to_exclude = None):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "    except:\n",
    "        pass\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/Tensor\n",
    "\n",
    "    # columns_to_convert: list of strings or string containing the names of columns\n",
    "    # that you want to convert. Use this if you want to convert only a subset of the dataframe. \n",
    "    # Example: columns_to_convert = ['col1', 'col2']; columns_to_convert = 'predictor';\n",
    "    # columns_to_convert = ['predictor'] will create a tensor with only the specified columns;\n",
    "    # If None, the whole dataframe will be converted.\n",
    "    # ATTENTION: This argument only works for Pandas dataframes.\n",
    "    \n",
    "    # columns_to_exclude: Alternative parameter. \n",
    "    # list of strings or string containing the names of columns that you want to exclude from the\n",
    "    # returned tensor. Use this if you want to convert only a subset of the dataframe. \n",
    "    # Example: columns_to_exclude = ['col1', 'col2']; columns_to_exclude = 'predictor';\n",
    "    # columns_to_exclude = ['predictor'] will create a tensor with all columns from the dataframe\n",
    "    # except the specified ones. This argument will only be used if the previous one was not.\n",
    "    # ATTENTION: This argument only works for Pandas dataframes.\n",
    "\n",
    "    try:\n",
    "        # Set a local copy of the dataframe to manipulate:\n",
    "        DATASET = df_or_array_to_convert.copy(deep = True)\n",
    "\n",
    "        if (columns_to_convert is not None):\n",
    "            # Subset the dataframe:\n",
    "            # Check if features_columns and response_columns are lists:\n",
    "            if ((type(columns_to_convert) != list) & (type(columns_to_convert) != tuple)):\n",
    "                #put inside a list:\n",
    "                columns_to_convert = [columns_to_convert]\n",
    "            \n",
    "            elif (type(columns_to_convert) == tuple):\n",
    "                columns_to_convert = list(columns_to_convert)\n",
    "\n",
    "            # Now, filter the dataframe:\n",
    "            DATASET = DATASET[columns_to_convert]\n",
    "\n",
    "        elif (columns_to_exclude is not None):\n",
    "            # Run only if the dataframe was not subset:\n",
    "            if ((type(columns_to_exclude) != list) & (type(columns_to_exclude) != tuple)):\n",
    "                #put inside a list:\n",
    "                columns_to_exclude = [columns_to_exclude]\n",
    "            \n",
    "            elif (type(columns_to_exclude) == tuple):\n",
    "                columns_to_exclude = list(columns_to_exclude)\n",
    "            \n",
    "            # Drop the columns:\n",
    "            DATASET = DATASET.drop(columns_to_exclude, axis = 1)\n",
    "    \n",
    "    except:\n",
    "        # It is an array or iterable:\n",
    "        DATASET = np.array(df_or_array_to_convert)\n",
    "        \n",
    "        if (len(DATASET.shape) == 1):\n",
    "            # It is a tuple like (1,) - array like [1, 2, 3,...]\n",
    "            DATASET =  DATASET.reshape(-1, 1)\n",
    "            # Now, its format is like [[1], [2], [3],...] - shape like (4, 1)\n",
    "\n",
    "    # Try the conversion to tensor. Since the values should not be modified, we\n",
    "    # will create the tensors as tf.constant, instead of tf.Variable:\n",
    "    try:\n",
    "\n",
    "        X = tf.constant(DATASET)\n",
    "\n",
    "        \"\"\"\n",
    "            Tensor with format as:\n",
    "            <tf.Tensor: shape=(253, 12), dtype=float64, numpy=\n",
    "            array([[ 1.        ,  1.        ,  1.        , ...,  4.18450387,\n",
    "                10.49874623,  2.09639084],\n",
    "               ...,\n",
    "               [12.        ,  4.        ,  6.        , ...,  4.40752786,\n",
    "                10.71241577,  3.30032431]])>\n",
    "        \"\"\"\n",
    "\n",
    "    except:\n",
    "\n",
    "        # Simply convert them to NumPy arrays. The arrays can be processed through\n",
    "        # deep learning and do not add features names to the model information (what\n",
    "        # raises error if we try to use the model to a set without names):\n",
    "        X = np.array(DATASET)\n",
    "        \n",
    "    print(\"Check the 5 first elements from the tensor or array obtained:\\n\")\n",
    "    print(X[:5])\n",
    "    print(\"\\n\")\n",
    "    print(f\"Shape of the complete X tensor or array = {X.shape}\\n\")\n",
    "    # shape attribute is common to tf.Tensor, pd.DataFrame, pd.Series, and np.array\n",
    "\n",
    "    # Notice that tensors and arrays are sliced in the same way as lists.\n",
    "    # The slicing also modify the shape attribute from Tensors.\n",
    "    # We can convert a tf.Tensor object named tensor to a np.array object by\n",
    "    # simply making array = np.array(tensor) \n",
    "\n",
    "    # Now, since the arrays do not have a column header, let's create a mapping dictionary, correlating\n",
    "    # the array position with the original column name:\n",
    "    column_map_dict = {}\n",
    "    try:\n",
    "        \n",
    "        for column_number, column in enumerate(list(DATASET.columns)):\n",
    "            # The enumerate object created from a list can be decoupled into two values:\n",
    "            # The index (number) - position in the list, and the element itself. example:\n",
    "            # 0, 'first_column':\n",
    "            # Add it to the features dictionary, with the column number as key:\n",
    "            column_map_dict[column_number] = column\n",
    "\n",
    "        print(\"The mapping of the arrays' positions with the columns original names was returned as 'column_map_dict'.\")\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return X, column_map_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for creating a TensorFlow windowed dataset from multiple-feature time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator:\n",
    "    \n",
    "    # original algorithm:\n",
    "    # https://www.tensorflow.org/tutorials/structured_data/time_series?hl=en&%3Bauthuser=1&authuser=1\n",
    "  \n",
    "    def __init__(self, dataset, shift, use_past_responses_for_prediction = True, \n",
    "                 sequence_stride = 1, sampling_rate = 1, label_columns = None, \n",
    "                 train_pct = 70, val_pct = 10):\n",
    "        \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import tensorflow as tf\n",
    "        \n",
    "        # Return an error if the percents are out of the allowable range:\n",
    "        assert ((train_pct >= 0) & (train_pct <= 100))\n",
    "        assert ((val_pct >= 0) & (val_pct <= 100))\n",
    "        \n",
    "        df = dataset.copy(deep = True)\n",
    "        \n",
    "        # Store the raw data.\n",
    "        self.df = df\n",
    "        self.sequence_stride = sequence_stride\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.shift = shift\n",
    "        \n",
    "        n = len(dataset)\n",
    "        # Store the fractions for training and validation:\n",
    "        self.train_boundary = int(n*(train_pct/100))\n",
    "        self.val_boundary = int(n*(100 - val_pct)/100)\n",
    "        \n",
    "        \n",
    "        # Set the response columns as a list, if it is a simple string:\n",
    "    \n",
    "        if ((type(label_columns) == tuple)|(type(label_columns) == set)):\n",
    "            self.label_columns = list(label_columns)\n",
    "\n",
    "        elif (type(label_columns) != list):\n",
    "            self.label_columns = [label_columns]\n",
    "        \n",
    "        else:\n",
    "            self.label_columns = label_columns\n",
    "        \n",
    "        # Set responses and features datasets\n",
    "        y = (df[self.label_columns]).copy(deep = True)\n",
    "        \n",
    "        if (use_past_responses_for_prediction):\n",
    "            # we use all the columns as predictors for the time series dataset:\n",
    "            X = df\n",
    "            \n",
    "        else:\n",
    "            # Since they will not be used, eliminate them\n",
    "            X = df.drop(columns = self.label_columns)\n",
    "        \n",
    "        self.feature_columns = list(X.columns)\n",
    "        self.num_features = X.shape[1]\n",
    "        \n",
    "        # Define each one of the train, test and validation dataframes as arrays:\n",
    "        self.X_train = np.array(X[0:self.train_boundary])\n",
    "        self.y_train = np.array(y[0:self.train_boundary])\n",
    "        self.X_test = np.array(X[self.train_boundary:self.val_boundary])\n",
    "        self.y_test = np.array(y[self.train_boundary:self.val_boundary])\n",
    "        self.X_val = np.array(X[self.val_boundary:])\n",
    "        self.y_val = np.array(y[self.val_boundary:])\n",
    "              \n",
    "        # In the time series TF dataset, all columns are used as predictors.\n",
    "        # You can use entries on times t1, t2, t3 to predict t4, for example.\n",
    "        # The predicted columns are the ones indicated as label_columns, i.e., columns that\n",
    "        # will be used as the labels y.\n",
    "        \n",
    "        if label_columns is not None:\n",
    "            \n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                        enumerate(self.label_columns)}\n",
    "            \n",
    "        self.column_indices = {name: i for i, name in\n",
    "                                   enumerate(self.df.columns)}\n",
    "        \n",
    "        \"\"\"\n",
    "        slice object: object that defines the slicing interval. slice(x,y) is equivalent\n",
    "        to defining the interval [x:y] for slicing.\n",
    "        Example: a = list(range(0,99))\n",
    "        b = slice(10,22)\n",
    "        c = a[b] is equivalent to c = a[10:22], resulting in [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
    "        if b = slice(1,3), c = [1, 2], which are the indices 1 and 2 (indexing starting from 0)\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "    def split_as_labels_and_inputs (self, X, y):\n",
    "        \n",
    "        import numpy as np\n",
    "        import tensorflow as tf\n",
    "        \n",
    "        shift = self.shift\n",
    "        # shift: the sequence of timesteps i, i+1, ... will be used for predicting the\n",
    "        # timestep i + shift\n",
    "        stride = self.sequence_stride\n",
    "        # if a sequence starts in index i, the next sequence will start from i + stride\n",
    "        sampling = self.sampling_rate\n",
    "        # the sequence will be formed by timesteps i, i + sampling_rate, i + 2* sampling_rate, ...\n",
    "        \"\"\"\n",
    "        Example from TensorFlow documentation: \n",
    "        https://www.tensorflow.org/api_docs/python/tf/keras/utils/timeseries_dataset_from_array\n",
    "        \n",
    "        Consider indices [0, 1, ... 99]. With sequence_length=10, sampling_rate=2, sequence_stride=3, \n",
    "        shuffle=False, the dataset will yield batches of sequences composed of the following indices:\n",
    "\n",
    "        First sequence:  [0  2  4  6  8 10 12 14 16 18]\n",
    "        Second sequence: [3  5  7  9 11 13 15 17 19 21]\n",
    "        Third sequence:  [6  8 10 12 14 16 18 20 22 24]\n",
    "        ...\n",
    "        Last sequence:   [78 80 82 84 86 88 90 92 94 96]\n",
    "        \"\"\"\n",
    "        \n",
    "        total_elements = len(X)\n",
    "        start_index = 0\n",
    "        stop_index = start_index + shift\n",
    "        \n",
    "        # List to store all arrays\n",
    "        list_of_inputs = []\n",
    "        list_of_labels = []\n",
    "        \n",
    "        while (start_index < total_elements):\n",
    "            \n",
    "            try:\n",
    "                # Slice the X array from start to stop, with step = sampling\n",
    "                # Array slice: [start:stop:sampling]\n",
    "                added_input = X[start_index:stop_index:sampling]\n",
    "                # Notice that stop_index is not added. It is actually the index to be picked from y:\n",
    "                added_label = y[stop_index]\n",
    "                \n",
    "                # add them to the lists of arrays:\n",
    "                list_of_inputs.append(np.array(added_input))\n",
    "                list_of_labels.append(np.array(added_label))\n",
    "                # Update the start and stop indices:\n",
    "                start_index = start_index + stride\n",
    "                stop_index = stop_index + stride\n",
    "            \n",
    "            except:\n",
    "                # The elements actually finished, due to shifting and striding, so stop the loop\n",
    "                break\n",
    "        \n",
    "        # Convert lists to arrays and then to tensors:\n",
    "        inputs_tensor = np.array(list_of_inputs)\n",
    "        labels_tensor = np.array(list_of_labels)\n",
    "        \n",
    "        inputs_tensor = tf.constant(inputs_tensor)\n",
    "        labels_tensor = tf.constant(labels_tensor)\n",
    "        \n",
    "        return inputs_tensor, labels_tensor\n",
    "    \n",
    "    \n",
    "    def make_tensors (self):\n",
    "        \n",
    "        # start a tensors dictionary\n",
    "        tensors_dict = {}\n",
    "        \n",
    "        for group in ['train', 'test', 'val']:\n",
    "            \n",
    "            # Use vars function to access the correct attributes storing the desired arrays.\n",
    "            # The vars function allows you to access an attribute as a string\n",
    "            X = vars(self)[('X_' + group)]\n",
    "            y = vars(self)[('y_' + group)]\n",
    "            \n",
    "            # Split into inputs and labels\n",
    "            inputs_tensor, labels_tensor = self.split_as_labels_and_inputs(X = X, y = y)\n",
    "            # Store them in the tensors dictionary:\n",
    "            tensors_dict[group] = {'inputs': inputs_tensor, 'labels': labels_tensor}\n",
    "        \n",
    "        # Save the dictionary as class variable:\n",
    "        self.tensors_dict = tensors_dict\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_columns_time_series_tensors (df, response_columns, sequence_stride = 1, sampling_rate = 1, shift = 1, use_past_responses_for_prediction = True, percent_of_data_used_for_model_training = 70, percent_of_training_data_used_for_model_validation = 10):\n",
    "   \n",
    "    # original algorithm: \n",
    "    # https://www.tensorflow.org/tutorials/structured_data/time_series?hl=en&%3Bauthuser=1&authuser=1\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    # response_columns: string or list of strings with the response columns\n",
    "    \n",
    "    # The time series may be represented as a sequence of times like: t = 0, t = 1, t = 2, ..., t = N.\n",
    "    # When preparing the dataset, we pick a given number of 'times' (indexes), and use them for\n",
    "    # predicting a time in the future.\n",
    "    # So, the input_width represents how much times will be used for prediction. If input_width = 6,\n",
    "    # we use 6 values for prediction, e.g., t = 0, t = 1, ..., t = 5 will be a prediction window.\n",
    "    # In turns, if input_width = 3, 3 values are used: t = 0, t = 1, t = 2; if input_width = N, N\n",
    "    # consecutive values will be used: t = 0, t = 1, t = 2, ..., t = N. And so on.\n",
    "    # label_width, in turns, represent how much times will be predicted. If label_width = 1, a single\n",
    "    # value will be predicted. If label_width = 2, two consecutive values are predicted; if label_width =\n",
    "    # N, N consecutive values are predicted; and so on.\n",
    "    \n",
    "    # shift, sampling_rate, and sequence_stride: integers\n",
    "    \n",
    "    # shift represents the offset, i.e., given the input values, which value in the time sequence will\n",
    "    # be predicted. So, suppose input_width = 6 and label_width = 1\n",
    "    # If shift = 1, the label, i.e., the predicted value, will be the first after the sequence used for\n",
    "    # prediction. So, if  t = 0, t = 1, ..., t = 5 will be a prediction window and t = 6 will be the\n",
    "    # predicted value. Notice that the complete window has a total width = 7: t = 0, ..., t = 7. \n",
    "    # If label_width = 2, then t = 6 and t = 7 will be predicted (total width = 8).\n",
    "    # Another example: suppose input_width = 24. So the predicted window is: t = 0, t = 1, ..., t = 23.\n",
    "    # If shift = 24, the 24th element after the prediction sequence will be used as label, i.e., will\n",
    "    # be predicted. So, t = 24 is the 1st after the sequence, t = 25 is the second, ... t = 47 is the\n",
    "    # 24th after. If label_with = 1, then the sequence t = 0, t = 1, ..., t = 23 will be used for\n",
    "    # predicting t = 47. Naturally, the total width of the window = 47 in this case.\n",
    "    \n",
    "    # Also, notice that the label is used by the model as the response (predicted) variable.\n",
    "    \n",
    "    # So for a given shift: the sequence of timesteps i, i+1, ... will be used for predicting the\n",
    "    # timestep i + shift\n",
    "    # If a sequence starts in index i, the next sequence will start from i + sequence_stride.\n",
    "    # The sequence will be formed by timesteps i, i + sampling_rate, i + 2* sampling_rate, ...\n",
    "    # Example: Consider indices [0, 1, ... 99]. With sequence_length=10, sampling_rate=2, \n",
    "    # sequence_stride=3, the dataset will yield batches of sequences composed of the following \n",
    "    # indices:\n",
    "    # First sequence:  [0  2  4  6  8 10 12 14 16 18]\n",
    "    # Second sequence: [3  5  7  9 11 13 15 17 19 21]\n",
    "    # Third sequence:  [6  8 10 12 14 16 18 20 22 24]\n",
    "    # ...\n",
    "    # Last sequence:   [78 80 82 84 86 88 90 92 94 96]\n",
    "\n",
    "    # percent_of_data_used_for_model_training: float from 0 to 100,\n",
    "    # representing the percent of data used for training the model\n",
    "    \n",
    "    # If you want to use cross-validation, separate a percent of the training data for validation.\n",
    "    # Declare this percent as percent_of_training_data_used_for_model_validation (float from 0 to 100).\n",
    "    \n",
    "    # If PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 70, and \n",
    "    # PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 10, \n",
    "    # training dataset slice goes from 0 to 0.7 (70%) of the dataset;\n",
    "    # testing slicing goes from 0.7 x dataset to ((1 - 0.1) = 0.9) x dataset\n",
    "    # validation slicing goes from 0.9 x dataset to the end of the dataset.\n",
    "    # Here, consider the time sequence t = 0, t = 1, ... , t = N, for a dataset with length N:\n",
    "    # training: from t = 0 to t = (0.7 x N); testing: from t = ((0.7 x N) + 1) to (0.9 x N);\n",
    "    # validation: from t = ((0.9 x N) + 1) to N (the fractions 0.7 x N and 0.9 x N are rounded to\n",
    "    # the closest integer).\n",
    "    \n",
    "    # use_past_responses_for_prediction: True if the past responses will be used for predicting their\n",
    "    # value in the future; False if you do not want to use them.\n",
    "\n",
    "    \n",
    "    # Create a local copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    # Instantiate an object from WindowGenerator class:\n",
    "    w = WindowGenerator (dataset = DATASET, shift = shift, use_past_responses_for_prediction = use_past_responses_for_prediction, sequence_stride = sequence_stride, sampling_rate = sampling_rate, label_columns = response_columns, train_pct = percent_of_data_used_for_model_training, val_pct = percent_of_training_data_used_for_model_validation)\n",
    "    # Make the tensors:\n",
    "    w = w.make_tensors()\n",
    "    # Retrieve tensors dictionary:\n",
    "    tensors_dict = w.tensors_dict\n",
    "\n",
    "    print(\"Finished preparing the time series datasets for training, testing, and validation. Check their shapes.\\n\")\n",
    "    \n",
    "    for key in tensors_dict.keys():\n",
    "        \n",
    "        print(f\"{key}-tensors obtained:\")\n",
    "        nested_dict = tensors_dict[key]\n",
    "        print(f\"Inputs tensor shape = {nested_dict['inputs'].shape}\")\n",
    "        print(f\"Labels tensor shape = {nested_dict['labels'].shape}\\n\")\n",
    "    \n",
    "    return tensors_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Function for the union of several 1-dimensional tensors (obtained from single columns) into a single tensor**\n",
    "- Each 1-dimensional tensor or array becomes a column from the new tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_1_dim_tensors (list_of_tensors_or_arrays):\n",
    "    \n",
    "    # list of tensors: list containing the 1-dimensional tensors or arrays that the function will union.\n",
    "    # the operation will be performed in the order that the tensors are declared.\n",
    "    # One-dimensional tensors have shape (X,), where X is the number of elements. Example: a column\n",
    "    # of the dataframe with elements 1, 2, 3 in this order may result in an array like array([1, 2, 3])\n",
    "    # and a Tensor with shape (3,). With we union it with the tensor from the column with elements\n",
    "    # 4, 5, 6, the output will be array([[1,4], [2,5], [3,6]]). Alternatively, this new array could\n",
    "    # be converted into a Pandas dataframe where each column would be correspondent to one individual\n",
    "    # tensor.\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    # Convert each element from the list to a numpy array, in case they are tensors:\n",
    "    list_of_arrays = [np.array(tensor) for tensor in list_of_tensors_or_arrays]\n",
    "    \n",
    "    # Now, stack all elements from list_of_arrays into a single array, using the columns' axis\n",
    "    # (axis = 1).\n",
    "    # https://numpy.org/doc/stable/reference/generated/numpy.stack.html\n",
    "    \n",
    "    \"\"\"\n",
    "    Example: suppose a = np.array([1, 2, 3]), b = np.array([4, 5, 6]), c = np.array([7, 8, 9])\n",
    "    If we do np.stack([a,b,c], axis = 1), the resultant will be array([[1, 4, 7],[2, 5, 8],[3, 6, 9]]),\n",
    "    what would be converted into a dataframe where each original tensor would correspond to a column.\n",
    "    \n",
    "    On the other hand, by doing np.stack([a,b,c], axis = 0), the resultant would be array([[1, 2, 3],\n",
    "    [4, 5, 6],[7, 8, 9]]) - in a dataframe originated from this array, each original tensor would\n",
    "    correspond to a row.\n",
    "    \"\"\"\n",
    "    stacked_array = np.stack(list_of_arrays, axis = 1)\n",
    "    \n",
    "    # Finally, convert it to tensor and return it:\n",
    "    tensors_union = tf.constant(stacked_array)\n",
    "    \n",
    "    # Notice that this operation is equivalent to firstly converting all to tensors and then performing:\n",
    "    # tf.stack([a,b,c], axis = 1), where [a, b, c] is a list of tensors a, b, c (substitute it by\n",
    "    # list_of_tensors).\n",
    "    \n",
    "    print(\"Tensors union complete. Check the resulting tensor below:\\n\")\n",
    "    print(tensors_union)\n",
    "    \n",
    "    return tensors_union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b0ab74d3-c6a4-430e-b5e2-38007f2a1d1a"
   },
   "source": [
    "# **Function for making predictions with the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_predictions (model_object, X, dataframe_for_concatenating_predictions = None, column_with_predictions_suffix = None, architecture = None):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    # The function will automatically detect if it is dealing with lists, NumPy arrays\n",
    "    # or Pandas dataframes. If X is a list or a single-dimension array, predict_for\n",
    "    # will be set as 'single_entry'. If X is a multi-dimension NumPy array (as the\n",
    "    # outputs for preparing data - even single_entry - for deep learning models), or if\n",
    "    # it is a Pandas dataframe, the function will set predict_for = 'subset'\n",
    "    \n",
    "    # X = subset of predictive variables (dataframe, NumPy array, or list).\n",
    "    # If PREDICT_FOR = 'single_entry', X should be a list of parameters values.\n",
    "    # e.g. X = [1.2, 3, 4] (dot is the decimal case separator, comma separate values). \n",
    "    # Notice that the list should contain only the numeric values, in the same order of the\n",
    "    # correspondent columns.\n",
    "    # If PREDICT_FOR = 'subset' (prediction for multiple entries), X should be a dataframe \n",
    "    # (subset) or a multi-dimensional NumPy array of the parameters values, as usual.\n",
    "    \n",
    "    # model_object: object containing the model that will be analyzed. e.g.\n",
    "    # model_object = elastic_net_linear_reg_model\n",
    "    \n",
    "    # dataframe_for_concatenating_predictions: if you want to concatenate the predictions\n",
    "    # to a dataframe, pass it here:\n",
    "    # e.g. dataframe_for_concatenating_predictions = df\n",
    "    # If the dataframe must be the same one passed as X, repeat the dataframe object here:\n",
    "    # X = dataset, dataframe_for_concatenating_predictions = dataset.\n",
    "    # Alternatively, if dataframe_for_concatenating_predictions = None, \n",
    "    # the prediction will be returned as a series or NumPy array, depending on the input format.\n",
    "    # Notice that the concatenated predictions will be added as a new column.\n",
    "    \n",
    "    # column_with_predictions_suffix = None. If the predictions are added as a new column\n",
    "    # of the dataframe dataframe_for_concatenating_predictions, you can declare this\n",
    "    # parameter as string with a suffix for identifying the model. If no suffix is added, the new\n",
    "    # column will be named 'y_pred'.\n",
    "    # e.g. column_with_predictions_suffix = '_keras' will create a column named \"y_pred_keras\". This\n",
    "    # parameter is useful when working with multiple models. Always start the suffix with underscore\n",
    "    # \"_\" so that no blank spaces are added; the suffix will not be merged to the column; and there\n",
    "    # will be no confusion with the dot (.) notation for methods, JSON attributes, etc.\n",
    "    \n",
    "    # architecture: some models require inputs in a proper format. Declare here if you are using\n",
    "    # one of these architectures. Example: architecture = 'cnn_lstm' from class tf_models require\n",
    "    # a special reshape before getting predictions. You can keep None or put the name of the\n",
    "    # architecture, if no special reshape is needed.\n",
    "    \n",
    "    \n",
    "    # Check the type of input: if we are predicting the output for a subset (NumPy array reshaped\n",
    "    # for deep learning models or Pandas dataframe); or predicting for a single entry (single-\n",
    "    # dimension NumPy array or Python list).\n",
    "    \n",
    "    # 1. Check if a list was input. Lists do not have the attribute shape, present in dataframes\n",
    "    # and NumPy arrays. Accessing the attribute shape from a list will raise the Exception error\n",
    "    # named AttributeError\n",
    "    # Try to access the attribute shape. If the error AttributeError is raised, it is a list, so\n",
    "    # set predict_for = 'single_entry':\n",
    "    \n",
    "    predict_for = 'subset'\n",
    "    # map if we are dealing with a subset or single entry\n",
    "    \n",
    "    if ((type(X) == list) | (type(X) == tuple)):\n",
    "        # Single entry as list or tuple\n",
    "        # Convert it to NumPy array:\n",
    "        X = np.array(X)\n",
    "    \n",
    "    # Run even if it come from list or tuple:\n",
    "    if ((type(X) == np.ndarray) & (len(X.shape) == 1)):\n",
    "        # If X.shape has len == 1, it is a tuple like (4,)\n",
    "        # Convert the numpy array to the correct shape. It runs even if the list or tuple was\n",
    "        # converted.\n",
    "        X = X.reshape(1, -1)\n",
    "        # generates an array like array([[1, 2, 3, 4]])\n",
    "        # The reshape (-1, 1) generates an array like ([1], [2], ...) with format for the y-vector\n",
    "        # used for training.\n",
    "        \n",
    "        # Update the predict_for variable:\n",
    "        predict_for = 'single_entry'\n",
    "    \n",
    "    # Finally, convert to Tensor:\n",
    "    X = tf.constant(X)\n",
    "    \n",
    "    if (architecture == 'cnn_lstm'):\n",
    "        # Get the hybrid cnn-lstm time series model from class tf_models:\n",
    "        X = (lambda x: tf.constant(((x.numpy()).reshape(x.numpy().shape[0], 2, 2, 1))))(X)\n",
    "     \n",
    "    \n",
    "    # prediction for a subset\n",
    "    y_pred = model_object.predict(X)\n",
    "    print(\"Attention: for classification with Keras/TensorFlow and other deep learning frameworks, this output will not be a class, but an array of probabilities correspondent to the probability that the entry belongs to each class. In this case, it is better to use the function calculate_class_probability below, setting model_type == \\'deep_learning\\'. This function will result into dataframes containing the classes as columns and the probabilities in the respective row.\\n\")\n",
    "    print(\"The output class from the deep learning model is the class with higher probability indicated by the predict method. Again, the order of classes is the order they appear in the training dataset. For instance, when using the ImageDataGenerator, the 1st class is the name of the 1st read directory, the 2nd class is the 2nd directory, and so on.\\n\")\n",
    "        \n",
    "    # If y_pred came from a RNN with the parameter return_sequences = True and/or\n",
    "    # return_states = True, then the hidden and/or cell states from the LSTMs\n",
    "    # were returned. So, the returned array has at least one extra dimensions (two\n",
    "    # if both parameters are True). On the other hand, we want only the first dimension,\n",
    "    # correspondent to the actual output.\n",
    "        \n",
    "    # Remember that, due to the reshapes for preparing data for deep learning models,\n",
    "    # y_pred must have at least 2 dimensions: (N, 1), where N is the number of rows of\n",
    "    # the original dataset. But y_pred returned from a model with return_sequences = True\n",
    "    # or return_states = True will be of dimension (N, N, 1). If both parameters are True,\n",
    "    # the dimension is (N, N, N, 1), since there are extra arrays for both the hidden and\n",
    "    # cell states.\n",
    "        \n",
    "    # The conclusion is that there is a third dimension only for models where return_sequences\n",
    "    # = True or return_states = True\n",
    "        \n",
    "    if (len(y_pred.shape) > 2):\n",
    "                \n",
    "        # The shape is a tuple containing 3 or more dimensions\n",
    "        # If we could access the third_dimension, than return_states and\n",
    "        # or return_sequences = True\n",
    "\n",
    "        # We want only the values stored as the 1st dimension\n",
    "        # y_pred is an array where each element is an array with two elements. \n",
    "        # To get only the first elements:\n",
    "        # (slice the arrays: get all values only for dimension 0, the 1st dim):\n",
    "        y_pred = y_pred[:,0]\n",
    "        # if we used y_pred[:,1] we would get the second element, \n",
    "        # which is the hidden state h (input of the next LSTM unit).\n",
    "        # It happens because of the parameter return_sequences = True. \n",
    "        # If return_states = True, there would be a third element, corresponding \n",
    "        # to the cell state c.\n",
    "        # Notice that we want only the 1st dimension (0), no matter the case.\n",
    "        \n",
    "    # Check if there is a dataframe to concatenate the predictions\n",
    "    if not (dataframe_for_concatenating_predictions is None):\n",
    "            \n",
    "        # there is a dataframe for concatenating the predictions    \n",
    "        # concatenate the predicted values with dataframe_for_concatenating_predictions.\n",
    "        # Add the predicted values as a column:\n",
    "        \n",
    "        # check if there is a suffix:\n",
    "        if not (column_with_predictions_suffix is None):\n",
    "            # There is a suffix declared\n",
    "            # Since there is a suffix, concatenate it to 'y_pred':\n",
    "            col_name = \"y_pred\" + column_with_predictions_suffix\n",
    "            \n",
    "        else:\n",
    "            # Create the column name as the standard.\n",
    "            # The name of the new column is simply 'y_pred'\n",
    "            col_name = \"y_pred\"\n",
    "            \n",
    "        # Set a local copy of the dataframe to manipulate:\n",
    "        X_copy = dataframe_for_concatenating_predictions.copy(deep = True)\n",
    "            \n",
    "        # Add the predictions as the new column named col_name:\n",
    "        # If y is a tensor, convert to NumPy array before adding. The numpy.array function\n",
    "        # has no effect in numpy arrays, but is equivalent to the .numpy method for tensors\n",
    "        X_copy[col_name] = np.array(y_pred)\n",
    "            \n",
    "        print(f\"The prediction was added as the new column {col_name} of the dataframe, and this dataframe was returned. Check its 10 first rows:\\n\")\n",
    "        try:\n",
    "            # only works in Jupyter Notebook:\n",
    "            from IPython.display import display\n",
    "            display(X_copy.head(10))\n",
    "                    \n",
    "        except: # regular mode\n",
    "            print(X_copy.head(10))\n",
    "            \n",
    "        return X_copy\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        if (predict_for == 'single_entry'):\n",
    "            \n",
    "            print(\"Making prediction for a single entry X.\\n\")\n",
    "            print(f\"Output value predicted for the entry parameters = {y_pred}\\n\")    \n",
    "            print(\"Returning only the predicted value.\")\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            print(\"Returning only the predicted values. Check the 10 first values of the series:\\n\")\n",
    "            print(y_pred[:10]) # slice until 10th element from the series or list\n",
    "            # dataset[:,10]: all rows for column 10 of dataset\n",
    "            # dataset[1,:] - slice of all rows for row 1 of dataset.\n",
    "            \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "9cafee20-5a5d-4d99-95a1-e3869eefcad5"
   },
   "source": [
    "# **Function for calculating probabilities associated to each class**\n",
    "- Set the list_of_classes as the input of this function.\n",
    "- The predictions (outputs) from deep learning models (e.g. Keras/TensorFlow models) are themselves the probabilities associated to each possible class.\n",
    "    - For Scikit-learn and XGBoost, we must use a specific method for retrieving the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_probability (model_object, X, list_of_classes, type_of_model = 'other', dataframe_for_concatenating_predictions = None, architecture = None):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    # predict_for = 'subset' or predict_for = 'single_entry'\n",
    "    # The function will automatically detect if it is dealing with lists, NumPy arrays\n",
    "    # or Pandas dataframes. If X is a list or a single-dimension array, predict_for\n",
    "    # will be set as 'single_entry'. If X is a multi-dimension NumPy array (as the\n",
    "    # outputs for preparing data - even single_entry - for deep learning models), or if\n",
    "    # it is a Pandas dataframe, the function will set predict_for = 'subset'\n",
    "    \n",
    "    # X = subset of predictive variables (dataframe, NumPy array, or list).\n",
    "    # If PREDICT_FOR = 'single_entry', X should be a list of parameters values.\n",
    "    # e.g. X = [1.2, 3, 4] (dot is the decimal case separator, comma separate values). \n",
    "    # Notice that the list should contain only the numeric values, in the same order of the\n",
    "    # correspondent columns.\n",
    "    # If PREDICT_FOR = 'subset' (prediction for multiple entries), X should be a dataframe \n",
    "    # (subset) or a multi-dimensional NumPy array of the parameters values, as usual.\n",
    "    \n",
    "    # model_object: object containing the model that will be analyzed. e.g.\n",
    "    # model_object = elastic_net_linear_reg_model\n",
    "    \n",
    "    # list_of_classes is the list of classes effectively used for training\n",
    "    # the model. Set this parameter as the object returned from function\n",
    "    # retrieve_classes_used_to_train\n",
    "    \n",
    "    # type_of_model = 'other' or type_of_model = 'deep_learning'\n",
    "    \n",
    "    # Notice that the output will be an array of probabilities, where each\n",
    "    # element corresponds to a possible class, in the order classes appear.\n",
    "    \n",
    "    # dataframe_for_concatenating_predictions: if you want to concatenate the predictions\n",
    "    # to a dataframe, pass it here:\n",
    "    # e.g. dataframe_for_concatenating_predictions = df\n",
    "    # If the dataframe must be the same one passed as X, repeat the dataframe object here:\n",
    "    # X = dataset, dataframe_for_concatenating_predictions = dataset.\n",
    "    # Alternatively, if dataframe_for_concatenating_predictions = None, \n",
    "    # the prediction will be returned as a series or NumPy array, depending on the input format.\n",
    "    # Notice that the concatenated predictions will be added as a new column.\n",
    "    \n",
    "    # architecture: some models require inputs in a proper format. Declare here if you are using\n",
    "    # one of these architectures. Example: architecture = 'cnn_lstm' from class tf_models require\n",
    "    # a special reshape before getting predictions. You can keep None or put the name of the\n",
    "    # architecture, if no special reshape is needed.\n",
    "    \n",
    "    \n",
    "    # All of the new columns (appended or not) will have the prefix \"prob_class_\" followed\n",
    "    # by the correspondent class name to identify them.\n",
    "    \n",
    "       \n",
    "    # 1. Check if a list was input. Lists do not have the attribute shape, present in dataframes\n",
    "    # and NumPy arrays. Accessing the attribute shape from a list will raise the Exception error\n",
    "    # named AttributeError\n",
    "    # Try to access the attribute shape. If the error AttributeError is raised, it is a list, so\n",
    "    # set predict_for = 'single_entry':\n",
    "    \n",
    "    predict_for = 'subset'\n",
    "    # map if we are dealing with a subset or single entry\n",
    "    \n",
    "    if ((type(X) == list) | (type(X) == tuple)):\n",
    "        # Single entry as list or tuple\n",
    "        # Convert it to NumPy array:\n",
    "        X = np.array(X)\n",
    "    \n",
    "    # Run even if it come from list or tuple:\n",
    "    if ((type(X) == np.ndarray) & (len(X.shape) == 1)):\n",
    "        # If X.shape has len == 1, it is a tuple like (4,)\n",
    "        # Convert the numpy array to the correct shape. It runs even if the list or tuple was\n",
    "        # converted.\n",
    "        X = X.reshape(1, -1)\n",
    "        # generates an array like array([[1, 2, 3, 4]])\n",
    "        # The reshape (-1, 1) generates an array like ([1], [2], ...) with format for the y-vector\n",
    "        # used for training.\n",
    "        \n",
    "        # Update the predict_for variable:\n",
    "        predict_for = 'single_entry'      \n",
    "    \n",
    "    # Finally, convert to Tensor:\n",
    "    X = tf.constant(X)\n",
    "    \n",
    "    if (architecture == 'cnn_lstm'):\n",
    "        # Get the hybrid cnn-lstm time series model from class tf_models:\n",
    "        X = (lambda x: tf.constant(((x.numpy()).reshape(x.numpy().shape[0], 2, 2, 1))))(X)\n",
    "    \n",
    "        \n",
    "    # Check if it is a keras or other deep learning framework; or if it is a sklearn or xgb model:\n",
    "    boolean_check = (type_of_model == 'deep_learning')\n",
    "    \n",
    "    if (boolean_check): # run if it is True\n",
    "        print(\"The predictions (outputs) from deep learning models are themselves the probabilities associated to each possible class.\")\n",
    "        print(\"\\n\") #line break\n",
    "        print(\"The output will be an array of float values: each float represents the probability of one class, in the order the classes appear. For a binary classifier, the first element will correspond to class 0; and the second element will be the probability of class 1.\")\n",
    "    \n",
    "    \n",
    "    if (predict_for == 'single_entry'):\n",
    "        \n",
    "        print(\"Calculating probabilities for a single entry X.\\n\")\n",
    "       \n",
    "        if (boolean_check): \n",
    "            # Use the predict method itself for deep learning models.\n",
    "            # These models do not have the predict_proba method.\n",
    "            # Their output is itself the probability for each class.\n",
    "            y_pred_probabilities = model_object.predict(X)\n",
    "        \n",
    "        else:\n",
    "            # use the predict_proba method from sklearn and xgboost:\n",
    "            y_pred_probabilities = model_object.predict_proba(X.numpy())\n",
    "        \n",
    "        print(\"Probabilities calculated using the entry parameters.\") \n",
    "        print(f\"Probabilities calculated for each one of the classes {list_of_classes} (in the order of classes) = {y_pred_probabilities}\\n\")\n",
    "        \n",
    "        # create a dictionary with the possible classes and the correspondent probabilities:\n",
    "        # Use the list attribute to guarantee that the probabilities are\n",
    "        # retrieved as a list:\n",
    "        probability_dict = {'class': list_of_classes,\n",
    "                            'probability': list(y_pred_probabilities)}\n",
    "            \n",
    "        # Convert it to a Pandas dataframe:\n",
    "        probabilities_df = pd.DataFrame(data = probability_dict)\n",
    "            \n",
    "        print(\"Returning a dataframe containing the classes and the probabilities calculated for the entry to belong to each class. Check it below:\")\n",
    "        try:\n",
    "            # only works in Jupyter Notebook:\n",
    "            from IPython.display import display\n",
    "            display(probabilities_df)\n",
    "                \n",
    "        except: # regular mode\n",
    "            print(probabilities_df)\n",
    "            \n",
    "        return probabilities_df\n",
    "    \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # prediction for a subset\n",
    "        \n",
    "        if (boolean_check): \n",
    "            # Use the predict method itself for deep learning models.\n",
    "            # These models do not have the predict_proba method.\n",
    "            # Their output is itself the probability for each class.\n",
    "            y_pred_probabilities = model_object.predict(X)\n",
    "            \n",
    "            # If y_pred_probabilities came from a RNN with the parameter return_sequences = True \n",
    "            # and/or return_states = True, then the hidden and/or cell states from the LSTMs\n",
    "            # were returned. So, the returned array has at least one extra dimensions (two\n",
    "            # if both parameters are True). On the other hand, we want only the first dimension,\n",
    "            # correspondent to the actual output.\n",
    "\n",
    "            # Remember that, due to the reshapes for preparing data for deep learning models,\n",
    "            # y_pred_probabilities must have at least 2 dimensions: (N, 1), where N is the number \n",
    "            # of rows of the original dataset. But y_pred_probabilities returned from a model \n",
    "            # with return_sequences = True or return_states = True will be of dimension (N, N, 1). \n",
    "            # If both parameters are True, the dimension is (N, N, N, 1), since there are extra \n",
    "            # arrays for both the hidden and cell states.\n",
    "\n",
    "            # The conclusion is that there is a third dimension only for models where \n",
    "            # return_sequences = True or return_states = True\n",
    "\n",
    "            # Check if y_pred_probabilities is a numpy array, instead of a Pandas dataframe:\n",
    "\n",
    "            if (len(y_pred_probabilities.shape) > 2):\n",
    "                \n",
    "                # The shape is a tuple containing 3 or more dimensions\n",
    "                # If we could access the third_dimension, than return_states and\n",
    "                # or return_sequences = True\n",
    "\n",
    "                # We want only the values stored as the 1st dimension\n",
    "                # y_pred_probabilities is an array where each element is an array with \n",
    "                # two elements. To get only the first elements:\n",
    "                # (slice the arrays: get all values only for dimension 0, the 1st dim):\n",
    "                y_pred_probabilities = y_pred_probabilities[:,0]\n",
    "                # if we used y_pred_probabilities[:,1] we would get the second element, \n",
    "                # which is the hidden state h (input of the next LSTM unit).\n",
    "                # It happens because of the parameter return_sequences = True. \n",
    "                # If return_states = True, there would be a third element, corresponding \n",
    "                # to the cell state c.\n",
    "                # Notice that we want only the 1st dimension (0), no matter the case.\n",
    "        \n",
    "        else:\n",
    "            # use the predict_proba method from sklearn and xgboost:\n",
    "            y_pred_probabilities = model_object.predict_proba(X.numpy())\n",
    "        \n",
    "        # y_pred_probabilities is a column containing arrays of probabilities\n",
    "        # Let's create a dataframe separating each element of the array into\n",
    "        # a separate column\n",
    "        \n",
    "        # Get the size of each array. It is the total of elements from\n",
    "        # list_of_classes (total of possible classes):\n",
    "        total_of_classes = len(list_of_classes)\n",
    "        \n",
    "        # Starts a dictionary. This dictionary will have the class as the\n",
    "        # key and a list of the probabilities that the element belong to that\n",
    "        # class as the value (in the dataframe, the class will be column,\n",
    "        # with its calculated probability in each row):\n",
    "        probability_dict = {}\n",
    "        \n",
    "        # Loop through each possible class:\n",
    "        for class_name in list_of_classes:\n",
    "            \n",
    "            # Let's concatenate the prefix \"prob_class_\" to this strings.\n",
    "            # This string will be used as column name, so it will be clear \n",
    "            # in the output dataframe that the column is referrent to the \n",
    "            # probability calculated for the class. Since the elements may \n",
    "            # have been saved as numbers use the str attribute to guarantee \n",
    "            # that the element was read as a string, and concatenate the\n",
    "            # prefix to its left:\n",
    "            class_name = \"prob_class_\" + str(class_name)\n",
    "            # Get the index in the list:\n",
    "            class_index = list_of_class.index(class_name)\n",
    "            \n",
    "            # Start a list of probabilities:\n",
    "            prob_list = []\n",
    "            \n",
    "            # Now loop through each row j from the dataframe\n",
    "            # to retrieve the array in the column y_pred_probabilities:\n",
    "            \n",
    "            for i in range(len(y_pred_probabilities)):\n",
    "                # goes from j = 0 (first row of the dataframe) to\n",
    "                # j = y_pred_probabilities - 1, index of the last row\n",
    "                # Get the array of probabilities for that row:\n",
    "                # If y is a tensor, convert to NumPy array before adding. The numpy.array function\n",
    "                # has no effect in numpy arrays, but is equivalent to the .numpy method for tensors\n",
    "                prob_array = np.array(y_pred_probabilities[i])\n",
    "                \n",
    "                # Append the (class_index)-th element of that array in prob_list\n",
    "                # The (class_index)-th position of the array is the probability\n",
    "                # of the class being analyzed in the i-th iteration of\n",
    "                # the main loop\n",
    "                prob_list.append(prob_array[(class_index)])\n",
    "            \n",
    "            # Now that the probabilities for the class correspondent to\n",
    "            # each row were retrieved as the list prob_list, update the\n",
    "            # dictionary. Use the class name saved as class_name as the\n",
    "            # key, and put the prob_list as the correspondent value:\n",
    "            probability_dict[class_name] = prob_list\n",
    "        \n",
    "        # Now that we finished the loop, the probability dictionary contains\n",
    "        # each one of the classes as its keys, and the list of probabilities\n",
    "        # for each row as the correspondent values. \n",
    "        # Also, the keys are identified with the prefix 'prob_class' to\n",
    "        # indicate that they are referrent to the probability of belonging to\n",
    "        # one class. Let's convert this dictionary to a Pandas dataframe:\n",
    "        \n",
    "        probabilities_df = pd.DataFrame(data = probability_dict)\n",
    "        \n",
    "        # Check if there is a dataframe to concatenate the predictions\n",
    "        if not (dataframe_for_concatenating_predictions is None):\n",
    "            \n",
    "            # there is a dataframe for concatenating the predictions.\n",
    "            \n",
    "            # Set a local copy of the dataframe to manipulate:\n",
    "            X_copy = X.copy(deep = True)\n",
    "            \n",
    "            # Append the columns from probabilities_df with Pandas concat\n",
    "            # method, setting axis = 1 (axis = 0  appends rows)\n",
    "            # Use the pandas 'inner' join, which removes entries without\n",
    "            # correspondence. It is the same strategy used for concatenating\n",
    "            # the dataframe obtained from One-Hot Encoding transformation in the\n",
    "            # ETL Workflow (3_Dataset_Transformation)\n",
    "            X_copy = pd.concat([X_copy, probabilities_df], axis = 1, join = \"inner\")\n",
    "      \n",
    "            print(f\"The dataframe X was concatenated to the probabilities calculated for each class and returned. Check its first 10 entries:\\n\")\n",
    "            try:\n",
    "                # only works in Jupyter Notebook:\n",
    "                from IPython.display import display\n",
    "                display(X_copy.head(10))\n",
    "                    \n",
    "            except: # regular mode\n",
    "                print(X_copy.head(10))\n",
    "            \n",
    "            return X_copy\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            print(\"Returning only the dataframe with the probabilities calculated for each class. Check its first 10 entries:\\n\")\n",
    "            try:\n",
    "                # only works in Jupyter Notebook:\n",
    "                from IPython.display import display\n",
    "                display(probabilities_df.head(10))\n",
    "                    \n",
    "            except: # regular mode\n",
    "                print(probabilities_df.head(10))\n",
    "            \n",
    "            return probabilities_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c1e2da99-8017-4077-ab25-5b865161e86b"
   },
   "source": [
    "# **Function for merging (joining) dataframes on given keys; and sorting the merged table**\n",
    "- Merge (join) types:\n",
    "    - 'inner': resultant dataframe contains only the rows on the left dataframe with correspondent values on the right dataframe. Can be used for filtering a set of labelled rows. Results in no missing values;\n",
    "    - 'left': resultant dataframe contains all the rows from the left table (even those without correspondence on the right); and the rows from the right table that have correspondence on the left one. Since rows from the left table may not have correspondence, it may result in missing values.\n",
    "    - 'right': resultant dataframe contains all the rows from the right table (even those without correspondence on the right); and the rows from the left table that have correspondence on the right one. Since rows from the right table may not have correspondence, it may result in missing values.\n",
    "    - 'outer': in SQL, the Pandas 'outer' merge usually corresponds to the FULL OUTER JOIN: the resultant dataframe contains all rows from both tables, not taking in account if there is correspondence. So, it may result in missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a22ec301-56e3-4bb1-af63-9411f54223cb",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def MERGE_AND_SORT_DATAFRAMES (df_left, df_right, left_key, right_key, how_to_join = \"inner\", merged_suffixes = ('_left', '_right'), sort_merged_df = False, column_to_sort = None, ascending_sorting = True):\n",
    "    \n",
    "    #WARNING: Only two dataframes can be merged on each call of the function.\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # df_left: dataframe to be joined as the left one.\n",
    "    \n",
    "    # df_right: dataframe to be joined as the right one\n",
    "    \n",
    "    # left_key: (String) name of column of the left dataframe to be used as key for joining.\n",
    "    \n",
    "    # right_key: (String) name of column of the right dataframe to be used as key for joining.\n",
    "    \n",
    "    # how_to_join: joining method: \"inner\", \"outer\", \"left\", \"right\". The default is \"inner\".\n",
    "    \n",
    "    # merge_method: which pandas merging method will be applied:\n",
    "    # merge_method = 'ordered' for using the .merge_ordered method.\n",
    "    # merge_method = \"asof\" for using the .merge_asof method.\n",
    "    # WARNING: .merge_asof uses fuzzy matching, so the how_to_join parameter is not applicable.\n",
    "    \n",
    "    # merged_suffixes = ('_left', '_right') - tuple of the suffixes to be added to columns\n",
    "    # with equal names. Simply modify the strings inside quotes to modify the standard\n",
    "    # values. If no tuple is provided, the standard denomination will be used.\n",
    "    \n",
    "    # sort_merged_df = False not to sort the merged dataframe. If you want to sort it,\n",
    "    # set as True. If sort_merged_df = True and column_to_sort = None, the dataframe will\n",
    "    # be sorted by its first column.\n",
    "    \n",
    "    # column_to_sort = None. Keep it None if the dataframe should not be sorted.\n",
    "    # Alternatively, pass a string with a column name to sort, such as:\n",
    "    # column_to_sort = 'col1'; or a list of columns to use for sorting: column_to_sort = \n",
    "    # ['col1', 'col2']\n",
    "    \n",
    "    # ascending_sorting = True. If you want to sort the column(s) passed on column_to_sort in\n",
    "    # ascending order, set as True. Set as False if you want to sort in descending order. If\n",
    "    # you want to sort each column passed as list column_to_sort in a specific order, pass a \n",
    "    # list of booleans like ascending_sorting = [False, True] - the first column of the list\n",
    "    # will be sorted in descending order, whereas the 2nd will be in ascending. Notice that\n",
    "    # the correspondence is element-wise: the boolean in list ascending_sorting will correspond \n",
    "    # to the sorting order of the column with the same position in list column_to_sort.\n",
    "    # If None, the dataframe will be sorted in ascending order.\n",
    "    \n",
    "    # Create dataframe local copies to manipulate, avoiding that Pandas operates on\n",
    "    # the original objects; or that Pandas tries to set values on slices or copies,\n",
    "    # resulting in unpredictable results.\n",
    "    # Use the copy method to effectively create a second object with the same properties\n",
    "    # of the input parameters, but completely independent from it.\n",
    "    DF_LEFT = df_left.copy(deep = True)\n",
    "    DF_RIGHT = df_right.copy(deep = True)\n",
    "    \n",
    "    # check if the keys are the same:\n",
    "    boolean_check = (left_key == right_key)\n",
    "    # if boolean_check is True, we will merge using the on parameter, instead of left_on and right_on:\n",
    "    \n",
    "    if (boolean_check): # runs if it is True:\n",
    "        \n",
    "        merged_df = DF_LEFT.merge(DF_RIGHT, on = left_key, how = how_to_join, suffixes = merged_suffixes)\n",
    "    \n",
    "    else:\n",
    "        # use left_on and right_on\n",
    "        merged_df = DF_LEFT.merge(DF_RIGHT, left_on = left_key, right_on = right_key, how = how_to_join, suffixes = merged_suffixes)\n",
    "    \n",
    "    # Check if the dataframe should be sorted:\n",
    "    if (sort_merged_df == True):\n",
    "        \n",
    "        # check if column_to_sort = None. If it is, set it as the first column (index 0):\n",
    "        if (column_to_sort is None):\n",
    "            \n",
    "            column_to_sort = merged_df.columns[0]\n",
    "            print(f\"Sorting merged dataframe by its first column = {column_to_sort}\\n\")\n",
    "        \n",
    "        # check if ascending_sorting is None. If it is, set it as True:\n",
    "        if (ascending_sorting is None):\n",
    "            \n",
    "            ascending_sorting = True\n",
    "            print(\"Sorting merged dataframe in ascending order.\\n\")\n",
    "        \n",
    "        # Now, sort the dataframe according to the parameters:\n",
    "        merged_df = merged_df.sort_values(by = column_to_sort, ascending = ascending_sorting)\n",
    "        #sort by the first column, with index 0.\n",
    "    \n",
    "        # Now, reset index positions:\n",
    "        merged_df = merged_df.reset_index(drop = True)\n",
    "        print(\"Merged dataframe successfully sorted.\\n\")\n",
    "    \n",
    "    # Pandas .head(Y) method results in a dataframe containing the first Y rows of the \n",
    "    # original dataframe. The default .head() is Y = 5. Print first 10 rows of the \n",
    "    # new dataframe:\n",
    "    print(\"Dataframe successfully merged. Check its 10 first rows:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(merged_df.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(merged_df.head(10))\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "93e16842-8a32-47ba-bd5a-7f42750f9dda"
   },
   "source": [
    "# **Function for concatenating (SQL UNION) multiple dataframes**\n",
    "- Vertical concatenation of the dataframes.\n",
    "- Equivalent to SQL Union: vertical stack/append of the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a834dd42-1251-4791-ac8b-08e6312dcf05",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def UNION_DATAFRAMES (list_of_dataframes, what_to_append = 'rows', ignore_index_on_union = True, sort_values_on_union = True, union_join_type = None):\n",
    "    \n",
    "    import pandas as pd\n",
    "    #JOIN can be 'inner' to perform an inner join, eliminating the missing values\n",
    "    #The default (None) is 'outer': the dataframes will be stacked on the columns with\n",
    "    #same names but, in case there is no correspondence, the row will present a missing\n",
    "    #value for the columns which are not present in one of the dataframes.\n",
    "    #When using the 'inner' method, only the common columns will remain\n",
    "    \n",
    "    #list_of_dataframes must be a list containing the dataframe objects\n",
    "    # example: list_of_dataframes = [df1, df2, df3, df4]\n",
    "    #Notice that the dataframes are objects, not strings. Therefore, they should not\n",
    "    # be declared inside quotes.\n",
    "    # There is no limit of dataframes. In this example, we will concatenate 4 dataframes.\n",
    "    # If list_of_dataframes = [df1, df2, df3] we would concatenate 3, and if\n",
    "    # list_of_dataframes = [df1, df2, df3, df4, df5] we would concatenate 5 dataframes.\n",
    "    \n",
    "    # what_to_append = 'rows' for appending the rows from one dataframe\n",
    "    # into the other; what_to_append = 'columns' for appending the columns\n",
    "    # from one dataframe into the other (horizontal or lateral append).\n",
    "    \n",
    "    # When what_to_append = 'rows', Pandas .concat method is defined as\n",
    "    # axis = 0, i.e., the operation occurs in the row level, so the rows\n",
    "    # of the second dataframe are added to the bottom of the first one.\n",
    "    # It is the SQL union, and creates a dataframe with more rows, and\n",
    "    # total of columns equals to the total of columns of the first dataframe\n",
    "    # plus the columns of the second one that were not in the first dataframe.\n",
    "    # When what_to_append = 'columns', Pandas .concat method is defined as\n",
    "    # axis = 1, i.e., the operation occurs in the column level: the two\n",
    "    # dataframes are laterally merged using the index as the key, \n",
    "    # preserving all columns from both dataframes. Therefore, the number of\n",
    "    # rows will be the total of rows of the dataframe with more entries,\n",
    "    # and the total of columns will be the sum of the total of columns of\n",
    "    # the first dataframe with the total of columns of the second dataframe.\n",
    "    \n",
    "    #The other parameters are the same from Pandas .concat method.\n",
    "    # ignore_index_on_union = ignore_index;\n",
    "    # sort_values_on_union = sort\n",
    "    # union_join_type = join\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\n",
    "    \n",
    "    #Check Datacamp course Joining Data with pandas, Chap.3, \n",
    "    # Advanced Merging and Concatenating\n",
    "    \n",
    "    # Create dataframe local copies to manipulate, avoiding that Pandas operates on\n",
    "    # the original objects; or that Pandas tries to set values on slices or copies,\n",
    "    # resulting in unpredictable results.\n",
    "    # Use the copy method to effectively create a second object with the same properties\n",
    "    # of the input parameters, but completely independent from it.\n",
    "    \n",
    "    # Start a list of copied dataframes:\n",
    "    LIST_OF_DATAFRAMES = []\n",
    "    \n",
    "    # Loop through each element from list_of_dataframes:\n",
    "    for dataframe in list_of_dataframes:\n",
    "        \n",
    "        # create a copy of the object:\n",
    "        copied_df = dataframe.copy(deep = True)\n",
    "        # Append this element to the LIST_OF_DATAFRAMES:\n",
    "        LIST_OF_DATAFRAMES.append(copied_df)\n",
    "    \n",
    "    # Check axis:\n",
    "    if (what_to_append == 'rows'):\n",
    "        \n",
    "        AXIS = 0\n",
    "    \n",
    "    elif (what_to_append == 'columns'):\n",
    "        \n",
    "        AXIS = 1\n",
    "        \n",
    "        # In this case, we must save a list of columns of each one of the dataframes, containing\n",
    "        # the different column names observed. That is because the concat method eliminates the\n",
    "        # original column names when AXIS = 1\n",
    "        # We can start the LIST_OF_COLUMNS as the columns from the first object on the\n",
    "        # LIST_OF_DATAFRAMES, eliminating one iteration cycle. Since the columns method generates\n",
    "        # an array, we use the list attribute to convert the array to a regular list:\n",
    "        \n",
    "        i = 0\n",
    "        analyzed_df = LIST_OF_DATAFRAMES[i]\n",
    "        LIST_OF_COLUMNS = list(analyzed_df.columns)\n",
    "        \n",
    "        # Now, loop through each other element on LIST_OF_DATAFRAMES. Since index 0 was already\n",
    "        # considered, start from index 1:\n",
    "        for i in range (1, len(LIST_OF_DATAFRAMES)):\n",
    "            \n",
    "            analyzed_df = LIST_OF_DATAFRAMES[i]\n",
    "            \n",
    "            # Now, loop through each column, named 'col', from the list of columns of analyzed_df:\n",
    "            for col in list(analyzed_df.columns):\n",
    "                \n",
    "                # If 'col' is not in LIST_OF_COLUMNS, append it to the list with its current name.\n",
    "                # The order of the columns on the concatenated dataframe will be the same (the order\n",
    "                # they appear):\n",
    "                if not (col in LIST_OF_COLUMNS):\n",
    "                    LIST_OF_COLUMNS.append(col)\n",
    "                \n",
    "                else:\n",
    "                    # There is already a column with this name. So, append col with a suffix:\n",
    "                    LIST_OF_COLUMNS.append(col + \"_df_\" + str(i))\n",
    "                    \n",
    "        # Now, we have a list of all column names, that we will use for retrieving the headers after\n",
    "        # concatenation.\n",
    "    \n",
    "    else:\n",
    "        print(\"No valid string was input to what_to_append, so appending rows (vertical append, equivalent to SQL UNION).\\n\")\n",
    "        AXIS = 0\n",
    "    \n",
    "    if (union_join_type == 'inner'):\n",
    "        \n",
    "        print(\"Warning: concatenating dataframes using the \\'inner\\' join method, that removes missing values.\\n\")\n",
    "        concat_df = pd.concat(LIST_OF_DATAFRAMES, axis = AXIS, ignore_index = ignore_index_on_union, sort = sort_values_on_union, join = union_join_type)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #In case None or an invalid value is provided, use the default 'outer', by simply\n",
    "        # not declaring the 'join':\n",
    "        concat_df = pd.concat(LIST_OF_DATAFRAMES, axis = AXIS, ignore_index = ignore_index_on_union, sort = sort_values_on_union)\n",
    "    \n",
    "    if (AXIS == 1):\n",
    "        # If we concatentated columns, we lost the columns' names (headers). So, use the list\n",
    "        # LIST_OF_COLUMNS as the new headers for this case:\n",
    "        concat_df.columns = LIST_OF_COLUMNS\n",
    "    \n",
    "    # Pandas .head(Y) method results in a dataframe containing the first Y rows of the \n",
    "    # original dataframe. The default .head() is Y = 5. Print first 10 rows of the \n",
    "    # new dataframe:\n",
    "    print(\"Dataframes successfully concatenated. Check the 10 first rows of new dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(concat_df.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(concat_df.head(10))\n",
    "    \n",
    "    #Now return the concatenated dataframe:\n",
    "    \n",
    "    return concat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d482dc7d-bfee-4682-874a-db703b85c7d9"
   },
   "source": [
    "# **Function for plotting the bar chart**\n",
    "- Bars may be vertically or horizontally oriented.\n",
    "- Bar charts are plotted after selecting an aggregation function, and the cumulative percent curve may be obtained and plotted with the bars (in secondary axis).\n",
    "- To obtain a **Pareto chart**, keep `aggregate_function = 'sum'`, `plot_cumulative_percent = True`, and `orientation = 'vertical'`.\n",
    "- For obtaining the **data distribution of categorical variables**, select any numeric column as the response, and set `aggregate_function = 'count'`. You can also set `plot_cumulative_percent = True` to compare the frequencies of each possible value.\n",
    "\n",
    "### Use this function for obtaining the statistical distributions for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "0a998b18-2bd4-48c3-887c-21cbf944fe8d",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def bar_chart (df, categorical_var_name, response_var_name, aggregate_function = 'sum', add_suffix_to_aggregated_col = True, suffix = None, calculate_and_plot_cumulative_percent = True, orientation = 'vertical', limit_of_plotted_categories = None, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, x_axis_rotation = 70, y_axis_rotation = 0, grid = True, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy import stats\n",
    "    \n",
    "    # df: dataframe being analyzed\n",
    "    \n",
    "    # categorical_var_name: string (inside quotes) containing the name \n",
    "    # of the column to be analyzed. e.g. \n",
    "    # categorical_var_name = \"column1\"\n",
    "    \n",
    "    # response_var_name: string (inside quotes) containing the name \n",
    "    # of the column that stores the response correspondent to the\n",
    "    # categories. e.g. response_var_name = \"response_feature\" \n",
    "    \n",
    "    # aggregate_function = 'sum': String defining the aggregation \n",
    "    # method that will be applied. Possible values:\n",
    "    # 'median', 'mean', 'mode', 'sum', 'min', 'max', 'variance', 'count',\n",
    "    # 'standard_deviation', '10_percent_quantile', '20_percent_quantile',\n",
    "    # '25_percent_quantile', '30_percent_quantile', '40_percent_quantile',\n",
    "    # '50_percent_quantile', '60_percent_quantile', '70_percent_quantile',\n",
    "    # '75_percent_quantile', '80_percent_quantile', '90_percent_quantile',\n",
    "    # '95_percent_quantile', 'kurtosis', 'skew', 'interquartile_range',\n",
    "    # 'mean_standard_error', 'entropy'\n",
    "    # To use another aggregate function, you can use the .agg method, passing \n",
    "    # the aggregate as argument, such as in:\n",
    "    # .agg(scipy.stats.mode), \n",
    "    # where the argument is a Scipy aggregate function.\n",
    "    # If None or an invalid function is input, 'sum' will be used.\n",
    "    \n",
    "    # add_suffix_to_aggregated_col = True will add a suffix to the\n",
    "    # aggregated column. e.g. 'responseVar_mean'. If add_suffix_to_aggregated_col \n",
    "    # = False, the aggregated column will have the original column name.\n",
    "    \n",
    "    # suffix = None. Keep it None if no suffix should be added, or if\n",
    "    # the name of the aggregate function should be used as suffix, after\n",
    "    # \"_\". Alternatively, set it as a string. As recommendation, put the\n",
    "    # \"_\" sign in the beginning of this string to separate the suffix from\n",
    "    # the original column name. e.g. if the response variable is 'Y' and\n",
    "    # suffix = '_agg', the new aggregated column will be named as 'Y_agg'\n",
    "    \n",
    "    # calculate_and_plot_cumulative_percent = True to calculate and plot\n",
    "    # the line of cumulative percent, or \n",
    "    # calculate_and_plot_cumulative_percent = False to omit it.\n",
    "    # This feature is only shown when aggregate_function = 'sum', 'median',\n",
    "    # 'mean', or 'mode'. So, it will be automatically set as False if \n",
    "    # another aggregate is selected.\n",
    "    \n",
    "    # orientation = 'vertical' is the standard, and plots vertical bars\n",
    "    # (perpendicular to the X axis). In this case, the categories are shown\n",
    "    # in the X axis, and the correspondent responses are in Y axis.\n",
    "    # Alternatively, orientation = 'horizontal' results in horizontal bars.\n",
    "    # In this case, categories are in Y axis, and responses in X axis.\n",
    "    # If None or invalid values are provided, orientation is set as 'vertical'.\n",
    "    \n",
    "    # Note: to obtain a Pareto chart, keep aggregate_function = 'sum',\n",
    "    # plot_cumulative_percent = True, and orientation = 'vertical'.\n",
    "    \n",
    "    # limit_of_plotted_categories: integer value that represents\n",
    "    # the maximum of categories that will be plot. Keep it None to plot\n",
    "    # all categories. Alternatively, set an integer value. e.g.: if\n",
    "    # limit_of_plotted_categories = 4, but there are more categories,\n",
    "    # the dataset will be sorted in descending order and: 1) The remaining\n",
    "    # categories will be sum in a new category named 'others' if the\n",
    "    # aggregate function is 'sum'; 2) Or the other categories will be simply\n",
    "    # omitted from the plot, for other aggregate functions. Notice that\n",
    "    # it limits only the variables in the plot: all of them will be\n",
    "    # returned in the dataframe.\n",
    "    # Use this parameter to obtain a cleaner plot. Notice that the remaining\n",
    "    # columns will be aggregated as 'others' even if there is a single column\n",
    "    # beyond the limit.\n",
    "    \n",
    "    \n",
    "    # Create a local copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    # Before calling the method, we must guarantee that the variables may be\n",
    "    # used for that aggregate. Some aggregations are permitted only for numeric variables, so calling\n",
    "    # the methods before selecting the variables may raise warnings or errors.\n",
    "    \n",
    "    \n",
    "    list_of_aggregates = ['median', 'mean', 'mode', 'sum', 'min', 'max', 'variance', 'count',\n",
    "                          'standard_deviation', '10_percent_quantile', '20_percent_quantile', \n",
    "                          '25_percent_quantile', '30_percent_quantile', '40_percent_quantile', \n",
    "                          '50_percent_quantile', '60_percent_quantile', '70_percent_quantile', \n",
    "                          '75_percent_quantile', '80_percent_quantile', '90_percent_quantile', \n",
    "                          '95_percent_quantile', 'kurtosis', 'skew', 'interquartile_range', \n",
    "                          'mean_standard_error', 'entropy']\n",
    "    \n",
    "    list_of_numeric_aggregates = ['median', 'mean', 'sum', 'min', 'max', 'variance',\n",
    "                                  'standard_deviation', '10_percent_quantile', '20_percent_quantile', \n",
    "                                  '25_percent_quantile', '30_percent_quantile', '40_percent_quantile', \n",
    "                                  '50_percent_quantile', '60_percent_quantile', '70_percent_quantile', \n",
    "                                  '75_percent_quantile', '80_percent_quantile', '90_percent_quantile',\n",
    "                                  '95_percent_quantile', 'kurtosis', 'skew', 'interquartile_range', \n",
    "                                  'mean_standard_error']\n",
    "    \n",
    "    # Check if an invalid or no aggregation function was selected:\n",
    "    if ((aggregate_function not in (list_of_aggregates)) | (aggregate_function is None)):\n",
    "        \n",
    "        aggregate_function = 'sum'\n",
    "        print(\"Invalid or no aggregation function input, so using the default \\'sum\\'.\\n\")\n",
    "    \n",
    "    # List the possible numeric data types for a Pandas dataframe column:\n",
    "    numeric_dtypes = [np.int16, np.int32, np.int64, np.float16, np.float32, np.float64]\n",
    "    \n",
    "    # Check if a numeric aggregate was selected:\n",
    "    if (aggregate_function in list_of_numeric_aggregates):\n",
    "        \n",
    "        column_data_type = DATASET[response_var_name].dtype\n",
    "        \n",
    "        if (column_data_type not in numeric_dtypes):\n",
    "            \n",
    "                # If the Pandas series was defined as an object, it means it is categorical\n",
    "                # (string, date, etc).\n",
    "                print(\"Numeric aggregate selected, but categorical variable indicated as response variable.\")\n",
    "                print(\"Setting aggregate_function = \\'mode\\', to make aggregate compatible with data type.\\n\")\n",
    "                \n",
    "                aggregate_function = 'mode'\n",
    "    \n",
    "    else: # categorical aggregate function\n",
    "        \n",
    "        column_data_type = DATASET[response_var_name].dtype\n",
    "        \n",
    "        if ((column_data_type in numeric_dtypes) & (aggregate_function != 'count')):\n",
    "                # count is the only aggregate for categorical that can be used for numerical variables as well.\n",
    "                \n",
    "                print(\"Categorical aggregate selected, but numeric variable indicated as response variable.\")\n",
    "                print(\"Setting aggregate_function = \\'sum\\', to make aggregate compatible with data type.\\n\")\n",
    "                \n",
    "                aggregate_function = 'sum'\n",
    "    \n",
    "    # Before grouping, let's remove the missing values, avoiding the raising of TypeError.\n",
    "    # Pandas deprecated the automatic dropna with aggregation:\n",
    "    DATASET = DATASET.dropna(axis = 0)\n",
    "    \n",
    "    # Convert categorical_var_name to Pandas 'category' type. If the variable is represented by\n",
    "    # a number, the dataframe will be grouped in terms of an aggregation of the variable, instead\n",
    "    # of as a category. It will prevents this to happen:\n",
    "    DATASET[categorical_var_name] = DATASET[categorical_var_name].astype(\"category\")    \n",
    "    \n",
    "    # If an aggregate function different from 'sum', 'mean', 'median' or 'mode' \n",
    "    # is used with plot_cumulative_percent = True, \n",
    "    # set plot_cumulative_percent = False:\n",
    "    # (check if aggregate function is not in the list of allowed values):\n",
    "    if ((aggregate_function not in ['sum', 'mean', 'median', 'mode', 'count']) & (calculate_and_plot_cumulative_percent == True)):\n",
    "        \n",
    "        calculate_and_plot_cumulative_percent = False\n",
    "        print(\"The cumulative percent is only calculated when aggregate_function = \\'sum\\', \\'mean\\', \\'median\\', \\'mode\\', or \\'count\\'. So, plot_cumulative_percent was set as False.\")\n",
    "    \n",
    "    # Guarantee that the columns from the aggregated dataset have the correct names\n",
    "    \n",
    "    # Groupby according to the selection.\n",
    "    # Here, there is a great gain of performance in not using a dictionary of methods:\n",
    "    # If using a dictionary of methods, Pandas would calculate the results for each one of the methods.\n",
    "    \n",
    "    # Pandas groupby method documentation:\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html?msclkid=7b3531a6cff211ec9086f4edaddb94ba\n",
    "    # argument as_index = False: prevents the grouper variable to be set as index of the new dataframe.\n",
    "    # (default: as_index = True);\n",
    "    # dropna = False: do not removes the missing values (default: dropna = True, used here to avoid\n",
    "    # compatibility and version issues)\n",
    "    \n",
    "    if (aggregate_function == 'median'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].agg('median')\n",
    "\n",
    "    elif (aggregate_function == 'mean'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].mean()\n",
    "    \n",
    "    elif (aggregate_function == 'mode'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].agg(stats.mode)\n",
    "    \n",
    "    elif (aggregate_function == 'sum'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].sum()\n",
    "    \n",
    "    elif (aggregate_function == 'count'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].count()\n",
    "\n",
    "    elif (aggregate_function == 'min'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].min()\n",
    "    \n",
    "    elif (aggregate_function == 'max'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].max()\n",
    "    \n",
    "    elif (aggregate_function == 'variance'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].var()\n",
    "\n",
    "    elif (aggregate_function == 'standard_deviation'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].std()\n",
    "    \n",
    "    elif (aggregate_function == '10_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.10)\n",
    "    \n",
    "    elif (aggregate_function == '20_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.20)\n",
    "    \n",
    "    elif (aggregate_function == '25_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.25)\n",
    "    \n",
    "    elif (aggregate_function == '30_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.30)\n",
    "    \n",
    "    elif (aggregate_function == '40_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.40)\n",
    "    \n",
    "    elif (aggregate_function == '50_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.50)\n",
    "\n",
    "    elif (aggregate_function == '60_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.60)\n",
    "    \n",
    "    elif (aggregate_function == '70_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.30)\n",
    "\n",
    "    elif (aggregate_function == '75_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.75)\n",
    "\n",
    "    elif (aggregate_function == '80_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.80)\n",
    "    \n",
    "    elif (aggregate_function == '90_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.90)\n",
    "    \n",
    "    elif (aggregate_function == '95_percent_quantile'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].quantile(0.95)\n",
    "\n",
    "    elif (aggregate_function == 'kurtosis'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].agg(stats.kurtosis)\n",
    "    \n",
    "    elif (aggregate_function == 'skew'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].agg(stats.skew)\n",
    "\n",
    "    elif (aggregate_function == 'interquartile_range'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].agg(stats.iqr)\n",
    "    \n",
    "    elif (aggregate_function == 'mean_standard_error'):\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].agg(stats.sem)\n",
    "    \n",
    "    else: # entropy\n",
    "        \n",
    "        DATASET = DATASET.groupby(by = categorical_var_name, as_index = False, sort = True)[response_var_name].agg(stats.entropy)\n",
    "\n",
    "    \n",
    "    # List of columns of the aggregated dataset:\n",
    "    list_of_columns = list(DATASET.columns) # convert to a list\n",
    "    \n",
    "    if (add_suffix_to_aggregated_col == True):\n",
    "            \n",
    "        if (suffix is None):\n",
    "                \n",
    "            suffix = \"_\" + aggregate_function\n",
    "            \n",
    "        new_columns = [(str(name) + suffix) for name in list_of_columns]\n",
    "        # Do not consider the first element, which is the aggregate function with a suffix.\n",
    "        # Concatenate the correct name with the columns from the second element of the list:\n",
    "        new_columns = [categorical_var_name] + new_columns[1:]\n",
    "        # Make it the new columns:\n",
    "        DATASET.columns = new_columns\n",
    "        # Update the list of columns:\n",
    "        list_of_columns = DATASET.columns\n",
    "    \n",
    "    if (aggregate_function == 'mode'):\n",
    "        \n",
    "        # The columns was saved as a series of Tuples. Each row contains a tuple like:\n",
    "        # ([calculated_mode], [counting_of_occurrences]). We want only the calculated mode.\n",
    "        # On the other hand, if we do column[0], we will get the columns first row. So, we have to\n",
    "        # go through each column, retrieving only the mode:\n",
    "        \n",
    "        # Loop through each column:\n",
    "        for column in list_of_columns:\n",
    "            \n",
    "            # Save the series as a list:\n",
    "            list_of_modes_arrays = list(DATASET[column])\n",
    "            # Start a list of modes:\n",
    "            list_of_modes = []\n",
    "            \n",
    "            # Loop through each element from the list of arrays:\n",
    "            for mode_array in list_of_modes_arrays:\n",
    "                # mode array is like:\n",
    "                # ModeResult(mode=array([calculated_mode]), count=array([counting_of_occurrences]))\n",
    "                # To retrieve only the mode, we must access the element [0][0] from this array:\n",
    "                try:\n",
    "                    list_of_modes.append(mode_array[0][0])\n",
    "                \n",
    "                except IndexError:\n",
    "                    # This error is generated when trying to access an array storing no values.\n",
    "                    # (i.e., with missing values). Since there is no dimension, it is not possible\n",
    "                    # to access the [0][0] position. In this case, simply append the np.nan \n",
    "                    # the (missing value):\n",
    "                    list_of_modes.append(np.nan)\n",
    "            \n",
    "            # Make the list of modes the column itself:\n",
    "            DATASET[column] = list_of_modes\n",
    "    \n",
    "            \n",
    "    # the name of the response variable is now the second element from the list of column:\n",
    "    response_var_name = list(DATASET.columns)[1]\n",
    "    # the categorical variable name was not changed.\n",
    "    \n",
    "    # Let's sort the dataframe.\n",
    "    \n",
    "    # Order the dataframe in descending order by the response.\n",
    "    # If there are equal responses, order them by category, in\n",
    "    # ascending order; put the missing values in the first position\n",
    "    # To pass multiple columns and multiple types of ordering, we use\n",
    "    # lists. If there was a single column to order by, we would declare\n",
    "    # it as a string. If only one order of ascending was used, we would\n",
    "    # declare it as a simple boolean\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html\n",
    "    \n",
    "    DATASET = DATASET.sort_values(by = [response_var_name, categorical_var_name], ascending = [False, True], na_position = 'first')\n",
    "    \n",
    "    # Now, reset index positions:\n",
    "    DATASET = DATASET.reset_index(drop = True)\n",
    "    \n",
    "    if (aggregate_function == 'count'):\n",
    "        \n",
    "        # Here, the column represents the counting, no matter the variable set as response.\n",
    "        DATASET.columns = [categorical_var_name, 'count_of_entries']\n",
    "        response_var_name = 'count_of_entries'\n",
    "    \n",
    "    # plot_cumulative_percent = True, create a column to store the\n",
    "    # cumulative percent:\n",
    "    if (calculate_and_plot_cumulative_percent): \n",
    "        # Run the following code if the boolean value is True (implicity)\n",
    "        # Only calculates cumulative percent in case aggregate is 'sum' or 'mode'\n",
    "        \n",
    "        # Create a column series for the cumulative sum:\n",
    "        cumsum_col = response_var_name + \"_cumsum\"\n",
    "        DATASET[cumsum_col] = DATASET[response_var_name].cumsum()\n",
    "        \n",
    "        # total sum is the last element from this series\n",
    "        # (i.e. the element with index len(DATASET) - 1)\n",
    "        total_sum = DATASET[cumsum_col][(len(DATASET) - 1)]\n",
    "        \n",
    "        # Now, create a column for the accumulated percent\n",
    "        # by dividing cumsum_col by total_sum and multiplying it by\n",
    "        # 100 (%):\n",
    "        cum_pct_col = response_var_name + \"_cum_pct\"\n",
    "        DATASET[cum_pct_col] = (DATASET[cumsum_col])/(total_sum) * 100\n",
    "        print(f\"Successfully calculated cumulative sum and cumulative percent correspondent to the response variable {response_var_name}.\")\n",
    "    \n",
    "    print(\"Successfully aggregated and ordered the dataset to plot. Check the 10 first rows of this returned dataset:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(DATASET.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(DATASET.head(10))\n",
    "    \n",
    "    # Check if the total of plotted categories is limited:\n",
    "    if not (limit_of_plotted_categories is None):\n",
    "        \n",
    "        # Since the value is not None, we have to limit it\n",
    "        # Check if the limit is lower than or equal to the length of the dataframe.\n",
    "        # If it is, we simply copy the columns to the series (there is no need of\n",
    "        # a memory-consuming loop or of applying the head method to a local copy\n",
    "        # of the dataframe):\n",
    "        df_length = len(DATASET)\n",
    "            \n",
    "        if (df_length <= limit_of_plotted_categories):\n",
    "            # Simply copy the columns to the graphic series:\n",
    "            categories = DATASET[categorical_var_name]\n",
    "            responses = DATASET[response_var_name]\n",
    "            # If there is a cum_pct column, copy it to a series too:\n",
    "            if (calculate_and_plot_cumulative_percent):\n",
    "                cum_pct = DATASET[cum_pct_col]\n",
    "        \n",
    "        else:\n",
    "            # The limit is lower than the total of categories,\n",
    "            # so we actually have to limit the size of plotted df:\n",
    "        \n",
    "            # If aggregate_function is not 'sum', we simply apply\n",
    "            # the head method to obtain the first rows (number of\n",
    "            # rows input as parameter; if no parameter is input, the\n",
    "            # number of 5 rows is used):\n",
    "            \n",
    "            # Limit to the number limit_of_plotted_categories:\n",
    "            # create another local copy of the dataframe not to\n",
    "            # modify the returned dataframe object:\n",
    "            plotted_df = DATASET.copy(deep = True).head(limit_of_plotted_categories)\n",
    "\n",
    "            # Create the series of elements to plot:\n",
    "            categories = list(plotted_df[categorical_var_name])\n",
    "            responses = list(plotted_df[response_var_name])\n",
    "            # If the cumulative percent was obtained, create the series for it:\n",
    "            if (calculate_and_plot_cumulative_percent):\n",
    "                cum_pct = list(plotted_df[cum_pct_col])\n",
    "            \n",
    "            # Start variable to store the aggregates from the others:\n",
    "            other_responses = 0\n",
    "            \n",
    "            # Loop through each row from DATASET:\n",
    "            for i in range(0, len(DATASET)):\n",
    "                \n",
    "                # Check if the category is not in categories:\n",
    "                category = DATASET[categorical_var_name][i]\n",
    "                \n",
    "                if (category not in categories):\n",
    "                    \n",
    "                    # sum the value in the response variable to other_responses:\n",
    "                    other_responses = other_responses + DATASET[response_var_name][i]\n",
    "            \n",
    "            # Now we finished the sum of the other responses, let's add these elements to\n",
    "            # the lists:\n",
    "            categories.append(\"others\")\n",
    "            responses.append(other_responses)\n",
    "            # If there is a cumulative percent, append 100% to the list:\n",
    "            if (calculate_and_plot_cumulative_percent):\n",
    "                cum_pct.append(100)\n",
    "                # The final cumulative percent must be the total, 100%\n",
    "            \n",
    "            else:\n",
    "\n",
    "                # Firstly, copy the elements that will be kept to x, y and (possibly) cum_pct\n",
    "                # lists.\n",
    "                # Start the lists:\n",
    "                categories = []\n",
    "                responses = []\n",
    "                if (calculate_and_plot_cumulative_percent):\n",
    "                    cum_pct = [] # start this list only if its needed to save memory\n",
    "\n",
    "                for i in range (0, limit_of_plotted_categories):\n",
    "                    # i goes from 0 (first index) to limit_of_plotted_categories - 1\n",
    "                    # (index of the last category to be kept):\n",
    "                    # copy the elements from the DATASET to the list\n",
    "                    # category is the 1st column (column 0); response is the 2nd (col 1);\n",
    "                    # and cumulative percent is the 4th (col 3):\n",
    "                    categories.append(DATASET.iloc[i, 0])\n",
    "                    responses.append(DATASET.iloc[i, 1])\n",
    "                    \n",
    "                    if (calculate_and_plot_cumulative_percent):\n",
    "                        cum_pct.append(DATASET.iloc[i, 3]) # only if there is something to iloc\n",
    "                    \n",
    "                # Now, i = limit_of_plotted_categories - 1\n",
    "                # Create a variable to store the sum of other responses\n",
    "                other_responses = 0\n",
    "                # loop from i = limit_of_plotted_categories to i = df_length-1, index\n",
    "                # of the last element. Notice that this loop may have a single call, if there\n",
    "                # is only one element above the limit:\n",
    "                for i in range (limit_of_plotted_categories, (df_length - 1)):\n",
    "                    \n",
    "                    other_responses = other_responses + (DATASET.iloc[i, 1])\n",
    "                \n",
    "                # Now, add the last elements to the series:\n",
    "                # The last category is named 'others':\n",
    "                categories.append('others')\n",
    "                # The correspondent aggregated response is the value \n",
    "                # stored in other_responses:\n",
    "                responses.append(other_responses)\n",
    "                # The cumulative percent is 100%, since this must be the sum of all\n",
    "                # elements (the previous ones plus the ones aggregated as 'others'\n",
    "                # must totalize 100%).\n",
    "                # On the other hand, the cumulative percent is stored only if needed:\n",
    "                cum_pct.append(100)\n",
    "    \n",
    "    else:\n",
    "        # This is the situation where there is no limit of plotted categories. So, we\n",
    "        # simply copy the columns to the plotted series (it is equivalent to the \n",
    "        # situation where there is a limit, but the limit is equal or inferior to the\n",
    "        # size of the dataframe):\n",
    "        categories = DATASET[categorical_var_name]\n",
    "        responses = DATASET[response_var_name]\n",
    "        # If there is a cum_pct column, copy it to a series too:\n",
    "        if (calculate_and_plot_cumulative_percent):\n",
    "            cum_pct = DATASET[cum_pct_col]\n",
    "    \n",
    "    \n",
    "    # Now the data is prepared and we only have to plot \n",
    "    # categories, responses, and cum_pct:\n",
    "    \n",
    "    # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "    # so that the bars do not completely block other views.\n",
    "    OPACITY = 0.95\n",
    "    \n",
    "    # Set labels and titles for the case they are None\n",
    "    if (plot_title is None):\n",
    "        \n",
    "        if (aggregate_function == 'count'):\n",
    "            # The graph is the same count, no matter the response\n",
    "            plot_title = f\"Bar_chart_count_of_{categorical_var_name}\"\n",
    "        \n",
    "        else:\n",
    "            plot_title = f\"Bar_chart_for_{response_var_name}_by_{categorical_var_name}\"\n",
    "    \n",
    "    if (horizontal_axis_title is None):\n",
    "\n",
    "        horizontal_axis_title = categorical_var_name\n",
    "\n",
    "    if (vertical_axis_title is None):\n",
    "        # Notice that response_var_name already has the suffix indicating the\n",
    "        # aggregation function\n",
    "        vertical_axis_title = response_var_name\n",
    "    \n",
    "    fig, ax1 = plt.subplots(figsize = (12, 8))\n",
    "    # Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "\n",
    "    #ROTATE X AXIS IN XX DEGREES\n",
    "    plt.xticks(rotation = x_axis_rotation)\n",
    "    # XX = 70 DEGREES x_axis (Default)\n",
    "    #ROTATE Y AXIS IN XX DEGREES:\n",
    "    plt.yticks(rotation = y_axis_rotation)\n",
    "    # XX = 0 DEGREES y_axis (Default)\n",
    "    \n",
    "    plt.title(plot_title)\n",
    "    \n",
    "    if (orientation == 'horizontal'):\n",
    "        \n",
    "        # invert the axes in relation to the default (vertical, below)\n",
    "        ax1.set_ylabel(horizontal_axis_title)\n",
    "        ax1.set_xlabel(vertical_axis_title, color = 'darkblue')\n",
    "        \n",
    "        # Horizontal bars used - barh method (bar horizontal):\n",
    "        # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.barh.html\n",
    "        # Now, the categorical variables stored in series categories must be\n",
    "        # positioned as the vertical axis Y, whereas the correspondent responses\n",
    "        # must be in the horizontal axis X.\n",
    "        ax1.barh(categories, responses, color = 'darkblue', alpha = OPACITY, label = categorical_var_name)\n",
    "        #.barh(y, x, ...)\n",
    "        \n",
    "        if (calculate_and_plot_cumulative_percent):\n",
    "            # Let's plot the line for the cumulative percent\n",
    "            # Set the grid for the bar chart as False. If it is True, there will\n",
    "            # be to grids, one for the bars and other for the percents, making \n",
    "            # the image difficult to interpretate:\n",
    "            ax1.grid(False)\n",
    "            \n",
    "            # Create the twin plot for the cumulative percent:\n",
    "            # for the vertical orientation, we use the twinx. Here, we use twiny\n",
    "            ax2 = ax1.twiny()\n",
    "            # Here, the x axis must be the cum_pct value, and the Y\n",
    "            # axis must be categories (it must be correspondent to the\n",
    "            # bar chart)\n",
    "            ax2.plot(cum_pct, categories, '-ro', label = \"cumulative\\npercent\")\n",
    "            #.plot(x, y, ...)\n",
    "            ax2.tick_params('x', color = 'red')\n",
    "            ax2.set_xlabel(\"Cumulative Percent (%)\", color = 'red')\n",
    "            ax2.legend()\n",
    "            ax2.grid(grid) # shown if user set grid = True\n",
    "            # If user wants to see the grid, it is shown only for the cumulative line.\n",
    "        \n",
    "        else:\n",
    "            # There is no cumulative line, so the parameter grid must control \n",
    "            # the bar chart's grid\n",
    "            ax1.legend()\n",
    "            ax1.grid(grid)\n",
    "        \n",
    "    else: \n",
    "        \n",
    "        ax1.set_xlabel(horizontal_axis_title)\n",
    "        ax1.set_ylabel(vertical_axis_title, color = 'darkblue')\n",
    "        # If None or an invalid orientation was used, set it as vertical\n",
    "        # Use Matplotlib standard bar method (vertical bar):\n",
    "        # https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html#matplotlib.pyplot.bar\n",
    "        \n",
    "        # In this standard case, the categorical variables (categories) are positioned\n",
    "        # as X, and the responses as Y:\n",
    "        ax1.bar(categories, responses, color = 'darkblue', alpha = OPACITY, label = categorical_var_name)\n",
    "        #.bar(x, y, ...)\n",
    "        \n",
    "        if (calculate_and_plot_cumulative_percent):\n",
    "            # Let's plot the line for the cumulative percent\n",
    "            # Set the grid for the bar chart as False. If it is True, there will\n",
    "            # be to grids, one for the bars and other for the percents, making \n",
    "            # the image difficult to interpretate:\n",
    "            ax1.grid(False)\n",
    "            \n",
    "            # Create the twin plot for the cumulative percent:\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.plot(categories, cum_pct, '-ro', label = \"cumulative\\npercent\")\n",
    "            #.plot(x, y, ...)\n",
    "            ax2.tick_params('y', color = 'red')\n",
    "            ax2.set_ylabel(\"Cumulative Percent (%)\", color = 'red', rotation = 270)\n",
    "            # rotate the twin axis so that its label is inverted in relation to the main\n",
    "            # vertical axis.\n",
    "            ax2.legend()\n",
    "            ax2.grid(grid) # shown if user set grid = True\n",
    "            # If user wants to see the grid, it is shown only for the cumulative line.\n",
    "        \n",
    "        else:\n",
    "            # There is no cumulative line, so the parameter grid must control \n",
    "            # the bar chart's grid\n",
    "            ax1.legend()\n",
    "            ax1.grid(grid)\n",
    "    \n",
    "    # Notice that the .plot method is used for generating the plot for both orientations.\n",
    "    # It is different from .bar and .barh, which specify the orientation of a bar; or\n",
    "    # .hline (creation of an horizontal constant line); or .vline (creation of a vertical\n",
    "    # constant line).\n",
    "    \n",
    "    # Now the parameters specific to the configurations are finished, so we can go back\n",
    "    # to the general code:\n",
    "    \n",
    "    if (export_png == True):\n",
    "        # Image will be exported\n",
    "        import os\n",
    "        \n",
    "        #check if the user defined a directory path. If not, set as the default root path:\n",
    "        if (directory_to_save is None):\n",
    "            #set as the default\n",
    "            directory_to_save = \"\"\n",
    "        \n",
    "        #check if the user defined a file name. If not, set as the default name for this\n",
    "        # function.\n",
    "        if (file_name is None):\n",
    "            #set as the default\n",
    "            file_name = \"bar_chart\"\n",
    "        \n",
    "        #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "        # resolution.\n",
    "        if (png_resolution_dpi is None):\n",
    "            #set as 330 dpi\n",
    "            png_resolution_dpi = 330\n",
    "        \n",
    "        #Get the new_file_path\n",
    "        new_file_path = os.path.join(directory_to_save, file_name)\n",
    "        \n",
    "        #Export the file to this new path:\n",
    "        # The extension will be automatically added by the savefig method:\n",
    "        plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "        #quality could be set from 1 to 100, where 100 is the best quality\n",
    "        #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "        #transparent = True or False\n",
    "        # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "        print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "    \n",
    "    #fig.tight_layout()\n",
    "    \n",
    "    ## Show an image read from an image file:\n",
    "    ## import matplotlib.image as pltimg\n",
    "    ## img=pltimg.imread('mydecisiontree.png')\n",
    "    ## imgplot = plt.imshow(img)\n",
    "    ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "    ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "    ##  '03_05_END.ipynb'\n",
    "    plt.show()\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "3e71c266-f1df-4e2c-802b-5cfc289ecc66"
   },
   "source": [
    "# **Function for time series visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "54bc10eb-9097-4849-bd6e-76eeb0843539",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def time_series_vis (data_in_same_column = False, df = None, column_with_predict_var_x = None, column_with_response_var_y = None, column_with_labels = None, list_of_dictionaries_with_series_to_analyze = [{'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}], x_axis_rotation = 70, y_axis_rotation = 0, grid = True, add_splines_lines = True, add_scatter_dots = False, horizontal_axis_title = None, vertical_axis_title = None, plot_title = None, export_png = False, directory_to_save = None, file_name = None, png_resolution_dpi = 330):\n",
    "     \n",
    "    import random\n",
    "    # Python Random documentation:\n",
    "    # https://docs.python.org/3/library/random.html?msclkid=9d0c34b2d13111ec9cfa8ddaee9f61a1\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.colors as mcolors\n",
    "    \n",
    "    # matplotlib.colors documentation:\n",
    "    # https://matplotlib.org/3.5.0/api/colors_api.html?msclkid=94286fa9d12f11ec94660321f39bf47f\n",
    "    \n",
    "    # Matplotlib list of colors:\n",
    "    # https://matplotlib.org/stable/gallery/color/named_colors.html?msclkid=0bb86abbd12e11ecbeb0a2439e5b0d23\n",
    "    # Matplotlib colors tutorial:\n",
    "    # https://matplotlib.org/stable/tutorials/colors/colors.html\n",
    "    # Matplotlib example of Python code using matplotlib.colors:\n",
    "    # https://matplotlib.org/stable/_downloads/0843ee646a32fc214e9f09328c0cd008/colors.py\n",
    "    # Same example as Jupyter Notebook:\n",
    "    # https://matplotlib.org/stable/_downloads/2a7b13c059456984288f5b84b4b73f45/colors.ipynb\n",
    "    \n",
    "        \n",
    "    # data_in_same_column = False: set as True if all the values to plot are in a same column.\n",
    "    # If data_in_same_column = True, you must specify the dataframe containing the data as df;\n",
    "    # the column containing the predict variable (X) as column_with_predict_var_x; the column \n",
    "    # containing the responses to plot (Y) as column_with_response_var_y; and the column \n",
    "    # containing the labels (subgroup) indication as column_with_labels. \n",
    "    # df is an object, so do not declare it in quotes. The other three arguments (columns' names) \n",
    "    # are strings, so declare in quotes. \n",
    "    \n",
    "    # Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare. \n",
    "    # All the results for both groups are in a column named 'results', wich will be plot against\n",
    "    # the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
    "    # an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
    "    # column 'group' shows the value 'B'. In this example:\n",
    "    # data_in_same_column = True,\n",
    "    # df = dataset,\n",
    "    # column_with_predict_var_x = 'time',\n",
    "    # column_with_response_var_y = 'results', \n",
    "    # column_with_labels = 'group'\n",
    "    # If you want to declare a list of dictionaries, keep data_in_same_column = False and keep\n",
    "    # df = None (the other arguments may be set as None, but it is not mandatory: \n",
    "    # column_with_predict_var_x = None, column_with_response_var_y = None, column_with_labels = None).\n",
    "    \n",
    "\n",
    "    # Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
    "    # list_of_dictionaries_with_series_to_analyze: if data is already converted to series, lists\n",
    "    # or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
    "    # even if there is a single dictionary.\n",
    "    # Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
    "    # (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
    "    # keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
    "    # If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "    # and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "    # Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "    # same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
    "    # represents the series and label of the added dictionary (you can pass 'lab': None, but if \n",
    "    # 'x' or 'y' are None, the new dictionary will be ignored).\n",
    "    \n",
    "    # Examples:\n",
    "    # list_of_dictionaries_with_series_to_analyze = \n",
    "    # [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
    "    # will plot a single variable. In turns:\n",
    "    # list_of_dictionaries_with_series_to_analyze = \n",
    "    # [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
    "    # will plot two series, Y1 x X and Y2 x X.\n",
    "    # Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
    "    # If None is provided to 'lab', an automatic label will be generated.\n",
    "    \n",
    "    \n",
    "    # List the possible numeric data types for a Pandas dataframe column:\n",
    "    numeric_dtypes = [np.int16, np.int32, np.int64, np.float16, np.float32, np.float64]\n",
    "    \n",
    "    if (data_in_same_column == True):\n",
    "        \n",
    "        print(\"Data to be plotted in a same column.\\n\")\n",
    "        \n",
    "        if (df is None):\n",
    "            \n",
    "            print(\"Please, input a valid dataframe as df.\\n\")\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "            # The code will check the size of this list on the next block.\n",
    "            # If it is zero, code is simply interrupted.\n",
    "            # Instead of returning an error, we use this code structure that can be applied\n",
    "            # on other graphic functions that do not return a summary (and so we should not\n",
    "            # return a value like 'error' to interrupt the function).\n",
    "        \n",
    "        elif (column_with_predict_var_x is None):\n",
    "            \n",
    "            print(\"Please, input a valid column name as column_with_predict_var_x.\\n\")\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "           \n",
    "        elif (column_with_response_var_y is None):\n",
    "            \n",
    "            print(\"Please, input a valid column name as column_with_response_var_y.\\n\")\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            # set a local copy of the dataframe:\n",
    "            DATASET = df.copy(deep = True)\n",
    "            \n",
    "            if (column_with_labels is None):\n",
    "            \n",
    "                print(\"Using the whole series (column) for correlation.\\n\")\n",
    "                column_with_labels = 'whole_series_' + column_with_response_var_y\n",
    "                DATASET[column_with_labels] = column_with_labels\n",
    "            \n",
    "            # sort DATASET; by column_with_predict_var_x; by column column_with_labels\n",
    "            # and by column_with_response_var_y, all in Ascending order\n",
    "            # Since we sort by label (group), it is easier to separate the groups.\n",
    "            DATASET = DATASET.sort_values(by = [column_with_predict_var_x, column_with_labels, column_with_response_var_y], ascending = [True, True, True])\n",
    "            \n",
    "            # Reset indices:\n",
    "            DATASET = DATASET.reset_index(drop = True)\n",
    "            \n",
    "            # If column_with_predict_var_x is an object, the user may be trying to pass a date as x. \n",
    "            # So, let's try to convert it to datetime:\n",
    "            if ((DATASET[column_with_predict_var_x]).dtype not in numeric_dtypes):\n",
    "                  \n",
    "                try:\n",
    "                    DATASET[column_with_predict_var_x] = (DATASET[column_with_predict_var_x]).astype('datetime64[ns]')\n",
    "                    print(\"Variable X successfully converted to datetime64[ns].\\n\")\n",
    "                    \n",
    "                except:\n",
    "                    # Simply ignore it\n",
    "                    pass\n",
    "            \n",
    "            # Get a series of unique values of the labels, and save it as a list using the\n",
    "            # list attribute:\n",
    "            unique_labels = list(DATASET[column_with_labels].unique())\n",
    "            print(f\"{len(unique_labels)} different labels detected: {unique_labels}.\\n\")\n",
    "            \n",
    "            # Start a list to store the dictionaries containing the keys:\n",
    "            # 'x': list of predict variables; 'y': list of responses; 'lab': the label (group)\n",
    "            list_of_dictionaries_with_series_to_analyze = []\n",
    "            \n",
    "            # Loop through each possible label:\n",
    "            for lab in unique_labels:\n",
    "                # loop through each element from the list unique_labels, referred as lab\n",
    "                \n",
    "                # Set a filter for the dataset, to select only rows correspondent to that\n",
    "                # label:\n",
    "                boolean_filter = (DATASET[column_with_labels] == lab)\n",
    "                \n",
    "                # Create a copy of the dataset, with entries selected by that filter:\n",
    "                ds_copy = (DATASET[boolean_filter]).copy(deep = True)\n",
    "                # Sort again by X and Y, to guarantee the results are in order:\n",
    "                ds_copy = ds_copy.sort_values(by = [column_with_predict_var_x, column_with_response_var_y], ascending = [True, True])\n",
    "                # Restart the index of the copy:\n",
    "                ds_copy = ds_copy.reset_index(drop = True)\n",
    "                \n",
    "                # Re-extract the X and Y series and convert them to NumPy arrays \n",
    "                # (these arrays will be important later in the function):\n",
    "                x = np.array(ds_copy[column_with_predict_var_x])\n",
    "                y = np.array(ds_copy[column_with_response_var_y])\n",
    "            \n",
    "                # Then, create the dictionary:\n",
    "                dict_of_values = {'x': x, 'y': y, 'lab': lab}\n",
    "                \n",
    "                # Now, append dict_of_values to list_of_dictionaries_with_series_to_analyze:\n",
    "                list_of_dictionaries_with_series_to_analyze.append(dict_of_values)\n",
    "                \n",
    "            # Now, we have a list of dictionaries with the same format of the input list.\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        # The user input a list_of_dictionaries_with_series_to_analyze\n",
    "        # Create a support list:\n",
    "        support_list = []\n",
    "        \n",
    "        # Loop through each element on the list list_of_dictionaries_with_series_to_analyze:\n",
    "        \n",
    "        for i in range (0, len(list_of_dictionaries_with_series_to_analyze)):\n",
    "            # from i = 0 to i = len(list_of_dictionaries_with_series_to_analyze) - 1, index of the\n",
    "            # last element from the list\n",
    "            \n",
    "            # pick the i-th dictionary from the list:\n",
    "            dictionary = list_of_dictionaries_with_series_to_analyze[i]\n",
    "            \n",
    "            # access 'x', 'y', and 'lab' keys from the dictionary:\n",
    "            x = dictionary['x']\n",
    "            y = dictionary['y']\n",
    "            lab = dictionary['lab']\n",
    "            # Remember that all this variables are series from a dataframe, so we can apply\n",
    "            # the astype function:\n",
    "            # https://www.askpython.com/python/built-in-methods/python-astype?msclkid=8f3de8afd0d411ec86a9c1a1e290f37c\n",
    "            \n",
    "            # check if at least x and y are not None:\n",
    "            if ((x is not None) & (y is not None)):\n",
    "                \n",
    "                # If column_with_predict_var_x is an object, the user may be trying to pass a date as x. \n",
    "                # So, let's try to convert it to datetime:\n",
    "                if (x.dtype not in numeric_dtypes):\n",
    "\n",
    "                    try:\n",
    "                        x = (x).astype('datetime64[ns]')\n",
    "                        print(f\"Variable X from {i}-th dictionary successfully converted to datetime64[ns].\\n\")\n",
    "\n",
    "                    except:\n",
    "                        # Simply ignore it\n",
    "                        pass\n",
    "                \n",
    "                # Possibly, x and y are not ordered. Firstly, let's merge them into a temporary\n",
    "                # dataframe to be able to order them together.\n",
    "                # Use the 'list' attribute to guarantee that x and y were read as lists. These lists\n",
    "                # are the values for a dictionary passed as argument for the constructor of the\n",
    "                # temporary dataframe. When using the list attribute, we make the series independent\n",
    "                # from its origin, even if it was created from a Pandas dataframe. Then, we have a\n",
    "                # completely independent dataframe that may be manipulated and sorted, without worrying\n",
    "                # that it may modify its origin:\n",
    "                \n",
    "                temp_df = pd.DataFrame(data = {'x': list(x), 'y': list(y)})\n",
    "                # sort this dataframe by 'x' and 'y':\n",
    "                temp_df = temp_df.sort_values(by = ['x', 'y'], ascending = [True, True])\n",
    "                # restart index:\n",
    "                temp_df = temp_df.reset_index(drop = True)\n",
    "                \n",
    "                # Re-extract the X and Y series and convert them to NumPy arrays \n",
    "                # (these arrays will be important later in the function):\n",
    "                x = np.array(temp_df['x'])\n",
    "                y = np.array(temp_df['y'])\n",
    "                \n",
    "                # check if lab is None:\n",
    "                if (lab is None):\n",
    "                    # input a default label.\n",
    "                    # Use the str attribute to convert the integer to string, allowing it\n",
    "                    # to be concatenated\n",
    "                    lab = \"X\" + str(i) + \"_x_\" + \"Y\" + str(i)\n",
    "                    \n",
    "                # Then, create the dictionary:\n",
    "                dict_of_values = {'x': x, 'y': y, 'lab': lab}\n",
    "                \n",
    "                # Now, append dict_of_values to support list:\n",
    "                support_list.append(dict_of_values)\n",
    "            \n",
    "        # Now, support_list contains only the dictionaries with valid entries, as well\n",
    "        # as labels for each collection of data. The values are independent from their origin,\n",
    "        # and now they are ordered and in the same format of the data extracted directly from\n",
    "        # the dataframe.\n",
    "        # So, make the list_of_dictionaries_with_series_to_analyze the support_list itself:\n",
    "        list_of_dictionaries_with_series_to_analyze = support_list\n",
    "        print(f\"{len(list_of_dictionaries_with_series_to_analyze)} valid series input.\\n\")\n",
    "\n",
    "        \n",
    "    # Now that both methods of input resulted in the same format of list, we can process both\n",
    "    # with the same code.\n",
    "    \n",
    "    # Each dictionary in list_of_dictionaries_with_series_to_analyze represents a series to\n",
    "    # plot. So, the total of series to plot is:\n",
    "    total_of_series = len(list_of_dictionaries_with_series_to_analyze)\n",
    "    \n",
    "    if (total_of_series <= 0):\n",
    "        \n",
    "        print(\"No valid series to plot. Please, provide valid arguments.\\n\")\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        # Continue to plotting and calculating the fitting.\n",
    "        # Notice that we sorted the all the lists after they were separated and before\n",
    "        # adding them to dictionaries. Also, the timestamps were converted to datetime64 variables\n",
    "        # Now we finished the loop, list_of_dictionaries_with_series_to_analyze \n",
    "        # contains all series converted to NumPy arrays, with timestamps parsed as datetimes.\n",
    "        # This list will be the object returned at the end of the function. Since it is an\n",
    "        # JSON-formatted list, we can use the function json_obj_to_pandas_dataframe to convert\n",
    "        # it to a Pandas dataframe.\n",
    "        \n",
    "        \n",
    "        # Now, we can plot the figure.\n",
    "        # we set alpha = 0.95 (opacity) to give a degree of transparency (5%), \n",
    "        # so that one series do not completely block the visualization of the other.\n",
    "        \n",
    "        # Let's retrieve the list of Matplotlib CSS colors:\n",
    "        css4 = mcolors.CSS4_COLORS\n",
    "        # css4 is a dictionary of colors: {'aliceblue': '#F0F8FF', 'antiquewhite': '#FAEBD7', ...}\n",
    "        # Each key of this dictionary is a color name to be passed as argument color on the plot\n",
    "        # function. So let's retrieve the array of keys, and use the list attribute to convert this\n",
    "        # array to a list of colors:\n",
    "        list_of_colors = list(css4.keys())\n",
    "        \n",
    "        # In 11 May 2022, this list of colors had 148 different elements\n",
    "        # Since this list is in alphabetic order, let's create a random order for the colors.\n",
    "        \n",
    "        # Function random.sample(input_sequence, number_of_samples): \n",
    "        # this function creates a list containing a total of elements equals to the parameter \n",
    "        # \"number_of_samples\", which must be an integer.\n",
    "        # This list is obtained by ramdomly selecting a total of \"number_of_samples\" elements from the\n",
    "        # list \"input_sequence\" passed as parameter.\n",
    "        \n",
    "        # Function random.choices(input_sequence, k = number_of_samples):\n",
    "        # similarly, randomly select k elements from the sequence input_sequence. This function is\n",
    "        # newer than random.sample\n",
    "        # Since we want to simply randomly sort the sequence, we can pass k = len(input_sequence)\n",
    "        # to obtain the randomly sorted sequence:\n",
    "        list_of_colors = random.choices(list_of_colors, k = len(list_of_colors))\n",
    "        # Now, we have a random list of colors to use for plotting the charts\n",
    "        \n",
    "        if (add_splines_lines == True):\n",
    "            LINE_STYLE = '-'\n",
    "\n",
    "        else:\n",
    "            LINE_STYLE = ''\n",
    "        \n",
    "        if (add_scatter_dots == True):\n",
    "            MARKER = 'o'\n",
    "            \n",
    "        else:\n",
    "            MARKER = ''\n",
    "        \n",
    "        # Matplotlib linestyle:\n",
    "        # https://matplotlib.org/stable/gallery/lines_bars_and_markers/linestyles.html?msclkid=68737f24d16011eca9e9c4b41313f1ad\n",
    "        \n",
    "        if (plot_title is None):\n",
    "            # Set graphic title\n",
    "            plot_title = f\"Y_x_timestamp\"\n",
    "\n",
    "        if (horizontal_axis_title is None):\n",
    "            # Set horizontal axis title\n",
    "            horizontal_axis_title = \"timestamp\"\n",
    "\n",
    "        if (vertical_axis_title is None):\n",
    "            # Set vertical axis title\n",
    "            vertical_axis_title = \"Y\"\n",
    "        \n",
    "        # Let's put a small degree of transparency (1 - OPACITY) = 0.05 = 5%\n",
    "        # so that the bars do not completely block other views.\n",
    "        OPACITY = 0.95\n",
    "        \n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        fig = plt.figure(figsize = (12, 8))\n",
    "        ax = fig.add_subplot()\n",
    "\n",
    "        i = 0 # Restart counting for the loop of colors\n",
    "        \n",
    "        # Loop through each dictionary from list_of_dictionaries_with_series_and_predictions:\n",
    "        for dictionary in list_of_dictionaries_with_series_to_analyze:\n",
    "            \n",
    "            # Try selecting a color from list_of_colors:\n",
    "            try:\n",
    "                \n",
    "                COLOR = list_of_colors[i]\n",
    "                # Go to the next element i, so that the next plot will use a different color:\n",
    "                i = i + 1\n",
    "            \n",
    "            except IndexError:\n",
    "                \n",
    "                # This error will be raised if list index is out of range, \n",
    "                # i.e. if i >= len(list_of_colors) - we used all colors from the list (at least 148).\n",
    "                # So, return the index to zero to restart the colors from the beginning:\n",
    "                i = 0\n",
    "                COLOR = list_of_colors[i]\n",
    "                i = i + 1\n",
    "            \n",
    "            # Access the arrays and label from the dictionary:\n",
    "            X = dictionary['x']\n",
    "            Y = dictionary['y']\n",
    "            LABEL = dictionary['lab']\n",
    "            \n",
    "            # Scatter plot:\n",
    "            ax.plot(X, Y, linestyle = LINE_STYLE, marker = MARKER, color = COLOR, alpha = OPACITY, label = LABEL)\n",
    "            # Axes.plot documentation:\n",
    "            # https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.plot.html?msclkid=42bc92c1d13511eca8634a2c93ab89b5\n",
    "            \n",
    "            # x and y are positional arguments: they are specified by their position in function\n",
    "            # call, not by an argument name like 'marker'.\n",
    "            \n",
    "            # Matplotlib markers:\n",
    "            # https://matplotlib.org/stable/api/markers_api.html?msclkid=36c5eec5d16011ec9583a5777dc39d1f\n",
    "            \n",
    "        # Now we finished plotting all of the series, we can set the general configuration:\n",
    "        \n",
    "        #ROTATE X AXIS IN XX DEGREES\n",
    "        plt.xticks(rotation = x_axis_rotation)\n",
    "        # XX = 0 DEGREES x_axis (Default)\n",
    "        #ROTATE Y AXIS IN XX DEGREES:\n",
    "        plt.yticks(rotation = y_axis_rotation)\n",
    "        # XX = 0 DEGREES y_axis (Default)\n",
    "\n",
    "        ax.set_title(plot_title)\n",
    "        ax.set_xlabel(horizontal_axis_title)\n",
    "        ax.set_ylabel(vertical_axis_title)\n",
    "\n",
    "        ax.grid(grid) # show grid or not\n",
    "        ax.legend(loc = 'upper left')\n",
    "        # position options: 'upper right'; 'upper left'; 'lower left'; 'lower right';\n",
    "        # 'right', 'center left'; 'center right'; 'lower center'; 'upper center', 'center'\n",
    "        # https://www.statology.org/matplotlib-legend-position/\n",
    "\n",
    "        if (export_png == True):\n",
    "            # Image will be exported\n",
    "            import os\n",
    "\n",
    "            #check if the user defined a directory path. If not, set as the default root path:\n",
    "            if (directory_to_save is None):\n",
    "                #set as the default\n",
    "                directory_to_save = \"\"\n",
    "\n",
    "            #check if the user defined a file name. If not, set as the default name for this\n",
    "            # function.\n",
    "            if (file_name is None):\n",
    "                #set as the default\n",
    "                file_name = \"time_series_vis\"\n",
    "\n",
    "            #check if the user defined an image resolution. If not, set as the default 110 dpi\n",
    "            # resolution.\n",
    "            if (png_resolution_dpi is None):\n",
    "                #set as 330 dpi\n",
    "                png_resolution_dpi = 330\n",
    "\n",
    "            #Get the new_file_path\n",
    "            new_file_path = os.path.join(directory_to_save, file_name)\n",
    "\n",
    "            #Export the file to this new path:\n",
    "            # The extension will be automatically added by the savefig method:\n",
    "            plt.savefig(new_file_path, dpi = png_resolution_dpi, quality = 100, format = 'png', transparent = False) \n",
    "            #quality could be set from 1 to 100, where 100 is the best quality\n",
    "            #format (str, supported formats) = 'png', 'pdf', 'ps', 'eps' or 'svg'\n",
    "            #transparent = True or False\n",
    "            # For other parameters of .savefig method, check https://indianaiproduction.com/matplotlib-savefig/\n",
    "            print (f\"Figure exported as \\'{new_file_path}.png\\'. Any previous file in this root path was overwritten.\")\n",
    "\n",
    "        #Set image size (x-pixels, y-pixels) for printing in the notebook's cell:\n",
    "        #plt.figure(figsize = (12, 8))\n",
    "        #fig.tight_layout()\n",
    "\n",
    "        ## Show an image read from an image file:\n",
    "        ## import matplotlib.image as pltimg\n",
    "        ## img=pltimg.imread('mydecisiontree.png')\n",
    "        ## imgplot = plt.imshow(img)\n",
    "        ## See linkedIn Learning course: \"Supervised machine learning and the technology boom\",\n",
    "        ##  Ex_Files_Supervised_Learning, Exercise Files, lesson '03. Decision Trees', '03_05', \n",
    "        ##  '03_05_END.ipynb'\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "30e909aa-186e-45b7-bff2-454066e125c7"
   },
   "source": [
    "# **Function for column filtering (selecting); ordering; or renaming all columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "de66d1bd-465d-4e49-9d4a-a5b2a7d3fdcc",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def select_order_or_rename_columns (df, columns_list, mode = 'select_or_order_columns'):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # MODE = 'select_or_order_columns' for filtering only the list of columns passed as columns_list,\n",
    "    # and setting a new column order. In this mode, you can pass the columns in any order: \n",
    "    # the order of elements on the list will be the new order of columns.\n",
    "\n",
    "    # MODE = 'rename_columns' for renaming the columns with the names passed as columns_list. In this\n",
    "    # mode, the list must have same length and same order of the columns of the dataframe. That is because\n",
    "    # the columns will sequentially receive the names in the list. So, a mismatching of positions\n",
    "    # will result into columns with incorrect names.\n",
    "    \n",
    "    # columns_list = list of strings containing the names (headers) of the columns to select\n",
    "    # (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "    # For instance: columns_list = ['col1', 'col2', 'col3'] will \n",
    "    # select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "    # Declare the names inside quotes.\n",
    "    \n",
    "    # Set a local copy of the dataframe to manipulate:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    print(f\"Original columns in the dataframe:\\n{DATASET.columns}\\n\")\n",
    "    \n",
    "    if ((columns_list is None) | (columns_list == np.nan)):\n",
    "        # empty list\n",
    "        columns_list = []\n",
    "    \n",
    "    if (len(columns_list) == 0):\n",
    "        print(\"Please, input a valid list of columns.\\n\")\n",
    "        return DATASET\n",
    "    \n",
    "    if (mode == 'select_or_order_columns'):\n",
    "        \n",
    "        #filter the dataframe so that it will contain only the cols_list.\n",
    "        DATASET = DATASET[columns_list]\n",
    "        print(\"Dataframe filtered according to the list provided.\\n\")\n",
    "        print(\"Check the new dataframe:\\n\")\n",
    "        \n",
    "        try:\n",
    "            # only works in Jupyter Notebook:\n",
    "            from IPython.display import display\n",
    "            display(DATASET)\n",
    "\n",
    "        except: # regular mode\n",
    "            print(DATASET)\n",
    "        \n",
    "    elif (mode == 'rename_columns'):\n",
    "        \n",
    "        # Check if the number of columns of the dataset is equal to the number of elements\n",
    "        # of the new list. It will avoid raising an exception error.\n",
    "        boolean_filter = (len(columns_list) == len(DATASET.columns))\n",
    "        \n",
    "        if (boolean_filter == False):\n",
    "            #Impossible to rename, number of elements are different.\n",
    "            print(\"The number of columns of the dataframe is different from the number of elements of the list. Please, provide a list with number of elements equals to the number of columns.\\n\")\n",
    "            return DATASET\n",
    "        \n",
    "        else:\n",
    "            #Same number of elements, so that we can update the columns' names.\n",
    "            DATASET.columns = columns_list\n",
    "            print(\"Dataframe columns renamed according to the list provided.\\n\")\n",
    "            print(\"Warning: the substitution is element-wise: the first element of the list is now the name of the first column, and so on, ..., so that the last element is the name of the last column.\\n\")\n",
    "            print(\"Check the new dataframe:\\n\")\n",
    "            try:\n",
    "                # only works in Jupyter Notebook:\n",
    "                from IPython.display import display\n",
    "                display(DATASET)\n",
    "\n",
    "            except: # regular mode\n",
    "                print(DATASET)\n",
    "        \n",
    "    else:\n",
    "        print(\"Enter a valid mode: \\'select_or_order_columns\\' or \\'rename_columns\\'.\")\n",
    "        return DATASET\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "70b4d825-6e6f-4950-ae9b-49d3b2ef69fb"
   },
   "source": [
    "# **Function for reversing the log-transform - applying the exponential transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "d2816e91-e621-4d2d-8889-336e6e16a0d8",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def reverse_log_transform (df, subset = None, create_new_columns = True, new_columns_suffix = \"_originalScale\"):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    #### WARNING: This function will eliminate rows where the selected variables present \n",
    "    #### values lower or equal to zero (condition for the logarithm to be applied).\n",
    "    \n",
    "    # subset = None\n",
    "    # Set subset = None to transform the whole dataset. Alternatively, pass a list with \n",
    "    # columns names for the transformation to be applied. For instance:\n",
    "    # subset = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "    # as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "    # Declaring the full list of columns is equivalent to setting subset = None.\n",
    "    \n",
    "    # create_new_columns = True\n",
    "    # Alternatively, set create_new_columns = True to store the transformed data into new\n",
    "    # columns. Or set create_new_columns = False to overwrite the existing columns\n",
    "    \n",
    "    # new_columns_suffix = \"_originalScale\"\n",
    "    # This value has effect only if create_new_column = True.\n",
    "    # The new column name will be set as column + new_columns_suffix. Then, if the original\n",
    "    # column was \"column1\" and the suffix is \"_originalScale\", the new column will be named \n",
    "    # as \"column1_originalScale\".\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name.\n",
    "    \n",
    "    \n",
    "    # Start a local copy of the dataframe:\n",
    "    DATASET = df.copy(deep = True)\n",
    "    \n",
    "    # Check if a subset was defined. If so, make columns_list = subset \n",
    "    if not (subset is None):\n",
    "        \n",
    "        columns_list = subset\n",
    "    \n",
    "    else:\n",
    "        #There is no declared subset. Then, make columns_list equals to the list of\n",
    "        # numeric columns of the dataframe.\n",
    "        columns_list = list(DATASET.columns)\n",
    "        \n",
    "    # Let's check if there are categorical columns in columns_list. Only numerical\n",
    "    # columns should remain\n",
    "    # Start a support list:\n",
    "    support_list = []\n",
    "    # List the possible numeric data types for a Pandas dataframe column:\n",
    "    numeric_dtypes = [np.int16, np.int32, np.int64, np.float16, np.float32, np.float64]\n",
    "\n",
    "    # Loop through each column in columns_list:\n",
    "    for column in columns_list:\n",
    "        \n",
    "        # Check the Pandas series (column) data type:\n",
    "        column_type = DATASET[column].dtype\n",
    "            \n",
    "        # If it is not categorical (object), append it to the support list:\n",
    "        if (column_type in numeric_dtypes):\n",
    "                \n",
    "            support_list.append(column)\n",
    "    \n",
    "    # Finally, make the columns_list support_list itself:\n",
    "    columns_list = support_list\n",
    "    \n",
    "    #Loop through each column to apply the transform:\n",
    "    for column in columns_list:\n",
    "        #access each element in the list column_list. The element is named 'column'.\n",
    "        \n",
    "        # The exponential transformation can be applied to zero and negative values,\n",
    "        # so we remove the boolean filter.\n",
    "        \n",
    "        #Check if a new column will be created, or if the original column should be\n",
    "        # substituted.\n",
    "        if (create_new_columns == True):\n",
    "            # Create a new column.\n",
    "            \n",
    "            # The new column name will be set as column + new_columns_suffix\n",
    "            new_column_name = column + new_columns_suffix\n",
    "        \n",
    "        else:\n",
    "            # Overwrite the existing column. Simply set new_column_name as the value 'column'\n",
    "            new_column_name = column\n",
    "        \n",
    "        # Calculate the column value as the log transform of the original series (column)\n",
    "        DATASET[new_column_name] = np.exp(DATASET[column])\n",
    "    \n",
    "    print(\"The log_transform was successfully reversed through the exponential transformation. Check the 10 first rows of the new dataset:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(DATASET.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(DATASET.head(10))\n",
    "    \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a23c7fab-c45c-4ccc-a12a-68c713832e5b"
   },
   "source": [
    "# **Function for reversing Box-Cox transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c477161c-72f6-4303-97c4-5579584fcc8e",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def reverse_box_cox (df, column_to_transform, lambda_boxcox, suffix = '_ReversedBoxCox'):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # This function will process a single column column_to_transform \n",
    "    # of the dataframe df per call.\n",
    "    \n",
    "    # Check https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html\n",
    "    ## Box-Cox transform is given by:\n",
    "    ## y = (x**lmbda - 1) / lmbda,  for lmbda != 0\n",
    "    ## log(x),                  for lmbda = 0\n",
    "    \n",
    "    # column_to_transform must be a string with the name of the column.\n",
    "    # e.g. column_to_transform = 'column1' to transform a column named as 'column1'\n",
    "    \n",
    "    # lambda_boxcox must be a float value. e.g. lamda_boxcox = 1.7\n",
    "    # If you calculated lambda from the function box_cox_transform and saved the\n",
    "    # transformation data summary dictionary as data_sum_dict, simply set:\n",
    "    # lambda_boxcox = data_sum_dict['lambda_boxcox']\n",
    "    # This will access the value on the key 'lambda_boxcox' of the dictionary, which\n",
    "    # contains the lambda. \n",
    "    \n",
    "    # Analogously, spec_lim_dict['Inf_spec_lim_transf'] access\n",
    "    # the inferior specification limit transformed; and spec_lim_dict['Sup_spec_lim_transf'] \n",
    "    # access the superior specification limit transformed.\n",
    "    \n",
    "    #suffix: string (inside quotes).\n",
    "    # How the transformed column will be identified in the returned data_transformed_df.\n",
    "    # If y_label = 'Y' and suffix = '_ReversedBoxCox', the transformed column will be\n",
    "    # identified as '_ReversedBoxCox'.\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name\n",
    "    \n",
    "    \n",
    "    # Start a local copy of the dataframe:\n",
    "    DATASET = df.copy(deep = True)\n",
    "\n",
    "    y = DATASET[column_to_transform]\n",
    "    \n",
    "    if (lambda_boxcox == 0):\n",
    "        #ytransf = np.log(y), according to Box-Cox definition. Then\n",
    "        #y_retransform = np.exp(y)\n",
    "        #In the case of this function, ytransf is passed as the argument y.\n",
    "        y_transform = np.exp(y)\n",
    "    \n",
    "    else:\n",
    "        #apply Box-Cox function:\n",
    "        #y_transf = (y**lmbda - 1) / lmbda. Then,\n",
    "        #y_retransf ** (lmbda) = (y_transf * lmbda) + 1\n",
    "        #y_retransf = ((y_transf * lmbda) + 1) ** (1/lmbda), where ** is the potentiation\n",
    "        #In the case of this function, ytransf is passed as the argument y.\n",
    "        y_transform = ((y * lambda_boxcox) + 1) ** (1/lambda_boxcox)\n",
    "    \n",
    "    if not (suffix is None):\n",
    "        #only if a suffix was declared\n",
    "        #concatenate the column name to the suffix\n",
    "        new_col = column_to_transform + suffix\n",
    "    \n",
    "    else:\n",
    "        #concatenate the column name to the standard '_ReversedBoxCox' suffix\n",
    "        new_col = column_to_transform + '_ReversedBoxCox'\n",
    "    \n",
    "    DATASET[new_col] = y_transform\n",
    "    #dataframe contendo os dados transformados\n",
    "    \n",
    "    print(\"Data successfully retransformed. Check the 10 first retransformed rows:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(DATASET.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(DATASET.head(10))\n",
    "    \n",
    "    print(\"\\n\") #line break\n",
    " \n",
    "    return DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "86cc0d94-11d5-40cd-89dc-a35df48f0161"
   },
   "source": [
    "# **Function for One-Hot Encoding categorical features**\n",
    "- Transform categorical values without notion of order into numerical (binary) features.\n",
    "- For each category, the One-Hot Encoder creates a new column in the dataset. This new column is represented by a binary variable which is equals to zero if the row is not classified in that category; and is equals to 1 when the row represents an element in that category.\n",
    "- The new columns will be named as the original columns + \"_\" + possible categories + \"OneHotEnc\".\n",
    "- Each column is a binary variable of the type \"is classified in this category or not\".\n",
    "\n",
    "Therefore, for a category \"A\", a column named \"A\" is created.\n",
    "- If the row is an element from category \"A\", the value for the column \"A\" is 1.\n",
    "- If not, the value for column \"A\" is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "61b7e54d-53be-4bd3-8a89-51fd5fd63b6c",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def OneHotEncode_df (df, subset_of_features_to_be_encoded):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "    \n",
    "    # df: the whole dataframe to be processed.\n",
    "    \n",
    "    # subset_of_features_to_be_encoded: list of strings (inside quotes), \n",
    "    # containing the names of the columns with the categorical variables that will be \n",
    "    # encoded. If a single column will be encoded, declare this parameter as list with\n",
    "    # only one element e.g.subset_of_features_to_be_encoded = [\"column1\"] \n",
    "    # will analyze the column named as 'column1'; \n",
    "    # subset_of_features_to_be_encoded = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "    # with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "    \n",
    "    #Start an encoding list empty (it will be a JSON object):\n",
    "    encoding_list = []\n",
    "    \n",
    "    # Start a copy of the original dataframe. This copy will be updated to create the new\n",
    "    # transformed dataframe. Then, we avoid manipulating the original object.\n",
    "    new_df = df.copy(deep = True)\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display  \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    #loop through each column of the subset:\n",
    "    for column in subset_of_features_to_be_encoded:\n",
    "        \n",
    "        # Start two empty dictionaries:\n",
    "        encoding_dict = {}\n",
    "        nested_dict = {}\n",
    "        \n",
    "        # Add the column to encoding_dict as the key 'column':\n",
    "        encoding_dict['column'] = column\n",
    "        \n",
    "        # Loop through each element (named 'column') of the list of columns to analyze,\n",
    "        # subset_of_features_to_be_encoded\n",
    "        \n",
    "        # We could process the whole subset at once, but it could make us lose information\n",
    "        # about the generated columns\n",
    "        \n",
    "        # set a subset of the dataframe X containing 'column' as the only column:\n",
    "        # it will be equivalent to using .reshape(-1,1) to set a 1D-series\n",
    "        # or array in the shape for scikit-learn:\n",
    "        # For doing so, pass a list of columns for column filtering, containing\n",
    "        # the object column as its single element:\n",
    "        X  = df[[column]]\n",
    "        \n",
    "        #Start the OneHotEncoder object:\n",
    "        OneHot_enc_obj = OneHotEncoder()\n",
    "        \n",
    "        #Fit the object to that column:\n",
    "        OneHot_enc_obj = OneHot_enc_obj.fit(X)\n",
    "        # Get the transformed columns as a SciPy sparse matrix: \n",
    "        transformed_columns = OneHot_enc_obj.transform(X)\n",
    "        # Convert the sparse matrix to a NumPy dense array:\n",
    "        transformed_columns = transformed_columns.toarray()\n",
    "        \n",
    "        # Now, let's retrieve the encoding information and save it:\n",
    "        # Show encoded categories and store this array. \n",
    "        # It will give the proper columns' names:\n",
    "        encoded_columns = OneHot_enc_obj.categories_\n",
    "\n",
    "        # encoded_columns is an array containing a single element.\n",
    "        # This element is an array like:\n",
    "        # array(['cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8'], dtype=object)\n",
    "        # Then, this array is the element of index 0 from the list encoded_columns.\n",
    "        # It is represented as encoded_columns[0]\n",
    "\n",
    "        # Therefore, we actually want the array which is named as encoded_columns[0]\n",
    "        # Each element of this array is the name of one of the encoded columns. In the\n",
    "        # example above, the element 'cat2' would be accessed as encoded_columns[0][1],\n",
    "        # since it is the element of index [1] (second element) from the array \n",
    "        # encoded_columns[0].\n",
    "        \n",
    "        new_columns = encoded_columns[0]\n",
    "        # To identify the column that originated these new columns, we can join the\n",
    "        # string column to each element from new_columns:\n",
    "        \n",
    "        # Update the nested dictionary: store the new_columns as the key 'categories':\n",
    "        nested_dict['categories'] = new_columns\n",
    "        # Store the encoder object as the key 'OneHot_enc_obj'\n",
    "        # Add the encoder object to the dictionary:\n",
    "        nested_dict['OneHot_enc_obj'] = OneHot_enc_obj\n",
    "        \n",
    "        # Store the nested dictionary in the encoding_dict as the key 'OneHot_encoder':\n",
    "        encoding_dict['OneHot_encoder'] = nested_dict\n",
    "        # Append the encoding_dict as an element from list encoding_list:\n",
    "        encoding_list.append(encoding_dict)\n",
    "        \n",
    "        # Now we saved all encoding information, let's transform the data:\n",
    "        \n",
    "        # Start a support_list to store the concatenated strings:\n",
    "        support_list = []\n",
    "        \n",
    "        for encoded_col in new_columns:\n",
    "            # Use the str attribute to guarantee that the array stores only strings.\n",
    "            # Add an underscore \"_\" to separate the strings and an identifier of the transform:\n",
    "            new_column = column + \"_\" + str(encoded_col) + \"_OneHotEnc\"\n",
    "            # Append it to the support_list:\n",
    "            support_list.append(new_column)\n",
    "            \n",
    "        # Convert the support list to NumPy array, and make new_columns the support list itself:\n",
    "        new_columns = np.array(support_list)\n",
    "        \n",
    "        # Crete a Pandas dataframe from the array transformed_columns:\n",
    "        encoded_X_df = pd.DataFrame(transformed_columns)\n",
    "        \n",
    "        # Modify the name of the columns to make it equal to new_columns:\n",
    "        encoded_X_df.columns = new_columns\n",
    "        \n",
    "        #Inner join the new dataset with the encoded dataset.\n",
    "        # Use the index as the key, since indices are necessarily correspondent.\n",
    "        # To use join on index, we apply pandas .concat method.\n",
    "        # To join on a specific key, we could use pandas .merge method with the arguments\n",
    "        # left_on = 'left_key', right_on = 'right_key'; or, if the keys have same name,\n",
    "        # on = 'key':\n",
    "        # Check Pandas merge and concat documentation:\n",
    "        # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge.html\n",
    "        \n",
    "        new_df = pd.concat([new_df, encoded_X_df], axis = 1, join = \"inner\")\n",
    "        # When axis = 0, the .concat operation occurs in the row level, so the rows\n",
    "        # of the second dataframe are added to the bottom of the first one.\n",
    "        # It is the SQL union, and creates a dataframe with more rows, and\n",
    "        # total of columns equals to the total of columns of the first dataframe\n",
    "        # plus the columns of the second one that were not in the first dataframe.\n",
    "        # When axis = 1, the operation occurs in the column level: the two\n",
    "        # dataframes are laterally merged using the index as the key, \n",
    "        # preserving all columns from both dataframes. Therefore, the number of\n",
    "        # rows will be the total of rows of the dataframe with more entries,\n",
    "        # and the total of columns will be the sum of the total of columns of\n",
    "        # the first dataframe with the total of columns of the second dataframe.\n",
    "        \n",
    "        print(f\"Successfully encoded column \\'{column}\\' and merged the encoded columns to the dataframe.\\n\")\n",
    "        print(\"Check first 5 rows of the encoded table that was merged:\\n\")\n",
    "        \n",
    "        try:\n",
    "            display(encoded_X_df.head())\n",
    "        except: # regular mode\n",
    "            print(encoded_X_df.head())\n",
    "        \n",
    "        # The default of the head method, when no parameter is printed, is to show 5 rows; if an\n",
    "        # integer number Y is passed as argument .head(Y), Pandas shows the first Y-rows.\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    print(\"Finished One-Hot Encoding. Returning the new transformed dataframe; and an encoding list.\\n\")\n",
    "    print(\"Each element from this list is a dictionary with the original column name as key \\'column\\', and a nested dictionary as the key \\'OneHot_encoder\\'.\\n\")\n",
    "    print(\"In turns, the nested dictionary shows the different categories as key \\'categories\\' and the encoder object as the key \\'OneHot_enc_obj\\'.\\n\")\n",
    "    print(\"Use the encoder object to inverse the One-Hot Encoding in the correspondent function.\\n\")\n",
    "    print(f\"For each category in the columns \\'{subset_of_features_to_be_encoded}\\', a new column has value 1, if it is the actual category of that row; or is 0 if not.\\n\")\n",
    "    print(\"Check the first 10 rows of the new dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        display(new_df.head(10))\n",
    "    except:\n",
    "        print(new_df.head(10))\n",
    "\n",
    "    #return the transformed dataframe and the encoding dictionary:\n",
    "    return new_df, encoding_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "7e688fb5-9eea-4388-b184-63e65766a927"
   },
   "source": [
    "# **Function for reversing the scaling of the features**\n",
    "- `mode = 'standard'`.\n",
    "- `mode = 'min_max'`.\n",
    "- `mode = 'factor'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "06dc2064-e600-4acb-9a94-5881748e69b0",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def reverse_feature_scaling (df, subset_of_features_to_scale, list_of_scaling_params, mode = 'min_max', suffix = '_reverseScaling'):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    # Scikit-learn Preprocessing data guide:\n",
    "    # https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler\n",
    "    # Standard scaler documentation:\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "    # Min-Max scaler documentation:\n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler.set_params\n",
    "    \n",
    "    ## mode = 'standard': reverses the standard scaling, \n",
    "    ##  which creates a new variable with mean = 0; and standard deviation = 1.\n",
    "    ##  Each value Y is transformed as Ytransf = (Y - u)/s, where u is the mean \n",
    "    ##  of the training samples, and s is the standard deviation of the training samples.\n",
    "    \n",
    "    ## mode = 'min_max': reverses min-max normalization, with a resultant feature \n",
    "    ## ranging from 0 to 1. each value Y is transformed as \n",
    "    ## Ytransf = (Y - Ymin)/(Ymax - Ymin), where Ymin and Ymax are the minimum and \n",
    "    ## maximum values of Y, respectively.\n",
    "    ## mode = 'factor': reverses the division of the whole series by a numeric value \n",
    "    # provided as argument. \n",
    "    ## For a factor F, the new Y transformed values are Ytransf = Y/F.\n",
    "    # Notice that if the original mode was 'normalize_by_maximum', then the maximum value used\n",
    "    # must be declared as any other factor.\n",
    "    \n",
    "    # df: the whole dataframe to be processed.\n",
    "    \n",
    "    # subset_of_features_to_be_scaled: list of strings (inside quotes), \n",
    "    # containing the names of the columns with the categorical variables that will be \n",
    "    # encoded. If a single column will be encoded, declare this parameter as list with\n",
    "    # only one element e.g.subset_of_features_to_be_scaled = [\"column1\"] \n",
    "    # will analyze the column named as 'column1'; \n",
    "    # subset_of_features_to_be_scaled = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "    # with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "    \n",
    "    # list_of_scaling_params is a list of dictionaries with the same format of the list returned\n",
    "    # from this function. Each dictionary must correspond to one of the features that will be scaled,\n",
    "    # but the list do not have to be in the same order of the columns - it will check one of the\n",
    "    # dictionary keys.\n",
    "    # The first key of the dictionary must be 'column'. This key must store a string with the exact\n",
    "    # name of the column that will be scaled.\n",
    "    # the second key must be 'scaler'. This key must store a dictionary. The dictionary must store\n",
    "    # one of two keys: 'scaler_obj' - sklearn scaler object to be used; or 'scaler_details' - the\n",
    "    # numeric parameters for re-calculating the scaler without the object. The key 'scaler_details', \n",
    "    # must contain a nested dictionary. For the mode 'min_max', this dictionary should contain \n",
    "    # two keys: 'min', with the minimum value of the variable, and 'max', with the maximum value. \n",
    "    # For mode 'standard', the keys should be 'mu', with the mean value, and 'sigma', with its \n",
    "    # standard deviation. For the mode 'factor', the key should be 'factor', and should contain the \n",
    "    # factor for division (the scaling value. e.g 'factor': 2.0 will divide the column by 2.0.).\n",
    "    # Again, if you want to normalize by the maximum, declare the maximum value as any other factor for\n",
    "    # division.\n",
    "    # The key 'scaler_details' will not create an object: the transform will be directly performed \n",
    "    # through vectorial operations.\n",
    "    \n",
    "    # suffix: string (inside quotes).\n",
    "    # How the transformed column will be identified in the returned data_transformed_df.\n",
    "    # If y_label = 'Y' and suffix = '_reverseScaling', the transformed column will be\n",
    "    # identified as '_reverseScaling'.\n",
    "    # Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "    # start the suffix with \"_\" to separate it from the original name\n",
    "      \n",
    "    if (suffix is None):\n",
    "        #set as the default\n",
    "        suffix = '_reverseScaling'\n",
    "    \n",
    "    #Start a copy of the original dataframe. This copy will be updated to create the new\n",
    "    # transformed dataframe. Then, we avoid manipulating the original object.\n",
    "    new_df = df.copy(deep = True)\n",
    "    \n",
    "    #Start an scaling list empty (it will be a JSON object):\n",
    "    scaling_list = []\n",
    "    \n",
    "    # Use a previously obtained scaler:\n",
    "    \n",
    "    for column in subset_of_features_to_scale:\n",
    "        \n",
    "        # Create a dataframe X by subsetting only the analyzed column\n",
    "        # it will be equivalent to using .reshape(-1,1) to set a 1D-series\n",
    "        # or array in the shape for scikit-learn:\n",
    "        # For doing so, pass a list of columns for column filtering, containing\n",
    "        # the object column as its single element:\n",
    "        X = new_df[[column]]\n",
    "\n",
    "        # Loop through each element of the list:\n",
    "            \n",
    "        for scaling_dict in list_of_scaling_params:\n",
    "                \n",
    "            # check if the dictionary is from that column:\n",
    "            if (scaling_dict['column'] == column):\n",
    "                    \n",
    "                # We found the correct dictionary. Let's retrieve the information:\n",
    "                # retrieve the nested dictionary:\n",
    "                nested_dict = scaling_dict['scaler']\n",
    "                    \n",
    "                # try accessing the scaler object:\n",
    "                try:\n",
    "                    scaler = nested_dict['scaler_obj']\n",
    "                    #calculate the reversed scaled feature, and store it as new array:\n",
    "                    rev_scaled_feature = scaler.inverse_transform(X)\n",
    "                        \n",
    "                    # Add the parameters to the nested dictionary:\n",
    "                    nested_dict['scaling_params'] = scaler.get_params(deep = True)\n",
    "                        \n",
    "                    if (mode == 'standard'):\n",
    "                            \n",
    "                        nested_dict['scaler_details'] = {\n",
    "                                'mu': rev_scaled_feature.mean(),\n",
    "                                'sigma': rev_scaled_feature.std()\n",
    "                            }\n",
    "                        \n",
    "                    elif (mode == 'min_max'):\n",
    "                            \n",
    "                        nested_dict['scaler_details'] = {\n",
    "                                'min': rev_scaled_feature.min(),\n",
    "                                'max': rev_scaled_feature.max()\n",
    "                            }\n",
    "                    \n",
    "                except:\n",
    "                        \n",
    "                    try:\n",
    "                        # As last alternative, let's try accessing the scaler details dict\n",
    "                        scaler_details = nested_dict['scaler_details']\n",
    "                                \n",
    "                        if (mode == 'standard'):\n",
    "                                \n",
    "                            nested_dict['scaling_params'] = 'standard_scaler_manually_defined'\n",
    "                            mu = scaler_details['mu']\n",
    "                            sigma = scaler_details['sigma']\n",
    "                                    \n",
    "                            if (sigma != 0):\n",
    "                                # scaled_feature = (X - mu)/sigma\n",
    "                                rev_scaled_feature = (X * sigma) + mu\n",
    "                            else:\n",
    "                                # scaled_feature = (X - mu)\n",
    "                                rev_scaled_feature = (X + mu)\n",
    "                                \n",
    "                        elif (mode == 'min_max'):\n",
    "                                    \n",
    "                            nested_dict['scaling_params'] = 'min_max_scaler_manually_defined'\n",
    "                            minimum = scaler_details['min']\n",
    "                            maximum = scaler_details['max']\n",
    "                                    \n",
    "                            if ((maximum - minimum) != 0):\n",
    "                                # scaled_feature = (X - minimum)/(maximum - minimum)\n",
    "                                rev_scaled_feature = (X * (maximum - minimum)) + minimum\n",
    "                            else:\n",
    "                                # scaled_feature = X/maximum\n",
    "                                rev_scaled_feature = (X * maximum)\n",
    "                                \n",
    "                        elif (mode == 'factor'):\n",
    "                                \n",
    "                            nested_dict['scaling_params'] = 'normalization_by_factor'\n",
    "                            factor = scaler_details['factor']\n",
    "                            # scaled_feature = X/(factor)\n",
    "                            rev_scaled_feature = (X * factor)\n",
    "                                \n",
    "                        else:\n",
    "                            print(\"Select a valid mode: standard, min_max, or factor.\\n\")\n",
    "                            return \"error\", \"error\"\n",
    "                            \n",
    "                    except:\n",
    "                                \n",
    "                        print(f\"No valid scaling dictionary was input for column {column}.\\n\")\n",
    "                        return \"error\", \"error\"\n",
    "         \n",
    "                # Create the new_column name:\n",
    "                new_column = column + suffix\n",
    "                # Create the new_column by dividing the previous column by the scaling factor:\n",
    "\n",
    "                # Set the new column as rev_scaled_feature\n",
    "                new_df[new_column] = rev_scaled_feature\n",
    "\n",
    "                # Add the nested dictionary to the scaling_dict:\n",
    "                scaling_dict['scaler'] = nested_dict\n",
    "\n",
    "                # Finally, append the scaling_dict to the list scaling_list:\n",
    "                scaling_list.append(scaling_dict)\n",
    "\n",
    "                print(f\"Successfully re-scaled column {column}.\\n\")\n",
    "                \n",
    "    print(\"Successfully re-scaled the dataframe.\\n\")\n",
    "    print(\"Check 10 first rows of the new dataframe:\\n\")\n",
    "    \n",
    "    try:\n",
    "        # only works in Jupyter Notebook:\n",
    "        from IPython.display import display\n",
    "        display(new_df.head(10))\n",
    "            \n",
    "    except: # regular mode\n",
    "        print(new_df.head(10))\n",
    "                \n",
    "    return new_df, scaling_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "2f5de8a2-cebb-4134-a4d9-5b98e285ddaf",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "## **Call the functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "0639929e-1c96-4901-ac44-0f8e5e5bfa18",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "2feeca77-2db7-4780-9e74-ea126a62971d",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "SOURCE = 'aws'\n",
    "# SOURCE = 'google' for mounting the google drive;\n",
    "# SOURCE = 'aws' for accessing an AWS S3 bucket\n",
    "\n",
    "## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN SOURCE == 'aws':\n",
    "\n",
    "PATH_TO_STORE_IMPORTED_S3_BUCKET = ''\n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET: path of the Python environment to which the\n",
    "# S3 bucket contents will be imported. If it is None; or if it is an empty string; or if \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = '/', bucket will be imported to the root path. \n",
    "# Alternatively, input the path as a string (in quotes). e.g. \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = 'copied_s3_bucket'\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for fetching AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "mount_storage_system (source = SOURCE, path_to_store_imported_s3_bucket = PATH_TO_STORE_IMPORTED_S3_BUCKET, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "6aa991e6-9314-479f-bcd5-2dd01f7a8ad8"
   },
   "source": [
    "## **Downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a34b5c8b-373a-4478-ac58-5bc7c86e0483"
   },
   "source": [
    "#### Case 1: upload a file to Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a221de4b-9063-4baa-9db6-8298b0e0cbd6",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model named keras_model, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'keras_model.h5'\n",
    "\n",
    "# Dictionary storing the uploaded files returned as colab_files_dict.\n",
    "# Simply modify this object on the left of the equality:\n",
    "colab_files_dict = upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "cc1a90dc-2ca8-40e5-8f5f-375c3592301b"
   },
   "source": [
    "#### Case 2: download a file from Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a0d16ab2-463c-4156-ab13-462f85c318da",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'download'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model nameACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b6939a1e-111d-4dbe-85a5-7a8c5a2169d3",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "80c817f6-2ead-4309-a692-673a9a1d1646",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "## JSON, txt, or CSV (comma separated values) files.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
    "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "# FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "# Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
    "\n",
    "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
    "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True \n",
    "# if you want to read a file with txt extension containing a text formatted as JSON \n",
    "# (but not saved as JSON).\n",
    "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the \n",
    "# function (below) must be set. If not, an error message will be raised.\n",
    "\n",
    "HOW_MISSING_VALUES_ARE_REGISTERED = None\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "# empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "# This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "#‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "# ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "# If a different denomination is used, indicate it as a string. e.g.\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "# If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "# only in column 'numeric_col', you can specify the following dictionary:\n",
    "# how_missing_values_are_registered = {'numeric-col': 0}\n",
    "\n",
    "    \n",
    "HAS_HEADER = True\n",
    "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
    "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
    "\n",
    "DECIMAL_SEPARATOR = '.'\n",
    "# DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "# the decimal separator. Alternatively, specify here the separator.\n",
    "# e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "# It manipulates the argument 'decimal' from Pandas functions.\n",
    "\n",
    "TXT_CSV_COL_SEP = \"comma\"\n",
    "# txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "# or 'csv'. It informs how the different columns are separated.\n",
    "# Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "# for columns separated by comma;\n",
    "# txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "# for columns separated by simple spaces.\n",
    "# You can also set a specific separator as string. For example:\n",
    "# txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "# is used as separator for the columns - '\\t' represents the tab character).\n",
    "\n",
    "## Parameters for loading Excel files:\n",
    "\n",
    "LOAD_ALL_SHEETS_AT_ONCE = False\n",
    "# LOAD_ALL_SHEETS_AT_ONCE = False - This parameter has effect only when for Excel files.\n",
    "# If LOAD_ALL_SHEETS_AT_ONCE = True, the function will return a list of dictionaries, each\n",
    "# dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
    "# value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
    "# and its value will be the pandas dataframe object obtained from that sheet.\n",
    "# This argument has preference over SHEET_TO_LOAD. If it is True, all sheets will be loaded.\n",
    "    \n",
    "SHEET_TO_LOAD = None\n",
    "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
    "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
    "# will be loaded.\n",
    "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
    "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
    "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "# name to load the sheet with that name.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = load_pandas_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, how_missing_values_are_registered = HOW_MISSING_VALUES_ARE_REGISTERED, has_header = HAS_HEADER, decimal_separator = DECIMAL_SEPARATOR, txt_csv_col_sep = TXT_CSV_COL_SEP, load_all_sheets_at_once = LOAD_ALL_SHEETS_AT_ONCE, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)\n",
    "\n",
    "# OBS: If an Excel file is loaded and LOAD_ALL_SHEETS_AT_ONCE = True, then the object\n",
    "# dataset will be a list of dictionaries, with 'sheet' as key containing the sheet name; and 'df'\n",
    "# as key correspondent to the Pandas dataframe. So, to access the 3rd dataframe (index 2, since\n",
    "# indexing starts from zero): df = dataframe[2]['df'], where dataframe is the list returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "4c64f872-48a9-4673-8356-ce72b1e04db0",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Converting JSON object to dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "d1b0b065-a57b-476a-8241-5868e85f86c9",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# JSON object in terms of Python structure: list of dictionaries, where each value of a\n",
    "# dictionary may be a dictionary or a list of dictionaries (nested structures).\n",
    "# example of highly nested structure saved as a list 'json_formatted_list'. Note that the same\n",
    "# structure could be declared and stored into a string variable. For instance, if you have a txt\n",
    "# file containing JSON, you could read the txt and save its content as a string.\n",
    "# json_formatted_list = [{'field1': val1, 'field2': {'dict_val': dict_val}, 'field3': [{\n",
    "# 'nest1': nest_val1}, {'nest2': nestval2}]}, {'field1': val1, 'field2': {'dict_val': dict_val}, \n",
    "# 'field3': [{'nest1': nest_val1}, {'nest2': nestval2}]}]\n",
    "\n",
    "JSON_OBJ_TO_CONVERT = json_object #Alternatively: object containing the JSON to be converted\n",
    "\n",
    "# JSON_OBJ_TO_CONVERT: object containing JSON, or string with JSON content to parse.\n",
    "# Objects may be: string with JSON formatted text;\n",
    "# list with nested dictionaries (JSON formatted);\n",
    "# dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "\n",
    "JSON_OBJ_TYPE = 'list'\n",
    "# JSON_OBJ_TYPE = 'list', in case the object was saved as a list of dictionaries (JSON format)\n",
    "# JSON_OBJ_TYPE = 'string', in case it was saved as a string (text) containing JSON.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: [{'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]}]\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = json_obj_to_pandas_dataframe (json_obj_to_convert = JSON_OBJ_TO_CONVERT, json_obj_type = JSON_OBJ_TYPE, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f029a23c-ee35-429c-9367-1f2eed795145",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing or exporting models and dictionaries (or lists)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f2683ad0-8a2f-418c-9070-5051d0c81b05"
   },
   "source": [
    "#### Case 1: import only a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "49c328f2-5834-4d1b-9369-715705b2a43b",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'tensorflow_general'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model.\n",
    "# Simply modify this object on the left of equality:\n",
    "model = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b8a0b0df-881d-492e-bb09-5d4266e9f48f"
   },
   "source": [
    "#### Case 2: import only a dictionary or a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "7640fc8b-8b60-4ab6-a472-84278c204320",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'dict_or_list_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'tensorflow_general'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Dictionary or list saved as imported_dict_or_list.\n",
    "# Simply modify this object on the left of equality:\n",
    "imported_dict_or_list = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "0bf7407c-48b8-4b8e-9e7d-fc625a8a88e0"
   },
   "source": [
    "#### Case 3: import a model and a dictionary (or a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "e53b93f2-fb31-4744-a111-52b70e11dd31",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_and_dict'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'tensorflow_general'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model. Dictionary or list saved as imported_dict_or_list.\n",
    "# Simply modify these objects on the left of equality:\n",
    "model, imported_dict_or_list = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "9d7e9253-5f97-4478-b61c-592ab39aa7e1"
   },
   "source": [
    "#### Case 4: export a model and/or a dictionary (or a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "bbe74636-fad6-49c0-93ff-547c684978f6",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'export'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'tensorflow_general'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8baabc99-7e73-430d-becf-0864b139f109"
   },
   "source": [
    "### **Separating and preparing features and responses tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "fa93563d-282e-420f-ab3c-66f28f353ea7",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset  #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "FEATURES_COLUMNS = ['col1', 'col2']\n",
    "# FEATURES_COLUMNS: list of strings or string containing the names of columns\n",
    "# with predictive variables in the original dataframe. \n",
    "# Example: FEATURES_COLUMNS = ['col1', 'col2']; FEATURES_COLUMNS = 'predictor';\n",
    "# FEATURES_COLUMNS = ['predictor'].\n",
    "\n",
    "RESPONSE_COLUMNS = \"response\"\n",
    "# RESPONSE_COLUMNS: list of strings or string containing the names of columns\n",
    "# with response variables in the original dataframe. \n",
    "# Example: RESPONSE_COLUMNS= ['col3', 'col4']; RESPONSE_COLUMNS = 'response';\n",
    "# RESPONSE_COLUMNS = ['response']\n",
    "\n",
    "# Arrays or tensors containing features and responses returned as X and y, respectively.\n",
    "# Mapping dictionary correlating the position in array or tensor to the original column name\n",
    "# returned as column_map_dict.\n",
    "# Simply modify these objects on the left of equality:\n",
    "X, y, column_map_dict = separate_and_prepare_features_and_responses (df = DATASET, features_columns = FEATURES_COLUMNS, response_columns = RESPONSE_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "dfad986a-4abc-4bae-b819-3c4e70eadcbd"
   },
   "source": [
    "### **Converting a whole dataframe or array-like object to tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "3e74f1ff-a590-4374-88eb-bc8e2c24f07e",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET_OR_ARRAY_TO_CONVERT = dataset  \n",
    "# Alternatively: object containing the dataset or array-like object to be converted and reshaped.\n",
    "\n",
    "COLUMNS_TO_CONVERT = None\n",
    "# ATTENTION: This argument only works for Pandas dataframes.\n",
    "# COLUMNS_TO_CONVERT: list of strings or string containing the names of columns\n",
    "# that you want to convert. Use this if you want to convert only a subset of the dataframe. \n",
    "# Example: COLUMNS_TO_CONVERT = ['col1', 'col2']; COLUMNS_TO_CONVERT = 'predictor';\n",
    "# COLUMNS_TO_CONVERT = ['predictor'] will create a tensor with only the specified columns;\n",
    "# If None, the whole dataframe will be converted.\n",
    "\n",
    "COLUMNS_TO_EXCLUDE = None\n",
    "# ATTENTION: This argument only works for Pandas dataframes.\n",
    "# COLUMNS_TO_EXCLUDE: Alternative parameter. \n",
    "# list of strings or string containing the names of columns that you want to exclude from the\n",
    "# returned tensor. Use this if you want to convert only a subset of the dataframe. \n",
    "# Example: COLUMNS_TO_EXCLUDE = ['col1', 'col2']; COLUMNS_TO_EXCLUDE = 'predictor';\n",
    "# COLUMNS_TO_EXCLUDE = ['predictor'] will create a tensor with all columns from the dataframe\n",
    "# except the specified ones. This argument will only be used if the previous one was not.\n",
    "\n",
    "\n",
    "# Array or tensor returned as X. Mapping dictionary correlating the position in array or tensor \n",
    "# to the original column name returned as column_map_dict.\n",
    "# Simply modify these objects on the left of equality:\n",
    "X, column_map_dict = convert_to_tensor (df_or_array_to_convert = DATASET_OR_ARRAY_TO_CONVERT, columns_to_convert = COLUMNS_TO_CONVERT, columns_to_exclude = COLUMNS_TO_EXCLUDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating a TensorFlow windowed dataset from multiple-feature time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset\n",
    "# Alternatively: object containing the Pandas dataframe to be converted and reshaped.\n",
    "\n",
    "RESPONSE_COLUMNS = 'response_variable'\n",
    "# RESPONSE_COLUMNS: string or list of strings with the response columns\n",
    "\n",
    "SEQUENCE_STRIDE = 1\n",
    "SAMPLING_RATE = 1\n",
    "SHIFT = 1\n",
    "# SHIFT, SAMPLING_RATE, and SEQUENCE_STRIDE: integers\n",
    "\n",
    "# The time series may be represented as a sequence of times like: t = 0, t = 1, t = 2, ..., t = N.\n",
    "# When preparing the dataset, we pick a given number of 'times' (indexes), and use them for\n",
    "# predicting a time in the future.\n",
    "# So, the INPUT_WIDTH represents how much times will be used for prediction. If INPUT_WIDTH = 6,\n",
    "# we use 6 values for prediction, e.g., t = 0, t = 1, ..., t = 5 will be a prediction window.\n",
    "# In turns, if INPUT_WIDTH = 3, 3 values are used: t = 0, t = 1, t = 2; if INPUT_WIDTH = N, N\n",
    "# consecutive values will be used: t = 0, t = 1, t = 2, ..., t = N. And so on.\n",
    "# LABEL_WIDTH, in turns, represent how much times will be predicted. If LABEL_WIDTH = 1, a single\n",
    "# value will be predicted. If LABEL_WIDTH = 2, two consecutive values are predicted; if LABEL_WIDTH =\n",
    "# N, N consecutive values are predicted; and so on.\n",
    "        \n",
    "# SHIFT represents the offset, i.e., given the input values, which value in the time sequence will\n",
    "# be predicted. So, suppose INPUT_WIDTH = 6 and LABEL_WIDTH = 1\n",
    "# If SHIFT = 1, the label, i.e., the predicted value, will be the first after the sequence used for\n",
    "# prediction. So, if  t = 0, t = 1, ..., t = 5 will be a prediction window and t = 6 will be the\n",
    "# predicted value. Notice that the complete window has a total width = 7: t = 0, ..., t = 7. \n",
    "# If LABEL_WIDTH = 2, then t = 6 and t = 7 will be predicted (total width = 8).\n",
    "# Another example: suppose INPUT_WIDTH = 24. So the predicted window is: t = 0, t = 1, ..., t = 23.\n",
    "# If SHIFT = 24, the 24th element after the prediction sequence will be used as label, i.e., will\n",
    "# be predicted. So, t = 24 is the 1st after the sequence, t = 25 is the second, ... t = 47 is the\n",
    "# 24th after. If label_with = 1, then the sequence t = 0, t = 1, ..., t = 23 will be used for\n",
    "# predicting t = 47. Naturally, the total width of the window = 47 in this case.\n",
    "# Also, notice that the label is used by the model as the response (predicted) variable.\n",
    "\n",
    "# So for a given SHIFT: the sequence of timesteps i, i+1, ... will be used for predicting the\n",
    "# timestep i + SHIFT\n",
    "# If a sequence starts in index i, the next sequence will start from i + SEQUENCE_STRIDE.\n",
    "# The sequence will be formed by timesteps i, i + SAMPLING_RATE, i + 2* SAMPLING_RATE, ...\n",
    "# Example: Consider indices [0, 1, ... 99]. With sequence_length=10, SAMPLING_RATE=2, \n",
    "# SEQUENCE_STRIDE=3, the dataset will yield batches of sequences composed of the following indices:\n",
    "# First sequence:  [0  2  4  6  8 10 12 14 16 18]\n",
    "# Second sequence: [3  5  7  9 11 13 15 17 19 21]\n",
    "# Third sequence:  [6  8 10 12 14 16 18 20 22 24]\n",
    "# ...\n",
    "# Last sequence:   [78 80 82 84 86 88 90 92 94 96]\n",
    "\n",
    "USE_PAST_RESPONSES_FOR_PREDICTION = True\n",
    "# USE_PAST_RESPONSES_FOR_PREDICTION: True if the past responses will be used for predicting their\n",
    "# value in the future; False if you do not want to use them.\n",
    "\n",
    "PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 70   \n",
    "# percent_of_data_used_for_model_training: float from 0 to 100,\n",
    "# representing the percent of data used for training the model\n",
    "\n",
    "PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 10\n",
    "# If you want to use cross-validation, separate a percent of the training data for validation.\n",
    "# Declare this percent as PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION (float from 0 to 100).\n",
    "\n",
    "# If PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 70, and \n",
    "# PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 10, \n",
    "# training dataset slice goes from 0 to 0.7 (70%) of the dataset;\n",
    "# testing slicing goes from 0.7 x dataset to ((1 - 0.1) = 0.9) x dataset\n",
    "# validation slicing goes from 0.9 x dataset to the end of the dataset.\n",
    "# Here, consider the time sequence t = 0, t = 1, ... , t = N, for a dataset with length N:\n",
    "# training: from t = 0 to t = (0.7 x N); testing: from t = ((0.7 x N) + 1) to (0.9 x N);\n",
    "# validation: from t = ((0.9 x N) + 1) to N (the fractions 0.7 x N and 0.9 x N are rounded to\n",
    "# the closest integer).\n",
    "    \n",
    "\n",
    "# Dictionary with inputs and labels tensors returned as tensors_dict.\n",
    "# Simply modify this object on the left of equality:\n",
    "tensors_dict = multi_columns_time_series_tensors (df = DATASET, response_columns = RESPONSE_COLUMNS, sequence_stride = SEQUENCE_STRIDE, sampling_rate = SAMPLING_RATE, shift = SHIFT, use_past_responses_for_prediction = USE_PAST_RESPONSES_FOR_PREDICTION, percent_of_data_used_for_model_training = PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING, percent_of_training_data_used_for_model_validation = PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union of several 1-dimensional tensors (obtained from single columns) into a single tensor\n",
    "- Each 1-dimensional tensor or array becomes a column from the new tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_TENSORS_OR_ARRAYS = [tensor1, tensor2]\n",
    "# list of tensors: list containing the 1-dimensional tensors or arrays that the function will union.\n",
    "# the operation will be performed in the order that the tensors are declared.\n",
    "# Substitue tensor1, tensor2, tensor3,... by the tensor objects, in the correct sequence.\n",
    "# If the resulting tensor will contain the responses for a multi-response tensor, declare them in the\n",
    "# orders of the responses (tensor 1 corresponding to response 1, tensor 2 to response 2, etc.)\n",
    "\n",
    "# One-dimensional tensors have shape (X,), where X is the number of elements. Example: a column\n",
    "# of the dataframe with elements 1, 2, 3 in this order may result in an array like array([1, 2, 3])\n",
    "# and a Tensor with shape (3,). With we union it with the tensor from the column with elements\n",
    "# 4, 5, 6, the output will be array([[1,4], [2,5], [3,6]]). Alternatively, this new array could\n",
    "# be converted into a Pandas dataframe where each column would be correspondent to one individual\n",
    "# tensor.\n",
    "\n",
    "# Tensor resulting from the union of multiple single-dimension tensor returned as tensors_union.\n",
    "# Simply modify this object on the left of equality:\n",
    "tensors_union = union_1_dim_tensors (list_of_tensors_or_arrays = LIST_OF_TENSORS_OR_ARRAYS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d30bd598-f1bc-45b1-9def-3a9acd442dab"
   },
   "source": [
    "### **Making predictions with the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "05f93b26-1ca4-4c17-86b4-3df873ffec42",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "MODEL_OBJECT = model # Alternatively: object storing another model\n",
    "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
    "# MODEL_OBJECT = elastic_net_linear_reg_model\n",
    "\n",
    "X_tensor = X\n",
    "# predict_for = 'subset' or predict_for = 'single_entry'\n",
    "# The function will automatically detect if it is dealing with lists, NumPy arrays\n",
    "# or Pandas dataframes. If X_df is a list or a single-dimension array, predict_for\n",
    "# will be set as 'single_entry'. If X is a multi-dimension NumPy array (as the\n",
    "# outputs for preparing data - even single_entry - for deep learning models), or if\n",
    "# it is a Pandas dataframe, the function will set predict_for = 'subset'\n",
    "    \n",
    "# X_df = subset of predictive variables (dataframe, NumPy array, or list).\n",
    "# If PREDICT_FOR = 'single_entry', X_df should be a list of parameters values.\n",
    "# e.g. X_df = [1.2, 3, 4] (dot is the decimal case separator, comma separate values). \n",
    "# Notice that the list should contain only the numeric values, in the same order of the\n",
    "# correspondent columns.\n",
    "# If PREDICT_FOR = 'subset' (prediction for multiple entries), X_df should be a dataframe \n",
    "# (subset) or a multi-dimensional NumPy array of the parameters values, as usual.\n",
    "\n",
    "DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset  \n",
    "# DATAFRAME_FOR_CONCATENATING_PREDICTIONS: if you want to concatenate the predictions\n",
    "# to a dataframe, pass it here:\n",
    "# e.g. DATAFRAME_FOR_CONCATENATING_PREDICTIONS = df\n",
    "# If the dataframe must be the same one passed as X, repeat the dataframe object here:\n",
    "# X_df = dataset, DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset.\n",
    "# Alternatively, if DATAFRAME_FOR_CONCATENATING_PREDICTIONS = None, \n",
    "# the prediction will be returned as a series or NumPy array, depending on the input format.\n",
    "# Notice that the concatenated predictions will be added as a new column.\n",
    "\n",
    "COLUMN_WITH_PREDICTIONS_SUFFIX = None\n",
    "# COLUMN_WITH_PREDICTIONS_SUFFIX = None. If the predictions are added as a new column\n",
    "# of the dataframe DATAFRAME_FOR_CONCATENATING_PREDICTIONS, you can declare this\n",
    "# parameter as string with a suffix for identifying the model. If no suffix is added, the new\n",
    "# column will be named 'y_pred'.\n",
    "# e.g. COLUMN_WITH_PREDICTIONS_SUFFIX = '_keras' will create a column named \"y_pred_keras\". This\n",
    "# parameter is useful when working with multiple models. Always start the suffix with underscore\n",
    "# \"_\" so that no blank spaces are added; the suffix will not be merged to the column; and there\n",
    "# will be no confusion with the dot (.) notation for methods, JSON attributes, etc.\n",
    "\n",
    "ARCHITECTURE = None\n",
    "# ARCHITECTURE: some models require inputs in a proper format. Declare here if you are using\n",
    "# one of these architectures. Example: ARCHITECTURE = 'cnn_lstm' from class tf_models require\n",
    "# a special reshape before getting predictions. You can keep None or put the name of the\n",
    "# architecture, if no special reshape is needed.\n",
    "\n",
    "\n",
    "# Predictions returned as prediction_output\n",
    "# Simply modify this object (or variable) on the left of equality:\n",
    "prediction_output = make_model_predictions (model_object = MODEL_OBJECT, X = X_df, dataframe_for_concatenating_predictions = DATAFRAME_FOR_CONCATENATING_PREDICTIONS, column_with_predictions_suffix = COLUMN_WITH_PREDICTIONS_SUFFIX, architecture = ARCHITECTURE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "043f5c0c-6e34-4df9-b2f3-736c2b183614"
   },
   "source": [
    "### **Calculating probabilities associated to each class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OBJECT = model # Alternatively: object storing another model\n",
    "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
    "# MODEL_OBJECT = mlp_model\n",
    "\n",
    "X_tensor = X\n",
    "# predict_for = 'subset' or predict_for = 'single_entry'\n",
    "# The function will automatically detect if it is dealing with lists, NumPy arrays\n",
    "# or Pandas dataframes. If X_df is a list or a single-dimension array, predict_for\n",
    "# will be set as 'single_entry'. If X is a multi-dimension NumPy array (as the\n",
    "# outputs for preparing data - even single_entry - for deep learning models), or if\n",
    "# it is a Pandas dataframe, the function will set predict_for = 'subset'\n",
    "    \n",
    "# X_df = subset of predictive variables (dataframe, NumPy array, or list).\n",
    "# If PREDICT_FOR = 'single_entry', X_df should be a list of parameters values.\n",
    "# e.g. X_df = [1.2, 3, 4] (dot is the decimal case separator, comma separate values). \n",
    "# Notice that the list should contain only the numeric values, in the same order of the\n",
    "# correspondent columns.\n",
    "# If PREDICT_FOR = 'subset' (prediction for multiple entries), X_df should be a dataframe \n",
    "# (subset) or a multi-dimensional NumPy array of the parameters values, as usual.\n",
    "\n",
    "LIST_OF_CLASSES = list_of_classes\n",
    "# LIST_OF_CLASSES is the list of classes effectively used for training\n",
    "# the model. Set this parameter as the object returned from function\n",
    "# retrieve_classes_used_to_train\n",
    "\n",
    "TYPE_OF_MODEL = 'deep_learning'\n",
    "# TYPE_OF_MODEL = 'deep_learning' if Keras/TensorFlow or other deep learning\n",
    "# framework was used to obtain the model;\n",
    "# TYPE_OF_MODEL = 'other' for Scikit-learn or XGBoost models.\n",
    "\n",
    "DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset  \n",
    "# DATAFRAME_FOR_CONCATENATING_PREDICTIONS: if you want to concatenate the predictions\n",
    "# to a dataframe, pass it here:\n",
    "# e.g. DATAFRAME_FOR_CONCATENATING_PREDICTIONS = df\n",
    "# If the dataframe must be the same one passed as X, repeat the dataframe object here:\n",
    "# X_df = dataset, DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset.\n",
    "# Alternatively, if DATAFRAME_FOR_CONCATENATING_PREDICTIONS = None, \n",
    "# the prediction will be returned as a series or NumPy array, depending on the input format.\n",
    "# Notice that the concatenated predictions will be added as a new column.    \n",
    "# All of the new columns (appended or not) will have the prefix \"prob_class_\" followed\n",
    "# by the correspondent class name to identify them.\n",
    "\n",
    "ARCHITECTURE = None\n",
    "# ARCHITECTURE: some models require inputs in a proper format. Declare here if you are using\n",
    "# one of these architectures. Example: ARCHITECTURE = 'cnn_lstm' from class tf_models require\n",
    "# a special reshape before getting predictions. You can keep None or put the name of the\n",
    "# architecture, if no special reshape is needed.\n",
    "\n",
    "\n",
    "# Probabilities returned as calculated_probability\n",
    "# Simply modify this object (or variable) on the left of equality:\n",
    "calculated_probability = calculate_class_probability (model_object = MODEL_OBJECT, X = X_tensor, list_of_classes = LIST_OF_CLASSES, type_of_model = TYPE_OF_MODEL, dataframe_for_concatenating_predictions = DATAFRAME_FOR_CONCATENATING_PREDICTIONS, architecture = ARCHITECTURE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b3408f07-a6be-43e4-bb3d-3ed2437db1a1"
   },
   "source": [
    "### **Merging (joining) dataframes on given keys; and sorting the merged table**\n",
    "- Merge (join) types:\n",
    "    - 'inner': resultant dataframe contains only the rows on the left dataframe with correspondent values on the right dataframe. Can be used for filtering a set of labelled rows. Results in no missing values;\n",
    "    - 'left': resultant dataframe contains all the rows from the left table (even those without correspondence on the right); and the rows from the right table that have correspondence on the left one. Since rows from the left table may not have correspondence, it may result in missing values.\n",
    "    - 'right': resultant dataframe contains all the rows from the right table (even those without correspondence on the right); and the rows from the left table that have correspondence on the right one. Since rows from the right table may not have correspondence, it may result in missing values.\n",
    "    - 'outer': in SQL, the Pandas 'outer' merge usually corresponds to the FULL OUTER JOIN: the resultant dataframe contains all rows from both tables, not taking in account if there is correspondence. So, it may result in missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "95e2a7a2-f4a4-40b3-a58e-e4772fb9f048",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DF_LEFT = dataset1 #Alternatively: object containing the dataset to be joined on the left\n",
    "DF_RIGHT = dataset2 #Alternatively: object containing the dataset to be joined on the right\n",
    "\n",
    "LEFT_KEY = \"left_key_column\" \n",
    "#Alternatively: (string) name of the column of the left dataframe to be used as key for \n",
    "# joining. Keep inside quotes.\n",
    "RIGHT_KEY = \"right_key_column\"\n",
    "#Alternatively: (string) name of the column of the right dataframe to be used as key for \n",
    "# joining. Keep inside quotes.\n",
    "\n",
    "HOW_TO_JOIN = \"inner\"\n",
    "#Alternatively: \"inner\", \"outer\", \"left\", \"right\".\n",
    "\n",
    "MERGED_SUFFIXES = ('_left', '_right')\n",
    "# SUFFIXES = ('_left', '_right') - tuple of the suffixes to be added to columns.\n",
    "# Example: suppose both datasets have the column 'Value'. The column from the left dataset\n",
    "# will be renamed as \"Value_left\", and the column from the right dataset will be renamed as\n",
    "# \"Value_right\".\n",
    "# Alternatively: modify the strings inside quotes to modify the standard values. \n",
    "# Do not eliminate the parenthesis that indicate the tuple object.\n",
    "# Any unmutable list is a tuple. A tuple can be also declared as an unmutable list of two\n",
    "# objects inside parenthesis instead of the brackets used for lists: []\n",
    "\n",
    "SORT_MERGED_DF = False\n",
    "# SORT_MERGED_DF = False not to sort the merged dataframe. If you want to sort it,\n",
    "# set as True. If SORT_MERGED_DF = True and COLUMN_TO_SORT = None, the dataframe will\n",
    "# be sorted by its first column.\n",
    "\n",
    "COLUMN_TO_SORT = None\n",
    "# COLUMN_TO_SORT = None. Keep it None if the dataframe should not be sorted.\n",
    "# Alternatively, pass a string with a column name to sort, such as:\n",
    "# COLUMN_TO_SORT = 'col1'; or a list of columns to use for sorting: COLUMN_TO_SORT = \n",
    "# ['col1', 'col2']\n",
    "\n",
    "ASCENDING_SORTING = True\n",
    "# ascending_sorting = True. If you want to sort the column(s) passed on column_to_sort in\n",
    "# ascending order, set as True. Set as False if you want to sort in descending order. If\n",
    "# you want to sort each column passed as list column_to_sort in a specific order, pass a \n",
    "# list of booleans like ASCENDING_SORTING = [False, True] - the first column of the list\n",
    "# will be sorted in descending order, whereas the 2nd will be in ascending. Notice that\n",
    "# the correspondence is element-wise: the boolean in list ASCENDING_SORTING will correspond \n",
    "# to the sorting order of the column with the same position in list COLUMN_TO_SORT.\n",
    "# If None, the dataframe will be sorted in ascending order.\n",
    "    \n",
    "\n",
    "#New dataframe saved as merged_df. Simply modify this object on the left of equality:\n",
    "merged_df = MERGE_AND_SORT_DATAFRAMES (df_left = DF_LEFT, df_right = DF_RIGHT, left_key = LEFT_KEY, right_key = RIGHT_KEY, how_to_join = HOW_TO_JOIN, merged_suffixes = MERGED_SUFFIXES, sort_merged_df = SORT_MERGED_DF, column_to_sort = COLUMN_TO_SORT, ascending_sorting = ASCENDING_SORTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "25468c5b-5532-45c5-8ee2-c70cc7b5fa4e"
   },
   "source": [
    "### **Concatenating (SQL UNION) multiple dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "8d90434e-5073-4d1b-87e9-f961cbf32bb8",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "LIST_OF_DATAFRAMES = [dataset1, dataset2]\n",
    "# LIST_OF_DATAFRAMES must be a list containing the dataframe objects\n",
    "# example: list_of_dataframes = [df1, df2, df3, df4]\n",
    "# Notice that the dataframes are objects, not strings. Therefore, they should not\n",
    "# be declared inside quotes.\n",
    "# There is no limit of dataframes. In this example, we will concatenate 4 dataframes.\n",
    "# If LIST_OF_DATAFRAMES = [df1, df2, df3] we would concatenate 3, and if\n",
    "# LIST_OF_DATAFRAMES = [df1, df2, df3, df4, df5] we would concatenate 5 dataframes.\n",
    "\n",
    "WHAT_TO_APPEND = 'rows'\n",
    "# WHAT_TO_APPEND = 'rows' for appending the rows from one dataframe\n",
    "# into the other; WHAT_TO_APPEND = 'columns' for appending the columns\n",
    "# from one dataframe into the other (horizontal or lateral append).\n",
    "\n",
    "IGNORE_INDEX_ON_UNION = True # Alternatively: True or False\n",
    "\n",
    "SORT_VALUES_ON_UNION = True # Alternatively: True or False\n",
    "\n",
    "UNION_JOIN_TYPE = None\n",
    "# JOIN can be 'inner' to perform an inner join, eliminating the missing values\n",
    "# The default (None) is 'outer': the dataframes will be stacked on the columns with\n",
    "# same names but, in case there is no correspondence, the row will present a missing\n",
    "# value for the columns which are not present in one of the dataframes.\n",
    "# When using the 'inner' method, only the common columns will remain.\n",
    "# Alternatively, keep UNION_JOIN_TYPE = None for the standard outer join; or set\n",
    "# UNION_JOIN_TYPE = \"inner\" (inside quotes) for using the inner join.\n",
    "    \n",
    "#These 3 last parameters are the same from Pandas .concat method:\n",
    "# IGNORE_INDEX_ON_UNION = ignore_index;\n",
    "# SORT_VALUES_ON_UNION = sort\n",
    "# UNION_JOIN_TYPE = join\n",
    "# Check Datacamp course Joining Data with pandas, Chap.3, \n",
    "# Advanced Merging and Concatenating\n",
    "    \n",
    "\n",
    "#New dataframe saved as concat_df. Simply modify this object on the left of equality:\n",
    "concat_df = UNION_DATAFRAMES (list_of_dataframes = LIST_OF_DATAFRAMES, what_to_append = WHAT_TO_APPEND, ignore_index_on_union = IGNORE_INDEX_ON_UNION, sort_values_on_union = SORT_VALUES_ON_UNION, union_join_type = UNION_JOIN_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "7b78bbb9-7b68-4bb6-aa47-97f7fdba571c"
   },
   "source": [
    "### **Filtering (selecting); ordering; or renaming columns from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "29a534e9-b95a-4cf9-8db6-83be61dc4125",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "MODE = 'select_or_order_columns'\n",
    "# MODE = 'select_or_order_columns' for filtering only the list of columns passed as COLUMNS_LIST,\n",
    "# and setting a new column order. In this mode, you can pass the columns in any order: \n",
    "# the order of elements on the list will be the new order of columns.\n",
    "\n",
    "# MODE = 'rename_columns' for renaming the columns with the names passed as COLUMNS_LIST. In this\n",
    "# mode, the list must have same length and same order of the columns of the dataframe. That is because\n",
    "# the columns will sequentially receive the names in the list. So, a mismatching of positions\n",
    "# will result into columns with incorrect names.\n",
    "\n",
    "COLUMNS_LIST = ['column1', 'column2', 'column3']\n",
    "# COLUMNS_LIST = list of strings containing the names (headers) of the columns to select\n",
    "# (filter); or to be set as the new columns' names, according to the selected mode.\n",
    "# For instance: COLUMNS_LIST = ['col1', 'col2', 'col3'] will \n",
    "# select columns 'col1', 'col2', and 'col3' (or rename the columns with these names). \n",
    "# Declare the names inside quotes.\n",
    "# Simply substitute the list by the list of columns that you want to select; or the\n",
    "# list of the new names you want to give to the dataset columns.\n",
    "\n",
    "# New dataframe saved as new_df. Simply modify this object on the left of equality:\n",
    "new_df = select_order_or_rename_columns (df = DATASET, columns_list = COLUMNS_LIST, mode = MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "4887e188-a297-40da-bfe2-d5aff4e6c1f8",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Reversing the log-transform - Exponentially transforming variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "0d98b47e-5e0d-4900-96fa-5defff7fe4dd",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "SUBSET = None\n",
    "# Set SUBSET = None to transform the whole dataset. Alternatively, pass a list with \n",
    "# columns names for the transformation to be applied. For instance:\n",
    "# SUBSET = ['col1', 'col2', 'col3'] will apply the transformation to the columns named\n",
    "# as 'col1', 'col2', and 'col3'. Declare the names inside quotes.\n",
    "# Declaring the full list of columns is equivalent to setting SUBSET = None.\n",
    "\n",
    "CREATE_NEW_COLUMNS = True\n",
    "# Alternatively, set CREATE_NEW_COLUMNS = True to store the transformed data into new\n",
    "# columns. Or set CREATE_NEW_COLUMNS = False to overwrite the existing columns\n",
    "    \n",
    "NEW_COLUMNS_SUFFIX = \"_originalScale\"\n",
    "# This value has effect only if CREATE_NEW_COLUMNS = True.\n",
    "# The new column name will be set as column + NEW_COLUMNS_SUFFIX. Then, if the original\n",
    "# column was \"column1\" and the suffix is \"_originalScale\", the new column will be named as\n",
    "# \"column1_originalScale\".\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name.\n",
    "\n",
    "#New dataframe saved as rescaled_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "rescaled_df = reverse_log_transform(df = DATASET, subset = SUBSET, create_new_columns = CREATE_NEW_COLUMNS, new_columns_suffix = NEW_COLUMNS_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f6f2c5ec-454d-4943-b3c4-f0bbfbf13a39",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Reversing Box-Cox transform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "b64bf50f-ad07-42eb-be30-618177769ba7",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# This function will process a single column column_to_transform of the dataframe df \n",
    "# per call.\n",
    "\n",
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "COLUMN_TO_TRANSFORM = 'column_to_transform'\n",
    "# COLUMN_TO_TRANSFORM must be a string with the name of the column.\n",
    "# e.g. COLUMN_TO_TRANSFORM = 'column1' to transform a column named as 'column1'\n",
    "\n",
    "LAMBDA_BOXCOX = None\n",
    "# LAMBDA_BOXCOX must be a float value. e.g. lamda_boxcox = 1.7\n",
    "# If you calculated lambda from the function box_cox_transform and saved the\n",
    "# transformation data summary dictionary as data_sum_dict, simply set:\n",
    "## LAMBDA_BOXCOX = data_sum_dict['lambda_boxcox']\n",
    "# This will access the value on the key 'lambda_boxcox' of the dictionary, which\n",
    "# contains the lambda. \n",
    "# If lambda_boxcox is None, the mode will be automatically set as 'calculate_and_apply'.\n",
    "\n",
    "SUFFIX = '_ReversedBoxCox'\n",
    "#suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_ReversedBoxCox', the transformed column will be\n",
    "# identified as 'Y_ReversedBoxCox'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "#New dataframe saved as retransformed_df.\n",
    "# Simply modify this object on the left of equality:\n",
    "retransformed_df = reverse_box_cox (df = DATASET, column_to_transform = COLUMN_TO_TRANSFORM, lambda_boxcox = LAMBDA_BOXCOX, suffix = SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "80adbe3a-129f-489e-8578-ef6699816623",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **One-Hot Encoding the categorical variables**\n",
    "- For each category, the One-Hot Encoder creates a new column in the dataset. This new column is represented by a binary variable which is equals to zero if the row is not classified in that category; and is equals to 1 when the row represents an element in that category.For a category \"A\", a column named \"A\" is created.\n",
    "    - If the row is an element from category \"A\", the value for the column \"A\" is 1.\n",
    "    - If not, the value for column \"A\" is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "8273f03e-4483-4b2a-bb5b-0e0f0ec06673",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "SUBSET_OF_FEATURES_TO_BE_ENCODED = ['COLUMN1', 'COLUMN2', 'COLUMN3']\n",
    "# SUBSET_OF_FEATURES_TO_BE_ENCODED: list of strings (inside quotes), \n",
    "# containing the names of the columns with the categorical variables that will be \n",
    "# encoded. If a single column will be encoded, declare this parameter as list with\n",
    "# only one element e.g.SUBSET_OF_FEATURES_TO_BE_ENCODED = [\"column1\"] \n",
    "# will analyze the column named as 'column1'; \n",
    "# SUBSET_OF_FEATURES_TO_BE_ENCODED = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "# with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "\n",
    "# New dataframe saved as one_hot_encoded_df; list of encoding information,\n",
    "# including different categories and encoder objects as OneHot_encoding_list.\n",
    "# Simply modify this object on the left of equality:\n",
    "one_hot_encoded_df, OneHot_encoding_list = OneHotEncode_df (df = DATASET, subset_of_features_to_be_encoded = SUBSET_OF_FEATURES_TO_BE_ENCODED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "4c74575b-9dac-4c7f-89eb-38d9c8d3ab6d"
   },
   "source": [
    "### **Reversing scaling of the features - Standard scaler, Min-Max scaler, division by factor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "3acb1db2-1f6f-4f1c-bbb9-ece595838fa8",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be processed\n",
    "\n",
    "SUBSET_OF_FEATURES_TO_SCALE = ['COLUMN1', 'COLUMN2', 'COLUMN3']\n",
    "#subset_of_features_to_be_encoded: list of strings (inside quotes), \n",
    "# containing the names of the columns with the categorical variables that will be \n",
    "# encoded. If a single column will be encoded, declare this parameter as list with\n",
    "# only one element e.g.subset_of_features_to_be_encoded = [\"column1\"] \n",
    "# will analyze the column named as 'column1'; \n",
    "# subset_of_features_to_be_encoded = [\"col1\", 'col2', 'col3'] will analyze 3 columns\n",
    "# with categorical variables: 'col1', 'col2', and 'col3'.\n",
    "\n",
    "MODE = 'min_max'\n",
    "## Alternatively: MODE = 'standard', MODE = 'min_max', MODE = 'factor'\n",
    "## This function provides 3 methods (modes) of scaling:\n",
    "## MODE = 'standard': applies the standard scaling, \n",
    "##  which creates a new variable with mean = 0; and standard deviation = 1.\n",
    "##  Each value Y is transformed as Ytransf = (Y - u)/s, where u is the mean \n",
    "##  of the training samples, and s is the standard deviation of the training samples.\n",
    "    \n",
    "## MODE = 'min_max': applies min-max normalization, with a resultant feature \n",
    "## ranging from 0 to 1. each value Y is transformed as \n",
    "## Ytransf = (Y - Ymin)/(Ymax - Ymin), where Ymin and Ymax are the minimum and \n",
    "## maximum values of Y, respectively.\n",
    "    \n",
    "## MODE = 'factor': divides the whole series by a numeric value provided as argument. \n",
    "## For a factor F, the new Y values will be Ytransf = Y/F.\n",
    "\n",
    "LIST_OF_SCALING_PARAMS = [\n",
    "                            {'column': None,\n",
    "                            'scaler': {'scaler_obj': None, \n",
    "                                      'scaler_details': None}},\n",
    "                            {'column': None,\n",
    "                            'scaler': {'scaler_obj': None, \n",
    "                                      'scaler_details': None}}\n",
    "                            \n",
    "                         ]\n",
    "# LIST_OF_SCALING_PARAMS is a list of dictionaries with the same format of the list returned\n",
    "# from this function. Each dictionary must correspond to one of the features that will be scaled,\n",
    "# but the list do not have to be in the same order of the columns - it will check one of the\n",
    "# dictionary keys.\n",
    "# The first key of the dictionary must be 'column'. This key must store a string with the exact\n",
    "# name of the column that will be scaled.\n",
    "# the second key must be 'scaler'. This key must store a dictionary. The dictionary must store\n",
    "# one of two keys: 'scaler_obj' - sklearn scaler object to be used; or 'scaler_details' - the\n",
    "# numeric parameters for re-calculating the scaler without the object. The key 'scaler_details', \n",
    "# must contain a nested dictionary. For the mode 'min_max', this dictionary should contain \n",
    "# two keys: 'min', with the minimum value of the variable, and 'max', with the maximum value. \n",
    "# For mode 'standard', the keys should be 'mu', with the mean value, and 'sigma', with its \n",
    "# standard deviation. For the mode 'factor', the key should be 'factor', and should contain the \n",
    "# factor for division (the scaling value. e.g 'factor': 2.0 will divide the column by 2.0.).\n",
    "# Again, if you want to normalize by the maximum, declare the maximum value as any other factor for\n",
    "# division.\n",
    "\n",
    "SUFFIX = '_reverseScaling'\n",
    "# suffix: string (inside quotes).\n",
    "# How the transformed column will be identified in the returned data_transformed_df.\n",
    "# If y_label = 'Y' and suffix = '_reverseScaling', the transformed column will be\n",
    "# identified as 'Y_reverseScaling'.\n",
    "# Alternatively, input inside quotes a string with the desired suffix. Recommendation:\n",
    "# start the suffix with \"_\" to separate it from the original name\n",
    "\n",
    "# New dataframe saved as rescaled_df; list of scaling parameters saved as scaling_list\n",
    "# Simply modify this object on the left of equality:\n",
    "rescaled_df, scaling_list = reverse_feature_scaling (df = DATASET, subset_of_features_to_scale = SUBSET_OF_FEATURES_TO_SCALE, list_of_scaling_params = LIST_OF_SCALING_PARAMS, mode = MODE, suffix = SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "2c793054-2ca7-4b8c-9e1d-f28df32fa518"
   },
   "source": [
    "### **Plotting a bar chart**\n",
    "- To obtain a **Pareto chart**, keep `aggregate_function = 'sum'`, `plot_cumulative_percent = True`, and `orientation = 'vertical'`.\n",
    "- For obtaining the **data distribution of categorical variables**, select any numeric column as the response, and set `aggregate_function = 'count'`. You can also set `plot_cumulative_percent = True` to compare the frequencies of each possible value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "0a694acc-c1f3-40d2-a3bc-e69f54212128",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "CATEGORICAL_VAR_NAME = 'categorical_column_name'\n",
    "# CATEGORICAL_VAR_NAME: string (inside quotes) containing the name \n",
    "# of the column to be analyzed. e.g. \n",
    "# CATEGORICAL_VAR_NAME = \"column1\"\n",
    "\n",
    "RESPONSE_VAR_NAME = \"response_column_name\"\n",
    "# RESPONSE_VAR_NAME: string (inside quotes) containing the name \n",
    "# of the column that stores the response correspondent to the\n",
    "# categories. e.g. RESPONSE_VAR_NAME = \"response_feature\"\n",
    "\n",
    "AGGREGATE_FUNCTION = 'sum'\n",
    "# AGGREGATE_FUNCTION = 'sum': String defining the aggregation \n",
    "# method that will be applied. Possible values:\n",
    "# 'median', 'mean', 'mode', 'sum', 'min', 'max', 'variance', 'count',\n",
    "# 'standard_deviation','10_percent_quantile', '20_percent_quantile',\n",
    "# '25_percent_quantile', '30_percent_quantile', '40_percent_quantile',\n",
    "# '50_percent_quantile', '60_percent_quantile', '70_percent_quantile',\n",
    "# '75_percent_quantile', '80_percent_quantile', '90_percent_quantile',\n",
    "# and '95_percent_quantile'.\n",
    "# To use another aggregate function, the method must be added to the\n",
    "# dictionary of methods agg_methods_dict, defined in the function.\n",
    "# If None or an invalid function is input, 'sum' will be used.\n",
    "\n",
    "ADD_SUFFIX_TO_AGGREGATED_COL = True\n",
    "# ADD_SUFFIX_TO_AGGREGATED_COL = True will add a suffix to the\n",
    "# aggregated column. e.g. 'responseVar_mean'. If ADD_SUFFIX_TO_AGGREGATED_COL\n",
    "# = False, the aggregated column will have the original column name.\n",
    "SUFFIX = None\n",
    "# suffix = None. Keep it None if no suffix should be added, or if\n",
    "# the name of the aggregate function should be used as suffix, after\n",
    "# \"_\". Alternatively, set it as a string. As recommendation, put the\n",
    "# \"_\" sign in the beginning of this string to separate the suffix from\n",
    "# the original column name. e.g. if the response variable is 'Y' and\n",
    "# suffix = '_agg', the new aggregated column will be named as 'Y_agg'\n",
    "CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = True\n",
    "# CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = True to calculate and plot\n",
    "# the line of cumulative percent, or \n",
    "# CALCULATE_AND_PLOT_CUMULATIVE_PERCENT = False to omit it.\n",
    "# This feature is only shown when AGGREGATE_FUNCTION = 'sum', 'median',\n",
    "# 'mean', or 'mode'. So, it will be automatically set as False if \n",
    "# another aggregate is selected.\n",
    "ORIENTATION = 'vertical'\n",
    "# ORIENTATION = 'vertical' is the standard, and plots vertical bars\n",
    "# (perpendicular to the X axis). In this case, the categories are shown\n",
    "# in the X axis, and the correspondent responses are in Y axis.\n",
    "# Alternatively, ORIENTATION = 'horizontal' results in horizontal bars.\n",
    "# In this case, categories are in Y axis, and responses in X axis.\n",
    "# If None or invalid values are provided, orientation is set as 'vertical'.\n",
    "LIMIT_OF_PLOTTED_CATEGORIES = None\n",
    "# LIMIT_OF_PLOTTED_CATEGORIES: integer value that represents\n",
    "# the maximum of categories that will be plot. Keep it None to plot\n",
    "# all categories. Alternatively, set an integer value. e.g.: if\n",
    "# LIMIT_OF_PLOTTED_CATEGORIES = 4, but there are more categories,\n",
    "# the dataset will be sorted in descending order and: 1) The remaining\n",
    "# categories will be sum in a new category named 'others' if the\n",
    "# aggregate function is 'sum'; 2) Or the other categories will be simply\n",
    "# omitted from the plot, for other aggregate functions. Notice that\n",
    "# it limits only the variables in the plot: all of them will be\n",
    "# returned in the dataframe.\n",
    "# Use this parameter to obtain a cleaner plot. Notice that the remaining\n",
    "# columns will be aggregated as 'others' even if there is a single column\n",
    "# beyond the limit.\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'bar_chart.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "# New dataframe saved as aggregated_sorted_df. \n",
    "# Simply modify this object on the left of equality:\n",
    "aggregated_sorted_df = bar_chart (df = DATASET, categorical_var_name = CATEGORICAL_VAR_NAME, response_var_name = RESPONSE_VAR_NAME, aggregate_function = AGGREGATE_FUNCTION, add_suffix_to_aggregated_col = ADD_SUFFIX_TO_AGGREGATED_COL, suffix = SUFFIX, calculate_and_plot_cumulative_percent = CALCULATE_AND_PLOT_CUMULATIVE_PERCENT, orientation = ORIENTATION, limit_of_plotted_categories = LIMIT_OF_PLOTTED_CATEGORIES, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "829ecd77-e938-4c64-8deb-2aa0a247e314"
   },
   "source": [
    "### **Visualizing time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "bd582ee3-dac4-4c2b-b832-b2a9c767a3a6",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATA_IN_SAME_COLUMN = False\n",
    "\n",
    "# Parameters to input when DATA_IN_SAME_COLUMN = True:\n",
    "DATASET = None #Alternatively: object containing the dataset to be analyzed (e.g. DATASET = dataset)\n",
    "COLUMN_WITH_PREDICT_VAR_X = 'X' # Alternatively: correct name for X-column\n",
    "COLUMN_WITH_RESPONSE_VAR_Y = 'Y' # Alternatively: correct name for Y-column\n",
    "COLUMN_WITH_LABELS = 'label_column' # Alternatively: correct name for column with the labels or groups\n",
    "\n",
    "# DATA_IN_SAME_COLUMN = False: set as True if all the values to plot are in a same column.\n",
    "# If DATA_IN_SAME_COLUMN = True, you must specify the dataframe containing the data as DATASET;\n",
    "# the column containing the predict variable (X) as COLUMN_WITH_PREDICT_VAR_X; the column \n",
    "# containing the responses to plot (Y) as COLUMN_WITH_RESPONSE_VAR_Y; and the column \n",
    "# containing the labels (subgroup) indication as COLUMN_WITH_LABELS. \n",
    "# DATASET is an object, so do not declare it in quotes. The other three arguments (columns' names) \n",
    "# are strings, so declare in quotes. \n",
    "\n",
    "# Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare. \n",
    "# All the results for both groups are in a column named 'results', wich will be plot against\n",
    "# the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
    "# an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
    "# column 'group' shows the value 'B'. In this example:\n",
    "# DATA_IN_SAME_COLUMN = True,\n",
    "# DATASET = dataset,\n",
    "# COLUMN_WITH_PREDICT_VAR_X = 'time',\n",
    "# COLUMN_WITH_RESPONSE_VAR_Y = 'results', \n",
    "# COLUMN_WITH_LABELS = 'group'\n",
    "# If you want to declare a list of dictionaries, keep DATA_IN_SAME_COLUMN = False and keep\n",
    "# DATASET = None (the other arguments may be set as None, but it is not mandatory: \n",
    "# COLUMN_WITH_PREDICT_VAR_X = None, COLUMN_WITH_RESPONSE_VAR_Y = None, COLUMN_WITH_LABELS = None).\n",
    "\n",
    "\n",
    "# Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
    "LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = [\n",
    "    \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}\n",
    "    \n",
    "]\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE: if data is already converted to series, lists\n",
    "# or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
    "# even if there is a single dictionary.\n",
    "# Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
    "# (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
    "# keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
    "# represents the series and label of the added dictionary (you can pass 'lab': None, but if \n",
    "# 'x' or 'y' are None, the new dictionary will be ignored).\n",
    "\n",
    "# Examples:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
    "# will plot a single variable. In turns:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
    "# will plot two series, Y1 x X and Y2 x X.\n",
    "# Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
    "# If None is provided to 'lab', an automatic label will be generated.\n",
    "\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "ADD_SPLINE_LINES = True #Alternatively: True or False\n",
    "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
    "# Since we are obtaining a scatter plot, there is no meaning in omitting the dots,\n",
    "# as we can do for the time series visualization function.\n",
    "ADD_SCATTER_DOTS = False\n",
    "# If ADD_SCATTER_DOTS = False, no dots representing the data points are shown.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'time_series_vis.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "time_series_vis (data_in_same_column = DATA_IN_SAME_COLUMN, df = DATASET, column_with_predict_var_x = COLUMN_WITH_PREDICT_VAR_X, column_with_response_var_y = COLUMN_WITH_RESPONSE_VAR_Y, column_with_labels = COLUMN_WITH_LABELS, list_of_dictionaries_with_series_to_analyze = LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, add_scatter_dots = ADD_SCATTER_DOTS, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1"
   },
   "source": [
    "****"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
