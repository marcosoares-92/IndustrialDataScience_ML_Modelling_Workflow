{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "02d59ea5-0973-46a3-899c-c6ad6c7e3131"
   },
   "source": [
    "# **Ensemble Models - Random Forests and Extreme Gradient Boosting (XGBoost)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "023ec098-2404-4a2d-b7ad-6e597ea243be"
   },
   "source": [
    "## _Machine Learning Modelling Workflow Notebook 2_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "75c54704-6557-4f61-8095-c632c6986f86"
   },
   "source": [
    "## Content:\n",
    "1. Splitting the dataframe into train and test subsets;\n",
    "2. Retrieving the list of classes used for training the classification models;\n",
    "3. Fitting the Random Forest Model;\n",
    "4. Fitting the Extreme Gradient Boosting (XGBoost) Model;\n",
    "5. Getting a general feature ranking;\n",
    "6. Calculating metrics for regression models;\n",
    "7. Calculating metrics for classification models;\n",
    "8. Making predictions with the models;\n",
    "9. Calculating probabilities associated to each class;\n",
    "10. Performing the SHAP feature importance analysis;\n",
    "11. Time series visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267"
   },
   "source": [
    "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
    "- marcosoares.feq@gmail.com\n",
    "- marco.soares@bayer.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c3a670bf-e0d2-4201-a807-8b3bd8e07c3a",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# To install a library (e.g. tensorflow), unmark and run:\n",
    "# ! pip install tensorflow\n",
    "# to update a library (e.g. tensorflow), unmark and run:\n",
    "# ! pip install tensorflow --upgrade\n",
    "# to update pip, unmark and run:\n",
    "# ! pip install pip --upgrade\n",
    "# to show if a library is installed and visualize its information, unmark and run\n",
    "# (e.g. tensorflow):\n",
    "# ! pip show tensorflow\n",
    "# To run a Python file (e.g idsw_etl.py) saved in the notebook's workspace directory,\n",
    "# unmark and run:\n",
    "# import idsw_etl\n",
    "# or:\n",
    "# import idsw_etl as etl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea"
   },
   "source": [
    "## **Load Python Libraries in Global Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "650819a3-c068-47c0-816c-c12e4c65ad88",
    "id": "bzZgOvXCyHHl",
    "language": "python",
    "tags": [
     "CELL_4"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# Import all needed functions and classes with original names, with no aliases:\n",
    "from idsw import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "72b58d9b-1ddd-4827-aa07-ed4c3ea17f6f",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "## **Call the functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "bc4223cd-95eb-46e3-9035-32eed697070a",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Mounting Google Drive or S3 (AWS Simple Storage Service) bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "132f6b8e-a81d-412f-b709-ce0ba438407c",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "SOURCE = 'aws'\n",
    "# SOURCE = 'google' for mounting the google drive;\n",
    "# SOURCE = 'aws' for accessing an AWS S3 bucket\n",
    "\n",
    "## THE FOLLOWING PARAMETERS HAVE EFFECT ONLY WHEN SOURCE == 'aws':\n",
    "\n",
    "PATH_TO_STORE_IMPORTED_S3_BUCKET = ''\n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET: path of the Python environment to which the\n",
    "# S3 bucket contents will be imported. If it is None; or if it is an empty string; or if \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = '/', bucket will be imported to the root path. \n",
    "# Alternatively, input the path as a string (in quotes). e.g. \n",
    "# PATH_TO_STORE_IMPORTED_S3_BUCKET = 'copied_s3_bucket'\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for fetching AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "mount_storage_system (source = SOURCE, path_to_store_imported_s3_bucket = PATH_TO_STORE_IMPORTED_S3_BUCKET, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1a3feca2-0e0e-4ee2-92f0-47fe99412fdd",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "## JSON, txt, or CSV (comma separated values) files. Tables in webpages or html files can also be read.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
    "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "# FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "# Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
    "# Also, html files and webpages may be also read.\n",
    "\n",
    "# You may input the path for an HTML file containing a table to be read; or \n",
    "# a string containing the address for a webpage containing the table. The address must start\n",
    "# with www or htpp. If a website is input, the full address can be input as FILE_DIRECTORY_PATH\n",
    "# or as FILE_NAME_WITH_EXTENSION.\n",
    "\n",
    "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
    "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True \n",
    "# if you want to read a file with txt extension containing a text formatted as JSON \n",
    "# (but not saved as JSON).\n",
    "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the \n",
    "# function (below) must be set. If not, an error message will be raised.\n",
    "\n",
    "HOW_MISSING_VALUES_ARE_REGISTERED = None\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "# empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "# This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "#‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "# ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "# If a different denomination is used, indicate it as a string. e.g.\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "# If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "# only in column 'numeric_col', you can specify the following dictionary:\n",
    "# how_missing_values_are_registered = {'numeric-col': 0}\n",
    "\n",
    "    \n",
    "HAS_HEADER = True\n",
    "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
    "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
    "\n",
    "DECIMAL_SEPARATOR = '.'\n",
    "# DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "# the decimal separator. Alternatively, specify here the separator.\n",
    "# e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "# It manipulates the argument 'decimal' from Pandas functions.\n",
    "\n",
    "TXT_CSV_COL_SEP = \"comma\"\n",
    "# txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "# or 'csv'. It informs how the different columns are separated.\n",
    "# Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "# for columns separated by comma;\n",
    "# txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "# for columns separated by simple spaces.\n",
    "# You can also set a specific separator as string. For example:\n",
    "# txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "# is used as separator for the columns - '\\t' represents the tab character).\n",
    "\n",
    "## Parameters for loading Excel files:\n",
    "\n",
    "LOAD_ALL_SHEETS_AT_ONCE = False\n",
    "# LOAD_ALL_SHEETS_AT_ONCE = False - This parameter has effect only when for Excel files.\n",
    "# If LOAD_ALL_SHEETS_AT_ONCE = True, the function will return a list of dictionaries, each\n",
    "# dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
    "# value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
    "# and its value will be the pandas dataframe object obtained from that sheet.\n",
    "# This argument has preference over SHEET_TO_LOAD. If it is True, all sheets will be loaded.\n",
    "    \n",
    "SHEET_TO_LOAD = None\n",
    "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
    "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
    "# will be loaded.\n",
    "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
    "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
    "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "# name to load the sheet with that name.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = load_pandas_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, how_missing_values_are_registered = HOW_MISSING_VALUES_ARE_REGISTERED, has_header = HAS_HEADER, decimal_separator = DECIMAL_SEPARATOR, txt_csv_col_sep = TXT_CSV_COL_SEP, load_all_sheets_at_once = LOAD_ALL_SHEETS_AT_ONCE, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)\n",
    "\n",
    "# OBS: If an Excel file is loaded and LOAD_ALL_SHEETS_AT_ONCE = True, then the object\n",
    "# dataset will be a list of dictionaries, with 'sheet' as key containing the sheet name; and 'df'\n",
    "# as key correspondent to the Pandas dataframe. So, to access the 3rd dataframe (index 2, since\n",
    "# indexing starts from zero): df = dataframe[2]['df'], where dataframe is the list returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b91bc8b8-b5f7-4283-b6b8-1c96a60d7ac2",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Converting JSON object to dataframe**\n",
    "- Objects may be:\n",
    "    - String with JSON formatted text;\n",
    "    - List with nested dictionaries (JSON formatted);\n",
    "    - Each dictionary may contain nested dictionaries, or nested lists of dictionaries (nested JSON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "a53dc6c2-940d-420a-81a2-928000bd6e71",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# JSON object in terms of Python structure: list of dictionaries, where each value of a\n",
    "# dictionary may be a dictionary or a list of dictionaries (nested structures).\n",
    "# example of highly nested structure saved as a list 'json_formatted_list'. Note that the same\n",
    "# structure could be declared and stored into a string variable. For instance, if you have a txt\n",
    "# file containing JSON, you could read the txt and save its content as a string.\n",
    "# json_formatted_list = [{'field1': val1, 'field2': {'dict_val': dict_val}, 'field3': [{\n",
    "# 'nest1': nest_val1}, {'nest2': nestval2}]}, {'field1': val1, 'field2': {'dict_val': dict_val}, \n",
    "# 'field3': [{'nest1': nest_val1}, {'nest2': nestval2}]}]\n",
    "\n",
    "JSON_OBJ_TO_CONVERT = json_object #Alternatively: object containing the JSON to be converted\n",
    "\n",
    "# JSON_OBJ_TO_CONVERT: object containing JSON, or string with JSON content to parse.\n",
    "# Objects may be: string with JSON formatted text;\n",
    "# list with nested dictionaries (JSON formatted);\n",
    "# dictionaries, possibly with nested dictionaries (JSON formatted).\n",
    "\n",
    "JSON_OBJ_TYPE = 'list'\n",
    "# JSON_OBJ_TYPE = 'list', in case the object was saved as a list of dictionaries (JSON format)\n",
    "# JSON_OBJ_TYPE = 'string', in case it was saved as a string (text) containing JSON.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: [{'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]}]\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = json_obj_to_pandas_dataframe (json_obj_to_convert = JSON_OBJ_TO_CONVERT, json_obj_type = JSON_OBJ_TYPE, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "cedf7c40-8c9a-4ed9-bcdb-4648c764576e"
   },
   "source": [
    "### **Separating and preparing features and responses tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "42b506a0-430d-4387-8b00-17b8031202dd",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset  #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "FEATURES_COLUMNS = ['col1', 'col2']\n",
    "# FEATURES_COLUMNS: list of strings or string containing the names of columns\n",
    "# with predictive variables in the original dataframe. \n",
    "# Example: FEATURES_COLUMNS = ['col1', 'col2']; FEATURES_COLUMNS = 'predictor';\n",
    "# FEATURES_COLUMNS = ['predictor'].\n",
    "\n",
    "RESPONSE_COLUMNS = \"response\"\n",
    "# RESPONSE_COLUMNS: list of strings or string containing the names of columns\n",
    "# with response variables in the original dataframe. \n",
    "# Example: RESPONSE_COLUMNS= ['col3', 'col4']; RESPONSE_COLUMNS = 'response';\n",
    "# RESPONSE_COLUMNS = ['response']\n",
    "\n",
    "# Arrays or tensors containing features and responses returned as X and y, respectively.\n",
    "# Mapping dictionary correlating the position in array or tensor to the original column name\n",
    "# returned as column_map_dict.\n",
    "# Simply modify these objects on the left of equality:\n",
    "X, y, column_map_dict = separate_and_prepare_features_and_responses (df = DATASET, features_columns = FEATURES_COLUMNS, response_columns = RESPONSE_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5a05f378-3fb7-4967-a0f2-059f8115e3f7"
   },
   "source": [
    "### **Converting a whole dataframe or array-like object to tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "1435b8c6-f520-4c1f-a8ab-30541fd3fb27",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET_OR_ARRAY_TO_CONVERT = dataset  \n",
    "# Alternatively: object containing the dataset or array-like object to be converted and reshaped.\n",
    "\n",
    "COLUMNS_TO_CONVERT = None\n",
    "# ATTENTION: This argument only works for Pandas dataframes.\n",
    "# COLUMNS_TO_CONVERT: list of strings or string containing the names of columns\n",
    "# that you want to convert. Use this if you want to convert only a subset of the dataframe. \n",
    "# Example: COLUMNS_TO_CONVERT = ['col1', 'col2']; COLUMNS_TO_CONVERT = 'predictor';\n",
    "# COLUMNS_TO_CONVERT = ['predictor'] will create a tensor with only the specified columns;\n",
    "# If None, the whole dataframe will be converted.\n",
    "\n",
    "COLUMNS_TO_EXCLUDE = None\n",
    "# ATTENTION: This argument only works for Pandas dataframes.\n",
    "# COLUMNS_TO_EXCLUDE: Alternative parameter. \n",
    "# list of strings or string containing the names of columns that you want to exclude from the\n",
    "# returned tensor. Use this if you want to convert only a subset of the dataframe. \n",
    "# Example: COLUMNS_TO_EXCLUDE = ['col1', 'col2']; COLUMNS_TO_EXCLUDE = 'predictor';\n",
    "# COLUMNS_TO_EXCLUDE = ['predictor'] will create a tensor with all columns from the dataframe\n",
    "# except the specified ones. This argument will only be used if the previous one was not.\n",
    "\n",
    "\n",
    "# Array or tensor returned as X. Mapping dictionary correlating the position in array or tensor \n",
    "# to the original column name returned as column_map_dict.\n",
    "# Simply modify these objects on the left of equality:\n",
    "X, column_map_dict = convert_to_tensor (df_or_array_to_convert = DATASET_OR_ARRAY_TO_CONVERT, columns_to_convert = COLUMNS_TO_CONVERT, columns_to_exclude = COLUMNS_TO_EXCLUDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "06ed356d-9264-4555-b799-7615069c662e"
   },
   "source": [
    "### **Splitting features and responses into train and test tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "b34acff9-eb3c-4699-9c0c-6c0c010f1c67",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "X_tensor = X\n",
    "# X_df = tensor or array of predictive variables. Alternatively, modify X, not X_tensor.\n",
    "Y_tensor = y\n",
    "# Y = tensor or array of response variables. Alternatively, modify y, not Y_tensor.\n",
    "\n",
    "PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 75   \n",
    "# percent_of_data_used_for_model_training: float from 0 to 100,\n",
    "# representing the percent of data used for training the model\n",
    "\n",
    "PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 0\n",
    "# If you want to use cross-validation, separate a percent of the training data for validation.\n",
    "# Declare this percent as PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION (float from 0 to 100).\n",
    "\n",
    "# Subset and series destined to training, testing and/or validation returned in the dictionary split_dictionary;\n",
    "# Simply modify this object on the left of equality:\n",
    "split_dictionary = split_data_into_train_and_test (X = X_tensor, y = Y_tensor, percent_of_data_used_for_model_training = PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING, percent_of_training_data_used_for_model_validation = PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c52953ff-eba6-420b-89b6-2a2e77a010a4"
   },
   "source": [
    "### **Splitting time series into train and test tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "3e0b3b36-36da-4090-9208-e8f7048c2798",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "X_tensor = X\n",
    "# X_df = tensor or array of predictive variables. Alternatively, modify X, not X_tensor.\n",
    "Y_tensor = y\n",
    "# Y = tensor or array of response variables. Alternatively, modify y, not Y_tensor.\n",
    "\n",
    "PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 75   \n",
    "# percent_of_data_used_for_model_training: float from 0 to 100,\n",
    "# representing the percent of data used for training the model\n",
    "\n",
    "PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 0\n",
    "# If you want to use cross-validation, separate a percent of the training data for validation.\n",
    "# Declare this percent as PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION (float from 0 to 100).\n",
    "\n",
    "# Subset and series destined to training, testing and/or validation returned in the dictionary split_dictionary;\n",
    "# Simply modify this object on the left of equality:\n",
    "split_dictionary = time_series_train_test_split (X = X_tensor, y = Y_tensor, percent_of_data_used_for_model_training = PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING, percent_of_training_data_used_for_model_validation = PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b4fe9b20-a4e9-4bb9-b3bb-8a839277a2bb"
   },
   "source": [
    "### **Creating a TensorFlow windowed dataset from a time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "9b214af0-e914-4ca6-8f30-5373d0391c4d",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "Y_tensor = y\n",
    "# Y = tensor or array of response variables. Alternatively, modify y, not Y_tensor.\n",
    "\n",
    "WINDOW_SIZE = 20\n",
    "# WINDOW_SIZE (integer): number of rows/ size of the time window used.\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "# BATCH_SIZE (integer): number of rows/ size of the batches used for training.\n",
    "\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "# SHUFFLE_BUFFER_SIZE (integer): number of rows/ size used for shuffling the entries.\n",
    "\n",
    "# TensorFlow Dataset obtained from the time series returned as dataset_from_time_series.\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset_from_time_series = windowed_dataset_from_time_series (y = Y_tensor, window_size = WINDOW_SIZE, batch_size = BATCH_SIZE, shuffle_buffer_size = SHUFFLE_BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating a TensorFlow windowed dataset from multiple-feature time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset\n",
    "# Alternatively: object containing the Pandas dataframe to be converted and reshaped.\n",
    "\n",
    "RESPONSE_COLUMNS = 'response_variable'\n",
    "# RESPONSE_COLUMNS: string or list of strings with the response columns\n",
    "\n",
    "SEQUENCE_STRIDE = 1\n",
    "SAMPLING_RATE = 1\n",
    "SHIFT = 1\n",
    "# SHIFT, SAMPLING_RATE, and SEQUENCE_STRIDE: integers\n",
    "\n",
    "# The time series may be represented as a sequence of times like: t = 0, t = 1, t = 2, ..., t = N.\n",
    "# When preparing the dataset, we pick a given number of 'times' (indexes), and use them for\n",
    "# predicting a time in the future.\n",
    "# So, the INPUT_WIDTH represents how much times will be used for prediction. If INPUT_WIDTH = 6,\n",
    "# we use 6 values for prediction, e.g., t = 0, t = 1, ..., t = 5 will be a prediction window.\n",
    "# In turns, if INPUT_WIDTH = 3, 3 values are used: t = 0, t = 1, t = 2; if INPUT_WIDTH = N, N\n",
    "# consecutive values will be used: t = 0, t = 1, t = 2, ..., t = N. And so on.\n",
    "# LABEL_WIDTH, in turns, represent how much times will be predicted. If LABEL_WIDTH = 1, a single\n",
    "# value will be predicted. If LABEL_WIDTH = 2, two consecutive values are predicted; if LABEL_WIDTH =\n",
    "# N, N consecutive values are predicted; and so on.\n",
    "        \n",
    "# SHIFT represents the offset, i.e., given the input values, which value in the time sequence will\n",
    "# be predicted. So, suppose INPUT_WIDTH = 6 and LABEL_WIDTH = 1\n",
    "# If SHIFT = 1, the label, i.e., the predicted value, will be the first after the sequence used for\n",
    "# prediction. So, if  t = 0, t = 1, ..., t = 5 will be a prediction window and t = 6 will be the\n",
    "# predicted value. Notice that the complete window has a total width = 7: t = 0, ..., t = 7. \n",
    "# If LABEL_WIDTH = 2, then t = 6 and t = 7 will be predicted (total width = 8).\n",
    "# Another example: suppose INPUT_WIDTH = 24. So the predicted window is: t = 0, t = 1, ..., t = 23.\n",
    "# If SHIFT = 24, the 24th element after the prediction sequence will be used as label, i.e., will\n",
    "# be predicted. So, t = 24 is the 1st after the sequence, t = 25 is the second, ... t = 47 is the\n",
    "# 24th after. If label_with = 1, then the sequence t = 0, t = 1, ..., t = 23 will be used for\n",
    "# predicting t = 47. Naturally, the total width of the window = 47 in this case.\n",
    "# Also, notice that the label is used by the model as the response (predicted) variable.\n",
    "\n",
    "# So for a given SHIFT: the sequence of timesteps i, i+1, ... will be used for predicting the\n",
    "# timestep i + SHIFT\n",
    "# If a sequence starts in index i, the next sequence will start from i + SEQUENCE_STRIDE.\n",
    "# The sequence will be formed by timesteps i, i + SAMPLING_RATE, i + 2* SAMPLING_RATE, ...\n",
    "# Example: Consider indices [0, 1, ... 99]. With sequence_length=10, SAMPLING_RATE=2, \n",
    "# SEQUENCE_STRIDE=3, the dataset will yield batches of sequences composed of the following indices:\n",
    "# First sequence:  [0  2  4  6  8 10 12 14 16 18]\n",
    "# Second sequence: [3  5  7  9 11 13 15 17 19 21]\n",
    "# Third sequence:  [6  8 10 12 14 16 18 20 22 24]\n",
    "# ...\n",
    "# Last sequence:   [78 80 82 84 86 88 90 92 94 96]\n",
    "\n",
    "USE_PAST_RESPONSES_FOR_PREDICTION = True\n",
    "# USE_PAST_RESPONSES_FOR_PREDICTION: True if the past responses will be used for predicting their\n",
    "# value in the future; False if you do not want to use them.\n",
    "\n",
    "PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 70   \n",
    "# percent_of_data_used_for_model_training: float from 0 to 100,\n",
    "# representing the percent of data used for training the model\n",
    "\n",
    "PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 10\n",
    "# If you want to use cross-validation, separate a percent of the training data for validation.\n",
    "# Declare this percent as PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION (float from 0 to 100).\n",
    "\n",
    "# If PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 70, and \n",
    "# PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 10, \n",
    "# training dataset slice goes from 0 to 0.7 (70%) of the dataset;\n",
    "# testing slicing goes from 0.7 x dataset to ((1 - 0.1) = 0.9) x dataset\n",
    "# validation slicing goes from 0.9 x dataset to the end of the dataset.\n",
    "# Here, consider the time sequence t = 0, t = 1, ... , t = N, for a dataset with length N:\n",
    "# training: from t = 0 to t = (0.7 x N); testing: from t = ((0.7 x N) + 1) to (0.9 x N);\n",
    "# validation: from t = ((0.9 x N) + 1) to N (the fractions 0.7 x N and 0.9 x N are rounded to\n",
    "# the closest integer).\n",
    "    \n",
    "\n",
    "# Dictionary with inputs and labels tensors returned as tensors_dict.\n",
    "# Simply modify this object on the left of equality:\n",
    "tensors_dict = multi_columns_time_series_tensors (df = DATASET, response_columns = RESPONSE_COLUMNS, sequence_stride = SEQUENCE_STRIDE, sampling_rate = SAMPLING_RATE, shift = SHIFT, use_past_responses_for_prediction = USE_PAST_RESPONSES_FOR_PREDICTION, percent_of_data_used_for_model_training = PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING, percent_of_training_data_used_for_model_validation = PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union of several 1-dimensional tensors (obtained from single columns) into a single tensor\n",
    "- Each 1-dimensional tensor or array becomes a column from the new tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_TENSORS_OR_ARRAYS = [tensor1, tensor2]\n",
    "# list of tensors: list containing the 1-dimensional tensors or arrays that the function will union.\n",
    "# the operation will be performed in the order that the tensors are declared.\n",
    "# Substitue tensor1, tensor2, tensor3,... by the tensor objects, in the correct sequence.\n",
    "# If the resulting tensor will contain the responses for a multi-response tensor, declare them in the\n",
    "# orders of the responses (tensor 1 corresponding to response 1, tensor 2 to response 2, etc.)\n",
    "\n",
    "# One-dimensional tensors have shape (X,), where X is the number of elements. Example: a column\n",
    "# of the dataframe with elements 1, 2, 3 in this order may result in an array like array([1, 2, 3])\n",
    "# and a Tensor with shape (3,). With we union it with the tensor from the column with elements\n",
    "# 4, 5, 6, the output will be array([[1,4], [2,5], [3,6]]). Alternatively, this new array could\n",
    "# be converted into a Pandas dataframe where each column would be correspondent to one individual\n",
    "# tensor.\n",
    "\n",
    "# Tensor resulting from the union of multiple single-dimension tensor returned as tensors_union.\n",
    "# Simply modify this object on the left of equality:\n",
    "tensors_union = union_1_dim_tensors (list_of_tensors_or_arrays = LIST_OF_TENSORS_OR_ARRAYS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f8f9f753-2559-40fd-aa08-7939f8d585a7"
   },
   "source": [
    "### **Fitting the Random Forest Model**\n",
    "- Bagging ensemble of decision trees.\n",
    "- Data is divided and sampled. Trees are trained simultaneously, giving no preference to a given subset.\n",
    "    - With bagging, some instances may be subject to sampling several times in each predictor, while others may not.\n",
    "    - By default, a Bagging Classifier samples m training instances with replacement (bootstrap = True), where m is the training size. It means that, on average, only 63% of training instances are sampled by each predictor.\n",
    "    - The 37% remaining training instances are called out-of-bag (oob) samples, and the 37% are not the same for all predictors. Once the predictor is not exposed to such instances during training, it may be evaluated with them with no need of a separated validation set.\n",
    "    - The ensemble itself may be evaluated as the average of each predictor regarding the oob evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "60f20e24-1497-4aaf-be5d-99c30ac2637b",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# split_dictionary.keys() == dict_keys(['X_train', 'y_train', 'X_test', 'y_test', 'X_valid', 'y_valid'])\n",
    "\n",
    "X_TRAIN = split_dictionary['X_train']\n",
    "# X_TRAIN = tensor of predictive variables.\n",
    "# Alternatively, modify X_train, not X_TRAIN\n",
    "Y_TRAIN = split_dictionary['y_train']\n",
    "# Y_TRAIN = tensor of response variables.\n",
    "# Alternatively, modify y_train, not Y_TRAIN\n",
    "\n",
    "TYPE_OF_PROBLEM = \"regression\"\n",
    "# TYPE_OF_PROBLEM = 'regression'; or TYPE_OF_PROBLEM = 'classification'\n",
    "# The default is 'regression', which will be used if no type is\n",
    "# provided.\n",
    "\n",
    "NUMBER_OF_TREES = 128\n",
    "# NUMBER_OF_TREES = 128 (integer) - number of trees in the forest\n",
    "# it is the n_estimators parameter of the model.\n",
    "# Usually, a number from 64 to 128 is used.\n",
    "\n",
    "MAX_TREE_DEPTH = 20\n",
    "# MAX_TREE_DEPTH = 20 - integer representing the maximum depth \n",
    "# permitted for the trees (base learners). If None, then nodes are expanded \n",
    "# until all leaves are pure or until all leaves contain less \n",
    "# than MIN_SAMPLES_TO_SPLIT_NODE samples.\n",
    "# it is the max_depth parameter of the model.\n",
    "\n",
    "MIN_SAMPLES_TO_SPLIT_NODE = 2\n",
    "# MIN_SAMPLES_TO_SPLIT_NODE = 2 (integer or float). It is the \n",
    "# min_samples_split parameter of the model.\n",
    "# The minimum number of samples required to split an internal node:\n",
    "# If int, then consider MIN_SAMPLES_TO_SPLIT_NODE as the minimum number.\n",
    "# If float, then MIN_SAMPLES_TO_SPLIT_NODE is a fraction and ceil\n",
    "# (MIN_SAMPLES_TO_SPLIT_NODE * NUMBER_OF_TREES) are the minimum number \n",
    "# of samples for each split.\n",
    "    \n",
    "MIN_SAMPLES_TO_MAKE_LEAF = 2\n",
    "# MIN_SAMPLES_TO_MAKE_LEAF = 2 (integer or float). It is the\n",
    "# min_samples_leaf parameter of the model.\n",
    "# The minimum number of samples required to be at a leaf node. \n",
    "# A split point at any depth will only be considered if it leaves at \n",
    "# least MIN_SAMPLES_TO_MAKE_LEAF training samples in each of the left and right branches. \n",
    "# This may have the effect of smoothing the model, especially in regression.\n",
    "# If int, then consider MIN_SAMPLES_TO_MAKE_LEAF as the minimum number.\n",
    "# If float, then MIN_SAMPLES_TO_MAKE_LEAF is a fraction and ceil\n",
    "# (MIN_SAMPLES_TO_MAKE_LEAF * NUMBER_OF_TREES) are the minimum number \n",
    "# of samples for each node.\n",
    "\n",
    "BOOTSTRAP_SAMPLES = True\n",
    "# BOOTSTRAP_SAMPLES = True. Parameter bootstrap of the model.\n",
    "# Whether bootstrap samples are used when building trees. If False, \n",
    "# the whole dataset is used to build each tree.\n",
    "\n",
    "USE_OUT_OF_BAG_ERROR = True\n",
    "# USE_OUT_OF_BAG_ERROR = True. Parameter oob_score of the model.\n",
    "# Whether to use out-of-bag (OOB) samples to estimate the generalization score. \n",
    "# Only available if BOOTSTRAP_SAMPLES = True.\n",
    "    \n",
    "# Importantly: random forest combines several decision trees, by randomnly selecting\n",
    "# variables for making the tree leafs and nodes; and ramdonly setting the depth of\n",
    "# the trees. The use of out-of-bag guarantees that the data used for the construction\n",
    "# of the trees is randomly selected.\n",
    "# If not using, the calculated metrics will be over estimated.\n",
    "# This phenomenon is characteristic from ensemble algorithms like random forests, and\n",
    "# is not usually observed on linear regressions.\n",
    "\n",
    "# Tensors of data separated for model testing:\n",
    "X_TEST = None\n",
    "Y_TEST = None\n",
    "#X_TEST = split_dictionary['X_test']\n",
    "#Y_TEST = split_dictionary['y_test']\n",
    "\n",
    "# Tensors of data separated for model validation:\n",
    "X_VALID = None\n",
    "Y_VALID = None\n",
    "#X_VALID = split_dictionary['X_valid']\n",
    "#Y_VALID = split_dictionary['y_valid']\n",
    "\n",
    "COLUMN_MAP_DICT = column_map_dict\n",
    "#COLUMN_MAP_DICT = None\n",
    "# COLUMN_MAP_DICT: Mapping dictionary correlating the position in array or tensor to the original \n",
    "# column name.\n",
    "\n",
    "ORIENTATION = 'vertical'\n",
    "# ORIENTATION = 'vertical' is the standard, and plots vertical bars\n",
    "# (perpendicular to the X axis). In this case, the categories are shown\n",
    "# in the X axis, and the correspondent responses are in Y axis.\n",
    "# Alternatively, ORIENTATION = 'horizontal' results in horizontal bars.\n",
    "# In this case, categories are in Y axis, and responses in X axis.\n",
    "# If None or invalid values are provided, orientation is set as 'vertical'.\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'feature_importance_ranking.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "# Model object returned as rf_model;\n",
    "# Summary dictionary storing the feature ranking importance dataframe; the metrics and the classes \n",
    "# count and possible values (for a classification problem) returned as summary_dict;\n",
    "# Simply modify these objects on the left of equality:\n",
    "rf_model, summary_dict = RANDOM_FOREST (X_train = X_TRAIN, y_train = Y_TRAIN, type_of_problem = TYPE_OF_PROBLEM, number_of_trees = NUMBER_OF_TREES, max_tree_depth = MAX_TREE_DEPTH, min_samples_to_split_node = MIN_SAMPLES_TO_SPLIT_NODE, min_samples_to_make_leaf = MIN_SAMPLES_TO_MAKE_LEAF, bootstrap_samples = BOOTSTRAP_SAMPLES, use_out_of_bag_error = USE_OUT_OF_BAG_ERROR, X_test = X_TEST, y_test = Y_TEST, X_valid = X_VALID, y_valid = Y_VALID, column_map_dict = COLUMN_MAP_DICT, orientation = ORIENTATION, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8b08c671-755c-49ce-b7ef-fe64d614bc5e"
   },
   "source": [
    "### **Fitting the Extreme Gradient Boosting (XGBoost) Model**\n",
    "- Boosting ensemble of decision trees.\n",
    "- Each new tree is trained preferentially with data for which the previous trees had difficulty on making good predictions (i.e., these data receives higher weights of importance).\n",
    "- This algorithm usally performs better than Random Forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "84413a48-35df-4a69-b088-9e400a529074",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# split_dictionary.keys() == dict_keys(['X_train', 'y_train', 'X_test', 'y_test', 'X_valid', 'y_valid'])\n",
    "\n",
    "X_TRAIN = split_dictionary['X_train']\n",
    "# X_TRAIN = tensor of predictive variables.\n",
    "# Alternatively, modify X_train, not X_TRAIN\n",
    "Y_TRAIN = split_dictionary['y_train']\n",
    "# Y_TRAIN = tensor of response variables.\n",
    "# Alternatively, modify y_train, not Y_TRAIN\n",
    "\n",
    "TYPE_OF_PROBLEM = \"regression\"\n",
    "# TYPE_OF_PROBLEM = 'regression'; or TYPE_OF_PROBLEM = 'classification'\n",
    "# The default is 'regression', which will be used if no type is\n",
    "# provided.\n",
    "\n",
    "NUMBER_OF_TREES = 128\n",
    "# NUMBER_OF_TREES = 128 (integer) - number of gradient boosted trees. \n",
    "# Equivalent to number of boosting rounds.\n",
    "# it is the n_estimators parameter of the model.\n",
    "# Usually, a number from 64 to 128 is used.\n",
    "\n",
    "MAX_TREE_DEPTH = 20\n",
    "# MAX_TREE_DEPTH = 20 - integer representing the maximum depth \n",
    "# permitted for the trees (base learners).\n",
    "\n",
    "PERCENT_OF_TRAINING_SET_TO_SUBSAMPLE = 75\n",
    "# PERCENT_OF_TRAINING_SET_TO_SUBSAMPLE = 75 (float or None).\n",
    "# If this value is set, it defines the percent of data that will be ramdonly\n",
    "# selected for training the models.\n",
    "# e.g. PERCENT_OF_TRAINING_SET_TO_SUBSAMPLE = 80 uses 80% of the data. If None,\n",
    "# it uses the whole training set (100%)\n",
    "    \n",
    "# When subsampling, training data is divided into several subsets, and these\n",
    "# subsets are used for separately training the model.\n",
    "# Importantly: random forest and XGBoost combine several decision trees, by randomnly selecting\n",
    "# variables for making the tree leafs and nodes; and ramdonly setting the depth of\n",
    "# the trees. The use of this strategy guarantees that the data used for the construction\n",
    "# of the trees is randomly selected.\n",
    "# If not using, the model will be highly susceptive of overfitting due to the use of\n",
    "# the whole dataset. Also, the calculated metrics will be over estimated.\n",
    "# This phenomenon is characteristic from ensemble algorithms like random forests, and\n",
    "# XGBoost, and is not usually observed on linear regressions.\n",
    "\n",
    "# Tensors of data separated for model testing:\n",
    "X_TEST = None\n",
    "Y_TEST = None\n",
    "#X_TEST = split_dictionary['X_test']\n",
    "#Y_TEST = split_dictionary['y_test']\n",
    "\n",
    "# Tensors of data separated for model validation:\n",
    "X_VALID = None\n",
    "Y_VALID = None\n",
    "#X_VALID = split_dictionary['X_valid']\n",
    "#Y_VALID = split_dictionary['y_valid']\n",
    "\n",
    "COLUMN_MAP_DICT = column_map_dict\n",
    "#COLUMN_MAP_DICT = None\n",
    "# COLUMN_MAP_DICT: Mapping dictionary correlating the position in array or tensor to the original \n",
    "# column name.\n",
    "\n",
    "ORIENTATION = 'vertical'\n",
    "# ORIENTATION = 'vertical' is the standard, and plots vertical bars\n",
    "# (perpendicular to the X axis). In this case, the categories are shown\n",
    "# in the X axis, and the correspondent responses are in Y axis.\n",
    "# Alternatively, ORIENTATION = 'horizontal' results in horizontal bars.\n",
    "# In this case, categories are in Y axis, and responses in X axis.\n",
    "# If None or invalid values are provided, orientation is set as 'vertical'.\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'feature_importance_ranking.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "# Model object returned as xgb_model;\n",
    "# Summary dictionary storing the feature ranking importance dataframe; the metrics and the classes \n",
    "# count and possible values (for a classification problem) returned as summary_dict;\n",
    "# Simply modify these objects on the left of equality:\n",
    "xgb_model, summary_dict = XGBOOST (X_train = X_TRAIN, y_train = Y_TRAIN, type_of_problem = TYPE_OF_PROBLEM, number_of_trees = NUMBER_OF_TREES, max_tree_depth = MAX_TREE_DEPTH, percent_of_training_set_to_subsample = PERCENT_OF_TRAINING_SET_TO_SUBSAMPLE, X_test = X_TEST, y_test = Y_TEST, X_valid = X_VALID, y_valid = Y_VALID, column_map_dict = COLUMN_MAP_DICT, orientation = ORIENTATION, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "822d46db-e22f-4681-bd9f-052d4191e233"
   },
   "source": [
    "### **Making predictions with the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OBJECT = model # Alternatively: object storing another model\n",
    "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
    "# MODEL_OBJECT = elastic_net_linear_reg_model\n",
    "\n",
    "X_tensor = X\n",
    "# predict_for = 'subset' or predict_for = 'single_entry'\n",
    "# The function will automatically detect if it is dealing with lists, NumPy arrays\n",
    "# or Pandas dataframes. If X_df is a list or a single-dimension array, predict_for\n",
    "# will be set as 'single_entry'. If X is a multi-dimension NumPy array (as the\n",
    "# outputs for preparing data - even single_entry - for deep learning models), or if\n",
    "# it is a Pandas dataframe, the function will set predict_for = 'subset'\n",
    "    \n",
    "# X_df = subset of predictive variables (dataframe, NumPy array, or list).\n",
    "# If PREDICT_FOR = 'single_entry', X_df should be a list of parameters values.\n",
    "# e.g. X_df = [1.2, 3, 4] (dot is the decimal case separator, comma separate values). \n",
    "# Notice that the list should contain only the numeric values, in the same order of the\n",
    "# correspondent columns.\n",
    "# If PREDICT_FOR = 'subset' (prediction for multiple entries), X_df should be a dataframe \n",
    "# (subset) or a multi-dimensional NumPy array of the parameters values, as usual.\n",
    "\n",
    "DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset  \n",
    "# DATAFRAME_FOR_CONCATENATING_PREDICTIONS: if you want to concatenate the predictions\n",
    "# to a dataframe, pass it here:\n",
    "# e.g. DATAFRAME_FOR_CONCATENATING_PREDICTIONS = df\n",
    "# If the dataframe must be the same one passed as X, repeat the dataframe object here:\n",
    "# X_df = dataset, DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset.\n",
    "# Alternatively, if DATAFRAME_FOR_CONCATENATING_PREDICTIONS = None, \n",
    "# the prediction will be returned as a series or NumPy array, depending on the input format.\n",
    "# Notice that the concatenated predictions will be added as a new column.\n",
    "\n",
    "COLUMN_WITH_PREDICTIONS_SUFFIX = None\n",
    "# COLUMN_WITH_PREDICTIONS_SUFFIX = None. If the predictions are added as a new column\n",
    "# of the dataframe DATAFRAME_FOR_CONCATENATING_PREDICTIONS, you can declare this\n",
    "# parameter as string with a suffix for identifying the model. If no suffix is added, the new\n",
    "# column will be named 'y_pred'.\n",
    "# e.g. COLUMN_WITH_PREDICTIONS_SUFFIX = '_keras' will create a column named \"y_pred_keras\". This\n",
    "# parameter is useful when working with multiple models. Always start the suffix with underscore\n",
    "# \"_\" so that no blank spaces are added; the suffix will not be merged to the column; and there\n",
    "# will be no confusion with the dot (.) notation for methods, JSON attributes, etc.\n",
    "\n",
    "FUNCTION_USED_FOR_FITTING_DL_MODEL = 'get_deep_learning_tf_model'\n",
    "# FUNCTION_USED_FOR_FITTING_DL_MODEL: the function you used for obtaining the deep learning model.\n",
    "# Example: 'get_deep_learning_tf_model' or 'get_siamese_networks_model'\n",
    "\n",
    "ARCHITECTURE = None\n",
    "# ARCHITECTURE: some models require inputs in a proper format. Declare here if you are using\n",
    "# one of these architectures. Example: ARCHITECTURE = 'cnn_lstm' from class tf_models require\n",
    "# a special reshape before getting predictions. You can keep None or put the name of the\n",
    "# architecture, if no special reshape is needed.\n",
    "\n",
    "LIST_OF_RESPONSES = RESPONSE_COLUMNS\n",
    "# You may declare the list RESPONSE_COLUMNS previously used for separating into features and responses tensors.\n",
    "# LIST_OF_RESPONSES = []. This parameter is obbligatory for multi-response models, such as the ones obtained from\n",
    "# function 'get_siamese_networks_model'. It must contain a list with the same order of the output responses.\n",
    "# Example: suppose your siamese model outputs 4 responses: 'temperature', 'pressure', 'flow_rate', and 'ph', in\n",
    "# this order. The list of responses must be declared as: \n",
    "# LIST_OF_RESPONSES = ['temperature', 'pressure', 'flow_rate', 'ph']\n",
    "# tuples and numpy arrays are also acceptable: LIST_OF_RESPONSES = ('temperature', 'pressure', 'flow_rate', 'ph')\n",
    "# Attention: the number of responses must be exactly the number of elements in list_of_responses, or an error will\n",
    "# be raised.\n",
    "\n",
    "\n",
    "# Predictions returned as prediction_output\n",
    "# Simply modify this object (or variable) on the left of equality:\n",
    "prediction_output = make_model_predictions (model_object = MODEL_OBJECT, X = X_tensor, dataframe_for_concatenating_predictions = DATAFRAME_FOR_CONCATENATING_PREDICTIONS, column_with_predictions_suffix = COLUMN_WITH_PREDICTIONS_SUFFIX, function_used_for_fitting_dl_model = FUNCTION_USED_FOR_FITTING_DL_MODEL, architecture = ARCHITECTURE, list_of_responses = LIST_OF_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f938cc7b-0261-463b-8a62-c679b00e51d9"
   },
   "source": [
    "### **Calculating probabilities associated to each class**\n",
    "- Set the list_of_classes as the input of this function.\n",
    "- The predictions (outputs) from deep learning models (e.g. Keras/TensorFlow models) are themselves the probabilities associated to each possible class.\n",
    "    - For Scikit-learn and XGBoost, we must use a specific method for retrieving the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "f4e3cae3-b142-44e1-b3c4-ac1d4589d6e6",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "MODEL_OBJECT = rf_model # Alternatively: object storing another model\n",
    "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
    "# MODEL_OBJECT = mlp_model\n",
    "\n",
    "X_tensor = X\n",
    "# predict_for = 'subset' or predict_for = 'single_entry'\n",
    "# The function will automatically detect if it is dealing with lists, NumPy arrays\n",
    "# or Pandas dataframes. If X_df is a list or a single-dimension array, predict_for\n",
    "# will be set as 'single_entry'. If X is a multi-dimension NumPy array (as the\n",
    "# outputs for preparing data - even single_entry - for deep learning models), or if\n",
    "# it is a Pandas dataframe, the function will set predict_for = 'subset'\n",
    "    \n",
    "# X_df = subset of predictive variables (dataframe, NumPy array, or list).\n",
    "# If PREDICT_FOR = 'single_entry', X_df should be a list of parameters values.\n",
    "# e.g. X_df = [1.2, 3, 4] (dot is the decimal case separator, comma separate values). \n",
    "# Notice that the list should contain only the numeric values, in the same order of the\n",
    "# correspondent columns.\n",
    "# If PREDICT_FOR = 'subset' (prediction for multiple entries), X_df should be a dataframe \n",
    "# (subset) or a multi-dimensional NumPy array of the parameters values, as usual.\n",
    "\n",
    "LIST_OF_CLASSES = list_of_classes\n",
    "# LIST_OF_CLASSES is the list of classes effectively used for training\n",
    "# the model. Set this parameter as the object returned from function\n",
    "# retrieve_classes_used_to_train\n",
    "\n",
    "TYPE_OF_MODEL = 'other'\n",
    "# TYPE_OF_MODEL = 'deep_learning' if Keras/TensorFlow or other deep learning\n",
    "# framework was used to obtain the model;\n",
    "# TYPE_OF_MODEL = 'other' for Scikit-learn or XGBoost models.\n",
    "\n",
    "DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset  \n",
    "# DATAFRAME_FOR_CONCATENATING_PREDICTIONS: if you want to concatenate the predictions\n",
    "# to a dataframe, pass it here:\n",
    "# e.g. DATAFRAME_FOR_CONCATENATING_PREDICTIONS = df\n",
    "# If the dataframe must be the same one passed as X, repeat the dataframe object here:\n",
    "# X_df = dataset, DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset.\n",
    "# Alternatively, if DATAFRAME_FOR_CONCATENATING_PREDICTIONS = None, \n",
    "# the prediction will be returned as a series or NumPy array, depending on the input format.\n",
    "# Notice that the concatenated predictions will be added as a new column.    \n",
    "# All of the new columns (appended or not) will have the prefix \"prob_class_\" followed\n",
    "# by the correspondent class name to identify them.\n",
    "\n",
    "ARCHITECTURE = None\n",
    "# ARCHITECTURE: some models require inputs in a proper format. Declare here if you are using\n",
    "# one of these architectures. Example: ARCHITECTURE = 'cnn_lstm' from class tf_models require\n",
    "# a special reshape before getting predictions. You can keep None or put the name of the\n",
    "# architecture, if no special reshape is needed.\n",
    "\n",
    "\n",
    "# Probabilities returned as calculated_probability\n",
    "# Simply modify this object (or variable) on the left of equality:\n",
    "calculated_probability = calculate_class_probability (model_object = MODEL_OBJECT, X = X_tensor, list_of_classes = LIST_OF_CLASSES, type_of_model = TYPE_OF_MODEL, dataframe_for_concatenating_predictions = DATAFRAME_FOR_CONCATENATING_PREDICTIONS, architecture = ARCHITECTURE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "7d140609-9f7c-4f6c-b248-19257e8db316"
   },
   "source": [
    "### **Performing the SHAP feature importance analysis**\n",
    "- SHAP was developed by a mathematician from Washington University.\n",
    "- It combines the obtained machine learning model with Game Theory algorithms to analyze the relative importance of each variable, as well as the **interactions between variables**.\n",
    "- SHAP returns us a SHAP value that represents the relative importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "404c4abb-9f58-4433-8e1d-94b8cb669c69",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "MODEL_OBJECT = rf_model # Alternatively: object storing another model\n",
    "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
    "# MODEL_OBJECT = xgb_model\n",
    "\n",
    "X_TRAIN = X_train\n",
    "# X_TRAIN = subset of predictive variables (dataframe).\n",
    "# Alternatively, modify X_train, not X_TRAIN\n",
    "\n",
    "MODEL_TYPE = 'tree'\n",
    "# MODEL_TYPE = 'linear' for linear models (OLS, Ridge, Lasso, ElasticNet,\n",
    "# Logistic Regression)\n",
    "# MODEL_TYPE = 'tree' for tree-based models (Random Forest and XGBoost)\n",
    "# MODEL_TYPE = 'ann' for artificial neural networks\n",
    "\n",
    "TOTAL_OF_SHAP_POINTS = 40\n",
    "# TOTAL_OF_SHAP_POINTS (integer): number of points from the \n",
    "# subset X_train that will be randomly selected for the SHAP \n",
    "# analysis. If the kernel is taking too long, reduce this value.\n",
    "\n",
    "PLOT_TYPE = 'waterfall'\n",
    "# PLOT_TYPE = 'waterfall', 'beeswarm', 'bar', 'heatmap' \n",
    "# 'scatter', 'force_plt' or 'summary': \n",
    "# sets the type of shap plot that will be shown\n",
    "\n",
    "MAX_NUMBER_OF_FEATURES_SHOWN = 10\n",
    "# MAX_NUMBER_OF_FEATURES_SHOWN = 10: (integer) limiting the number\n",
    "# of features shown in the plot.\n",
    "\n",
    "# Dictionary containing calculated metrics returned as shap_dict;\n",
    "# Simply modify this object on the left of equality:\n",
    "shap_dict = shap_feature_analysis (model_object = MODEL_OBJECT, X_train = X_TRAIN, model_type = MODEL_TYPE, total_of_shap_points = TOTAL_OF_SHAP_POINTS, plot_type = PLOT_TYPE, max_number_of_features_shown = MAX_NUMBER_OF_FEATURES_SHOWN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5d9cd846-94f7-43dc-8d01-b3f5e5a91c63"
   },
   "source": [
    "### **Visualizing time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "5b7b076c-453d-4e93-bd3d-ecaa6e5aa200",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATA_IN_SAME_COLUMN = False\n",
    "\n",
    "# Parameters to input when DATA_IN_SAME_COLUMN = True:\n",
    "DATASET = None #Alternatively: object containing the dataset to be analyzed (e.g. DATASET = dataset)\n",
    "COLUMN_WITH_PREDICT_VAR_X = 'X' # Alternatively: correct name for X-column\n",
    "COLUMN_WITH_RESPONSE_VAR_Y = 'Y' # Alternatively: correct name for Y-column\n",
    "COLUMN_WITH_LABELS = 'label_column' # Alternatively: correct name for column with the labels or groups\n",
    "\n",
    "# DATA_IN_SAME_COLUMN = False: set as True if all the values to plot are in a same column.\n",
    "# If DATA_IN_SAME_COLUMN = True, you must specify the dataframe containing the data as DATASET;\n",
    "# the column containing the predict variable (X) as COLUMN_WITH_PREDICT_VAR_X; the column \n",
    "# containing the responses to plot (Y) as COLUMN_WITH_RESPONSE_VAR_Y; and the column \n",
    "# containing the labels (subgroup) indication as COLUMN_WITH_LABELS. \n",
    "# DATASET is an object, so do not declare it in quotes. The other three arguments (columns' names) \n",
    "# are strings, so declare in quotes. \n",
    "\n",
    "# Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare. \n",
    "# All the results for both groups are in a column named 'results', wich will be plot against\n",
    "# the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
    "# an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
    "# column 'group' shows the value 'B'. In this example:\n",
    "# DATA_IN_SAME_COLUMN = True,\n",
    "# DATASET = dataset,\n",
    "# COLUMN_WITH_PREDICT_VAR_X = 'time',\n",
    "# COLUMN_WITH_RESPONSE_VAR_Y = 'results', \n",
    "# COLUMN_WITH_LABELS = 'group'\n",
    "# If you want to declare a list of dictionaries, keep DATA_IN_SAME_COLUMN = False and keep\n",
    "# DATASET = None (the other arguments may be set as None, but it is not mandatory: \n",
    "# COLUMN_WITH_PREDICT_VAR_X = None, COLUMN_WITH_RESPONSE_VAR_Y = None, COLUMN_WITH_LABELS = None).\n",
    "\n",
    "\n",
    "# Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
    "LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = [\n",
    "    \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}, \n",
    "    {'x': None, 'y': None, 'lab': None}\n",
    "    \n",
    "]\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE: if data is already converted to series, lists\n",
    "# or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
    "# even if there is a single dictionary.\n",
    "# Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
    "# (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
    "# keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
    "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
    "# and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
    "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
    "# same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
    "# represents the series and label of the added dictionary (you can pass 'lab': None, but if \n",
    "# 'x' or 'y' are None, the new dictionary will be ignored).\n",
    "\n",
    "# Examples:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
    "# will plot a single variable. In turns:\n",
    "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = \n",
    "# [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
    "# will plot two series, Y1 x X and Y2 x X.\n",
    "# Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
    "# If None is provided to 'lab', an automatic label will be generated.\n",
    "\n",
    "\n",
    "X_AXIS_ROTATION = 70\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "ADD_SPLINE_LINES = True #Alternatively: True or False\n",
    "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
    "# Since we are obtaining a scatter plot, there is no meaning in omitting the dots,\n",
    "# as we can do for the time series visualization function.\n",
    "ADD_SCATTER_DOTS = False\n",
    "# If ADD_SCATTER_DOTS = False, no dots representing the data points are shown.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'time_series_vis.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "time_series_vis (data_in_same_column = DATA_IN_SAME_COLUMN, df = DATASET, column_with_predict_var_x = COLUMN_WITH_PREDICT_VAR_X, column_with_response_var_y = COLUMN_WITH_RESPONSE_VAR_Y, column_with_labels = COLUMN_WITH_LABELS, list_of_dictionaries_with_series_to_analyze = LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, add_scatter_dots = ADD_SCATTER_DOTS, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "99102f11-3f0e-4be2-9a2f-1f6d081e90a7",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing or exporting models and dictionaries (or lists)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "fbefe227-1dd4-4eda-9266-d3fabdd670aa"
   },
   "source": [
    "#### Case 1: import only a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "62aae970-a4f3-4dc1-9cdb-8bf6d15dea62",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'sklearn'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
    "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model.\n",
    "# Simply modify this object on the left of equality:\n",
    "model = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "96f6de6f-a85c-4c23-9536-dfe5d8118e61"
   },
   "source": [
    "#### Case 2: import only a dictionary or a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c4569134-81dd-4426-86d4-2b35a9792d1f",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'dict_or_list_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'sklearn'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
    "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Dictionary or list saved as imported_dict_or_list.\n",
    "# Simply modify this object on the left of equality:\n",
    "imported_dict_or_list = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "65654ea3-fb1a-440e-b0f1-f68853e28430"
   },
   "source": [
    "#### Case 3: import a model and a dictionary (or a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "9a827ad3-c7f8-4f09-ac3e-5275fefac65e",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_and_dict'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'sklearn'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
    "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model. Dictionary or list saved as imported_dict_or_list.\n",
    "# Simply modify these objects on the left of equality:\n",
    "model, imported_dict_or_list = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "ee54a7a8-e89f-46f9-b7ef-d147ba928d27"
   },
   "source": [
    "#### Case 4: export a model and/or a dictionary (or a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "22224ba6-5c67-44df-9378-c6f118228762",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'export'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'sklearn'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
    "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "051e0176-d463-454f-8e6b-8cfa63b24898"
   },
   "source": [
    "## **Exporting the dataframe as CSV file (to notebook's workspace)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "89f2403b-01db-47e5-a371-d03a4947dbbb",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## WARNING: all files exported from this function are .csv (comma separated values)\n",
    "\n",
    "DATAFRAME_OBJ_TO_BE_EXPORTED = dataset\n",
    "# Alternatively: object containing the dataset to be exported.\n",
    "# DATAFRAME_OBJ_TO_BE_EXPORTED: dataframe object that is going to be exported from the\n",
    "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "# example: DATAFRAME_OBJ_TO_BE_EXPORTED = dataset will export the dataset object.\n",
    "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "NEW_FILE_NAME_WITHOUT_EXTENSION = \"dataset\"\n",
    "# NEW_FILE_NAME_WITHOUT_EXTENSION - (string, in quotes): input the name of the \n",
    "# file without the extension. e.g. set NEW_FILE_NAME_WITHOUT_EXTENSION = \"my_file\" \n",
    "# to export the CSV file 'my_file.csv' to notebook's workspace.\n",
    "\n",
    "export_pd_dataframe_as_csv (dataframe_obj_to_be_exported = DATAFRAME_OBJ_TO_BE_EXPORTED, new_file_name_without_extension = NEW_FILE_NAME_WITHOUT_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "e13d2bdc-3246-467d-bb2d-75ec2248e2e1"
   },
   "source": [
    "## **Downloading a file from Google Colab to the local machine; or uploading a file from the machine to Colab's instant memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "f04fa868-effc-4706-aebb-157bd4b6ace2"
   },
   "source": [
    "#### Case 1: upload a file to Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "aa4a5310-400e-42c9-a1ba-bce54dc62038",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model named keras_model, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'keras_model.h5'\n",
    "\n",
    "# Dictionary storing the uploaded files returned as colab_files_dict.\n",
    "# Simply modify this object on the left of the equality:\n",
    "colab_files_dict = upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "afb9b62f-75f0-4f38-89fb-4bbaf8f0a817"
   },
   "source": [
    "#### Case 2: download a file from Colab's workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "28b2e0af-bb77-4dc1-986b-795034ec148a",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'download'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "FILE_TO_DOWNLOAD_FROM_COLAB = None\n",
    "# FILE_TO_DOWNLOAD_FROM_COLAB = None. This parameter is obbligatory when\n",
    "# action = 'download'. \n",
    "# Declare as FILE_TO_DOWNLOAD_FROM_COLAB the file that you want to download, with\n",
    "# the correspondent extension.\n",
    "# It should not be declared in quotes.\n",
    "# e.g. to download a dictionary named dict, FILE_TO_DOWNLOAD_FROM_COLAB = 'dict.pkl'\n",
    "# To download a dataframe named df, declare FILE_TO_DOWNLOAD_FROM_COLAB = 'df.csv'\n",
    "# To export a model nameACTION = 'upload'\n",
    "# ACTION = 'download' to download the file to the local machine\n",
    "# ACTION = 'upload' to upload a file from local machine to Google Colab's \n",
    "# instant memory\n",
    "\n",
    "upload_to_or_download_file_from_colab (action = ACTION, file_to_download_from_colab = FILE_TO_DOWNLOAD_FROM_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "a9164a50-4c88-4f64-8da9-f935bb1ba65a"
   },
   "source": [
    "## **Exporting a list of files from notebook's workspace to AWS Simple Storage Service (S3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "17c78189-b9bf-4544-8868-36b3fd62ba79",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['s3_file1.txt', 's3_file2.txt']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS: list containing all the files to export to S3.\n",
    "# Declare it as a list even if only a single file will be exported.\n",
    "# It must be a list of strings containing the file names followed by the extensions.\n",
    "# Example, to a export a single file my_file.ext, where my_file is the name and ext is the\n",
    "# extension:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['my_file.ext']\n",
    "# To export 3 files, file1.ext1, file2.ext2, and file3.ext3:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['file1.ext1', 'file2.ext2', 'file3.ext3']\n",
    "# Other examples:\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['Screen_Shot.png', 'dataset.csv']\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = [\"dictionary.pkl\", \"model.h5\"]\n",
    "# LIST_OF_FILE_NAMES_WITH_EXTENSIONS = ['doc.pdf', 'model.dill']\n",
    "\n",
    "DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = ''\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT: directory from notebook's workspace\n",
    "# from which the files will be exported to S3. Keep it None, or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = \"/\"; or\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = '' (empty string) to export from\n",
    "# the root (main) directory.\n",
    "# Alternatively, set as a string containing only the directories and folders, not the file names.\n",
    "# Examples: DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1';\n",
    "# DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT = 'folder1/folder2/'\n",
    "    \n",
    "# For this function, all exported files must be located in the same directory.\n",
    "\n",
    "S3_BUCKET_NAME = 'my_bucket'\n",
    "## This parameter is obbligatory to access an AWS S3 bucket. Substitute it for a string\n",
    "# with the bucket's name. e.g. s3_bucket_name = \"aws-bucket-1\" access a bucket named as\n",
    "# \"aws-bucket-1\"\n",
    "\n",
    "S3_OBJECT_FOLDER_PREFIX = \"\"\n",
    "# S3_OBJECT_FOLDER_PREFIX = None. Keep it None; or as an empty string \n",
    "# (S3_OBJECT_FOLDER_PREFIX = ''); or as the root \"/\" to import the \n",
    "# whole bucket content, instead of a single object from it.\n",
    "# Alternatively, set it as a string containing the subfolder from the bucket to import:\n",
    "# Suppose that your bucket (admin-created) has four objects with the following object \n",
    "# keys: Development/Projects1.xls; Finance/statement1.pdf; Private/taxdocument.pdf; and\n",
    "# s3-dg.pdf. \n",
    "# The s3-dg.pdf key does not have a prefix, so its object appears directly \n",
    "# at the root level of the bucket. If you open the Development/ folder, you see \n",
    "# the Projects.xlsx object in it.\n",
    "# In summary, if the path of the file is: 'bucket/my_path/.../file.csv'\n",
    "# where 'bucket' is the bucket's name, prefix = 'my_path/.../', without the\n",
    "# 'file.csv' (file name with extension) last part.\n",
    "\n",
    "# So, declare the prefix as S3_OBJECT_FOLDER_PREFIX to import only files from\n",
    "# a given folder (directory) of the bucket.\n",
    "# DO NOT PUT A SLASH before (to the right of) the prefix;\n",
    "# DO NOT ADD THE BUCKET'S NAME TO THE right of the prefix:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/\"\n",
    "\n",
    "# Alternatively, provide the full path of a given file if you want to import only it:\n",
    "# S3_OBJECT_FOLDER_PREFIX = \"bucket_directory1/.../bucket_directoryN/my_file.ext\"\n",
    "# where my_file is the file's name, and ext is its extension.\n",
    "\n",
    "\n",
    "# Attention: after running this function for connecting with AWS Simple Storage System (S3), \n",
    "# your 'AWS Access key ID' and your 'Secret access key' will be requested.\n",
    "# The 'Secret access key' will be hidden through dots, so it cannot be visualized or copied by\n",
    "# other users. On the other hand, the same is not true for 'Access key ID', the bucket's name \n",
    "# and the prefix. All of these are sensitive information from the organization.\n",
    "# Therefore, after importing the information, always remember of cleaning the output of this cell\n",
    "# and of removing such information from the strings.\n",
    "# Remember that these data may contain privilege for accessing protected information, \n",
    "# so it should not be used for non-authorized people.\n",
    "\n",
    "# Also, remember of deleting the imported files from the workspace after finishing the analysis.\n",
    "# The costs for storing the files in S3 is quite inferior than those for storing directly in the\n",
    "# workspace. Also, files stored in S3 may be accessed for other users than those with access to\n",
    "# the notebook's workspace.\n",
    "export_files_to_s3 (list_of_file_names_with_extensions = LIST_OF_FILE_NAMES_WITH_EXTENSIONS, directory_of_notebook_workspace_storing_files_to_export = DIRECTORY_OF_NOTEBOOK_WORKSPACE_STORING_FILES_TO_EXPORT, s3_bucket_name = S3_BUCKET_NAME, s3_obj_prefix = S3_OBJECT_FOLDER_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1"
   },
   "source": [
    "****"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
