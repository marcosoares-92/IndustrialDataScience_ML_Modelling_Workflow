{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5edcdce4-06a8-4dc6-947d-20eee44ae7ba"
   },
   "source": [
    "# **Similarity, Clustering and Anomaly Detection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "d0c4fe20-ff25-4875-8113-3c16a46bc83f"
   },
   "source": [
    "## _Machine Learning Modelling Workflow Notebook 7_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "1721cd2c-7d13-49f3-a74e-0faf52df7b5c"
   },
   "source": [
    "## Content:\n",
    "1. Splitting the dataframe into train and test subsets;\n",
    "2. Obtaining the distance matrix to find similarities;\n",
    "3. Applying the K-Means Elbow method to find ideal number of clusters;\n",
    "4. K-Means Clustering;\n",
    "5. Anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267"
   },
   "source": [
    "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
    "- marcosoares.feq@gmail.com\n",
    "- marco.soares@bayer.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea"
   },
   "source": [
    "## **Load Python Libraries in Global Context**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "694895ef-4275-46c6-82ad-a08addf9ed8e",
    "id": "bzZgOvXCyHHl",
    "language": "python",
    "tags": [
     "CELL_4"
    ]
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Import all needed functions and classes with original names, with no aliases:\n",
    "from idsw import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "eee74d56-8e4c-4851-be92-759a8182901c",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "## **Call the functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "5fe1661a-23b2-42b6-b996-144eb951fd50",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt), \n",
    "## JSON, txt, or CSV (comma separated values) files. Tables in webpages or html files can also be read.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "\n",
    "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
    "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the \n",
    "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or, \n",
    "# FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
    "# Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
    "# Also, html files and webpages may be also read.\n",
    "\n",
    "# You may input the path for an HTML file containing a table to be read; or \n",
    "# a string containing the address for a webpage containing the table. The address must start\n",
    "# with www or htpp. If a website is input, the full address can be input as FILE_DIRECTORY_PATH\n",
    "# or as FILE_NAME_WITH_EXTENSION.\n",
    "\n",
    "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
    "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True \n",
    "# if you want to read a file with txt extension containing a text formatted as JSON \n",
    "# (but not saved as JSON).\n",
    "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the \n",
    "# function (below) must be set. If not, an error message will be raised.\n",
    "\n",
    "HOW_MISSING_VALUES_ARE_REGISTERED = None\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
    "# empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
    "# This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
    "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’, \n",
    "#‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’, \n",
    "# ‘n/a’, ‘nan’, ‘null’.\n",
    "\n",
    "# If a different denomination is used, indicate it as a string. e.g.\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
    "# HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
    "\n",
    "# If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
    "# only in column 'numeric_col', you can specify the following dictionary:\n",
    "# how_missing_values_are_registered = {'numeric-col': 0}\n",
    "\n",
    "    \n",
    "HAS_HEADER = True\n",
    "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
    "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
    "\n",
    "DECIMAL_SEPARATOR = '.'\n",
    "# DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
    "# the decimal separator. Alternatively, specify here the separator.\n",
    "# e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
    "# It manipulates the argument 'decimal' from Pandas functions.\n",
    "\n",
    "TXT_CSV_COL_SEP = \"comma\"\n",
    "# txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
    "# or 'csv'. It informs how the different columns are separated.\n",
    "# Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\" \n",
    "# for columns separated by comma;\n",
    "# txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \" \n",
    "# for columns separated by simple spaces.\n",
    "# You can also set a specific separator as string. For example:\n",
    "# txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
    "# is used as separator for the columns - '\\t' represents the tab character).\n",
    "\n",
    "## Parameters for loading Excel files:\n",
    "\n",
    "LOAD_ALL_SHEETS_AT_ONCE = False\n",
    "# LOAD_ALL_SHEETS_AT_ONCE = False - This parameter has effect only when for Excel files.\n",
    "# If LOAD_ALL_SHEETS_AT_ONCE = True, the function will return a list of dictionaries, each\n",
    "# dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
    "# value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
    "# and its value will be the pandas dataframe object obtained from that sheet.\n",
    "# This argument has preference over SHEET_TO_LOAD. If it is True, all sheets will be loaded.\n",
    "    \n",
    "SHEET_TO_LOAD = None\n",
    "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
    "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
    "# will be loaded.\n",
    "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
    "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
    "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
    "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
    "# name to load the sheet with that name.\n",
    "\n",
    "## Parameters for loading JSON files:\n",
    "\n",
    "JSON_RECORD_PATH = None\n",
    "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
    "# Path in each object to list of records. If not passed, data will be assumed to \n",
    "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
    "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
    "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
    "\n",
    "JSON_FIELD_SEPARATOR = \"_\"\n",
    "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
    "# Nested records will generate names separated by sep. \n",
    "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
    "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
    "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
    "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
    "\n",
    "JSON_METADATA_PREFIX_LIST = None\n",
    "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter \n",
    "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting \n",
    "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
    "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
    "\n",
    "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
    "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
    "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
    "# are 'name' and 'last'.\n",
    "# Then, JSON_RECORD_PATH = 'books'\n",
    "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
    "\n",
    "\n",
    "# The dataframe will be stored in the object named 'dataset':\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset = load_pandas_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, how_missing_values_are_registered = HOW_MISSING_VALUES_ARE_REGISTERED, has_header = HAS_HEADER, decimal_separator = DECIMAL_SEPARATOR, txt_csv_col_sep = TXT_CSV_COL_SEP, load_all_sheets_at_once = LOAD_ALL_SHEETS_AT_ONCE, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)\n",
    "\n",
    "# OBS: If an Excel file is loaded and LOAD_ALL_SHEETS_AT_ONCE = True, then the object\n",
    "# dataset will be a list of dictionaries, with 'sheet' as key containing the sheet name; and 'df'\n",
    "# as key correspondent to the Pandas dataframe. So, to access the 3rd dataframe (index 2, since\n",
    "# indexing starts from zero): df = dataframe[2]['df'], where dataframe is the list returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "fe6ecfda-9620-47c9-91b5-f68c298c159d"
   },
   "source": [
    "### **Separating and preparing features and responses tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "302dcfa9-30d9-436d-b97c-fa31972336fb",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET = dataset  #Alternatively: object containing the dataset to be analyzed\n",
    "\n",
    "FEATURES_COLUMNS = ['col1', 'col2']\n",
    "# FEATURES_COLUMNS: list of strings or string containing the names of columns\n",
    "# with predictive variables in the original dataframe. \n",
    "# Example: FEATURES_COLUMNS = ['col1', 'col2']; FEATURES_COLUMNS = 'predictor';\n",
    "# FEATURES_COLUMNS = ['predictor'].\n",
    "\n",
    "RESPONSE_COLUMNS = \"response\"\n",
    "# RESPONSE_COLUMNS: list of strings or string containing the names of columns\n",
    "# with response variables in the original dataframe. \n",
    "# Example: RESPONSE_COLUMNS= ['col3', 'col4']; RESPONSE_COLUMNS = 'response';\n",
    "# RESPONSE_COLUMNS = ['response']\n",
    "\n",
    "# Arrays or tensors containing features and responses returned as X and y, respectively.\n",
    "# Mapping dictionary correlating the position in array or tensor to the original column name\n",
    "# returned as column_map_dict.\n",
    "# Simply modify these objects on the left of equality:\n",
    "X, y, column_map_dict = separate_and_prepare_features_and_responses (df = DATASET, features_columns = FEATURES_COLUMNS, response_columns = RESPONSE_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b0ec8b20-ce3e-4f36-8ab7-24730702e392"
   },
   "source": [
    "### **Converting a whole dataframe or array-like object to tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "40232256-4dc5-4eca-935f-9bdc47c127e9",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "DATASET_OR_ARRAY_TO_CONVERT = dataset  \n",
    "# Alternatively: object containing the dataset or array-like object to be converted and reshaped.\n",
    "\n",
    "COLUMNS_TO_CONVERT = None\n",
    "# ATTENTION: This argument only works for Pandas dataframes.\n",
    "# COLUMNS_TO_CONVERT: list of strings or string containing the names of columns\n",
    "# that you want to convert. Use this if you want to convert only a subset of the dataframe. \n",
    "# Example: COLUMNS_TO_CONVERT = ['col1', 'col2']; COLUMNS_TO_CONVERT = 'predictor';\n",
    "# COLUMNS_TO_CONVERT = ['predictor'] will create a tensor with only the specified columns;\n",
    "# If None, the whole dataframe will be converted.\n",
    "\n",
    "COLUMNS_TO_EXCLUDE = None\n",
    "# ATTENTION: This argument only works for Pandas dataframes.\n",
    "# COLUMNS_TO_EXCLUDE: Alternative parameter. \n",
    "# list of strings or string containing the names of columns that you want to exclude from the\n",
    "# returned tensor. Use this if you want to convert only a subset of the dataframe. \n",
    "# Example: COLUMNS_TO_EXCLUDE = ['col1', 'col2']; COLUMNS_TO_EXCLUDE = 'predictor';\n",
    "# COLUMNS_TO_EXCLUDE = ['predictor'] will create a tensor with all columns from the dataframe\n",
    "# except the specified ones. This argument will only be used if the previous one was not.\n",
    "\n",
    "\n",
    "# Array or tensor returned as X. Mapping dictionary correlating the position in array or tensor \n",
    "# to the original column name returned as column_map_dict.\n",
    "# Simply modify these objects on the left of equality:\n",
    "X, column_map_dict = convert_to_tensor (df_or_array_to_convert = DATASET_OR_ARRAY_TO_CONVERT, columns_to_convert = COLUMNS_TO_CONVERT, columns_to_exclude = COLUMNS_TO_EXCLUDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "b975176a-0e1d-47cc-9151-456955980d1f"
   },
   "source": [
    "### **Splitting features and responses into train and test tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "57e8b5f4-453d-4342-a584-82b24f335f2b",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "X_tensor = X\n",
    "# X_tensor = tensor or array of predictive variables. Alternatively, modify X, not X_tensor.\n",
    "Y_tensor = y\n",
    "# Y_tensor = tensor or array of response variables. Alternatively, modify y, not Y_tensor.\n",
    "\n",
    "PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 75   \n",
    "# percent_of_data_used_for_model_training: float from 0 to 100,\n",
    "# representing the percent of data used for training the model\n",
    "\n",
    "PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 0\n",
    "# If you want to use cross-validation, separate a percent of the training data for validation.\n",
    "# Declare this percent as PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION (float from 0 to 100).\n",
    "\n",
    "# Subset and series destined to training, testing and/or validation returned in the dictionary split_dictionary;\n",
    "# Simply modify this object on the left of equality:\n",
    "split_dictionary = split_data_into_train_and_test (X = X_tensor, y = Y_tensor, percent_of_data_used_for_model_training = PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING, percent_of_training_data_used_for_model_validation = PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "ae327068-9685-495f-97ef-aebd96777b74"
   },
   "source": [
    "### **Splitting time series into train and test tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c4375c3f-c21f-4620-8701-ca22951fa55f",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "X_tensor = X\n",
    "# X_df = tensor or array of predictive variables. Alternatively, modify X, not X_tensor.\n",
    "Y_tensor = y\n",
    "# Y = tensor or array of response variables. Alternatively, modify y, not Y_tensor.\n",
    "\n",
    "PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 75   \n",
    "# percent_of_data_used_for_model_training: float from 0 to 100,\n",
    "# representing the percent of data used for training the model\n",
    "\n",
    "PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 0\n",
    "# If you want to use cross-validation, separate a percent of the training data for validation.\n",
    "# Declare this percent as PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION (float from 0 to 100).\n",
    "\n",
    "# Subset and series destined to training, testing and/or validation returned in the dictionary split_dictionary;\n",
    "# Simply modify this object on the left of equality:\n",
    "split_dictionary = time_series_train_test_split (X = X_tensor, y = Y_tensor, percent_of_data_used_for_model_training = PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING, percent_of_training_data_used_for_model_validation = PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "ffb76259-dfef-4409-8491-e961968e8eb6"
   },
   "source": [
    "### **Creating a TensorFlow windowed dataset from a time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c43031a7-4d55-47a8-b18b-31233b5fe688",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "Y_tensor = y\n",
    "# Y = tensor or array of response variables. Alternatively, modify y, not Y_tensor.\n",
    "\n",
    "WINDOW_SIZE = 20\n",
    "# WINDOW_SIZE (integer): number of rows/ size of the time window used.\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "# BATCH_SIZE (integer): number of rows/ size of the batches used for training.\n",
    "\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "# SHUFFLE_BUFFER_SIZE (integer): number of rows/ size used for shuffling the entries.\n",
    "\n",
    "# TensorFlow Dataset obtained from the time series returned as dataset_from_time_series.\n",
    "# Simply modify this object on the left of equality:\n",
    "dataset_from_time_series = windowed_dataset_from_time_series (y = Y_tensor, window_size = WINDOW_SIZE, batch_size = BATCH_SIZE, shuffle_buffer_size = SHUFFLE_BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating a TensorFlow windowed dataset from multiple-feature time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = dataset\n",
    "# Alternatively: object containing the Pandas dataframe to be converted and reshaped.\n",
    "\n",
    "RESPONSE_COLUMNS = 'response_variable'\n",
    "# RESPONSE_COLUMNS: string or list of strings with the response columns\n",
    "\n",
    "SEQUENCE_STRIDE = 1\n",
    "SAMPLING_RATE = 1\n",
    "SHIFT = 1\n",
    "# SHIFT, SAMPLING_RATE, and SEQUENCE_STRIDE: integers\n",
    "\n",
    "# The time series may be represented as a sequence of times like: t = 0, t = 1, t = 2, ..., t = N.\n",
    "# When preparing the dataset, we pick a given number of 'times' (indexes), and use them for\n",
    "# predicting a time in the future.\n",
    "# So, the INPUT_WIDTH represents how much times will be used for prediction. If INPUT_WIDTH = 6,\n",
    "# we use 6 values for prediction, e.g., t = 0, t = 1, ..., t = 5 will be a prediction window.\n",
    "# In turns, if INPUT_WIDTH = 3, 3 values are used: t = 0, t = 1, t = 2; if INPUT_WIDTH = N, N\n",
    "# consecutive values will be used: t = 0, t = 1, t = 2, ..., t = N. And so on.\n",
    "# LABEL_WIDTH, in turns, represent how much times will be predicted. If LABEL_WIDTH = 1, a single\n",
    "# value will be predicted. If LABEL_WIDTH = 2, two consecutive values are predicted; if LABEL_WIDTH =\n",
    "# N, N consecutive values are predicted; and so on.\n",
    "        \n",
    "# SHIFT represents the offset, i.e., given the input values, which value in the time sequence will\n",
    "# be predicted. So, suppose INPUT_WIDTH = 6 and LABEL_WIDTH = 1\n",
    "# If SHIFT = 1, the label, i.e., the predicted value, will be the first after the sequence used for\n",
    "# prediction. So, if  t = 0, t = 1, ..., t = 5 will be a prediction window and t = 6 will be the\n",
    "# predicted value. Notice that the complete window has a total width = 7: t = 0, ..., t = 7. \n",
    "# If LABEL_WIDTH = 2, then t = 6 and t = 7 will be predicted (total width = 8).\n",
    "# Another example: suppose INPUT_WIDTH = 24. So the predicted window is: t = 0, t = 1, ..., t = 23.\n",
    "# If SHIFT = 24, the 24th element after the prediction sequence will be used as label, i.e., will\n",
    "# be predicted. So, t = 24 is the 1st after the sequence, t = 25 is the second, ... t = 47 is the\n",
    "# 24th after. If label_with = 1, then the sequence t = 0, t = 1, ..., t = 23 will be used for\n",
    "# predicting t = 47. Naturally, the total width of the window = 47 in this case.\n",
    "# Also, notice that the label is used by the model as the response (predicted) variable.\n",
    "\n",
    "# So for a given SHIFT: the sequence of timesteps i, i+1, ... will be used for predicting the\n",
    "# timestep i + SHIFT\n",
    "# If a sequence starts in index i, the next sequence will start from i + SEQUENCE_STRIDE.\n",
    "# The sequence will be formed by timesteps i, i + SAMPLING_RATE, i + 2* SAMPLING_RATE, ...\n",
    "# Example: Consider indices [0, 1, ... 99]. With sequence_length=10, SAMPLING_RATE=2, \n",
    "# SEQUENCE_STRIDE=3, the dataset will yield batches of sequences composed of the following indices:\n",
    "# First sequence:  [0  2  4  6  8 10 12 14 16 18]\n",
    "# Second sequence: [3  5  7  9 11 13 15 17 19 21]\n",
    "# Third sequence:  [6  8 10 12 14 16 18 20 22 24]\n",
    "# ...\n",
    "# Last sequence:   [78 80 82 84 86 88 90 92 94 96]\n",
    "\n",
    "USE_PAST_RESPONSES_FOR_PREDICTION = True\n",
    "# USE_PAST_RESPONSES_FOR_PREDICTION: True if the past responses will be used for predicting their\n",
    "# value in the future; False if you do not want to use them.\n",
    "\n",
    "PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 70   \n",
    "# percent_of_data_used_for_model_training: float from 0 to 100,\n",
    "# representing the percent of data used for training the model\n",
    "\n",
    "PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 10\n",
    "# If you want to use cross-validation, separate a percent of the training data for validation.\n",
    "# Declare this percent as PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION (float from 0 to 100).\n",
    "\n",
    "# If PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 70, and \n",
    "# PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 10, \n",
    "# training dataset slice goes from 0 to 0.7 (70%) of the dataset;\n",
    "# testing slicing goes from 0.7 x dataset to ((1 - 0.1) = 0.9) x dataset\n",
    "# validation slicing goes from 0.9 x dataset to the end of the dataset.\n",
    "# Here, consider the time sequence t = 0, t = 1, ... , t = N, for a dataset with length N:\n",
    "# training: from t = 0 to t = (0.7 x N); testing: from t = ((0.7 x N) + 1) to (0.9 x N);\n",
    "# validation: from t = ((0.9 x N) + 1) to N (the fractions 0.7 x N and 0.9 x N are rounded to\n",
    "# the closest integer).\n",
    "    \n",
    "\n",
    "# Dictionary with inputs and labels tensors returned as tensors_dict.\n",
    "# Simply modify this object on the left of equality:\n",
    "tensors_dict = multi_columns_time_series_tensors (df = DATASET, response_columns = RESPONSE_COLUMNS, sequence_stride = SEQUENCE_STRIDE, sampling_rate = SAMPLING_RATE, shift = SHIFT, use_past_responses_for_prediction = USE_PAST_RESPONSES_FOR_PREDICTION, percent_of_data_used_for_model_training = PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING, percent_of_training_data_used_for_model_validation = PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union of several 1-dimensional tensors (obtained from single columns) into a single tensor\n",
    "- Each 1-dimensional tensor or array becomes a column from the new tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_OF_TENSORS_OR_ARRAYS = [tensor1, tensor2]\n",
    "# list of tensors: list containing the 1-dimensional tensors or arrays that the function will union.\n",
    "# the operation will be performed in the order that the tensors are declared.\n",
    "# Substitue tensor1, tensor2, tensor3,... by the tensor objects, in the correct sequence.\n",
    "# If the resulting tensor will contain the responses for a multi-response tensor, declare them in the\n",
    "# orders of the responses (tensor 1 corresponding to response 1, tensor 2 to response 2, etc.)\n",
    "\n",
    "# One-dimensional tensors have shape (X,), where X is the number of elements. Example: a column\n",
    "# of the dataframe with elements 1, 2, 3 in this order may result in an array like array([1, 2, 3])\n",
    "# and a Tensor with shape (3,). With we union it with the tensor from the column with elements\n",
    "# 4, 5, 6, the output will be array([[1,4], [2,5], [3,6]]). Alternatively, this new array could\n",
    "# be converted into a Pandas dataframe where each column would be correspondent to one individual\n",
    "# tensor.\n",
    "\n",
    "# Tensor resulting from the union of multiple single-dimension tensor returned as tensors_union.\n",
    "# Simply modify this object on the left of equality:\n",
    "tensors_union = union_1_dim_tensors (list_of_tensors_or_arrays = LIST_OF_TENSORS_OR_ARRAYS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c2452032-84b3-4cab-99f6-fc3f944917d1"
   },
   "source": [
    "### **Obtaining the distance matrix to find similarities**\n",
    "- Compute distances between data points.\n",
    "- Smaller distances in the n-dimensional space indicate closer (more similar) points.\n",
    "- You may compute distance between new points and the original dataset, new points themselves, or between each point from the original dataset.\n",
    "- This function will return the matrix of distances between each tensor data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "c0d29add-d3b2-494c-8ad4-c7e4f586d61a",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# split_dictionary.keys() == dict_keys(['X_train', 'y_train', 'X_test', 'y_test', 'X_valid', 'y_valid'])\n",
    "\n",
    "DF_TENSOR_OR_ARRAY = dataset\n",
    "# DF_TENSOR_OR_ARRAY = dataframe, array, or tensor with the data points. \n",
    "# At least one array must be provided as DF_TENSOR_OR_ARRAY\n",
    "\n",
    "LIST_OF_NEW_ARRAYS_TO_COMPARE = None\n",
    "# LIST_OF_NEW_ARRAYS_TO_COMPARE: if you want to compare new data points with the original dataset, \n",
    "# input them as a collection (array-like or list) of arrays. Each value in the array is the \n",
    "# correspondent value of a given variable for that new entry (i.e., a component of the vector). \n",
    "# For example, if you have a dataset with 3 columns, 'A', 'B', and 'C' and you want to\n",
    "# find the distance between the points (A, B, C) = (1, 2, 3) and (A, B, C) = (1.2, 2.4, 3.6) with \n",
    "# each point of the original array, then:\n",
    "# LIST_OF_NEW_ARRAYS_TO_COMPARE = [[1, 2, 3], [1.2, 2.4, 3.6]]\n",
    "# If you want only to compare the data points in the original dataset, you may keep \n",
    "# LIST_OF_NEW_ARRAYS_TO_COMPARE = None\n",
    "\n",
    "# NOTICE THAT you may input a single array in DF_TENSOR_OR_ARRAY and a single array as \n",
    "# LIST_OF_NEW_ARRAYS_TO_COMPARE. This will calculate the distance between two data points.\n",
    "\n",
    "WHAT_TO_COMPARE = 'only_original_array'\n",
    "# WHAT_TO_COMPARE = 'only_original_array' - if WHAT_TO_COMPARE = 'only_original_array', \n",
    "# the function will calculate the distance between each point in the original dataset only \n",
    "# (even if you input new arrays to compare).\n",
    "# If WHAT_TO_COMPARE = 'only_new_with_original' - in this case, only the comparison between new \n",
    "# arrays input as LIST_OF_NEW_ARRAYS_TO_COMPARE and the original arrays will be performed. \n",
    "# Notice that it will not calculate the distances between the data points of the original dataset, \n",
    "# except if no data was input as LIST_OF_NEW_ARRAYS_TO_COMPARE. \n",
    "# In this case, the default 'only_original_array' will be performed.\n",
    "# If WHAT_TO_COMPARE = 'everything', new data will be stacked on the bottom of the original array \n",
    "# and the distance between each data point will be obtained.\n",
    "\n",
    "DISTANCE_METRICS = 'euclidean'\n",
    "# DISTANCE_METRICS = 'euclidean' - string with the distance metrics that will be used.\n",
    "# The distance function can be ‘braycurtis’, ‘canberra’, ‘chebyshev’, ‘cityblock’, ‘correlation’, \n",
    "# ‘cosine’, ‘dice’, ‘euclidean’, ‘hamming’, ‘jaccard’, ‘jensenshannon’, ‘kulczynski1’, ‘mahalanobis’, \n",
    "# ‘matching’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, ‘sokalmichener’, \n",
    "# ‘sokalsneath’, ‘sqeuclidean’, ‘yule’.\n",
    "## ATTENTION: METRICS ARE CALCULATED AS DEFAULT, it is not possible to modify extra parameters of \n",
    "#  each one. Explanaition of each one in:\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html#scipy.spatial.distance.pdist\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html#scipy.spatial.distance.cdist\n",
    "# https://docs.scipy.org/doc/scipy/reference/spatial.distance.html\n",
    "\n",
    "\n",
    "# Matrix with distances between data points returned as distance_matrix.\n",
    "# Simply modify this object on the left of equality:\n",
    "distance_matrix = distances_between_each_data_point(df_tensor_or_array = DF_TENSOR_OR_ARRAY, list_of_new_arrays_to_compare = LIST_OF_NEW_ARRAYS_TO_COMPARE, what_to_compare = WHAT_TO_COMPARE, distance_metrics = DISTANCE_METRICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c2452032-84b3-4cab-99f6-fc3f944917d1"
   },
   "source": [
    "### **Applying the K-Means Elbow method to find ideal number of clusters**\n",
    "- Support for K-Means Clustering Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TENSOR = X\n",
    "# X_TENSOR = subset of predictive variables (dataframe). Since this is a non-supervised algorithm, \n",
    "# you may pass the full dataset.\n",
    "\n",
    "MAX_NUMBER_OF_CLUSTERS_TO_TEST = 100\n",
    "# MAX_NUMBER_OF_CLUSTERS_TO_TEST = 100 (integer). The algorithm will test 2 to the total of clusters you defined as\n",
    "# MAX_NUMBER_OF_CLUSTERS_TO_TEST.\n",
    "NUMBER_OF_INITIALIZATIONS = 10\n",
    "# NUMBER_OF_INITIALIZATIONS = 10 (integer). Number of times the k-means algorithm is run with \n",
    "# different centroid seeds. The final results is the best output of n_init consecutive runs in terms \n",
    "# of inertia. Several runs are recommended for sparse high-dimensional problems. \n",
    "# Manipulates the parameter n_init from KMeans Sklearn class.\n",
    "MAXIMUM_OF_ALLOWED_ITERATIONS = 20000\n",
    "# MAXIMUM_OF_ALLOWED_ITERATIONS = integer representing the maximum number of iterations\n",
    "# that the optimization algorithm can perform. Depending on data, convergence may not be\n",
    "# reached within this limit, so you may need to increase this hyperparameter.\n",
    "KMEANS_ALGORITHM = 'lloyd'\n",
    "# KMEANS_ALGORITHM = 'lloyd'. K-means algorithm to use. The classical EM-style algorithm is \"lloyd\". \n",
    "# The \"elkan\" variation can be more efficient on some datasets with well-defined clusters, by using \n",
    "# the triangle inequality. However it’s more memory intensive due to the allocation of an extra \n",
    "# array of shape (n_samples, n_clusters).\n",
    "# Options: “lloyd”, “elkan”, “auto”, “full”. Manipulates the parameter algorithm from KMeans \n",
    "# Sklearn class.\n",
    "TOLERANCE = 0.0001\n",
    "# TOLERANCE = 0.0001. Relative tolerance with regards to Frobenius norm of the difference in the \n",
    "# cluster centers of two consecutive iterations to declare convergence. \n",
    "# Manipulates the parameter tol from KMeans Sklearn class.\n",
    "\n",
    "X_AXIS_ROTATION = 0\n",
    "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "Y_AXIS_ROTATION = 0\n",
    "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
    "GRID = True #Alternatively: True or False\n",
    "# If GRID = False, no grid lines are shown in the graphic.\n",
    "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
    "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
    "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
    "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
    "\n",
    "EXPORT_PNG = False\n",
    "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
    "# Set EXPORT_PNG = True to export the obtained image.\n",
    "DIRECTORY_TO_SAVE = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\" \n",
    "# or DIRECTORY_TO_SAVE = \"folder\"\n",
    "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
    "# path, DIRECTORY_TO_SAVE = \"\"\n",
    "FILE_NAME = None\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# (string, in quotes): input the name you want for the file without the \n",
    "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png' \n",
    "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
    "# 'KMeans_elbow_plot.png'\n",
    "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
    "# the file will be overwritten.\n",
    "PNG_RESOLUTION_DPI = 330\n",
    "# This parameter has effect only if EXPORT_PNG = True.\n",
    "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
    "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
    "\n",
    "\n",
    "# The dataframe containing the number of cluster and correspondent inertia was returned as\n",
    "# elbow_df. Simply modify this object on the left of equality:\n",
    "elbow_df = kmeans_elbow_method (X_tensor = X_TENSOR, max_number_of_clusters_to_test = MAX_NUMBER_OF_CLUSTERS_TO_TEST, number_of_initializations = NUMBER_OF_INITIALIZATIONS, maximum_of_allowed_iterations = MAXIMUM_OF_ALLOWED_ITERATIONS, kmeans_algorithm = KMEANS_ALGORITHM, tolerance = TOLERANCE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c2452032-84b3-4cab-99f6-fc3f944917d1"
   },
   "source": [
    "### **K-Means Clustering**\n",
    "- Unsupervised learning method for assigning clusters to each data point.\n",
    "- Notice that the response predicted by the model is simply the cluster for a given entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TENSOR = X\n",
    "# X_TENSOR = subset of predictive variables (dataframe). Since this is a non-supervised algorithm, \n",
    "# you may pass the full dataset.\n",
    "\n",
    "NUMBER_OF_CLUSTERS = 8\n",
    "# NUMBER_OF_CLUSTERS = 8 (integer). The number of clusters to form as well as the number of centroids \n",
    "# to generate. Manipulates the parameter n_clusters from KMeans Sklearn class.\n",
    "NUMBER_OF_INITIALIZATIONS = 10\n",
    "# NUMBER_OF_INITIALIZATIONS = 10 (integer). Number of times the k-means algorithm is run with \n",
    "# different centroid seeds. The final results is the best output of n_init consecutive runs in terms \n",
    "# of inertia. Several runs are recommended for sparse high-dimensional problems. \n",
    "# Manipulates the parameter n_init from KMeans Sklearn class.\n",
    "MAXIMUM_OF_ALLOWED_ITERATIONS = 20000\n",
    "# MAXIMUM_OF_ALLOWED_ITERATIONS = integer representing the maximum number of iterations\n",
    "# that the optimization algorithm can perform. Depending on data, convergence may not be\n",
    "# reached within this limit, so you may need to increase this hyperparameter.\n",
    "KMEANS_ALGORITHM = 'lloyd'\n",
    "# KMEANS_ALGORITHM = 'lloyd'. K-means algorithm to use. The classical EM-style algorithm is \"lloyd\". \n",
    "# The \"elkan\" variation can be more efficient on some datasets with well-defined clusters, by using \n",
    "# the triangle inequality. However it’s more memory intensive due to the allocation of an extra \n",
    "# array of shape (n_samples, n_clusters).\n",
    "# Options: “lloyd”, “elkan”, “auto”, “full”. Manipulates the parameter algorithm from KMeans \n",
    "# Sklearn class.\n",
    "TOLERANCE = 0.0001\n",
    "# TOLERANCE = 0.0001. Relative tolerance with regards to Frobenius norm of the difference in the \n",
    "# cluster centers of two consecutive iterations to declare convergence. \n",
    "# Manipulates the parameter tol from KMeans Sklearn class.\n",
    "\n",
    "\n",
    "# The model object that may be used for predicting the clusters for new data was\n",
    "# returned as kmeans_model. The labels (clusters) correspondent to each entry in the input tensor \n",
    "# were returned as X_labels, and the model's centroids were returned as centroids.\n",
    "# Simply modify these objects on the left of equality:\n",
    "kmeans_model, X_labels, centroids = kmeans_clustering (X_tensor = X_TENSOR, number_of_clusters = NUMBER_OF_CLUSTERS, number_of_initializations = NUMBER_OF_INITIALIZATIONS, maximum_of_allowed_iterations = MAXIMUM_OF_ALLOWED_ITERATIONS, kmeans_algorithm = KMEANS_ALGORITHM, tolerance = TOLERANCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c2452032-84b3-4cab-99f6-fc3f944917d1"
   },
   "source": [
    "### **Anomaly Detection**\n",
    "- Unsupervised learning method for detecting anomaly points, based on the assumption that each variable is normal and independent.\n",
    "- Data points considered anomalous are labelled as 1; non-anomalous are labelled as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TENSOR = X\n",
    "# X_TENSOR = subset of predictive variables (dataframe). Since this is a non-supervised algorithm, \n",
    "# you may pass the full dataset.\n",
    "\n",
    "DEFINED_THRESHOLD = 0.00001\n",
    "# DEFINED_THRESHOLD = 0.00001 - represents the minimum value of the combined probabilities for a \n",
    "# given value to be considered anomalous. If no test/validation set is provided, the threshold must \n",
    "# be defined by the user.\n",
    "# If test/validation sets are provided, the best threshold will be calculated from it.\n",
    "# Notice that the response set is an array containing the label 1 for anomalous (and validated) points,\n",
    "# and 0 for non-anomalous ones.\n",
    "\n",
    "# Tensors of data separated for model testing:\n",
    "X_TEST = split_dictionary['X_test']\n",
    "Y_TEST = split_dictionary['y_test']\n",
    "#X_TEST = split_dictionary['X_test']\n",
    "#Y_TEST = split_dictionary['y_test']\n",
    "\n",
    "\n",
    "# The model object that may be used for evaluating if new data is anomalous was\n",
    "# returned as anomaly_detection_model. The labels correspondent to each entry in the input tensor \n",
    "# were returned as outliers. In this array, elements marked as anomalous are labelled as 1; whereas\n",
    "# non-anomalous are labelled as 0.\n",
    "# Simply modify these objects on the left of equality:\n",
    "anomaly_detection_model, outliers = anomaly_detection (X_tensor = X_TENSOR, defined_threshold = DEFINED_THRESHOLD, X_test = X_TEST, y_test = Y_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "0295f638-de93-4034-b7d5-42f6e69f40f9"
   },
   "source": [
    "### **Making predictions with the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OBJECT = model # Alternatively: object storing another model\n",
    "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
    "# MODEL_OBJECT = kmeans_model\n",
    "\n",
    "X_tensor = X\n",
    "# predict_for = 'subset' or predict_for = 'single_entry'\n",
    "# The function will automatically detect if it is dealing with lists, NumPy arrays\n",
    "# or Pandas dataframes. If X_df is a list or a single-dimension array, predict_for\n",
    "# will be set as 'single_entry'. If X is a multi-dimension NumPy array (as the\n",
    "# outputs for preparing data - even single_entry - for deep learning models), or if\n",
    "# it is a Pandas dataframe, the function will set predict_for = 'subset'\n",
    "    \n",
    "# X_df = subset of predictive variables (dataframe, NumPy array, or list).\n",
    "# If PREDICT_FOR = 'single_entry', X_df should be a list of parameters values.\n",
    "# e.g. X_df = [1.2, 3, 4] (dot is the decimal case separator, comma separate values). \n",
    "# Notice that the list should contain only the numeric values, in the same order of the\n",
    "# correspondent columns.\n",
    "# If PREDICT_FOR = 'subset' (prediction for multiple entries), X_df should be a dataframe \n",
    "# (subset) or a multi-dimensional NumPy array of the parameters values, as usual.\n",
    "\n",
    "DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset  \n",
    "# DATAFRAME_FOR_CONCATENATING_PREDICTIONS: if you want to concatenate the predictions\n",
    "# to a dataframe, pass it here:\n",
    "# e.g. DATAFRAME_FOR_CONCATENATING_PREDICTIONS = df\n",
    "# If the dataframe must be the same one passed as X, repeat the dataframe object here:\n",
    "# X_df = dataset, DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset.\n",
    "# Alternatively, if DATAFRAME_FOR_CONCATENATING_PREDICTIONS = None, \n",
    "# the prediction will be returned as a series or NumPy array, depending on the input format.\n",
    "# Notice that the concatenated predictions will be added as a new column.\n",
    "\n",
    "COLUMN_WITH_PREDICTIONS_SUFFIX = None\n",
    "# COLUMN_WITH_PREDICTIONS_SUFFIX = None. If the predictions are added as a new column\n",
    "# of the dataframe DATAFRAME_FOR_CONCATENATING_PREDICTIONS, you can declare this\n",
    "# parameter as string with a suffix for identifying the model. If no suffix is added, the new\n",
    "# column will be named 'y_pred'.\n",
    "# e.g. COLUMN_WITH_PREDICTIONS_SUFFIX = '_keras' will create a column named \"y_pred_keras\". This\n",
    "# parameter is useful when working with multiple models. Always start the suffix with underscore\n",
    "# \"_\" so that no blank spaces are added; the suffix will not be merged to the column; and there\n",
    "# will be no confusion with the dot (.) notation for methods, JSON attributes, etc.\n",
    "\n",
    "FUNCTION_USED_FOR_FITTING_DL_MODEL = 'get_deep_learning_tf_model'\n",
    "# FUNCTION_USED_FOR_FITTING_DL_MODEL: the function you used for obtaining the deep learning model.\n",
    "# Example: 'get_deep_learning_tf_model' or 'get_siamese_networks_model'\n",
    "\n",
    "ARCHITECTURE = None\n",
    "# ARCHITECTURE: some models require inputs in a proper format. Declare here if you are using\n",
    "# one of these architectures. Example: ARCHITECTURE = 'cnn_lstm' from class tf_models require\n",
    "# a special reshape before getting predictions. You can keep None or put the name of the\n",
    "# architecture, if no special reshape is needed.\n",
    "\n",
    "### ATTENTION: ALL MODELS WITH LSTM, CNN or OTHER SPECIAL LAYERS REQUIRE THIS ARGUMENT TO BE\n",
    "# DECLARED\n",
    "\n",
    "LIST_OF_RESPONSES = RESPONSE_COLUMNS\n",
    "# You may declare the list RESPONSE_COLUMNS previously used for separating into features and responses tensors.\n",
    "# LIST_OF_RESPONSES = []. This parameter is obbligatory for multi-response models, such as the ones obtained from\n",
    "# function 'get_siamese_networks_model'. It must contain a list with the same order of the output responses.\n",
    "# Example: suppose your siamese model outputs 4 responses: 'temperature', 'pressure', 'flow_rate', and 'ph', in\n",
    "# this order. The list of responses must be declared as: \n",
    "# LIST_OF_RESPONSES = ['temperature', 'pressure', 'flow_rate', 'ph']\n",
    "# tuples and numpy arrays are also acceptable: LIST_OF_RESPONSES = ('temperature', 'pressure', 'flow_rate', 'ph')\n",
    "# Attention: the number of responses must be exactly the number of elements in list_of_responses, or an error will\n",
    "# be raised.\n",
    "\n",
    "\n",
    "# Predictions returned as prediction_output\n",
    "# Simply modify this object (or variable) on the left of equality:\n",
    "prediction_output = make_model_predictions (model_object = MODEL_OBJECT, X = X_tensor, dataframe_for_concatenating_predictions = DATAFRAME_FOR_CONCATENATING_PREDICTIONS, column_with_predictions_suffix = COLUMN_WITH_PREDICTIONS_SUFFIX, function_used_for_fitting_dl_model = FUNCTION_USED_FOR_FITTING_DL_MODEL, architecture = ARCHITECTURE, list_of_responses = LIST_OF_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Performing the SHAP feature importance analysis**\n",
    "- SHAP was developed by a mathematician from Washington University.\n",
    "- It combines the obtained machine learning model with Game Theory algorithms to analyze the relative importance of each variable, as well as the **interactions between variables**.\n",
    "- SHAP returns us a SHAP value that represents the relative importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_OBJECT = kmeans_model # Alternatively: object storing another model\n",
    "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
    "# MODEL_OBJECT = xgb_model\n",
    "\n",
    "X_TRAIN = split_dictionary['X_train']\n",
    "# X_TRAIN = subset of predictive variables (dataframe).\n",
    "# Alternatively, modify X_train, not X_TRAIN\n",
    "\n",
    "MODEL_TYPE = 'general'\n",
    "# MODEL_TYPE = 'general' for the general case, including artificial neural networks.\n",
    "# MODEL_TYPE = 'linear' for Sklearn linear models (OLS, Ridge, Lasso, ElasticNet,\n",
    "# Logistic Regression).\n",
    "# MODEL_TYPE = 'tree' for tree-based models (Random Forest and XGBoost).\n",
    "# MODEL_TYPE = 'deep' for Deep Learning TensorFlow model.\n",
    "# Actually, any string different from 'linear', 'tree', or 'deep' (including blank string)\n",
    "# will apply the general case.\n",
    "\n",
    "TOTAL_OF_SHAP_POINTS = 40\n",
    "# TOTAL_OF_SHAP_POINTS (integer): number of points from the \n",
    "# subset X_train that will be randomly selected for the SHAP \n",
    "# analysis. If the kernel is taking too long, reduce this value.\n",
    "\n",
    "\n",
    "# Dictionary containing calculated metrics returned as shap_dict;\n",
    "# Simply modify this object on the left of equality:\n",
    "shap_dict = shap_feature_analysis (model_object = MODEL_OBJECT, X_train = X_TRAIN, model_type = MODEL_TYPE, total_of_shap_points = TOTAL_OF_SHAP_POINTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "df90ed9f-4bbd-4a4f-9986-e706702217d9",
    "id": "zHUhoX1XyHHm"
   },
   "source": [
    "### **Importing or exporting models and dictionaries (or lists)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "c83aac50-c444-4de1-a851-7c98b91ee1ce"
   },
   "source": [
    "#### Case 1: import only a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "19ceeae7-121c-4e55-9d41-3e498b2d57ca",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'sklearn'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
    "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model.\n",
    "# Simply modify this object on the left of equality:\n",
    "model = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "98416876-845b-4632-8ed8-ec685b13abee"
   },
   "source": [
    "#### Case 2: import only a dictionary or a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "ed0579c2-2dfc-4856-84b0-d2c6cd549efb",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'dict_or_list_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'sklearn'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
    "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Dictionary or list saved as imported_dict_or_list.\n",
    "# Simply modify this object on the left of equality:\n",
    "imported_dict_or_list = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "6a461900-d684-48e0-b09e-9bed23fb445d"
   },
   "source": [
    "#### Case 3: import a model and a dictionary (or a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "68be2dfa-cb28-4134-a5f5-b325a1b29595",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'import'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_and_dict'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'sklearn'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
    "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "# Model object saved as model. Dictionary or list saved as imported_dict_or_list.\n",
    "# Simply modify these objects on the left of equality:\n",
    "model, imported_dict_or_list = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "4d0c9875-ec07-40dd-bbdd-63fb620c22e1"
   },
   "source": [
    "#### Case 4: export a model and/or a dictionary (or a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "290ed3d6-656b-4ada-a931-55ee6ca8b5b1",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "ACTION = 'export'\n",
    "# ACTION = 'import' for importing a model and/or a dictionary;\n",
    "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
    "\n",
    "OBJECTS_MANIPULATED = 'model_only'\n",
    "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
    "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will \n",
    "#  be manipulated.\n",
    "\n",
    "MODEL_FILE_NAME = None\n",
    "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. MODEL_FILE_NAME = 'model'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
    "\n",
    "DICTIONARY_OR_LIST_FILE_NAME = None\n",
    "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary \n",
    "# (for 'import');\n",
    "# or of the name that the exported file will have (for 'export')\n",
    "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
    "# WARNING: Do not add the file extension.\n",
    "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary \n",
    "# or list will be manipulated.\n",
    "\n",
    "DIRECTORY_PATH = ''\n",
    "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
    "# or from which the model will be retrieved. If no value is provided,\n",
    "# the DIRECTORY_PATH will be the root: \"\"\n",
    "# Notice that the model and the dictionary must be stored in the same path.\n",
    "# If a model and a dictionary will be exported, they will be stored in the same\n",
    "# DIRECTORY_PATH.\n",
    "    \n",
    "MODEL_TYPE = 'sklearn'\n",
    "# This parameter has effect only when a model will be manipulated.\n",
    "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .h5\n",
    "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing \n",
    "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
    "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
    "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
    "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
    "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
    "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
    "\n",
    "DICT_OR_LIST_TO_EXPORT = None\n",
    "MODEL_TO_EXPORT = None \n",
    "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
    "# must be declared. If ACTION == 'export', keep:\n",
    "# DICT_OR_LIST_TO_EXPORT = None, \n",
    "# MODEL_TO_EXPORT = None\n",
    "# If one of these objects will be exported, substitute None by the name of the object\n",
    "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
    "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
    "# it is not a string, but an object.\n",
    "# For exporting a dictionary named as 'dict':\n",
    "# DICT_OR_LIST_TO_EXPORT = dict\n",
    "\n",
    "USE_COLAB_MEMORY = False\n",
    "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
    "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
    "# from Google Colaboratory: you will update or download the file and it will be available\n",
    "# only during the time when the kernel is running. It will be excluded when the kernel\n",
    "# dies, for instance, when you close the notebook.\n",
    "    \n",
    "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
    "# to your computer (running the cell will start the download).\n",
    "\n",
    "import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "717a95cc-68f7-478f-acc8-295187e25295"
   },
   "source": [
    "## **Exporting the dataframe as CSV file (to notebook's workspace)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "azdata_cell_guid": "6bf2552b-6799-4c5f-89a7-0b626bacedc6",
    "language": "python"
   },
   "outputs": [],
   "source": [
    "## WARNING: all files exported from this function are .csv (comma separated values)\n",
    "\n",
    "DATAFRAME_OBJ_TO_BE_EXPORTED = dataset\n",
    "# Alternatively: object containing the dataset to be exported.\n",
    "# DATAFRAME_OBJ_TO_BE_EXPORTED: dataframe object that is going to be exported from the\n",
    "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "# example: DATAFRAME_OBJ_TO_BE_EXPORTED = dataset will export the dataset object.\n",
    "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "NEW_FILE_NAME_WITHOUT_EXTENSION = \"dataset\"\n",
    "# NEW_FILE_NAME_WITHOUT_EXTENSION - (string, in quotes): input the name of the \n",
    "# file without the extension. e.g. set NEW_FILE_NAME_WITHOUT_EXTENSION = \"my_file\" \n",
    "# to export the CSV file 'my_file.csv' to notebook's workspace.\n",
    "\n",
    "export_pd_dataframe_as_csv (dataframe_obj_to_be_exported = DATAFRAME_OBJ_TO_BE_EXPORTED, new_file_name_without_extension = NEW_FILE_NAME_WITHOUT_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exporting dataframes as Excel file tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WARNING: all files exported from this function are .xlsx\n",
    "\n",
    "FILE_NAME_WITHOUT_EXTENSION = \"datasets\"\n",
    "# (string, in quotes): input the name of the \n",
    "# file without the extension. e.g. new_file_name_without_extension = \"my_file\" \n",
    "# will export a file 'my_file.xlsx' to notebook's workspace.\n",
    "\n",
    "EXPORTED_TABLES = [{'dataframe_obj_to_be_exported': None, \n",
    "                    'excel_sheet_name': None},]\n",
    "\n",
    "# exported_tables is a list of dictionaries. User may declare several dictionaries, \n",
    "# as long as the keys are always the same, and if the values stored in keys are not None.\n",
    "      \n",
    "# key 'dataframe_obj_to_be_exported': dataframe object that is going to be exported from the\n",
    "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
    "# example: dataframe_obj_to_be_exported = dataset will export the dataset object.\n",
    "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
    "\n",
    "# key 'excel_sheet_name': string containing the name of the sheet to be written on the\n",
    "# exported Excel file. Example: excel_sheet_name = 'tab_1' will save the dataframe in the\n",
    "# sheet 'tab_1' from the file named as file_name_without_extension.\n",
    "\n",
    "# examples: exported_tables = [{'dataframe_obj_to_be_exported': dataset1, \n",
    "# 'excel_sheet_name': 'sheet1'},]\n",
    "# will export only dataset1 as 'sheet1';\n",
    "# exported_tables = [{'dataframe_obj_to_be_exported': dataset1, 'excel_sheet_name': 'sheet1'},\n",
    "# {'dataframe_obj_to_be_exported': dataset2, 'excel_sheet_name': 'sheet2']\n",
    "# will export dataset1 as 'sheet1' and dataset2 as 'sheet2'.\n",
    "\n",
    "# Notice that if the file does not contain the exported sheets, they will be created. If it has,\n",
    "# the sheets will be replaced.\n",
    "\n",
    "FILE_DIRECTORY_PATH = \"\"\n",
    "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory \n",
    "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\" \n",
    "# or FILE_DIRECTORY_PATH = \"folder\"\n",
    "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
    "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
    "\n",
    "\n",
    "export_pd_dataframe_as_excel (file_name_without_extension = FILE_NAME_WITHOUT_EXTENSION, exported_tables = EXPORTED_TABLES, file_directory_path = FILE_DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1"
   },
   "source": [
    "****"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
