{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "instance_type": "ml.t3.medium",
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep Learning Models for Time Series Analysis - Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs)**"
      ],
      "metadata": {
        "azdata_cell_guid": "ac0ea76d-6884-4ddd-b392-0ff217565838",
        "id": "XDYhGk-oegE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## _Machine Learning Modelling Workflow Notebook 4_"
      ],
      "metadata": {
        "azdata_cell_guid": "72bcaabc-dba2-4b57-8337-5e27603d1209",
        "id": "9mfpSby0egFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content:\n",
        "1. Converting the datasets into NumPy arrays with correct format for CNN and RNN Architectures;\n",
        "2. Splitting the dataframe into train and test subsets;\n",
        "3. Retrieving the list of classes used for training the classification models;\n",
        "4. Convolutional Neural Network (CNN) Architecture;\n",
        "5. Simplified Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN) Architecture;\n",
        "6. Encoder-Decoder Recurrent Neural Network (RNN) Architecture;\n",
        "7. CNN-LSTM Hybrid Architecture;\n",
        "8. Calculating metrics for regression models;\n",
        "9. Calculating metrics for classification models;\n",
        "10. Making predictions with the models;\n",
        "11. Calculating probabilities associated to each class;\n",
        "12. Time series visualization."
      ],
      "metadata": {
        "azdata_cell_guid": "9a67e617-ab40-4d9c-b6ad-7191df57e893",
        "id": "QHVobIiJegFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Marco Cesar Prado Soares, Data Scientist Specialist - Bayer Crop Science LATAM\n",
        "- marcosoares.feq@gmail.com\n",
        "- marco.soares@bayer.com"
      ],
      "metadata": {
        "azdata_cell_guid": "c66b1173-e6ca-4990-bc83-3c20a9318267",
        "id": "R_KFCqJ2egFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Python Libraries in Global Context**"
      ],
      "metadata": {
        "azdata_cell_guid": "8dd5c4f4-f98c-457d-a4a9-42647d13fbea",
        "id": "6xDuw5SsegFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import load\n",
        "from idsw import *"
      ],
      "metadata": {
        "azdata_cell_guid": "c1851e1e-33cb-45ad-8388-a51b28d1918f",
        "id": "bzZgOvXCyHHl",
        "language": "python",
        "tags": [
          "CELL_4"
        ]
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Call the functions**"
      ],
      "metadata": {
        "azdata_cell_guid": "e698beba-7156-4b70-a430-9e0e28bedf1b",
        "id": "zHUhoX1XyHHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing the dataset**"
      ],
      "metadata": {
        "azdata_cell_guid": "bdb61cdf-152f-4a1f-a90c-5f51cd2f7342",
        "id": "4vS5BfpLegFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## WARNING: Use this function to load dataframes stored on Excel (xls, xlsx, xlsm, xlsb, odf, ods and odt),\n",
        "## JSON, txt, or CSV (comma separated values) files. Tables in webpages or html files can also be read.\n",
        "\n",
        "FILE_DIRECTORY_PATH = \"\"\n",
        "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\"\n",
        "# or FILE_DIRECTORY_PATH = \"folder\"\n",
        "\n",
        "FILE_NAME_WITH_EXTENSION = \"dataset.csv\"\n",
        "# FILE_NAME_WITH_EXTENSION - (string, in quotes): input the name of the file with the\n",
        "# extension. e.g. FILE_NAME_WITH_EXTENSION = \"file.xlsx\", or,\n",
        "# FILE_NAME_WITH_EXTENSION = \"file.csv\", \"file.txt\", or \"file.json\"\n",
        "# Again, the extensions may be: xls, xlsx, xlsm, xlsb, odf, ods, odt, json, txt or csv.\n",
        "# Also, html files and webpages may be also read.\n",
        "\n",
        "# You may input the path for an HTML file containing a table to be read; or\n",
        "# a string containing the address for a webpage containing the table. The address must start\n",
        "# with www or htpp. If a website is input, the full address can be input as FILE_DIRECTORY_PATH\n",
        "# or as FILE_NAME_WITH_EXTENSION.\n",
        "\n",
        "LOAD_TXT_FILE_WITH_JSON_FORMAT = False\n",
        "# LOAD_TXT_FILE_WITH_JSON_FORMAT = False. Set LOAD_TXT_FILE_WITH_JSON_FORMAT = True\n",
        "# if you want to read a file with txt extension containing a text formatted as JSON\n",
        "# (but not saved as JSON).\n",
        "# WARNING: if LOAD_TXT_FILE_WITH_JSON_FORMAT = True, all the JSON file parameters of the\n",
        "# function (below) must be set. If not, an error message will be raised.\n",
        "\n",
        "HOW_MISSING_VALUES_ARE_REGISTERED = None\n",
        "# HOW_MISSING_VALUES_ARE_REGISTERED = None: keep it None if missing values are registered as None,\n",
        "# empty or np.nan. Pandas automatically converts None to NumPy np.nan objects (floats).\n",
        "# This parameter manipulates the argument na_values (default: None) from Pandas functions.\n",
        "# By default the following values are interpreted as NaN: ‘’, ‘#N/A’, ‘#N/A N/A’, ‘#NA’, ‘-1.#IND’,\n",
        "#‘-1.#QNAN’, ‘-NaN’, ‘-nan’, ‘1.#IND’, ‘1.#QNAN’, ‘<NA>’, ‘N/A’, ‘NA’, ‘NULL’, ‘NaN’,\n",
        "# ‘n/a’, ‘nan’, ‘null’.\n",
        "\n",
        "# If a different denomination is used, indicate it as a string. e.g.\n",
        "# HOW_MISSING_VALUES_ARE_REGISTERED = '.' will convert all strings '.' to missing values;\n",
        "# HOW_MISSING_VALUES_ARE_REGISTERED = 0 will convert zeros to missing values.\n",
        "\n",
        "# If dict passed, specific per-column NA values. For example, if zero is the missing value\n",
        "# only in column 'numeric_col', you can specify the following dictionary:\n",
        "# how_missing_values_are_registered = {'numeric-col': 0}\n",
        "\n",
        "\n",
        "HAS_HEADER = True\n",
        "# HAS_HEADER = True if the the imported table has headers (row with columns names).\n",
        "# Alternatively, HAS_HEADER = False if the dataframe does not have header.\n",
        "\n",
        "DECIMAL_SEPARATOR = '.'\n",
        "# DECIMAL_SEPARATOR = '.' - String. Keep it '.' or None to use the period ('.') as\n",
        "# the decimal separator. Alternatively, specify here the separator.\n",
        "# e.g. DECIMAL_SEPARATOR = ',' will set the comma as the separator.\n",
        "# It manipulates the argument 'decimal' from Pandas functions.\n",
        "\n",
        "TXT_CSV_COL_SEP = \"comma\"\n",
        "# txt_csv_col_sep = \"comma\" - This parameter has effect only when the file is a 'txt'\n",
        "# or 'csv'. It informs how the different columns are separated.\n",
        "# Alternatively, txt_csv_col_sep = \"comma\", or txt_csv_col_sep = \",\"\n",
        "# for columns separated by comma;\n",
        "# txt_csv_col_sep = \"whitespace\", or txt_csv_col_sep = \" \"\n",
        "# for columns separated by simple spaces.\n",
        "# You can also set a specific separator as string. For example:\n",
        "# txt_csv_col_sep = '\\s+'; or txt_csv_col_sep = '\\t' (in this last example, the tabulation\n",
        "# is used as separator for the columns - '\\t' represents the tab character).\n",
        "\n",
        "## Parameters for loading Excel files:\n",
        "\n",
        "LOAD_ALL_SHEETS_AT_ONCE = False\n",
        "# LOAD_ALL_SHEETS_AT_ONCE = False - This parameter has effect only when for Excel files.\n",
        "# If LOAD_ALL_SHEETS_AT_ONCE = True, the function will return a list of dictionaries, each\n",
        "# dictionary containing 2 key-value pairs: the first key will be 'sheet', and its\n",
        "# value will be the name (or number) of the table (sheet). The second key will be 'df',\n",
        "# and its value will be the pandas dataframe object obtained from that sheet.\n",
        "# This argument has preference over SHEET_TO_LOAD. If it is True, all sheets will be loaded.\n",
        "\n",
        "SHEET_TO_LOAD = None\n",
        "# SHEET_TO_LOAD - This parameter has effect only when for Excel files.\n",
        "# keep SHEET_TO_LOAD = None not to specify a sheet of the file, so that the first sheet\n",
        "# will be loaded.\n",
        "# SHEET_TO_LOAD may be an integer or an string (inside quotes). SHEET_TO_LOAD = 0\n",
        "# loads the first sheet (sheet with index 0); SHEET_TO_LOAD = 1 loads the second sheet\n",
        "# of the file (index 1); SHEET_TO_LOAD = \"Sheet1\" loads a sheet named as \"Sheet1\".\n",
        "# Declare a number to load the sheet with that index, starting from 0; or declare a\n",
        "# name to load the sheet with that name.\n",
        "\n",
        "## Parameters for loading JSON files:\n",
        "\n",
        "JSON_RECORD_PATH = None\n",
        "# JSON_RECORD_PATH (string): manipulate parameter 'record_path' from json_normalize method.\n",
        "# Path in each object to list of records. If not passed, data will be assumed to\n",
        "# be an array of records. If a given field from the JSON stores a nested JSON (or a nested\n",
        "# dictionary) declare it here to decompose the content of the nested data. e.g. if the field\n",
        "# 'books' stores a nested JSON, declare, JSON_RECORD_PATH = 'books'\n",
        "\n",
        "JSON_FIELD_SEPARATOR = \"_\"\n",
        "# JSON_FIELD_SEPARATOR = \"_\" (string). Manipulates the parameter 'sep' from json_normalize method.\n",
        "# Nested records will generate names separated by sep.\n",
        "# e.g., for JSON_FIELD_SEPARATOR = \".\", {‘foo’: {‘bar’: 0}} -> foo.bar.\n",
        "# Then, if a given field 'main_field' stores a nested JSON with fields 'field1', 'field2', ...\n",
        "# the name of the columns of the dataframe will be formed by concatenating 'main_field', the\n",
        "# separator, and the names of the nested fields: 'main_field_field1', 'main_field_field2',...\n",
        "\n",
        "JSON_METADATA_PREFIX_LIST = None\n",
        "# JSON_METADATA_PREFIX_LIST: list of strings (in quotes). Manipulates the parameter\n",
        "# 'meta' from json_normalize method. Fields to use as metadata for each record in resulting\n",
        "# table. Declare here the non-nested fields, i.e., the fields in the principal JSON. They\n",
        "# will be repeated in the rows of the dataframe to give the metadata (context) of the rows.\n",
        "\n",
        "# e.g. Suppose a JSON with the following structure: {'name': 'Mary', 'last': 'Shelley',\n",
        "# 'books': [{'title': 'Frankestein', 'year': 1818}, {'title': 'Mathilda ', 'year': 1819},{'title': 'The Last Man', 'year': 1826}]},\n",
        "# Here, there are nested JSONs in the field 'books'. The fields that are not nested\n",
        "# are 'name' and 'last'.\n",
        "# Then, JSON_RECORD_PATH = 'books'\n",
        "# JSON_METADATA_PREFIX_LIST = ['name', 'last']\n",
        "\n",
        "\n",
        "# The dataframe will be stored in the object named 'dataset':\n",
        "# Simply modify this object on the left of equality:\n",
        "dataset = load_pandas_dataframe (file_directory_path = FILE_DIRECTORY_PATH, file_name_with_extension = FILE_NAME_WITH_EXTENSION, load_txt_file_with_json_format = LOAD_TXT_FILE_WITH_JSON_FORMAT, how_missing_values_are_registered = HOW_MISSING_VALUES_ARE_REGISTERED, has_header = HAS_HEADER, decimal_separator = DECIMAL_SEPARATOR, txt_csv_col_sep = TXT_CSV_COL_SEP, load_all_sheets_at_once = LOAD_ALL_SHEETS_AT_ONCE, sheet_to_load = SHEET_TO_LOAD, json_record_path = JSON_RECORD_PATH, json_field_separator = JSON_FIELD_SEPARATOR, json_metadata_prefix_list = JSON_METADATA_PREFIX_LIST)\n",
        "\n",
        "# OBS: If an Excel file is loaded and LOAD_ALL_SHEETS_AT_ONCE = True, then the object\n",
        "# dataset will be a list of dictionaries, with 'sheet' as key containing the sheet name; and 'df'\n",
        "# as key correspondent to the Pandas dataframe. So, to access the 3rd dataframe (index 2, since\n",
        "# indexing starts from zero): df = dataframe[2]['df'], where dataframe is the list returned."
      ],
      "metadata": {
        "azdata_cell_guid": "bfd159e7-bf8e-4426-8bbd-2833d1ab56b7",
        "language": "python",
        "id": "LhNi_ynfegFJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Separating and preparing features and responses tensors**"
      ],
      "metadata": {
        "azdata_cell_guid": "1c5ccf83-f6ae-4cb8-9543-732dc880886a",
        "id": "OC0We7gnegFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = dataset  #Alternatively: object containing the dataset to be analyzed\n",
        "\n",
        "FEATURES_COLUMNS = ['col1', 'col2']\n",
        "# FEATURES_COLUMNS: list of strings or string containing the names of columns\n",
        "# with predictive variables in the original dataframe.\n",
        "# Example: FEATURES_COLUMNS = ['col1', 'col2']; FEATURES_COLUMNS = 'predictor';\n",
        "# FEATURES_COLUMNS = ['predictor'].\n",
        "\n",
        "RESPONSE_COLUMNS = \"response\"\n",
        "# RESPONSE_COLUMNS: list of strings or string containing the names of columns\n",
        "# with response variables in the original dataframe.\n",
        "# Example: RESPONSE_COLUMNS= ['col3', 'col4']; RESPONSE_COLUMNS = 'response';\n",
        "# RESPONSE_COLUMNS = ['response']\n",
        "\n",
        "# Arrays or tensors containing features and responses returned as X and y, respectively.\n",
        "# Mapping dictionary correlating the position in array or tensor to the original column name\n",
        "# returned as column_map_dict.\n",
        "# Simply modify these objects on the left of equality:\n",
        "X, y, column_map_dict = separate_and_prepare_features_and_responses (df = DATASET, features_columns = FEATURES_COLUMNS, response_columns = RESPONSE_COLUMNS)"
      ],
      "metadata": {
        "azdata_cell_guid": "c07c2e25-d68f-4c63-bfb9-8b8c205b6afc",
        "language": "python",
        "id": "bJnb-c2HegFL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Converting a whole dataframe or array-like object to tensor**"
      ],
      "metadata": {
        "azdata_cell_guid": "e5bdf3a3-bb5a-4fc2-9aeb-e479b3002446",
        "id": "IEsczgyWegFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_OR_ARRAY_TO_CONVERT = dataset\n",
        "# Alternatively: object containing the dataset or array-like object to be converted and reshaped.\n",
        "\n",
        "COLUMNS_TO_CONVERT = None\n",
        "# ATTENTION: This argument only works for Pandas dataframes.\n",
        "# COLUMNS_TO_CONVERT: list of strings or string containing the names of columns\n",
        "# that you want to convert. Use this if you want to convert only a subset of the dataframe.\n",
        "# Example: COLUMNS_TO_CONVERT = ['col1', 'col2']; COLUMNS_TO_CONVERT = 'predictor';\n",
        "# COLUMNS_TO_CONVERT = ['predictor'] will create a tensor with only the specified columns;\n",
        "# If None, the whole dataframe will be converted.\n",
        "\n",
        "COLUMNS_TO_EXCLUDE = None\n",
        "# ATTENTION: This argument only works for Pandas dataframes.\n",
        "# COLUMNS_TO_EXCLUDE: Alternative parameter.\n",
        "# list of strings or string containing the names of columns that you want to exclude from the\n",
        "# returned tensor. Use this if you want to convert only a subset of the dataframe.\n",
        "# Example: COLUMNS_TO_EXCLUDE = ['col1', 'col2']; COLUMNS_TO_EXCLUDE = 'predictor';\n",
        "# COLUMNS_TO_EXCLUDE = ['predictor'] will create a tensor with all columns from the dataframe\n",
        "# except the specified ones. This argument will only be used if the previous one was not.\n",
        "\n",
        "\n",
        "# Array or tensor returned as X. Mapping dictionary correlating the position in array or tensor\n",
        "# to the original column name returned as column_map_dict.\n",
        "# Simply modify these objects on the left of equality:\n",
        "X, column_map_dict = convert_to_tensor (df_or_array_to_convert = DATASET_OR_ARRAY_TO_CONVERT, columns_to_convert = COLUMNS_TO_CONVERT, columns_to_exclude = COLUMNS_TO_EXCLUDE)"
      ],
      "metadata": {
        "azdata_cell_guid": "b1a29863-1113-4dd6-aa8c-f051cd2cbf8c",
        "language": "python",
        "id": "BDDk6w4DegFN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Splitting features and responses into train and test tensors**"
      ],
      "metadata": {
        "azdata_cell_guid": "2315771d-7707-4e86-93ef-a5e54d1d942b",
        "id": "CHXGYpWsegFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_tensor = X\n",
        "# X_df = tensor or array of predictive variables. Alternatively, modify X, not X_tensor.\n",
        "Y_tensor = y\n",
        "# Y = tensor or array of response variables. Alternatively, modify y, not Y_tensor.\n",
        "\n",
        "PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 75\n",
        "# percent_of_data_used_for_model_training: float from 0 to 100,\n",
        "# representing the percent of data used for training the model\n",
        "\n",
        "PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 0\n",
        "# If you want to use cross-validation, separate a percent of the training data for validation.\n",
        "# Declare this percent as PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION (float from 0 to 100).\n",
        "\n",
        "# Subset and series destined to training, testing and/or validation returned in the dictionary split_dictionary;\n",
        "# Simply modify this object on the left of equality:\n",
        "split_dictionary = split_data_into_train_and_test (X = X_tensor, y = Y_tensor, percent_of_data_used_for_model_training = PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING, percent_of_training_data_used_for_model_validation = PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION)"
      ],
      "metadata": {
        "azdata_cell_guid": "473b82e4-2ead-489c-a115-d620d63032f1",
        "language": "python",
        "id": "23y5YzVGegFN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Splitting time series into train and test tensors**"
      ],
      "metadata": {
        "azdata_cell_guid": "36695991-b0f6-461a-87c5-efb09e0687ec",
        "id": "Z4taqCQdegFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_tensor = X\n",
        "# X_df = tensor or array of predictive variables. Alternatively, modify X, not X_tensor.\n",
        "Y_tensor = y\n",
        "# Y = tensor or array of response variables. Alternatively, modify y, not Y_tensor.\n",
        "\n",
        "PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 75\n",
        "# percent_of_data_used_for_model_training: float from 0 to 100,\n",
        "# representing the percent of data used for training the model\n",
        "\n",
        "PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 0\n",
        "# If you want to use cross-validation, separate a percent of the training data for validation.\n",
        "# Declare this percent as PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION (float from 0 to 100).\n",
        "\n",
        "# Subset and series destined to training, testing and/or validation returned in the dictionary split_dictionary;\n",
        "# Simply modify this object on the left of equality:\n",
        "split_dictionary = time_series_train_test_split (X = X_tensor, y = Y_tensor, percent_of_data_used_for_model_training = PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING, percent_of_training_data_used_for_model_validation = PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION)"
      ],
      "metadata": {
        "azdata_cell_guid": "143d4855-90f2-473b-8bf1-8d1097ac6b25",
        "language": "python",
        "id": "ZGo15PADegFO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creating a TensorFlow windowed dataset from a time series**"
      ],
      "metadata": {
        "azdata_cell_guid": "5ff43655-ac0c-4864-b5af-0a676baf0823",
        "id": "zpTiAwiLegFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y_tensor = y\n",
        "# Y = tensor or array of response variables. Alternatively, modify y, not Y_tensor.\n",
        "\n",
        "WINDOW_SIZE = 20\n",
        "# WINDOW_SIZE (integer): number of rows/ size of the time window used.\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "# BATCH_SIZE (integer): number of rows/ size of the batches used for training.\n",
        "\n",
        "SHUFFLE_BUFFER_SIZE = 100\n",
        "# SHUFFLE_BUFFER_SIZE (integer): number of rows/ size used for shuffling the entries.\n",
        "\n",
        "# TensorFlow Dataset obtained from the time series returned as dataset_from_time_series.\n",
        "# Simply modify this object on the left of equality:\n",
        "dataset_from_time_series = windowed_dataset_from_time_series (y = Y_tensor, window_size = WINDOW_SIZE, batch_size = BATCH_SIZE, shuffle_buffer_size = SHUFFLE_BUFFER_SIZE)"
      ],
      "metadata": {
        "azdata_cell_guid": "b46255d5-5472-45fe-829c-7c9507f4393b",
        "language": "python",
        "id": "NkwdsBV2egFP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Creating a TensorFlow windowed dataset from multiple-feature time series**"
      ],
      "metadata": {
        "azdata_cell_guid": "393c43d2-024e-494a-a6ba-d2f9ec9351a5",
        "id": "uv23Hpi_egFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = dataset\n",
        "# Alternatively: object containing the Pandas dataframe to be converted and reshaped.\n",
        "\n",
        "RESPONSE_COLUMNS = 'response_variable'\n",
        "# RESPONSE_COLUMNS: string or list of strings with the response columns\n",
        "\n",
        "SEQUENCE_STRIDE = 1\n",
        "SAMPLING_RATE = 1\n",
        "SHIFT = 1\n",
        "# SHIFT, SAMPLING_RATE, and SEQUENCE_STRIDE: integers\n",
        "\n",
        "# The time series may be represented as a sequence of times like: t = 0, t = 1, t = 2, ..., t = N.\n",
        "# When preparing the dataset, we pick a given number of 'times' (indexes), and use them for\n",
        "# predicting a time in the future.\n",
        "# So, the INPUT_WIDTH represents how much times will be used for prediction. If INPUT_WIDTH = 6,\n",
        "# we use 6 values for prediction, e.g., t = 0, t = 1, ..., t = 5 will be a prediction window.\n",
        "# In turns, if INPUT_WIDTH = 3, 3 values are used: t = 0, t = 1, t = 2; if INPUT_WIDTH = N, N\n",
        "# consecutive values will be used: t = 0, t = 1, t = 2, ..., t = N. And so on.\n",
        "# LABEL_WIDTH, in turns, represent how much times will be predicted. If LABEL_WIDTH = 1, a single\n",
        "# value will be predicted. If LABEL_WIDTH = 2, two consecutive values are predicted; if LABEL_WIDTH =\n",
        "# N, N consecutive values are predicted; and so on.\n",
        "\n",
        "# SHIFT represents the offset, i.e., given the input values, which value in the time sequence will\n",
        "# be predicted. So, suppose INPUT_WIDTH = 6 and LABEL_WIDTH = 1\n",
        "# If SHIFT = 1, the label, i.e., the predicted value, will be the first after the sequence used for\n",
        "# prediction. So, if  t = 0, t = 1, ..., t = 5 will be a prediction window and t = 6 will be the\n",
        "# predicted value. Notice that the complete window has a total width = 7: t = 0, ..., t = 7.\n",
        "# If LABEL_WIDTH = 2, then t = 6 and t = 7 will be predicted (total width = 8).\n",
        "# Another example: suppose INPUT_WIDTH = 24. So the predicted window is: t = 0, t = 1, ..., t = 23.\n",
        "# If SHIFT = 24, the 24th element after the prediction sequence will be used as label, i.e., will\n",
        "# be predicted. So, t = 24 is the 1st after the sequence, t = 25 is the second, ... t = 47 is the\n",
        "# 24th after. If label_with = 1, then the sequence t = 0, t = 1, ..., t = 23 will be used for\n",
        "# predicting t = 47. Naturally, the total width of the window = 47 in this case.\n",
        "# Also, notice that the label is used by the model as the response (predicted) variable.\n",
        "\n",
        "# So for a given SHIFT: the sequence of timesteps i, i+1, ... will be used for predicting the\n",
        "# timestep i + SHIFT\n",
        "# If a sequence starts in index i, the next sequence will start from i + SEQUENCE_STRIDE.\n",
        "# The sequence will be formed by timesteps i, i + SAMPLING_RATE, i + 2* SAMPLING_RATE, ...\n",
        "# Example: Consider indices [0, 1, ... 99]. With sequence_length=10, SAMPLING_RATE=2,\n",
        "# SEQUENCE_STRIDE=3, the dataset will yield batches of sequences composed of the following indices:\n",
        "# First sequence:  [0  2  4  6  8 10 12 14 16 18]\n",
        "# Second sequence: [3  5  7  9 11 13 15 17 19 21]\n",
        "# Third sequence:  [6  8 10 12 14 16 18 20 22 24]\n",
        "# ...\n",
        "# Last sequence:   [78 80 82 84 86 88 90 92 94 96]\n",
        "\n",
        "USE_PAST_RESPONSES_FOR_PREDICTION = True\n",
        "# USE_PAST_RESPONSES_FOR_PREDICTION: True if the past responses will be used for predicting their\n",
        "# value in the future; False if you do not want to use them.\n",
        "\n",
        "PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 70\n",
        "# percent_of_data_used_for_model_training: float from 0 to 100,\n",
        "# representing the percent of data used for training the model\n",
        "\n",
        "PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 10\n",
        "# If you want to use cross-validation, separate a percent of the training data for validation.\n",
        "# Declare this percent as PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION (float from 0 to 100).\n",
        "\n",
        "# If PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING = 70, and\n",
        "# PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION = 10,\n",
        "# training dataset slice goes from 0 to 0.7 (70%) of the dataset;\n",
        "# testing slicing goes from 0.7 x dataset to ((1 - 0.1) = 0.9) x dataset\n",
        "# validation slicing goes from 0.9 x dataset to the end of the dataset.\n",
        "# Here, consider the time sequence t = 0, t = 1, ... , t = N, for a dataset with length N:\n",
        "# training: from t = 0 to t = (0.7 x N); testing: from t = ((0.7 x N) + 1) to (0.9 x N);\n",
        "# validation: from t = ((0.9 x N) + 1) to N (the fractions 0.7 x N and 0.9 x N are rounded to\n",
        "# the closest integer).\n",
        "\n",
        "\n",
        "# Dictionary with inputs and labels tensors returned as tensors_dict.\n",
        "# Simply modify this object on the left of equality:\n",
        "tensors_dict = multi_columns_time_series_tensors (df = DATASET, response_columns = RESPONSE_COLUMNS, sequence_stride = SEQUENCE_STRIDE, sampling_rate = SAMPLING_RATE, shift = SHIFT, use_past_responses_for_prediction = USE_PAST_RESPONSES_FOR_PREDICTION, percent_of_data_used_for_model_training = PERCENT_OF_DATA_USED_FOR_MODEL_TRAINING, percent_of_training_data_used_for_model_validation = PERCENT_OF_TRAINING_DATA_USED_FOR_MODEL_VALIDATION)"
      ],
      "metadata": {
        "azdata_cell_guid": "406491ea-b30f-417f-8b1c-c507f71925ff",
        "language": "python",
        "id": "s2EnKYtXegFQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Union of several 1-dimensional tensors (obtained from single columns) into a single tensor\n",
        "- Each 1-dimensional tensor or array becomes a column from the new tensor."
      ],
      "metadata": {
        "azdata_cell_guid": "30e5e3d1-1f8e-48ee-b885-ff7d534466c1",
        "id": "aq2QmYGEegFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LIST_OF_TENSORS_OR_ARRAYS = [tensor1, tensor2]\n",
        "# list of tensors: list containing the 1-dimensional tensors or arrays that the function will union.\n",
        "# the operation will be performed in the order that the tensors are declared.\n",
        "# Substitue tensor1, tensor2, tensor3,... by the tensor objects, in the correct sequence.\n",
        "# If the resulting tensor will contain the responses for a multi-response tensor, declare them in the\n",
        "# orders of the responses (tensor 1 corresponding to response 1, tensor 2 to response 2, etc.)\n",
        "\n",
        "# One-dimensional tensors have shape (X,), where X is the number of elements. Example: a column\n",
        "# of the dataframe with elements 1, 2, 3 in this order may result in an array like array([1, 2, 3])\n",
        "# and a Tensor with shape (3,). With we union it with the tensor from the column with elements\n",
        "# 4, 5, 6, the output will be array([[1,4], [2,5], [3,6]]). Alternatively, this new array could\n",
        "# be converted into a Pandas dataframe where each column would be correspondent to one individual\n",
        "# tensor.\n",
        "\n",
        "# Tensor resulting from the union of multiple single-dimension tensor returned as tensors_union.\n",
        "# Simply modify this object on the left of equality:\n",
        "tensors_union = union_1_dim_tensors (list_of_tensors_or_arrays = LIST_OF_TENSORS_OR_ARRAYS)"
      ],
      "metadata": {
        "azdata_cell_guid": "d35dfde7-8d3e-43c6-989e-6799d16e9870",
        "language": "python",
        "id": "lUQ1XFziegFS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architectures from tf_models class - Function for fitting a given architecture\n",
        "\n",
        "#### **1. Keras Dense Artificial Neural Network (ANN)**\n",
        "\n",
        "#### **2. Convolutional Neural Network (CNN) Architecture**\n",
        "- CNN applies a filter (kernel) destined to highlight important features of the data.\n",
        "    - In image classification, it is translated as the features most important to characterize a given object.\n",
        "    - In text processing, it is translated as the group of characteristics that make a text recieve a given classification (e.g. classification of comments as toxic).\n",
        "    - In sound processing (a special case of the text processing) it can be both a filter applied to the sound or a classifier of the message translated from the sound.\n",
        "    - In time series, it is a 1-dimensional filter as in text (and sound) processing. The filter will highlight important characteristics of the time series, like seasonality. So it can extract time characteristics in a manner different from the one performed by the RNNs.\n",
        "\n",
        "#### **3. Simplified Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN) Architecture**\n",
        "- The LSTM uses a combination of hidden and cell states to capture memory (time) effects.\n",
        "- The LSTM performs one cycle (iteration) for element of the sequence.\n",
        "    - In text processing, the sequence is the sentence, and each element is a token.\n",
        "    - In time series analysis, the sequence is a single entry (i.e. each array input into the model). The sequence length is the array's length: it is the number of predictive variables (columns) of the dataset (once each column is translated as one element of each array).\n",
        "    - Since the RNN may be viewed as a series of neural networks with total of networks equals to the number of cycles (or iterations), then this is equivalent to concatenating a number of neural networks equals to the total of predictors (e.g. if there are 3 predictors, then the LSTM performs 3 iterations, one for each column).\n",
        "\n",
        "#### **4. Encoder-Decoder Recurrent Neural Network (RNN) Architecture**\n",
        "- The Encoder-Decoder is a more complex RNN architecture that uses a combination of LSTMs.\n",
        "- This architecture is used for language translation and for programming simple chatbots: it is capable of memorizing more characteristics of the time series and generate sequences of length different from the original sequence's size.\n",
        "- So, it is a powerful architecture for capturing time effects. Basically:\n",
        "    - The encoder reads the original sequence and converts it into a simplified (coded) information;\n",
        "    - This information is passed to the decoder, which decodifies it and combines it to the information from its own previous cycles to obtain the outputs.\n",
        "    - The hidden states h from the Encoder are passed as inputs for the Decoder. Then, the parameter `return_sequences` is set as True, resulting in a bidimensional output (one dimension for the output, other for the hidden state h).\n",
        "\n",
        "#### **5. CNN-LSTM Hybrid Architecture**\n",
        "- The benefit of this model is that the model can support very long input sequences that can be read as blocks or subsequences by the CNN model, then pieced together by the LSTM model.\n",
        "     - This architecture can only be applied to sequences with even number of elements (or an even total number of columns): it uses two consecutive sequence elements for prediction."
      ],
      "metadata": {
        "azdata_cell_guid": "30e5e3d1-1f8e-48ee-b885-ff7d534466c1",
        "id": "kMMWD-Y7egFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split_dictionary.keys() == dict_keys(['X_train', 'y_train', 'X_test', 'y_test', 'X_valid', 'y_valid'])\n",
        "\n",
        "X_TRAIN = split_dictionary['X_train']\n",
        "# X_TRAIN = tensor of predictive variables.\n",
        "# Alternatively, modify X_train, not X_TRAIN\n",
        "Y_TRAIN = split_dictionary['y_train']\n",
        "# Y_TRAIN = tensor of response variables.\n",
        "# Alternatively, modify y_train, not Y_TRAIN\n",
        "\n",
        "\n",
        "ARCHITECTURE = 'simple_dense'\n",
        "# ARCHITECTURE = 'simple_dense': tf_simple_dense model from class tf_models;\n",
        "# ARCHITECTURE = 'double_dense';\n",
        "# ARCHITECTURE = 'cnn': tf_cnn time series model from class tf_models;\n",
        "# ARCHITECTURE = 'lstm': tf_lstm time series model from class tf_models;\n",
        "# ARCHITECTURE = 'encoder_decoder': encoder-decoder time series model from class tf_models;\n",
        "# ARCHITECTURE = 'cnn_lstm': hybrid cnn-lstm time series model from class tf_models.\n",
        "\n",
        "OPTIMIZER = None\n",
        "# OPTIMIZER: tf.keras.optimizers.Optimizer object:\n",
        "# keep it None to use the Adam optimizer.\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer\n",
        "# use the object to set parameters such as learning rate and selection of the optimizer\n",
        "\n",
        "# Tensors of data separated for model testing:\n",
        "X_TEST = split_dictionary['X_test']\n",
        "Y_TEST = split_dictionary['y_test']\n",
        "#X_TEST = split_dictionary['X_test']\n",
        "#Y_TEST = split_dictionary['y_test']\n",
        "\n",
        "# Tensors of data separated for model validation:\n",
        "X_VALID = split_dictionary['X_valid']\n",
        "Y_VALID = split_dictionary['y_valid']\n",
        "#X_VALID = split_dictionary['X_valid']\n",
        "#Y_VALID = split_dictionary['y_valid']\n",
        "\n",
        "TYPE_OF_PROBLEM = \"regression\"\n",
        "# TYPE_OF_PROBLEM = 'regression'; or TYPE_OF_PROBLEM = 'classification'\n",
        "# The default is 'regression', which will be used if no type is\n",
        "# provided.\n",
        "\n",
        "SIZE_OF_TRAINING_BATCH = 200\n",
        "# SIZE_OF_TRAINING_BATCH (integer): amount of data used on each training cycle (epoch).\n",
        "# If we had 20000 data and SIZE_OF_TRAINING_BATCH = 200, then there would be 100\n",
        "# batches (cycles) using 200 data. Training is more efficient when dividing the data into\n",
        "# smaller subsets (batches) of ramdonly chosen data and separately training the model\n",
        "# for each batch (in training cycles called epochs). Also, this helps preventing\n",
        "# overfitting: if we use at once all data, the model is much more prone to overfit\n",
        "# (memorizing effect), selecting non-general features highly specific from the data\n",
        "# for the description of it. Therefore, it will have lower capability of predicting\n",
        "# data for values it already did not observe.\n",
        "# This is the parameter batch_size of most of the algorithms.\n",
        "\n",
        "NUMBER_OF_TRAINING_EPOCHS = 200\n",
        "# NUMBER_OF_TRAINING_EPOCHS (integer): number of training cycles used.\n",
        "# This is the 'epochs' parameter of the algorithms.\n",
        "\n",
        "NUMBER_OF_OUTPUT_CLASSES = 2\n",
        "# NUMBER_OF_OUTPUT_CLASSES = None - if TYPE_OF_PROBLEM = 'classification',\n",
        "# this parameter should be specified as an integer. That is because the number of\n",
        "# neurons in the output layer should be equal to the number of classes (1 neuron per\n",
        "# possible class).\n",
        "# If we simply took the number of different labels on the training data as the number\n",
        "# of classes, there would be the risk that a given class is not present on the training\n",
        "# set. So, it is safer (and less computer consuming) to input this number.\n",
        "\n",
        "VERBOSE = 1\n",
        "# VERBOSE: whether to show the training status.\n",
        "# If you set VERBOSE = 0, It will show nothing. If you set VERBOSE = 1, It will\n",
        "# show the output like this: Epoch 1/200 55/55[=====] - 10s 307ms/step - loss: 0.56 -\n",
        "# accuracy: 0.4949\n",
        "\n",
        "COLUMN_MAP_DICT = column_map_dict\n",
        "#COLUMN_MAP_DICT = None\n",
        "# COLUMN_MAP_DICT: Mapping dictionary correlating the position in array or tensor to the original\n",
        "# column name.\n",
        "\n",
        "X_AXIS_ROTATION = 0\n",
        "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "Y_AXIS_ROTATION = 0\n",
        "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "GRID = True #Alternatively: True or False\n",
        "# If GRID = False, no grid lines are shown in the graphic.\n",
        "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
        "METRICS_VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for metrics plot title\n",
        "LOSS_VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for loss plot title\n",
        "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", METRICS_VERTICAL_AXIS_TITLE = \"training_metrics\",\n",
        "# LOSS_VERTICAL_AXIS_TITLE = \"training_loss\"\n",
        "EXPORT_PNG = False\n",
        "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
        "# Set EXPORT_PNG = True to export the obtained image.\n",
        "DIRECTORY_TO_SAVE = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\"\n",
        "# or DIRECTORY_TO_SAVE = \"folder\"\n",
        "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
        "# path, DIRECTORY_TO_SAVE = \"\"\n",
        "FILE_NAME = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the name you want for the file without the\n",
        "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png'\n",
        "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
        "# 'feature_importance_ranking.png'\n",
        "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
        "# the file will be overwritten.\n",
        "PNG_RESOLUTION_DPI = 330\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
        "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
        "\n",
        "\n",
        "# Model object returned as model;\n",
        "# Summary dictionary storing the metrics returned as metrics_dict;\n",
        "# training history object returned as history.\n",
        "# Simply modify these objects on the left of equality:\n",
        "model, metrics_dict, history = get_deep_learning_tf_model (X_train = X_TRAIN, y_train = Y_TRAIN, architecture = ARCHITECTURE, optimizer = OPTIMIZER, X_test = X_TEST, y_test = Y_TEST, X_valid = X_VALID, y_valid = Y_VALID, type_of_problem = TYPE_OF_PROBLEM, size_of_training_batch = SIZE_OF_TRAINING_BATCH, number_of_training_epochs = NUMBER_OF_TRAINING_EPOCHS, number_of_output_classes = NUMBER_OF_OUTPUT_CLASSES, verbose = VERBOSE, column_map_dict = COLUMN_MAP_DICT, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, metrics_vertical_axis_title = METRICS_VERTICAL_AXIS_TITLE, loss_vertical_axis_title = LOSS_VERTICAL_AXIS_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
      ],
      "metadata": {
        "azdata_cell_guid": "f280108e-ef74-4207-9bb1-a2d83f6bdc15",
        "language": "python",
        "id": "q2KG0HfaegFT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architectures from siamese_networks class - Function for fitting a given architecture"
      ],
      "metadata": {
        "azdata_cell_guid": "30e5e3d1-1f8e-48ee-b885-ff7d534466c1",
        "id": "DS7McCCwegFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split_dictionary.keys() == dict_keys(['X_train', 'y_train', 'X_test', 'y_test', 'X_valid', 'y_valid'])\n",
        "\n",
        "X_TRAIN = tensors_dict['train']['inputs']\n",
        "# X_TRAIN = tensor of predictive variables.\n",
        "# Alternatively, modify X_train, not X_TRAIN\n",
        "Y_TRAIN = tensors_dict['train']['labels']\n",
        "# Y_TRAIN = tensor of response variables.\n",
        "# Alternatively, modify y_train, not Y_TRAIN\n",
        "\n",
        "\"\"\"\n",
        "ATTENTION: Y tensors must contain all the responses in sequence. If they are not correctly\n",
        "shaped, use the function union_1_dim_tensors to prepare them before feeding to this architecture.\n",
        "\"\"\"\n",
        "\n",
        "OUTPUT_DICTIONARY = {'response_variable1': {'type': 'regression', 'number_of_classes': 1},\n",
        "                     'response_variable2': {'type': 'regression', 'number_of_classes': 1}\n",
        "                    }\n",
        "# OUTPUT_DICTIONARY structure:\n",
        "# {'response_variable': {\n",
        "# 'type': 'regression', 'number_of_classes':}}\n",
        "# 'response_variable': name of the column used as response for one of the outputs. This key\n",
        "# gives access to the nested dictionary containing the following keys:\n",
        "# 'type': type of problem. Must contain the string 'regression' or 'classification';\n",
        "# 'number_of_classes': integer. This key may not be declared for regression problems. Do not\n",
        "# include the key, set as 1, or set the number of classes used for training.\n",
        "\n",
        "\n",
        "ARCHITECTURE = 'simple_dense'\n",
        "# ARCHITECTURE = 'simple_dense';\n",
        "# ARCHITECTURE = 'double_dense';\n",
        "# ARCHITECTURE = 'cnn';\n",
        "# ARCHITECTURE = 'lstm';\n",
        "# ARCHITECTURE = 'encoder_decoder';\n",
        "# ARCHITECTURE = 'cnn_lstm': hybrid cnn-lstm time series model.\n",
        "\n",
        "OPTIMIZER = None\n",
        "# OPTIMIZER: tf.keras.optimizers.Optimizer object:\n",
        "# keep it None to use the Adam optimizer.\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Optimizer\n",
        "# use the object to set parameters such as learning rate and selection of the optimizer\n",
        "\n",
        "# Tensors of data separated for model testing:\n",
        "X_TEST = tensors_dict['test']['inputs']\n",
        "Y_TEST = tensors_dict['test']['labels']\n",
        "#X_TEST = tensors_dict['test']['inputs']\n",
        "#Y_TEST = tensors_dict['test']['labels']\n",
        "\n",
        "# Tensors of data separated for model validation:\n",
        "X_VALID = tensors_dict['val']['inputs']\n",
        "Y_VALID = tensors_dict['val']['labels']\n",
        "#X_VALID = tensors_dict['val']['inputs']\n",
        "#Y_VALID = tensors_dict['val']['labels']\n",
        "\n",
        "SIZE_OF_TRAINING_BATCH = 200\n",
        "# SIZE_OF_TRAINING_BATCH (integer): amount of data used on each training cycle (epoch).\n",
        "# If we had 20000 data and SIZE_OF_TRAINING_BATCH = 200, then there would be 100\n",
        "# batches (cycles) using 200 data. Training is more efficient when dividing the data into\n",
        "# smaller subsets (batches) of ramdonly chosen data and separately training the model\n",
        "# for each batch (in training cycles called epochs). Also, this helps preventing\n",
        "# overfitting: if we use at once all data, the model is much more prone to overfit\n",
        "# (memorizing effect), selecting non-general features highly specific from the data\n",
        "# for the description of it. Therefore, it will have lower capability of predicting\n",
        "# data for values it already did not observe.\n",
        "# This is the parameter batch_size of most of the algorithms.\n",
        "\n",
        "NUMBER_OF_TRAINING_EPOCHS = 200\n",
        "# NUMBER_OF_TRAINING_EPOCHS (integer): number of training cycles used.\n",
        "# This is the 'epochs' parameter of the algorithms.\n",
        "\n",
        "VERBOSE = 1\n",
        "# VERBOSE: whether to show the training status.\n",
        "# If you set VERBOSE = 0, It will show nothing. If you set VERBOSE = 1, It will\n",
        "# show the output like this: Epoch 1/200 55/55[=====] - 10s 307ms/step - loss: 0.56 -\n",
        "# accuracy: 0.4949\n",
        "\n",
        "COLUMN_MAP_DICT = column_map_dict\n",
        "#COLUMN_MAP_DICT = None\n",
        "# COLUMN_MAP_DICT: Mapping dictionary correlating the position in array or tensor to the original\n",
        "# column name.\n",
        "\n",
        "X_AXIS_ROTATION = 0\n",
        "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "Y_AXIS_ROTATION = 0\n",
        "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "GRID = True #Alternatively: True or False\n",
        "# If GRID = False, no grid lines are shown in the graphic.\n",
        "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
        "METRICS_VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for metrics plot title\n",
        "LOSS_VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for loss plot title\n",
        "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", METRICS_VERTICAL_AXIS_TITLE = \"training_metrics\",\n",
        "# LOSS_VERTICAL_AXIS_TITLE = \"training_loss\"\n",
        "EXPORT_PNG = False\n",
        "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
        "# Set EXPORT_PNG = True to export the obtained image.\n",
        "DIRECTORY_TO_SAVE = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\"\n",
        "# or DIRECTORY_TO_SAVE = \"folder\"\n",
        "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
        "# path, DIRECTORY_TO_SAVE = \"\"\n",
        "FILE_NAME = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the name you want for the file without the\n",
        "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png'\n",
        "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
        "# 'feature_importance_ranking.png'\n",
        "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
        "# the file will be overwritten.\n",
        "PNG_RESOLUTION_DPI = 330\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
        "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
        "\n",
        "\n",
        "# Model object returned as model;\n",
        "# Summary dictionary storing the metrics returned as metrics_dict;\n",
        "# training history object returned as history.\n",
        "# Simply modify these objects on the left of equality:\n",
        "model, metrics_dict, history = get_siamese_networks_model (X_train = X_TRAIN, y_train = Y_TRAIN, output_dictionary = OUTPUT_DICTIONARY, architecture = ARCHITECTURE, optimizer = OPTIMIZER, X_test = X_TEST, y_test = Y_TEST, X_valid = X_VALID, y_valid = Y_VALID, size_of_training_batch = SIZE_OF_TRAINING_BATCH, number_of_training_epochs = NUMBER_OF_TRAINING_EPOCHS, verbose = VERBOSE, column_map_dict = COLUMN_MAP_DICT, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, metrics_vertical_axis_title = METRICS_VERTICAL_AXIS_TITLE, loss_vertical_axis_title = LOSS_VERTICAL_AXIS_TITLE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
      ],
      "metadata": {
        "azdata_cell_guid": "eb9b6571-ea91-4eae-8e50-4fc0ae9992dd",
        "language": "python",
        "id": "PlwWRbo3egFU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Making predictions with the models**"
      ],
      "metadata": {
        "azdata_cell_guid": "73ffe647-9c76-41d9-b203-1e967071ef63",
        "id": "mnW7BLRCegFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_OBJECT = model # Alternatively: object storing another model\n",
        "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
        "# MODEL_OBJECT = elastic_net_linear_reg_model\n",
        "\n",
        "X_tensor = X\n",
        "# predict_for = 'subset' or predict_for = 'single_entry'\n",
        "# The function will automatically detect if it is dealing with lists, NumPy arrays\n",
        "# or Pandas dataframes. If X_df is a list or a single-dimension array, predict_for\n",
        "# will be set as 'single_entry'. If X is a multi-dimension NumPy array (as the\n",
        "# outputs for preparing data - even single_entry - for deep learning models), or if\n",
        "# it is a Pandas dataframe, the function will set predict_for = 'subset'\n",
        "\n",
        "# X_df = subset of predictive variables (dataframe, NumPy array, or list).\n",
        "# If PREDICT_FOR = 'single_entry', X_df should be a list of parameters values.\n",
        "# e.g. X_df = [1.2, 3, 4] (dot is the decimal case separator, comma separate values).\n",
        "# Notice that the list should contain only the numeric values, in the same order of the\n",
        "# correspondent columns.\n",
        "# If PREDICT_FOR = 'subset' (prediction for multiple entries), X_df should be a dataframe\n",
        "# (subset) or a multi-dimensional NumPy array of the parameters values, as usual.\n",
        "\n",
        "DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset\n",
        "# DATAFRAME_FOR_CONCATENATING_PREDICTIONS: if you want to concatenate the predictions\n",
        "# to a dataframe, pass it here:\n",
        "# e.g. DATAFRAME_FOR_CONCATENATING_PREDICTIONS = df\n",
        "# If the dataframe must be the same one passed as X, repeat the dataframe object here:\n",
        "# X_df = dataset, DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset.\n",
        "# Alternatively, if DATAFRAME_FOR_CONCATENATING_PREDICTIONS = None,\n",
        "# the prediction will be returned as a series or NumPy array, depending on the input format.\n",
        "# Notice that the concatenated predictions will be added as a new column.\n",
        "\n",
        "COLUMN_WITH_PREDICTIONS_SUFFIX = None\n",
        "# COLUMN_WITH_PREDICTIONS_SUFFIX = None. If the predictions are added as a new column\n",
        "# of the dataframe DATAFRAME_FOR_CONCATENATING_PREDICTIONS, you can declare this\n",
        "# parameter as string with a suffix for identifying the model. If no suffix is added, the new\n",
        "# column will be named 'y_pred'.\n",
        "# e.g. COLUMN_WITH_PREDICTIONS_SUFFIX = '_keras' will create a column named \"y_pred_keras\". This\n",
        "# parameter is useful when working with multiple models. Always start the suffix with underscore\n",
        "# \"_\" so that no blank spaces are added; the suffix will not be merged to the column; and there\n",
        "# will be no confusion with the dot (.) notation for methods, JSON attributes, etc.\n",
        "\n",
        "FUNCTION_USED_FOR_FITTING_DL_MODEL = 'get_deep_learning_tf_model'\n",
        "# FUNCTION_USED_FOR_FITTING_DL_MODEL: the function you used for obtaining the deep learning model.\n",
        "# Example: 'get_deep_learning_tf_model' or 'get_siamese_networks_model'\n",
        "\n",
        "ARCHITECTURE = None\n",
        "# ARCHITECTURE: some models require inputs in a proper format. Declare here if you are using\n",
        "# one of these architectures. Example: ARCHITECTURE = 'cnn_lstm' from class tf_models require\n",
        "# a special reshape before getting predictions. You can keep None or put the name of the\n",
        "# architecture, if no special reshape is needed.\n",
        "\n",
        "### ATTENTION: ALL MODELS WITH LSTM, CNN or OTHER SPECIAL LAYERS REQUIRE THIS ARGUMENT TO BE\n",
        "# DECLARED\n",
        "\n",
        "LIST_OF_RESPONSES = RESPONSE_COLUMNS\n",
        "# You may declare the list RESPONSE_COLUMNS previously used for separating into features and responses tensors.\n",
        "# LIST_OF_RESPONSES = []. This parameter is obbligatory for multi-response models, such as the ones obtained from\n",
        "# function 'get_siamese_networks_model'. It must contain a list with the same order of the output responses.\n",
        "# Example: suppose your siamese model outputs 4 responses: 'temperature', 'pressure', 'flow_rate', and 'ph', in\n",
        "# this order. The list of responses must be declared as:\n",
        "# LIST_OF_RESPONSES = ['temperature', 'pressure', 'flow_rate', 'ph']\n",
        "# tuples and numpy arrays are also acceptable: LIST_OF_RESPONSES = ('temperature', 'pressure', 'flow_rate', 'ph')\n",
        "# Attention: the number of responses must be exactly the number of elements in list_of_responses, or an error will\n",
        "# be raised.\n",
        "\n",
        "\n",
        "# Predictions returned as prediction_output\n",
        "# Simply modify this object (or variable) on the left of equality:\n",
        "prediction_output = make_model_predictions (model_object = MODEL_OBJECT, X = X_tensor, dataframe_for_concatenating_predictions = DATAFRAME_FOR_CONCATENATING_PREDICTIONS, column_with_predictions_suffix = COLUMN_WITH_PREDICTIONS_SUFFIX, function_used_for_fitting_dl_model = FUNCTION_USED_FOR_FITTING_DL_MODEL, architecture = ARCHITECTURE, list_of_responses = LIST_OF_RESPONSES)"
      ],
      "metadata": {
        "azdata_cell_guid": "aa6b4e31-0b40-4fd8-97bd-c902d1209a94",
        "language": "python",
        "id": "hIOrXYSZegFV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Calculating probabilities associated to each class**\n",
        "- Set the list_of_classes as the input of this function.\n",
        "- The predictions (outputs) from deep learning models (e.g. Keras/TensorFlow models) are themselves the probabilities associated to each possible class.\n",
        "    - For Scikit-learn and XGBoost, we must use a specific method for retrieving the probabilities."
      ],
      "metadata": {
        "azdata_cell_guid": "77e7c192-1669-4cec-9212-d957bb4d2724",
        "id": "NpnEYTUWegFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_OBJECT = model # Alternatively: object storing another model\n",
        "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
        "# MODEL_OBJECT = mlp_model\n",
        "\n",
        "X_tensor = X\n",
        "# predict_for = 'subset' or predict_for = 'single_entry'\n",
        "# The function will automatically detect if it is dealing with lists, NumPy arrays\n",
        "# or Pandas dataframes. If X_df is a list or a single-dimension array, predict_for\n",
        "# will be set as 'single_entry'. If X is a multi-dimension NumPy array (as the\n",
        "# outputs for preparing data - even single_entry - for deep learning models), or if\n",
        "# it is a Pandas dataframe, the function will set predict_for = 'subset'\n",
        "\n",
        "# X_df = subset of predictive variables (dataframe, NumPy array, or list).\n",
        "# If PREDICT_FOR = 'single_entry', X_df should be a list of parameters values.\n",
        "# e.g. X_df = [1.2, 3, 4] (dot is the decimal case separator, comma separate values).\n",
        "# Notice that the list should contain only the numeric values, in the same order of the\n",
        "# correspondent columns.\n",
        "# If PREDICT_FOR = 'subset' (prediction for multiple entries), X_df should be a dataframe\n",
        "# (subset) or a multi-dimensional NumPy array of the parameters values, as usual.\n",
        "\n",
        "LIST_OF_CLASSES = list_of_classes\n",
        "# LIST_OF_CLASSES is the list of classes effectively used for training\n",
        "# the model. Set this parameter as the object returned from function\n",
        "# retrieve_classes_used_to_train\n",
        "\n",
        "TYPE_OF_MODEL = 'deep_learning'\n",
        "# TYPE_OF_MODEL = 'deep_learning' if Keras/TensorFlow or other deep learning\n",
        "# framework was used to obtain the model;\n",
        "# TYPE_OF_MODEL = 'other' for Scikit-learn or XGBoost models.\n",
        "\n",
        "DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset\n",
        "# DATAFRAME_FOR_CONCATENATING_PREDICTIONS: if you want to concatenate the predictions\n",
        "# to a dataframe, pass it here:\n",
        "# e.g. DATAFRAME_FOR_CONCATENATING_PREDICTIONS = df\n",
        "# If the dataframe must be the same one passed as X, repeat the dataframe object here:\n",
        "# X_df = dataset, DATAFRAME_FOR_CONCATENATING_PREDICTIONS = dataset.\n",
        "# Alternatively, if DATAFRAME_FOR_CONCATENATING_PREDICTIONS = None,\n",
        "# the prediction will be returned as a series or NumPy array, depending on the input format.\n",
        "# Notice that the concatenated predictions will be added as a new column.\n",
        "# All of the new columns (appended or not) will have the prefix \"prob_class_\" followed\n",
        "# by the correspondent class name to identify them.\n",
        "\n",
        "ARCHITECTURE = None\n",
        "# ARCHITECTURE: some models require inputs in a proper format. Declare here if you are using\n",
        "# one of these architectures. Example: ARCHITECTURE = 'cnn_lstm' from class tf_models require\n",
        "# a special reshape before getting predictions. You can keep None or put the name of the\n",
        "# architecture, if no special reshape is needed.\n",
        "\n",
        "\n",
        "# Probabilities returned as calculated_probability\n",
        "# Simply modify this object (or variable) on the left of equality:\n",
        "calculated_probability = calculate_class_probability (model_object = MODEL_OBJECT, X = X_tensor, list_of_classes = LIST_OF_CLASSES, type_of_model = TYPE_OF_MODEL, dataframe_for_concatenating_predictions = DATAFRAME_FOR_CONCATENATING_PREDICTIONS, architecture = ARCHITECTURE)"
      ],
      "metadata": {
        "azdata_cell_guid": "67c8d44b-47ea-4044-9e5e-e0e43a426543",
        "language": "python",
        "id": "oPLBWyuCegFW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Performing the SHAP feature importance analysis**\n",
        "- SHAP was developed by a mathematician from Washington University.\n",
        "- It combines the obtained machine learning model with Game Theory algorithms to analyze the relative importance of each variable, as well as the **interactions between variables**.\n",
        "- SHAP returns us a SHAP value that represents the relative importance."
      ],
      "metadata": {
        "azdata_cell_guid": "8778623f-0e23-418a-9108-fe250511ea90",
        "id": "CSoh75YOegFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_OBJECT = model # Alternatively: object storing another model\n",
        "# MODEL_OBJECT: object containing the model that will be analyzed. e.g.\n",
        "# MODEL_OBJECT = xgb_model\n",
        "\n",
        "X_TRAIN = split_dictionary['X_train']\n",
        "# X_TRAIN = subset of predictive variables (dataframe).\n",
        "# Alternatively, modify X_train, not X_TRAIN\n",
        "\n",
        "MODEL_TYPE = 'deep'\n",
        "# MODEL_TYPE = 'general' for the general case, including artificial neural networks.\n",
        "# MODEL_TYPE = 'linear' for Sklearn linear models (OLS, Ridge, Lasso, ElasticNet,\n",
        "# Logistic Regression).\n",
        "# MODEL_TYPE = 'tree' for tree-based models (Random Forest and XGBoost).\n",
        "# MODEL_TYPE = 'deep' for Deep Learning TensorFlow model.\n",
        "# Actually, any string different from 'linear', 'tree', or 'deep' (including blank string)\n",
        "# will apply the general case.\n",
        "\n",
        "TOTAL_OF_SHAP_POINTS = 40\n",
        "# TOTAL_OF_SHAP_POINTS (integer): number of points from the\n",
        "# subset X_train that will be randomly selected for the SHAP\n",
        "# analysis. If the kernel is taking too long, reduce this value.\n",
        "\n",
        "\n",
        "# Dictionary containing calculated metrics returned as shap_dict;\n",
        "# Simply modify this object on the left of equality:\n",
        "shap_dict = shap_feature_analysis (model_object = MODEL_OBJECT, X_train = X_TRAIN, model_type = MODEL_TYPE, total_of_shap_points = TOTAL_OF_SHAP_POINTS)"
      ],
      "metadata": {
        "azdata_cell_guid": "2bd4e275-9726-409e-bf0c-a95b8ff48a79",
        "language": "python",
        "id": "ALHNeR1YegFW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualizing time series**"
      ],
      "metadata": {
        "azdata_cell_guid": "d0ffa059-868b-4b1b-ba2b-ce7a98781f76",
        "id": "V8etZM4HegFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_IN_SAME_COLUMN = False\n",
        "\n",
        "# Parameters to input when DATA_IN_SAME_COLUMN = True:\n",
        "DATASET = None #Alternatively: object containing the dataset to be analyzed (e.g. DATASET = dataset)\n",
        "COLUMN_WITH_PREDICT_VAR_X = 'X' # Alternatively: correct name for X-column\n",
        "COLUMN_WITH_RESPONSE_VAR_Y = 'Y' # Alternatively: correct name for Y-column\n",
        "COLUMN_WITH_LABELS = 'label_column' # Alternatively: correct name for column with the labels or groups\n",
        "\n",
        "# DATA_IN_SAME_COLUMN = False: set as True if all the values to plot are in a same column.\n",
        "# If DATA_IN_SAME_COLUMN = True, you must specify the dataframe containing the data as DATASET;\n",
        "# the column containing the predict variable (X) as COLUMN_WITH_PREDICT_VAR_X; the column\n",
        "# containing the responses to plot (Y) as COLUMN_WITH_RESPONSE_VAR_Y; and the column\n",
        "# containing the labels (subgroup) indication as COLUMN_WITH_LABELS.\n",
        "# DATASET is an object, so do not declare it in quotes. The other three arguments (columns' names)\n",
        "# are strings, so declare in quotes.\n",
        "\n",
        "# Example: suppose you have a dataframe saved as dataset, and two groups A and B to compare.\n",
        "# All the results for both groups are in a column named 'results', wich will be plot against\n",
        "# the time, saved as 'time' (X = 'time'; Y = 'results'). If the result is for\n",
        "# an entry from group A, then a column named 'group' has the value 'A'. If it is for group B,\n",
        "# column 'group' shows the value 'B'. In this example:\n",
        "# DATA_IN_SAME_COLUMN = True,\n",
        "# DATASET = dataset,\n",
        "# COLUMN_WITH_PREDICT_VAR_X = 'time',\n",
        "# COLUMN_WITH_RESPONSE_VAR_Y = 'results',\n",
        "# COLUMN_WITH_LABELS = 'group'\n",
        "# If you want to declare a list of dictionaries, keep DATA_IN_SAME_COLUMN = False and keep\n",
        "# DATASET = None (the other arguments may be set as None, but it is not mandatory:\n",
        "# COLUMN_WITH_PREDICT_VAR_X = None, COLUMN_WITH_RESPONSE_VAR_Y = None, COLUMN_WITH_LABELS = None).\n",
        "\n",
        "\n",
        "# Parameter to input when DATA_IN_SAME_COLUMN = False:\n",
        "LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE = [\n",
        "\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None},\n",
        "    {'x': None, 'y': None, 'lab': None}\n",
        "\n",
        "]\n",
        "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE: if data is already converted to series, lists\n",
        "# or arrays, provide them as a list of dictionaries. It must be declared as a list, in brackets,\n",
        "# even if there is a single dictionary.\n",
        "# Use always the same keys: 'x' for the X-series (predict variables); 'y' for the Y-series\n",
        "# (response variables); and 'lab' for the labels. If you do not want to declare a series, simply\n",
        "# keep as None, but do not remove or rename a key (ALWAYS USE THE KEYS SHOWN AS MODEL).\n",
        "# If you want, you can remove elements (dictionaries) from the list to declare fewer elements;\n",
        "# and you can also add more elements (dictionaries) to the lists, if you need to plot more series.\n",
        "# Simply put a comma after the last element from the list and declare a new dictionary, keeping the\n",
        "# same keys: {'x': x_series, 'y': y_series, 'lab': label}, where x_series, y_series and label\n",
        "# represents the series and label of the added dictionary (you can pass 'lab': None, but if\n",
        "# 'x' or 'y' are None, the new dictionary will be ignored).\n",
        "\n",
        "# Examples:\n",
        "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE =\n",
        "# [{'x': DATASET['X'], 'y': DATASET['Y'], 'lab': 'label'}]\n",
        "# will plot a single variable. In turns:\n",
        "# LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE =\n",
        "# [{'x': DATASET['X'], 'y': DATASET['Y1'], 'lab': 'label'}, {'x': DATASET['X'], 'y': DATASET['Y2'], 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}, {'x': None, 'y': None, 'lab': None}]\n",
        "# will plot two series, Y1 x X and Y2 x X.\n",
        "# Notice that all dictionaries where 'x' or 'y' are None are automatically ignored.\n",
        "# If None is provided to 'lab', an automatic label will be generated.\n",
        "\n",
        "\n",
        "X_AXIS_ROTATION = 70\n",
        "#Rotation of X axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "Y_AXIS_ROTATION = 0\n",
        "#Rotation of Y axis labels. Alternatively, insert any numeric value from 0 to 90 (degrees).\n",
        "GRID = True #Alternatively: True or False\n",
        "# If GRID = False, no grid lines are shown in the graphic.\n",
        "ADD_SPLINE_LINES = True #Alternatively: True or False\n",
        "# If ADD_SPLINE_LINES = False, no lines connecting the successive values are shown.\n",
        "# Since we are obtaining a scatter plot, there is no meaning in omitting the dots,\n",
        "# as we can do for the time series visualization function.\n",
        "ADD_SCATTER_DOTS = False\n",
        "# If ADD_SCATTER_DOTS = False, no dots representing the data points are shown.\n",
        "HORIZONTAL_AXIS_TITLE = None #Alternatively: string inside quotes for horizontal title\n",
        "VERTICAL_AXIS_TITLE = None #Alternatively: string inside quotes for vertical title\n",
        "PLOT_TITLE = None #Alternatively: string inside quotes for graphic title\n",
        "# e.g. HORIZONTAL_AXIS_TITLE = \"X\", VERTICAL_AXIS_TITLE = \"Y\", PLOT_TITLE = \"YxX\"\n",
        "\n",
        "EXPORT_PNG = False\n",
        "# Set EXPORT_PNG = False if you do not want to export the obtained image;\n",
        "# Set EXPORT_PNG = True to export the obtained image.\n",
        "DIRECTORY_TO_SAVE = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file will be stored. e.g. DIRECTORY_TO_SAVE = \"\"\n",
        "# or DIRECTORY_TO_SAVE = \"folder\"\n",
        "# If EXPORT_PNG = True and DIRECTORY_TO_SAVE = None, the file will be saved in the root\n",
        "# path, DIRECTORY_TO_SAVE = \"\"\n",
        "FILE_NAME = None\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# (string, in quotes): input the name you want for the file without the\n",
        "# extension, which will be 'png'. e.g. FILE_NAME = \"my_image\" will save a file 'my_image.png'\n",
        "# If EXPORT_PNG = True and FILE_NAME = None, the file will be saved as:\n",
        "# 'time_series_vis.png'\n",
        "# WARNING: if there is already a file in the path DIRECTORY_TO_SAVE saved as FILE_NAME,\n",
        "# the file will be overwritten.\n",
        "PNG_RESOLUTION_DPI = 330\n",
        "# This parameter has effect only if EXPORT_PNG = True.\n",
        "# Alternatively, input an integer that will correspond to the resolution of the exported\n",
        "# image in dpi. If PNG_RESOLUTION_DPI = None, it will be set as 330.\n",
        "\n",
        "\n",
        "time_series_vis (data_in_same_column = DATA_IN_SAME_COLUMN, df = DATASET, column_with_predict_var_x = COLUMN_WITH_PREDICT_VAR_X, column_with_response_var_y = COLUMN_WITH_RESPONSE_VAR_Y, column_with_labels = COLUMN_WITH_LABELS, list_of_dictionaries_with_series_to_analyze = LIST_OF_DICTIONARIES_WITH_SERIES_TO_ANALYZE, x_axis_rotation = X_AXIS_ROTATION, y_axis_rotation = Y_AXIS_ROTATION, grid = GRID, add_splines_lines = ADD_SPLINE_LINES, add_scatter_dots = ADD_SCATTER_DOTS, horizontal_axis_title = HORIZONTAL_AXIS_TITLE, vertical_axis_title = VERTICAL_AXIS_TITLE, plot_title = PLOT_TITLE, export_png = EXPORT_PNG, directory_to_save = DIRECTORY_TO_SAVE, file_name = FILE_NAME, png_resolution_dpi = PNG_RESOLUTION_DPI)"
      ],
      "metadata": {
        "azdata_cell_guid": "5842332c-e4ef-4bea-a875-ad6bf9664344",
        "language": "python",
        "id": "ydovL5r0egFX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing or exporting models and dictionaries (or lists)**"
      ],
      "metadata": {
        "azdata_cell_guid": "44260226-a302-4a02-8649-9b3bd66e2e70",
        "id": "wihhb75zegFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case 1: import only a model"
      ],
      "metadata": {
        "azdata_cell_guid": "fe316077-151f-4f67-a299-41a9fa488ee2",
        "id": "d3eGq3z2egFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTION = 'import'\n",
        "# ACTION = 'import' for importing a model and/or a dictionary;\n",
        "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
        "\n",
        "OBJECTS_MANIPULATED = 'model_only'\n",
        "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will\n",
        "#  be manipulated.\n",
        "\n",
        "MODEL_FILE_NAME = None\n",
        "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. MODEL_FILE_NAME = 'model'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
        "\n",
        "DICTIONARY_OR_LIST_FILE_NAME = None\n",
        "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary\n",
        "# (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary\n",
        "# or list will be manipulated.\n",
        "\n",
        "DIRECTORY_PATH = ''\n",
        "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
        "# or from which the model will be retrieved. If no value is provided,\n",
        "# the DIRECTORY_PATH will be the root: \"\"\n",
        "# Notice that the model and the dictionary must be stored in the same path.\n",
        "# If a model and a dictionary will be exported, they will be stored in the same\n",
        "# DIRECTORY_PATH.\n",
        "\n",
        "MODEL_TYPE = 'tensorflow_general'\n",
        "# This parameter has effect only when a model will be manipulated.\n",
        "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .keras\n",
        "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing\n",
        "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
        "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
        "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
        "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
        "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
        "\n",
        "DICT_OR_LIST_TO_EXPORT = None\n",
        "MODEL_TO_EXPORT = None\n",
        "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
        "# must be declared. If ACTION == 'export', keep:\n",
        "# DICT_OR_LIST_TO_EXPORT = None,\n",
        "# MODEL_TO_EXPORT = None\n",
        "# If one of these objects will be exported, substitute None by the name of the object\n",
        "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
        "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
        "# it is not a string, but an object.\n",
        "# For exporting a dictionary named as 'dict':\n",
        "# DICT_OR_LIST_TO_EXPORT = dict\n",
        "\n",
        "USE_COLAB_MEMORY = False\n",
        "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
        "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
        "# from Google Colaboratory: you will update or download the file and it will be available\n",
        "# only during the time when the kernel is running. It will be excluded when the kernel\n",
        "# dies, for instance, when you close the notebook.\n",
        "\n",
        "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
        "# to your computer (running the cell will start the download).\n",
        "\n",
        "# Model object saved as model.\n",
        "# Simply modify this object on the left of equality:\n",
        "model = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)"
      ],
      "metadata": {
        "azdata_cell_guid": "35261175-26b7-4b8c-b570-b40c45a0ee21",
        "language": "python",
        "id": "fDjVtmogegFY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case 2: import only a dictionary or a list"
      ],
      "metadata": {
        "azdata_cell_guid": "acf3e82f-7596-4edf-95e4-ced7622a951e",
        "id": "-YTRQ3WcegFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTION = 'import'\n",
        "# ACTION = 'import' for importing a model and/or a dictionary;\n",
        "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
        "\n",
        "OBJECTS_MANIPULATED = 'dict_or_list_only'\n",
        "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will\n",
        "#  be manipulated.\n",
        "\n",
        "MODEL_FILE_NAME = None\n",
        "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. MODEL_FILE_NAME = 'model'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
        "\n",
        "DICTIONARY_OR_LIST_FILE_NAME = None\n",
        "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary\n",
        "# (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary\n",
        "# or list will be manipulated.\n",
        "\n",
        "DIRECTORY_PATH = ''\n",
        "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
        "# or from which the model will be retrieved. If no value is provided,\n",
        "# the DIRECTORY_PATH will be the root: \"\"\n",
        "# Notice that the model and the dictionary must be stored in the same path.\n",
        "# If a model and a dictionary will be exported, they will be stored in the same\n",
        "# DIRECTORY_PATH.\n",
        "\n",
        "MODEL_TYPE = 'tensorflow_general'\n",
        "# This parameter has effect only when a model will be manipulated.\n",
        "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .keras\n",
        "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing\n",
        "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
        "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
        "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
        "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
        "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
        "\n",
        "DICT_OR_LIST_TO_EXPORT = None\n",
        "MODEL_TO_EXPORT = None\n",
        "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
        "# must be declared. If ACTION == 'export', keep:\n",
        "# DICT_OR_LIST_TO_EXPORT = None,\n",
        "# MODEL_TO_EXPORT = None\n",
        "# If one of these objects will be exported, substitute None by the name of the object\n",
        "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
        "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
        "# it is not a string, but an object.\n",
        "# For exporting a dictionary named as 'dict':\n",
        "# DICT_OR_LIST_TO_EXPORT = dict\n",
        "\n",
        "USE_COLAB_MEMORY = False\n",
        "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
        "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
        "# from Google Colaboratory: you will update or download the file and it will be available\n",
        "# only during the time when the kernel is running. It will be excluded when the kernel\n",
        "# dies, for instance, when you close the notebook.\n",
        "\n",
        "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
        "# to your computer (running the cell will start the download).\n",
        "\n",
        "# Dictionary or list saved as imported_dict_or_list.\n",
        "# Simply modify this object on the left of equality:\n",
        "imported_dict_or_list = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)"
      ],
      "metadata": {
        "azdata_cell_guid": "0637bea5-3f6f-460b-917f-fb9c006fbcbe",
        "language": "python",
        "id": "EMQ-wHFuegFg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case 3: import a model and a dictionary (or a list)"
      ],
      "metadata": {
        "azdata_cell_guid": "19209c7b-cf4b-4a0d-9c82-1b63ae3e3319",
        "id": "8jg7vKbdegFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTION = 'import'\n",
        "# ACTION = 'import' for importing a model and/or a dictionary;\n",
        "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
        "\n",
        "OBJECTS_MANIPULATED = 'model_and_dict'\n",
        "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will\n",
        "#  be manipulated.\n",
        "\n",
        "MODEL_FILE_NAME = None\n",
        "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. MODEL_FILE_NAME = 'model'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
        "\n",
        "DICTIONARY_OR_LIST_FILE_NAME = None\n",
        "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary\n",
        "# (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary\n",
        "# or list will be manipulated.\n",
        "\n",
        "DIRECTORY_PATH = ''\n",
        "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
        "# or from which the model will be retrieved. If no value is provided,\n",
        "# the DIRECTORY_PATH will be the root: \"\"\n",
        "# Notice that the model and the dictionary must be stored in the same path.\n",
        "# If a model and a dictionary will be exported, they will be stored in the same\n",
        "# DIRECTORY_PATH.\n",
        "\n",
        "MODEL_TYPE = 'tensorflow_general'\n",
        "# This parameter has effect only when a model will be manipulated.\n",
        "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .keras\n",
        "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing\n",
        "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
        "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
        "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
        "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
        "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
        "\n",
        "DICT_OR_LIST_TO_EXPORT = None\n",
        "MODEL_TO_EXPORT = None\n",
        "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
        "# must be declared. If ACTION == 'export', keep:\n",
        "# DICT_OR_LIST_TO_EXPORT = None,\n",
        "# MODEL_TO_EXPORT = None\n",
        "# If one of these objects will be exported, substitute None by the name of the object\n",
        "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
        "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
        "# it is not a string, but an object.\n",
        "# For exporting a dictionary named as 'dict':\n",
        "# DICT_OR_LIST_TO_EXPORT = dict\n",
        "\n",
        "USE_COLAB_MEMORY = False\n",
        "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
        "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
        "# from Google Colaboratory: you will update or download the file and it will be available\n",
        "# only during the time when the kernel is running. It will be excluded when the kernel\n",
        "# dies, for instance, when you close the notebook.\n",
        "\n",
        "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
        "# to your computer (running the cell will start the download).\n",
        "\n",
        "# Model object saved as model. Dictionary or list saved as imported_dict_or_list.\n",
        "# Simply modify these objects on the left of equality:\n",
        "model, imported_dict_or_list = import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)"
      ],
      "metadata": {
        "azdata_cell_guid": "dc261ce4-d3d9-4421-8881-18af2db88a63",
        "language": "python",
        "id": "kSCFdw68egFi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Case 4: export a model and/or a dictionary (or a list)"
      ],
      "metadata": {
        "azdata_cell_guid": "6bf07c8e-30b5-4994-a5ea-f719e32a61b4",
        "id": "w-MrBEnPegFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ACTION = 'export'\n",
        "# ACTION = 'import' for importing a model and/or a dictionary;\n",
        "# ACTION = 'export' for exporting a model and/or a dictionary.\n",
        "\n",
        "OBJECTS_MANIPULATED = 'model_only'\n",
        "# OBJECTS_MANIPULATED = 'model_only' if only a model will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'dict_or_list_only' if only a dictionary will be manipulated.\n",
        "# OBJECTS_MANIPULATED = 'model_and_dict' if both a model and a dictionary will\n",
        "#  be manipulated.\n",
        "\n",
        "MODEL_FILE_NAME = None\n",
        "# MODEL_FILE_NAME: string with the name of the file containing the model (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. MODEL_FILE_NAME = 'model'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep MODEL_FILE_NAME = None if no model will be manipulated.\n",
        "\n",
        "DICTIONARY_OR_LIST_FILE_NAME = None\n",
        "# DICTIONARY_OR_LIST_FILE_NAME: string with the name of the file containing the dictionary\n",
        "# (for 'import');\n",
        "# or of the name that the exported file will have (for 'export')\n",
        "# e.g. DICTIONARY_OR_LIST_FILE_NAME = 'history_dict'\n",
        "# WARNING: Do not add the file extension.\n",
        "# Keep it in quotes. Keep DICTIONARY_OR_LIST_FILE_NAME = None if no dictionary\n",
        "# or list will be manipulated.\n",
        "\n",
        "DIRECTORY_PATH = ''\n",
        "# DIRECTORY_PATH: path of the directory where the model will be saved,\n",
        "# or from which the model will be retrieved. If no value is provided,\n",
        "# the DIRECTORY_PATH will be the root: \"\"\n",
        "# Notice that the model and the dictionary must be stored in the same path.\n",
        "# If a model and a dictionary will be exported, they will be stored in the same\n",
        "# DIRECTORY_PATH.\n",
        "\n",
        "MODEL_TYPE = 'tensorflow_general'\n",
        "# This parameter has effect only when a model will be manipulated.\n",
        "# MODEL_TYPE: 'keras' for deep learning Keras/ TensorFlow models with extension .keras\n",
        "# MODEL_TYPE: 'tensorflow_general' for generic deep learning tensorflow models containing\n",
        "# custom layers, losses and architectures. Such models are compressed as tar.gz, tar, or zip.\n",
        "# MODEL_TYPE = 'sklearn' for models from Scikit-learn (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_regressor' for XGBoost regression models (non-deep learning)\n",
        "# MODEL_TYPE = 'xgb_classifier' for XGBoost classification models (non-deep learning)\n",
        "# MODEL_TYPE = 'arima' for ARIMA model (Statsmodels)\n",
        "# MODEL_TYPE = 'prophet' for Facebook Prophet model\n",
        "# MODEL_TYPE = 'anomaly_detector' for the Anomaly Detection model\n",
        "\n",
        "DICT_OR_LIST_TO_EXPORT = None\n",
        "MODEL_TO_EXPORT = None\n",
        "# These two parameters have effect only when ACTION == 'export'. In this case, they\n",
        "# must be declared. If ACTION == 'export', keep:\n",
        "# DICT_OR_LIST_TO_EXPORT = None,\n",
        "# MODEL_TO_EXPORT = None\n",
        "# If one of these objects will be exported, substitute None by the name of the object\n",
        "# e.g. if your model is stored in the global memory as 'keras_model' declare:\n",
        "# MODEL_TO_EXPORT = keras_model. Notice that it must be declared without quotes, since\n",
        "# it is not a string, but an object.\n",
        "# For exporting a dictionary named as 'dict':\n",
        "# DICT_OR_LIST_TO_EXPORT = dict\n",
        "\n",
        "USE_COLAB_MEMORY = False\n",
        "# USE_COLAB_MEMORY: this parameter has only effect when using Google Colab (or it will\n",
        "# raise an error). Set as USE_COLAB_MEMORY = True if you want to use the instant memory\n",
        "# from Google Colaboratory: you will update or download the file and it will be available\n",
        "# only during the time when the kernel is running. It will be excluded when the kernel\n",
        "# dies, for instance, when you close the notebook.\n",
        "\n",
        "# If ACTION == 'export' and USE_COLAB_MEMORY == True, then the file will be downloaded\n",
        "# to your computer (running the cell will start the download).\n",
        "\n",
        "import_export_model_list_dict (action = ACTION, objects_manipulated = OBJECTS_MANIPULATED, model_file_name = MODEL_FILE_NAME, dictionary_or_list_file_name = DICTIONARY_OR_LIST_FILE_NAME, directory_path = DIRECTORY_PATH, model_type = MODEL_TYPE, dict_or_list_to_export = DICT_OR_LIST_TO_EXPORT, model_to_export = MODEL_TO_EXPORT, use_colab_memory = USE_COLAB_MEMORY)"
      ],
      "metadata": {
        "azdata_cell_guid": "2ecac573-ebdb-42bb-bf9b-5ec449bc7992",
        "language": "python",
        "id": "3pP2b6IYegFi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exporting the dataframe as CSV file (to notebook's workspace)**"
      ],
      "metadata": {
        "azdata_cell_guid": "72a74f4b-c693-4811-98d0-e1af20aa6e61",
        "id": "QyDMJROJegFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## WARNING: all files exported from this function are .csv (comma separated values)\n",
        "\n",
        "DATAFRAME_OBJ_TO_BE_EXPORTED = dataset\n",
        "# Alternatively: object containing the dataset to be exported.\n",
        "# DATAFRAME_OBJ_TO_BE_EXPORTED: dataframe object that is going to be exported from the\n",
        "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
        "# example: DATAFRAME_OBJ_TO_BE_EXPORTED = dataset will export the dataset object.\n",
        "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
        "\n",
        "FILE_DIRECTORY_PATH = \"\"\n",
        "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\"\n",
        "# or FILE_DIRECTORY_PATH = \"folder\"\n",
        "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
        "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
        "\n",
        "NEW_FILE_NAME_WITHOUT_EXTENSION = \"dataset\"\n",
        "# NEW_FILE_NAME_WITHOUT_EXTENSION - (string, in quotes): input the name of the\n",
        "# file without the extension. e.g. set NEW_FILE_NAME_WITHOUT_EXTENSION = \"my_file\"\n",
        "# to export the CSV file 'my_file.csv' to notebook's workspace.\n",
        "\n",
        "export_pd_dataframe_as_csv (dataframe_obj_to_be_exported = DATAFRAME_OBJ_TO_BE_EXPORTED, new_file_name_without_extension = NEW_FILE_NAME_WITHOUT_EXTENSION, file_directory_path = FILE_DIRECTORY_PATH)"
      ],
      "metadata": {
        "azdata_cell_guid": "ed885b74-996a-400e-805c-8c332d55f3d4",
        "language": "python",
        "id": "2CiUC28fegFj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exporting dataframes as Excel file tables**"
      ],
      "metadata": {
        "azdata_cell_guid": "23f27c0a-d30b-43bc-8f75-788a3d8d3a94",
        "id": "mq54OwhkegFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## WARNING: all files exported from this function are .xlsx\n",
        "\n",
        "FILE_NAME_WITHOUT_EXTENSION = \"datasets\"\n",
        "# (string, in quotes): input the name of the\n",
        "# file without the extension. e.g. new_file_name_without_extension = \"my_file\"\n",
        "# will export a file 'my_file.xlsx' to notebook's workspace.\n",
        "\n",
        "EXPORTED_TABLES = [{'dataframe_obj_to_be_exported': None,\n",
        "                    'excel_sheet_name': None},]\n",
        "\n",
        "# exported_tables is a list of dictionaries. User may declare several dictionaries,\n",
        "# as long as the keys are always the same, and if the values stored in keys are not None.\n",
        "\n",
        "# key 'dataframe_obj_to_be_exported': dataframe object that is going to be exported from the\n",
        "# function. Since it is an object (not a string), it should not be declared in quotes.\n",
        "# example: dataframe_obj_to_be_exported = dataset will export the dataset object.\n",
        "# ATTENTION: The dataframe object must be a Pandas dataframe.\n",
        "\n",
        "# key 'excel_sheet_name': string containing the name of the sheet to be written on the\n",
        "# exported Excel file. Example: excel_sheet_name = 'tab_1' will save the dataframe in the\n",
        "# sheet 'tab_1' from the file named as file_name_without_extension.\n",
        "\n",
        "# examples: exported_tables = [{'dataframe_obj_to_be_exported': dataset1,\n",
        "# 'excel_sheet_name': 'sheet1'},]\n",
        "# will export only dataset1 as 'sheet1';\n",
        "# exported_tables = [{'dataframe_obj_to_be_exported': dataset1, 'excel_sheet_name': 'sheet1'},\n",
        "# {'dataframe_obj_to_be_exported': dataset2, 'excel_sheet_name': 'sheet2']\n",
        "# will export dataset1 as 'sheet1' and dataset2 as 'sheet2'.\n",
        "\n",
        "# Notice that if the file does not contain the exported sheets, they will be created. If it has,\n",
        "# the sheets will be replaced.\n",
        "\n",
        "FILE_DIRECTORY_PATH = \"\"\n",
        "# FILE_DIRECTORY_PATH - (string, in quotes): input the path of the directory\n",
        "# (e.g. folder path) where the file is stored. e.g. FILE_DIRECTORY_PATH = \"\"\n",
        "# or FILE_DIRECTORY_PATH = \"folder\"\n",
        "# If you want to export the file to AWS S3, this parameter will have no effect.\n",
        "# In this case, you can set FILE_DIRECTORY_PATH = None\n",
        "\n",
        "\n",
        "export_pd_dataframe_as_excel (file_name_without_extension = FILE_NAME_WITHOUT_EXTENSION, exported_tables = EXPORTED_TABLES, file_directory_path = FILE_DIRECTORY_PATH)"
      ],
      "metadata": {
        "azdata_cell_guid": "6ba0bb3f-dfef-4598-8faa-ef52101d6d11",
        "language": "python",
        "id": "SoxThC68egFk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "****"
      ],
      "metadata": {
        "azdata_cell_guid": "61af15ba-194d-4817-81b0-010b94ab93e1",
        "id": "O2fwGJqPegFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep Learning for Time Series Forecasting (crash-course)**\n",
        "- by Jason Brownlee @ ML Mastery\n",
        "- In: https://machinelearningmastery.com/wp-content/uploads/2021/03/deep_learning_time_series_forecasting_mini_course.pdf?__s=q55hvrcdgsdqj6usy4p2&utm_source=drip&utm_medium=email&utm_campaign=DLFTSF+Mini-Course&utm_content=Deep+Learning+for+Time+Series+Forecasting+%28crash-course%29\n",
        "\n",
        "## 1. Deep Learning\n",
        "\n",
        "Generally, neural networks like Multilayer Perceptrons or MLPs provide capabilities that are offered by few algorithms, such as:\n",
        "\n",
        "- Robust to Noise: Neural networks are robust to noise in input data and in the mapping function and can even support learning and prediction in the presence of missing values.\n",
        "- Nonlinear: Neural networks do not make strong assumptions about the mapping function and readily learn linear and nonlinear relationships.\n",
        "- Multivariate Inputs: An arbitrary number of input features can be specified, providing direct support for multivariate forecasting.\n",
        "- Multi-step Forecasts: An arbitrary number of output values can be specified, providing direct support for multi-step and even multivariate forecasting.\n",
        "\n",
        "For these capabilities alone, feedforward neural networks may be useful for time series forecasting.\n",
        "\n",
        "## 2. Multilayer Perceptron (MLP) model for Time Series Forecasting\n",
        "\n",
        "First, we can define the model.\n",
        "\n",
        "- We will define the number of input time steps as the total of features or columns on the original dataset and we will input this value via the `input_dim` argument on the first hidden layer. For N features:\n",
        "\n",
        "`input_dim = N`\n",
        "\n",
        "- In this case, we will use the efficient Adam version of stochastic gradient descent and optimizes the mean squared error ('mse') loss function.\n",
        "\n",
        "## 3. Convolutional Neural Network (CNN) model for Time Series Forecasting\n",
        "\n",
        "An important difference from the MLP model is that the CNN model expects three-dimensional input with the shape: [samples, timesteps, responses].\n",
        "- Then, we must convert the dataset X_train an array with the following format before training:\n",
        "\n",
        "[X_train.shape[0], X_train.shape[1], 1]\n",
        "\n",
        "    - Where X_train.shape[0] is the number of rows; and\n",
        "    - X_train.shape[1] is the number of (predictive) columns of the original dataframe.\n",
        "\n",
        "The array of responses y_train, in turns, must be of shape:\n",
        "\n",
        "[y_train.shape[0], 1]\n",
        "\n",
        "- So, we will define the data in the form [samples, timesteps] and reshape it accordingly.\n",
        "- We will define the number of input time steps as the total of features or columns on the original dataset; and the number of response variables as 1; and we will input these values via the via the `input_shape` argument on the first hidden layer. For N features:\n",
        "\n",
        "`input_shape = (N, 1)`\n",
        "\n",
        "- We will use one convolutional hidden layer followed by a max pooling layer.\n",
        "- The filter maps are then flattened before being interpreted by a Dense layer and outputting a prediction.\n",
        "- The model uses the efficient Adam version of stochastic gradient descent and optimizes the mean squared error ('mse') loss function.\n",
        "\n",
        "## 4. Long Short-Term Memory (LSTM) Neural Network model for Time Series Forecasting\n",
        "\n",
        "An important difference from the MLP model, and like the CNN model, is that the LSTM model expects three-dimensional input with the shape [samples, timesteps, responses].\n",
        "\n",
        "- Then, we must convert the dataset X_train an array with the following format before training:\n",
        "\n",
        "[X_train.shape[0], X_train.shape[1], 1]\n",
        "\n",
        "    - Where X_train.shape[0] is the number of rows; and\n",
        "    - X_train.shape[1] is the number of (predictive) columns of the original dataframe.\n",
        "\n",
        "Again, the array of responses y_train, must be of shape:\n",
        "\n",
        "[y_train.shape[0], 1]\n",
        "\n",
        "- So, we will define the data in the form [samples, timesteps] and reshape it accordingly.\n",
        "- We will define the number of input time steps as the total of features or columns on the original dataset; and the number of response variables as 1; and we will input these values via the via the `input_shape` argument on the first hidden layer. For N features:\n",
        "\n",
        "`input_shape = (N, 1)`\n",
        "\n",
        "- We will use one LSTM layer to process each input sub-sequence of N time steps, followed by a Dense layer to interpret the summary of the input sequence.\n",
        "- The model uses the efficient Adam version of stochastic gradient descent and optimizes the mean squared error ('mse') loss function.\n",
        "\n",
        "## 5. Encoder-Decoder LSTM Multi-step Forecasting\n",
        "\n",
        "The LSTM model expects three-dimensional input with the shape [samples, timesteps, responses].\n",
        "\n",
        "- Then, we must convert the dataset X_train an array with the following format before training:\n",
        "\n",
        "[X_train.shape[0], X_train.shape[1], 1]\n",
        "\n",
        "    - Where X_train.shape[0] is the number of rows; and\n",
        "    - X_train.shape[1] is the number of (predictive) columns of the original dataframe.\n",
        "\n",
        "- So, we will define the data in the form [samples, timesteps] and reshape it accordingly.\n",
        "\n",
        "Important particularity of the encoder-decoder architecture:\n",
        "- The output must also be shaped this way when using the Encoder-Decoder model (i.e., we must perform a final reshaping of the response variables before feeding  this model).\n",
        "- In other words, y_train must also have 3 dimensions. So, after reshaping the array of responses as [y_train.shape[0], 1], we must apply a second reshape to obtain a 3-dimensional array with format:\n",
        "\n",
        "[y_train.shape[0], y_train.shape[1], 1]\n",
        "\n",
        "- We will define the number of input time steps as the total of features or columns on the original dataset; and the number of response variables as 1; and we will input these values via the via the `input_shape` argument on the first hidden layer. For N features:\n",
        "\n",
        "`input_shape = (N, 1)`\n",
        "\n",
        "- We will define an LSTM encoder to read and encode the input sequences of N time steps.\n",
        "- The encoded sequence will be repeated 2 times by the model for the two output time steps required by the model using a RepeatVector layer.\n",
        "- These will be fed to a decoder LSTM layer before using a Dense output layer wrapped in a TimeDistributed layer that will produce one output for each step in the output sequence.\n",
        "- The model uses the efficient Adam version of stochastic gradient descent and optimizes the mean squared error ('mse') loss function.\n",
        "\n",
        "## 6. CNN-LSTM model for Time Series Forecasting\n",
        "\n",
        "The benefit of this model is that the model can support very long input sequences that can be read as blocks or subsequences by the CNN model, then pieced together by the LSTM model.\n",
        "\n",
        "We can define a simple univariate problem as a sequence of integers, fit the model on this sequence and have the model predict the next value in the sequence. We will frame the problem to have 4 inputs and 1 output, for example: [10, 20, 30, 40] as input and [50] as output.\n",
        "\n",
        "When using a hybrid CNN-LSTM model, we will further divide each sample into further subsequences. The CNN model will interpret each sub-sequence and the LSTM will piece together the interpretations from the subsequences. As such, we will split each sample into 2 subsequences of 2 times per subsequence.\n",
        "\n",
        "- So, for a total of M entries = X_train.shape[0] (entries of the original dataset), the data must now be converted into arrays of the following format before feeding the model:\n",
        "\n",
        "[X.shape[0], 2, 2, 1]\n",
        "\n",
        "    - Where X_train.shape[0] is the number of rows; and\n",
        "    - X_train.shape[1] is the number of (predictive) columns of the original dataframe.\n",
        "\n",
        "Again, the array of responses y_train, must be of shape:\n",
        "\n",
        "[y_train.shape[0], 1]\n",
        "\n",
        "- Since the number of response variables is 1, the `input_shape` argument on the first hidden layer now is:\n",
        "\n",
        "`input_shape = (None, 2, 1)`\n",
        "\n",
        "- The CNN will be defined to expect 2 time steps per subsequence with one feature.\n",
        "- The entire CNN model is then wrapped in TimeDistributed wrapper layers so that it can be applied to each subsequence in the sample.\n",
        "- The results are then interpreted by the LSTM layer before the model outputs a prediction.\n",
        "\n",
        "The model uses the efficient Adam version of stochastic gradient descent and optimizes the mean squared error ('mse') loss function."
      ],
      "metadata": {
        "azdata_cell_guid": "56fbd53c-f480-42e6-908b-c9d5a39fc67d",
        "id": "FKKz3SicegFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chosen of activation function - Background**\n",
        "- by Jason Brownlee on January 18, 2021 in Deep Learning\n",
        "- In: https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/?msclkid=b2ff61a7c17811eca0396af8009afac9\n",
        "\n",
        "Activation Functions\n",
        "An activation function in a neural network defines how the weighted sum of the input is transformed into an output from a node or nodes in a layer of the network.\n",
        "\n",
        "Sometimes the activation function is called a “transfer function.” If the output range of the activation function is limited, then it may be called a “squashing function.” Many activation functions are nonlinear and may be referred to as the “nonlinearity” in the layer or the network design.\n",
        "\n",
        "The choice of activation function has a large impact on the capability and performance of the neural network, and different activation functions may be used in different parts of the model.\n",
        "\n",
        "Technically, the activation function is used within or after the internal processing of each node in the network, although networks are designed to use the same activation function for all nodes in a layer.\n",
        "\n",
        "A network may have three types of layers: input layers that take raw input from the domain, hidden layers that take input from another layer and pass output to another layer, and output layers that make a prediction.\n",
        "\n",
        "All hidden layers typically use the same activation function. The output layer will typically use a different activation function from the hidden layers and is dependent upon the type of prediction required by the model.\n",
        "\n",
        "Activation functions are also typically differentiable, meaning the first-order derivative can be calculated for a given input value. This is required given that neural networks are typically trained using the backpropagation of error algorithm that requires the derivative of prediction error in order to update the weights of the model.\n",
        "\n",
        "There are many different types of activation functions used in neural networks, although perhaps only a small number of functions used in practice for hidden and output layers.\n",
        "\n",
        "Let’s take a look at the activation functions used for each type of layer in turn.\n",
        "\n",
        "Activation for Hidden Layers\n",
        "A hidden layer in a neural network is a layer that receives input from another layer (such as another hidden layer or an input layer) and provides output to another layer (such as another hidden layer or an output layer).\n",
        "\n",
        "A hidden layer does not directly contact input data or produce outputs for a model, at least in general.\n",
        "\n",
        "A neural network may have zero or more hidden layers.\n",
        "\n",
        "Typically, a differentiable nonlinear activation function is used in the hidden layers of a neural network. This allows the model to learn more complex functions than a network trained using a linear activation function.\n",
        "\n",
        "In order to get access to a much richer hypothesis space that would benefit from deep representations, you need a non-linearity, or activation function.\n",
        "\n",
        "— Page 72, Deep Learning with Python, 2017.\n",
        "\n",
        "There are perhaps three activation functions you may want to consider for use in hidden layers; they are:\n",
        "\n",
        " - Rectified Linear Activation (ReLU)\n",
        " - Logistic (Sigmoid)\n",
        " - Hyperbolic Tangent (Tanh)\n",
        "\n",
        "This is not an exhaustive list of activation functions used for hidden layers, but they are the most commonly used.\n",
        "\n",
        "Let’s take a closer look at each in turn.\n",
        "\n",
        "## **ReLU Hidden Layer Activation Function**\n",
        "The rectified linear activation function, or ReLU activation function, is perhaps the most common function used for hidden layers.\n",
        "\n",
        "It is common because it is both simple to implement and effective at overcoming the limitations of other previously popular activation functions, such as Sigmoid and Tanh. Specifically, it is less susceptible to vanishing gradients that prevent deep models from being trained, although it can suffer from other problems like saturated or “dead” units.\n",
        "\n",
        "The ReLU function is calculated as follows:\n",
        "\n",
        "`max(0.0, x)`\n",
        "\n",
        "This means that if the input value (x) is negative, then a value 0.0 is returned, otherwise, the value is returned.\n",
        "\n",
        "When using the ReLU function for hidden layers, it is a good practice to use a “He Normal” or “He Uniform” weight initialization and scale input data to the range 0-1 (normalize) prior to training.\n",
        "\n",
        "## **Sigmoid Hidden Layer Activation Function**\n",
        "The sigmoid activation function is also called the logistic function.\n",
        "\n",
        "It is the same function used in the logistic regression classification algorithm.\n",
        "\n",
        "The function takes any real value as input and outputs values in the range 0 to 1. The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to 0.0.\n",
        "\n",
        "The sigmoid activation function is calculated as follows:\n",
        "\n",
        "`1.0 / (1.0 + e^-x)`\n",
        "\n",
        "Where e is a mathematical constant, which is the base of the natural logarithm.\n",
        "\n",
        "When using the Sigmoid function for hidden layers, it is a good practice to use a “Xavier Normal” or “Xavier Uniform” weight initialization (also referred to Glorot initialization, named for Xavier Glorot) and scale input data to the range 0-1 (e.g. the range of the activation function) prior to training.\n",
        "\n",
        "## **Tanh Hidden Layer Activation Function**\n",
        "The hyperbolic tangent activation function is also referred to simply as the Tanh (also “tanh” and “TanH“) function.\n",
        "\n",
        "It is very similar to the sigmoid activation function and even has the same S-shape.\n",
        "\n",
        "The function takes any real value as input and outputs values in the range -1 to 1. The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0.\n",
        "\n",
        "The Tanh activation function is calculated as follows:\n",
        "\n",
        "`(e^x – e^-x) / (e^x + e^-x)`\n",
        "\n",
        "Where e is a mathematical constant that is the base of the natural logarithm.\n",
        "\n",
        "When using the TanH function for hidden layers, it is a good practice to use a “Xavier Normal” or “Xavier Uniform” weight initialization (also referred to Glorot initialization, named for Xavier Glorot) and scale input data to the range -1 to 1 (e.g. the range of the activation function) prior to training.\n",
        "\n",
        "## **How to Choose a Hidden Layer Activation Function**\n",
        "A neural network will almost always have the same activation function in all hidden layers.\n",
        "\n",
        "It is most unusual to vary the activation function through a network model.\n",
        "\n",
        "Traditionally, the sigmoid activation function was the default activation function in the 1990s. Perhaps through the mid to late 1990s to 2010s, the Tanh function was the default activation function for hidden layers.\n",
        "\n",
        "… the hyperbolic tangent activation function typically performs better than the logistic sigmoid.\n",
        "\n",
        "— Page 195, Deep Learning, 2016.\n",
        "\n",
        "Both the sigmoid and Tanh functions can make the model more susceptible to problems during training, via the so-called vanishing gradients problem.\n",
        "\n",
        "You can learn more about this problem in this tutorial:\n",
        "\n",
        "A Gentle Introduction to the Rectified Linear Unit (ReLU)\n",
        "The activation function used in hidden layers is typically chosen based on the type of neural network architecture.\n",
        "\n",
        "Modern neural network models with common architectures, such as MLP and CNN, will make use of the ReLU activation function, or extensions.\n",
        "\n",
        "In modern neural networks, the default recommendation is to use the rectified linear unit or ReLU …\n",
        "\n",
        "— Page 174, Deep Learning, 2016.\n",
        "\n",
        "Recurrent networks still commonly use Tanh or sigmoid activation functions, or even both. For example, the LSTM commonly uses the Sigmoid activation for recurrent connections and the Tanh activation for output.\n",
        "\n",
        " - **Multilayer Perceptron (MLP)**: ReLU activation function.\n",
        " - **Convolutional Neural Network (CNN)**: ReLU activation function.\n",
        " - **Recurrent Neural Network**: Tanh and/or Sigmoid activation function.\n",
        "\n",
        "If you’re unsure which activation function to use for your network, try a few and compare the results.\n",
        "\n",
        "The figure below summarizes how to choose an activation function for the hidden layers of your neural network model.\n",
        "\n",
        "![image.png](attachment:image.png)"
      ],
      "metadata": {
        "azdata_cell_guid": "3692ff9e-6bcd-4030-bc31-3ced066bfabe",
        "id": "UBwU6B0PegFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Activation for Output Layers**\n",
        "The output layer is the layer in a neural network model that directly outputs a prediction.\n",
        "\n",
        "All feed-forward neural network models have an output layer.\n",
        "\n",
        "There are perhaps three activation functions you may want to consider for use in the output layer; they are:\n",
        "\n",
        "- Linear\n",
        "- Logistic (Sigmoid)\n",
        "- Softmax\n",
        "\n",
        "This is not an exhaustive list of activation functions used for output layers, but they are the most commonly used.\n",
        "\n",
        "Let’s take a closer look at each in turn.\n",
        "\n",
        "## **Linear Output Activation Function**\n",
        "The linear activation function is also called **“identity” (multiplied by 1.0) or “no activation.”**\n",
        "\n",
        "This is because the linear activation function does not change the weighted sum of the input in any way and instead returns the value directly.\n",
        "\n",
        "`linear(x) = x`\n",
        "\n",
        "Target values used to train a model with a linear activation function in the output layer are **typically scaled prior to modeling using normalization or standardization transforms**.\n",
        "\n",
        "## **Sigmoid Output Activation Function**\n",
        "The sigmoid of logistic activation function was described in the previous section.\n",
        "\n",
        "Target labels used to train a model with a sigmoid activation function in the output layer will have the values 0 or 1.\n",
        "\n",
        "## **Softmax Output Activation Function**\n",
        "The softmax function outputs a vector of values that sum to 1.0 that can be **interpreted as probabilities of class membership.**\n",
        "\n",
        "It is related to the argmax function that outputs a 0 for all options and 1 for the chosen option. Softmax is a “softer” version of argmax that allows a probability-like output of a winner-take-all function.\n",
        "\n",
        "As such, the input to the function is a vector of real values and the output is a vector of the same length with values that sum to 1.0 like probabilities.\n",
        "\n",
        "The softmax function is calculated as follows:\n",
        "\n",
        "`e^x / sum(e^x)`\n",
        "\n",
        "Where x is a vector of outputs and e is a mathematical constant that is the base of the natural logarithm.\n",
        "\n",
        "Target labels used to train a model with the softmax activation function in the output layer will be vectors with 1 for the target class and 0 for all other classes.\n",
        "\n",
        "## **How to Choose an Output Activation Function**\n",
        "You must choose the activation function for your output layer based on the type of prediction problem that you are solving.\n",
        "\n",
        "Specifically, the type of variable that is being predicted.\n",
        "\n",
        "For example, you may divide prediction problems into two main groups, predicting a categorical variable (classification) and predicting a numerical variable (regression).\n",
        "\n",
        "If your problem is a regression problem, you should use a linear activation function.\n",
        "\n",
        " - **Regression**: One node, linear activation.\n",
        "\n",
        "If your problem is a classification problem, then there are three main types of classification problems and each may use a different activation function.\n",
        "\n",
        "Predicting a probability is not a regression problem; it is classification. In all cases of classification, your model will predict the probability of class membership (e.g. probability that an example belongs to each class) that you can convert to a crisp class label by rounding (for sigmoid) or argmax (for softmax).\n",
        "\n",
        "If there are two mutually exclusive classes (binary classification), then your output layer will have one node and a sigmoid activation function should be used. If there are more than two mutually exclusive classes (multiclass classification), then your output layer will have one node per class and a softmax activation should be used. If there are two or more mutually inclusive classes (multilabel classification), then your output layer will have one node for each class and a sigmoid activation function is used.\n",
        "\n",
        " - **Binary Classification**: One node, sigmoid activation.\n",
        " - **Multiclass Classification**: One node per class, softmax activation.\n",
        " - **Multilabel Classification**: One node per class, sigmoid activation.\n",
        "\n",
        "The figure below summarizes how to choose an activation function for the output layer of your neural network model.\n",
        "\n",
        "![image.png](attachment:image.png)"
      ],
      "metadata": {
        "azdata_cell_guid": "55f68cb9-1a34-478a-a494-677f389d2df8",
        "id": "qaVJgaCCegFl"
      }
    }
  ]
}